2025-05-29 23:19:10,299 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -                           cfg.name : bpe_5k_de_it
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -                     cfg.data.train : data/unprocessed/train.de-it
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -                       cfg.data.dev : data/unprocessed/dev.de-it
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -                      cfg.data.test : data/unprocessed/test.de-it
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -                  cfg.data.src.lang : de
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -       cfg.data.src.max_sent_length : 100
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : subword_models/5k/bpe_5k.joint_vocab
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : subword-nmt
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -   cfg.data.src.tokenizer_cfg.codes : subword_models/5k/bpe_5k.codes
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : it
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -       cfg.data.trg.max_sent_length : 100
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : subword_models/5k/bpe_5k.joint_vocab
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : subword-nmt
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -   cfg.data.trg.tokenizer_cfg.codes : subword_models/5k/bpe_5k.codes
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -                  cfg.testing.alpha : 1.0
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -            cfg.training.batch_size : 2048
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 1024
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -              cfg.training.patience : 8
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2025-05-29 23:19:10,300 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -                cfg.training.epochs : 10
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 500
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -           cfg.training.eval_metric : bleu
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -             cfg.training.model_dir : models/bpe_5k_de_it
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -             cfg.training.overwrite : False
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -              cfg.training.use_cuda : True
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 100
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3, 4]
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.3
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier_uniform
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier_uniform
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 4
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 2
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 512
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 1
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 2
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 512
2025-05-29 23:19:10,301 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0
2025-05-29 23:19:10,440 - INFO - joeynmt.data - Building tokenizer...
2025-05-29 23:19:10,458 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-29 23:19:10,458 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-29 23:19:10,458 - INFO - joeynmt.data - Loading train set...
2025-05-29 23:19:10,764 - INFO - joeynmt.data - Building vocabulary...
2025-05-29 23:19:10,975 - INFO - joeynmt.data - Loading dev set...
2025-05-29 23:19:10,981 - INFO - joeynmt.data - Loading test set...
2025-05-29 23:19:10,987 - INFO - joeynmt.data - Data loaded.
2025-05-29 23:19:10,987 - INFO - joeynmt.data - Train dataset: PlaintextDataset(split=train, len=208858, src_lang=de, trg_lang=it, has_trg=True, random_subset=-1)
2025-05-29 23:19:10,988 - INFO - joeynmt.data - Valid dataset: PlaintextDataset(split=dev, len=923, src_lang=de, trg_lang=it, has_trg=True, random_subset=-1)
2025-05-29 23:19:10,988 - INFO - joeynmt.data -  Test dataset: PlaintextDataset(split=test, len=1567, src_lang=de, trg_lang=it, has_trg=True, random_subset=-1)
2025-05-29 23:19:10,988 - INFO - joeynmt.data - First training example:
	[SRC] Al G@@ or@@ e@@ : Die Ab@@ wen@@ dung der Kli@@ ma@@ k@@ at@@ astr@@ op@@ he
	[TRG] Al G@@ or@@ e@@ : ar@@ rest@@ iamo il ris@@ cal@@ d@@ amento globale
2025-05-29 23:19:10,989 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ! (5) # (6) $ (7) % (8) &#91; (9) &#93;
2025-05-29 23:19:10,989 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ! (5) # (6) $ (7) % (8) &#91; (9) &#93;
2025-05-29 23:19:10,989 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 4992
2025-05-29 23:19:10,989 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 4992
2025-05-29 23:19:10,996 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-29 23:19:11,093 - INFO - joeynmt.model - Enc-dec model built.
2025-05-29 23:19:11,104 - INFO - joeynmt.model - Total params: 4177152
2025-05-29 23:19:11,105 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'src_embed.lut.weight']
2025-05-29 23:19:11,105 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=4992),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=4992),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.3))
2025-05-29 23:19:14,771 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=(0.9, 0.999))
2025-05-29 23:19:14,772 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, threshold_mode=abs, eps=0.0, factor=0.7, patience=8)
2025-05-29 23:19:14,772 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 2048
	effective batch size (w. parallel & accumulation): 2048
2025-05-29 23:19:14,773 - INFO - joeynmt.training - EPOCH 1
2025-05-29 23:19:19,216 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     4.272972, Batch Acc: 0.052756, Tokens per Sec:    14972, Lr: 0.000300
2025-05-29 23:19:23,048 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     4.087396, Batch Acc: 0.073795, Tokens per Sec:    17892, Lr: 0.000300
2025-05-29 23:19:26,794 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     4.008254, Batch Acc: 0.091340, Tokens per Sec:    18295, Lr: 0.000300
2025-05-29 23:19:30,508 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     3.752875, Batch Acc: 0.118898, Tokens per Sec:    18344, Lr: 0.000300
2025-05-29 23:19:34,225 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     3.694690, Batch Acc: 0.124840, Tokens per Sec:    18234, Lr: 0.000300
2025-05-29 23:19:34,226 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:19:34,226 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:19:44,930 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.75, ppl:  42.62, acc:   0.13, generation: 10.6802[sec], evaluation: 0.0000[sec]
2025-05-29 23:19:44,931 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:19:45,530 - INFO - joeynmt.training - Example #0
2025-05-29 23:19:45,530 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:19:45,531 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:19:45,531 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'il', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un']
2025-05-29 23:19:45,532 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:19:45,532 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:19:45,532 - INFO - joeynmt.training - 	Hypothesis: E il un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un
2025-05-29 23:19:45,532 - INFO - joeynmt.training - Example #1
2025-05-29 23:19:45,532 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:19:45,532 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:19:45,532 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'il', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un']
2025-05-29 23:19:45,533 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:19:45,533 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:19:45,533 - INFO - joeynmt.training - 	Hypothesis: E il un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un
2025-05-29 23:19:45,533 - INFO - joeynmt.training - Example #2
2025-05-29 23:19:45,533 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:19:45,533 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:19:45,533 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'il', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un']
2025-05-29 23:19:45,534 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:19:45,534 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:19:45,534 - INFO - joeynmt.training - 	Hypothesis: E il un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un
2025-05-29 23:19:45,534 - INFO - joeynmt.training - Example #3
2025-05-29 23:19:45,534 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:19:45,534 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:19:45,534 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'il', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un']
2025-05-29 23:19:45,535 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:19:45,535 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:19:45,535 - INFO - joeynmt.training - 	Hypothesis: E è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è il un un un un un un un un un un un un un un un un un un un un un un un un un un un un
2025-05-29 23:19:45,535 - INFO - joeynmt.training - Example #4
2025-05-29 23:19:45,535 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:19:45,535 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:19:45,535 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'il', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un']
2025-05-29 23:19:45,536 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:19:45,536 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:19:45,536 - INFO - joeynmt.training - 	Hypothesis: E il un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un
2025-05-29 23:19:49,540 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     3.625098, Batch Acc: 0.128461, Tokens per Sec:    15205, Lr: 0.000300
2025-05-29 23:19:53,216 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     3.599550, Batch Acc: 0.130946, Tokens per Sec:    18235, Lr: 0.000300
2025-05-29 23:19:56,892 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     3.561761, Batch Acc: 0.136401, Tokens per Sec:    18972, Lr: 0.000300
2025-05-29 23:20:00,568 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     3.579586, Batch Acc: 0.135882, Tokens per Sec:    18786, Lr: 0.000300
2025-05-29 23:20:04,218 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     3.554070, Batch Acc: 0.141025, Tokens per Sec:    18210, Lr: 0.000300
2025-05-29 23:20:04,219 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:20:04,219 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:20:14,737 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.61, ppl:  37.11, acc:   0.14, generation: 10.4937[sec], evaluation: 0.0000[sec]
2025-05-29 23:20:14,738 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:20:15,377 - INFO - joeynmt.training - Example #0
2025-05-29 23:20:15,378 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:20:15,378 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:20:15,378 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'non', 'è', 'la', 'mondo', 'di', 'un', 'persone', 'che', 'non', 'è', 'la', 'mondo', 'di', 'un', 'persone', 'che', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'è', 'la', 'mondo', 'di', 'un', 'persone', 'che', 'non', 'non', 'è', 'la', 'mondo', 'di', 'un', 'po@@', '<unk>', 'è', 'la', 'mondo', 'di', 'un', 'persone', 'che', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'non', 'è', 'la', 'mondo', 'che', 'si', 'si']
2025-05-29 23:20:15,379 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:20:15,379 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:20:15,379 - INFO - joeynmt.training - 	Hypothesis: E non è la mondo di un persone che non è la mondo di un persone che non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non non è la mondo di un persone che non non è la mondo di un po<unk> è la mondo di un persone che non non non non non non non non non non non non non non non non non non non non non non è la mondo che si si
2025-05-29 23:20:15,379 - INFO - joeynmt.training - Example #1
2025-05-29 23:20:15,379 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:20:15,379 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:20:15,379 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'mondo', 'che', 'non', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'persone', 'che', 'non', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è']
2025-05-29 23:20:15,380 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:20:15,380 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:20:15,380 - INFO - joeynmt.training - 	Hypothesis: E è un po<unk> è un mondo che non è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un persone che non è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è
2025-05-29 23:20:15,380 - INFO - joeynmt.training - Example #2
2025-05-29 23:20:15,380 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:20:15,380 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:20:15,380 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'un', 'è', 'un', 'Talk', 'Talk', 'Talk', 'Talk', 'Talk', 'Talk', 'Talk', 'Talk', 'Talk', 'Talk', 'Talk', 'Talk', 'Talk', 'Talk', 'Talk', 'Talk', 'Talk', 'Talk', 'Talk', 'Talk', 'Talk', 'Talk', 'Talk', 'Talk', 'Talk', 'Talk', 'Subtitles', 'si', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'Talk', 'Talk', 'Talk', 'Talk', 'Talk', 'Subtitles', 'si', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'mondo', 'di', 'un', 'persone', 'che', 'non', 'è', 'un', 'è', 'un', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'persone', 'che', 'non', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'è', 'un', 'po@@']
2025-05-29 23:20:15,381 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:20:15,381 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:20:15,381 - INFO - joeynmt.training - 	Hypothesis: E è un è un Talk Talk Talk Talk Talk Talk Talk Talk Talk Talk Talk Talk Talk Talk Talk Talk Talk Talk Talk Talk Talk Talk Talk Talk Talk Talk Subtitles si è un po<unk> è un po<unk> è un po<unk> è un Talk Talk Talk Talk Talk Subtitles si è un po<unk> è un mondo di un persone che non è un è un è un po<unk> è un persone che non è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un è un po
2025-05-29 23:20:15,381 - INFO - joeynmt.training - Example #3
2025-05-29 23:20:15,381 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:20:15,381 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:20:15,381 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'la', 'mondo', 'di', 'un', 'mondo', 'di', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'persone', 'di', 'un', 'persone', 'che', 'la', 'persone', 'di', 'un', 'persone', 'di', 'un', 'po@@', '<unk>', 'è', 'un', 'persone', 'di', 'un', 'persone', 'che', 'non', 'la', 'persone', 'che', 'non', 'la', 'persone', 'di', 'un', 'persone', 'di', 'un', 'po@@', '<unk>', 'è', 'un', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'persone', 'di', 'un', 'persone', 'che', 'non', 'la', 'persone', 'che', 'non', 'la', 'persone', 'che', 'non', 'la', 'persone', 'non', 'la', 'persone', 'non', 'la', 'persone', 'che', 'la', 'persone', 'che', 'la', 'persone', 'non', 'è', 'un', 'po@@', '<unk>']
2025-05-29 23:20:15,381 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:20:15,382 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:20:15,382 - INFO - joeynmt.training - 	Hypothesis: E la mondo di un mondo di un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un persone di un persone che la persone di un persone di un po<unk> è un persone di un persone che non la persone che non la persone di un persone di un po<unk> è un è un po<unk> è un persone di un persone che non la persone che non la persone che non la persone non la persone non la persone che la persone che la persone non è un po<unk>
2025-05-29 23:20:15,382 - INFO - joeynmt.training - Example #4
2025-05-29 23:20:15,382 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:20:15,382 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:20:15,382 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'un', 'è', 'un', 'po@@', '<unk>', 'è', 'la', 'mondo', 'che', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'mondo', 'di', 'un', 'po@@', '<unk>', 'è', 'un', 'mondo', 'di', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'la', 'mondo', 'di', 'un', 'mondo', 'di', 'un', 'mondo', 'di', 'un', 'po@@', '<unk>', 'è', 'la', 'persone', 'che', 'non', 'è', 'la', 'persone', 'che', 'è', 'un', 'è', 'la', 'persone', 'che', 'è', 'un', 'persone', 'che', 'non', 'è', 'la', 'mondo', 'di', 'un', 'po@@', '<unk>', 'è', 'la', 'persone', 'che', 'non', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'è', 'un', 'po@@', '<unk>']
2025-05-29 23:20:15,382 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:20:15,382 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:20:15,383 - INFO - joeynmt.training - 	Hypothesis: E è un è un po<unk> è la mondo che è un po<unk> è un po<unk> è un po<unk> è un mondo di un po<unk> è un mondo di un po<unk> è un po<unk> è un po<unk> è la mondo di un mondo di un mondo di un po<unk> è la persone che non è la persone che è un è la persone che è un persone che non è la mondo di un po<unk> è la persone che non è un po<unk> è un po<unk> è un po<unk> è un è un po<unk>
2025-05-29 23:20:19,080 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     3.666708, Batch Acc: 0.146742, Tokens per Sec:    15614, Lr: 0.000300
2025-05-29 23:20:22,789 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     3.763597, Batch Acc: 0.144138, Tokens per Sec:    19553, Lr: 0.000300
2025-05-29 23:20:26,457 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     3.570712, Batch Acc: 0.150585, Tokens per Sec:    19223, Lr: 0.000300
2025-05-29 23:20:30,103 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     3.459881, Batch Acc: 0.151893, Tokens per Sec:    18785, Lr: 0.000300
2025-05-29 23:20:33,730 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     3.584439, Batch Acc: 0.153729, Tokens per Sec:    18625, Lr: 0.000300
2025-05-29 23:20:33,730 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:20:33,730 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:20:44,379 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.52, ppl:  33.93, acc:   0.15, generation: 10.6304[sec], evaluation: 0.0000[sec]
2025-05-29 23:20:44,380 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:20:44,981 - INFO - joeynmt.training - Example #0
2025-05-29 23:20:44,982 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:20:44,982 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:20:44,982 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'non', 'sono', 'il', 'mio', 'modo', 'che', 'non', 'sono', 'il', 'mio', 'po@@', '<unk>', 'è', 'il', 'mio', 'po@@', '<unk>', 'è', 'il', 'mio', 'modo', 'che', 'non', 'sono', 'un', 'po@@', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', 'è', 'il', 'mio', 'modo', 'di', 'un', 'po@@', '<unk>', 'è', 'il', 'mio', 'po@@', '<unk>', 'è', 'il', 'mio', 'po@@', '<unk>', '.', '</s>']
2025-05-29 23:20:44,983 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:20:44,983 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:20:44,983 - INFO - joeynmt.training - 	Hypothesis: E non sono il mio modo che non sono il mio po<unk> è il mio po<unk> è il mio modo che non sono un po<unk> <unk> <unk> <unk> <unk> <unk> <unk> è il mio modo di un po<unk> è il mio po<unk> è il mio po<unk> .
2025-05-29 23:20:44,983 - INFO - joeynmt.training - Example #1
2025-05-29 23:20:44,983 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:20:44,983 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:20:44,983 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'non', 'è', 'il', 'mio', 'modo', 'di', 'un', 'po@@', '<unk>', 'ell@@', '<unk>', 'ell@@', '<unk>', '<unk>', '<unk>', '<unk>', 'ell@@', '<unk>', 'ell@@', '<unk>', 'ell@@', '<unk>', '<unk>', '<unk>', '<unk>', 'ell@@', '<unk>', 'ell@@', '<unk>', 'ell@@', '<unk>', '<unk>', '<unk>', 'ell@@', '<unk>', 'ell@@', '<unk>', 'ell@@', '<unk>', 'ell@@', '<unk>', 'ell@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'il', 'mio', 'modo', 'di', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', 'ell@@', '<unk>', 'è', 'il', 'mio', 'è', 'un', 'è', 'un', 'po@@', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', 'è', 'il', 'mio', 'po@@', '<unk>', 'è', 'il', 'mio', 'è', 'il', 'mio', 'po@@', '<unk>', 'è', 'il', 'mio', 'po@@', '<unk>', '<unk>', '<unk>', '<unk>', 'è', 'il', 'mio', 'è', 'il', 'mio', 'è']
2025-05-29 23:20:44,984 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:20:44,984 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:20:44,984 - INFO - joeynmt.training - 	Hypothesis: E non è il mio modo di un po<unk> ell<unk> ell<unk> <unk> <unk> <unk> ell<unk> ell<unk> ell<unk> <unk> <unk> <unk> ell<unk> ell<unk> ell<unk> <unk> <unk> ell<unk> ell<unk> ell<unk> ell<unk> ell<unk> è un po<unk> è il mio modo di un po<unk> è un po<unk> <unk> <unk> <unk> <unk> <unk> ell<unk> è il mio è un è un po<unk> <unk> <unk> <unk> <unk> è il mio po<unk> è il mio è il mio po<unk> è il mio po<unk> <unk> <unk> <unk> è il mio è il mio è
2025-05-29 23:20:44,984 - INFO - joeynmt.training - Example #2
2025-05-29 23:20:44,984 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:20:44,984 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:20:44,984 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'non', 'sono', 'un', 'po@@', '<unk>', '.', '</s>']
2025-05-29 23:20:44,985 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:20:44,985 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:20:44,985 - INFO - joeynmt.training - 	Hypothesis: E non sono un po<unk> .
2025-05-29 23:20:44,985 - INFO - joeynmt.training - Example #3
2025-05-29 23:20:44,985 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:20:44,985 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:20:44,985 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'il', 'mio', 'po@@', '<unk>', '</s>']
2025-05-29 23:20:44,986 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:20:44,986 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:20:44,986 - INFO - joeynmt.training - 	Hypothesis: E il mio po<unk>
2025-05-29 23:20:44,986 - INFO - joeynmt.training - Example #4
2025-05-29 23:20:44,986 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:20:44,986 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:20:44,986 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'il', 'mio', 'po@@', '<unk>', 'è', 'il', 'mio', 'po@@', '<unk>', 'è', 'il', 'modo', 'che', 'non', 'è', 'il', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'il', 'modo', 'di', 'un', 'po@@', '<unk>', 'è', 'il', 'modo', 'di', 'un', 'po@@', '<unk>', 'è', 'il', 'modo', 'che', 'non', 'è', 'il', 'po@@', '<unk>', 'è', 'il', 'mio', 'po@@', '<unk>', 'è', 'il', 'mio', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'il', 'modo', 'di', 'un', 'po@@', '<unk>', 'è', 'il', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'il', 'mio', 'po@@', '<unk>', 'è', 'il', 'mio', 'po@@', '<unk>', 'è', 'il', 'modo', 'che', 'non', 'è', 'un', 'è', 'il', 'mio', 'po@@', '<unk>', 'è', 'il', 'modo', 'che', 'non', 'è', 'il', 'po@@', '<unk>', 'è', 'il']
2025-05-29 23:20:44,987 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:20:44,987 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:20:44,987 - INFO - joeynmt.training - 	Hypothesis: E il mio po<unk> è il mio po<unk> è il modo che non è il po<unk> è un po<unk> è il modo di un po<unk> è il modo di un po<unk> è il modo che non è il po<unk> è il mio po<unk> è il mio po<unk> è un po<unk> è il modo di un po<unk> è il po<unk> è un po<unk> è il mio po<unk> è il mio po<unk> è il modo che non è un è il mio po<unk> è il modo che non è il po<unk> è il
2025-05-29 23:20:48,685 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     3.462384, Batch Acc: 0.153265, Tokens per Sec:    15976, Lr: 0.000300
2025-05-29 23:20:52,337 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     3.530962, Batch Acc: 0.155031, Tokens per Sec:    19126, Lr: 0.000300
2025-05-29 23:20:55,973 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     3.391096, Batch Acc: 0.159124, Tokens per Sec:    18674, Lr: 0.000300
2025-05-29 23:20:59,610 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     3.443987, Batch Acc: 0.163220, Tokens per Sec:    19015, Lr: 0.000300
2025-05-29 23:21:03,233 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     3.375415, Batch Acc: 0.164030, Tokens per Sec:    19282, Lr: 0.000300
2025-05-29 23:21:03,234 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:21:03,234 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:21:13,689 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.45, ppl:  31.53, acc:   0.16, generation: 10.4294[sec], evaluation: 0.0000[sec]
2025-05-29 23:21:13,689 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:21:14,312 - INFO - joeynmt.training - Example #0
2025-05-29 23:21:14,312 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:21:14,312 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:21:14,313 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'mia', 'persone', 'che', 'si', 'sono', 'un', 'po@@', ',', 'e', 'il', 'nostro', 'po@@', ',', 'e', 'il', 'nostro', 'po@@', ',', 'e', 'la', 'mia', 'persone', 'che', 'si', 'sono', 'un', 'po@@', ',', 'e', 'il', 'nostro', 'po@@', ',', 'e', 'il', 'modo', 'che', 'si', 'sono', 'un', 'po@@', ',', 'e', 'la', 'mia', 'persone', 'che', 'si', 'sono', 'un', 'po@@', ',', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e']
2025-05-29 23:21:14,313 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:21:14,313 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:21:14,313 - INFO - joeynmt.training - 	Hypothesis: La mia persone che si sono un po, e il nostro po, e il nostro po, e la mia persone che si sono un po, e il nostro po, e il modo che si sono un po, e la mia persone che si sono un po, e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e
2025-05-29 23:21:14,313 - INFO - joeynmt.training - Example #1
2025-05-29 23:21:14,314 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:21:14,314 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:21:14,314 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'non', 'è', 'il', 'modo', 'che', 'si', 'è', 'il', 'modo', 'di', 'un', 'po@@', ',', 'e', 'la', 'mia', 'persone', 'che', 'si', 'è', 'il', 'nostro', 'po@@', ',', 'e', 'il', 'modo', 'di', 'un', 'po@@', ',', 'e', 'il', 'modo', 'di', 'cui', 'si', 'è', 'un', 'po@@', ',', 'e', 'il', 'modo', 'che', 'si', 'è', 'un', 'po@@', ',', 'e', 'il', 'nostro', 'po@@', ',', 'e', 'il', 'nostro', 'po@@', ',', 'e', 'il', 'nostro', 'po@@', ',', 'e', 'il', 'nostro', 'po@@', ',', 'e', 'il', 'nostro', 'po@@', ',', 'e', 'il', 'nostro', 'po@@', ',', 'e', 'la', 'mia', 'persone', 'che', 'si', 'sono', 'un', 'po@@', ',', 'e', 'la', 'mia', 'persone', 'che', 'si', 'sono', 'un', 'po@@', ',', 'e', 'il', 'modo', 'di', 'un', 'po@@', ',']
2025-05-29 23:21:14,314 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:21:14,314 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:21:14,314 - INFO - joeynmt.training - 	Hypothesis: E non è il modo che si è il modo di un po, e la mia persone che si è il nostro po, e il modo di un po, e il modo di cui si è un po, e il modo che si è un po, e il nostro po, e il nostro po, e il nostro po, e il nostro po, e il nostro po, e il nostro po, e la mia persone che si sono un po, e la mia persone che si sono un po, e il modo di un po,
2025-05-29 23:21:14,314 - INFO - joeynmt.training - Example #2
2025-05-29 23:21:14,315 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:21:14,315 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:21:14,315 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'mia', 'cosa', 'che', 'si', 'sono', 'un', 'modo', 'di', 'un', 'modo', 'di', 'un', 'modo', 'di', 'un', 'modo', 'di', 'un', 'po@@', ',', 'e', 'il', 'modo', 'di', 'un', 'po@@', ',', 'e', 'il', 'modo', 'di', 'un', 'po@@', ',', 'e', 'il', 'modo', 'di', 'un', 'po@@', ',', 'e', 'la', 'mia', 'persone', 'che', 'si', 'sono', 'un', 'modo', 'di', 'un', 'modo', 'di', 'un', 'modo', 'di', 'un', 'modo', 'di', 'un', 'modo', 'di', 'un', 'po@@', ',', 'e', 'il', 'modo', 'di', 'un', 'po@@', ',', 'e', 'il', 'modo', 'di', 'un', 'modo', 'di', 'un', 'modo', 'di', 'un', 'modo', 'di', 'un', 'modo', 'di', 'un', 'modo', 'di', 'un', 'modo', 'di', 'un', 'modo', 'di', 'un', 'modo', 'di', 'un', 'po@@', ',', 'e', 'il', 'modo']
2025-05-29 23:21:14,315 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:21:14,315 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:21:14,315 - INFO - joeynmt.training - 	Hypothesis: La mia cosa che si sono un modo di un modo di un modo di un modo di un po, e il modo di un po, e il modo di un po, e il modo di un po, e la mia persone che si sono un modo di un modo di un modo di un modo di un modo di un po, e il modo di un po, e il modo di un modo di un modo di un modo di un modo di un modo di un modo di un modo di un modo di un po, e il modo
2025-05-29 23:21:14,315 - INFO - joeynmt.training - Example #3
2025-05-29 23:21:14,316 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:21:14,316 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:21:14,316 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'è', 'il', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'modo', 'di', 'cui', 'si', 'sono', 'un', 'po@@', ',', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'il', 'suo', 'po@@', '<unk>', 'ell@@', '<unk>', 'ell@@', '<unk>', '<unk>', 'ell@@', '<unk>', 'ell@@', '<unk>', 'ell@@', '<unk>', 'ell@@', '<unk>']
2025-05-29 23:21:14,316 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:21:14,316 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:21:14,316 - INFO - joeynmt.training - 	Hypothesis: L<unk> è il mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio mio modo di cui si sono un po, e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e il suo po<unk> ell<unk> ell<unk> <unk> ell<unk> ell<unk> ell<unk> ell<unk>
2025-05-29 23:21:14,316 - INFO - joeynmt.training - Example #4
2025-05-29 23:21:14,317 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:21:14,317 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:21:14,317 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'mia', 'cosa', 'che', 'si', 'sono', 'un', 'po@@', ',', 'e', 'il', 'nostro', 'po@@', ',', 'e', 'la', 'mia', 'cosa', 'che', 'si', 'si', 'si', 'si', 'si', 'sono', 'un', 'po@@', ',', 'e', 'il', 'modo', 'di', 'un', 'po@@', ',', 'e', 'il', 'modo', 'di', 'un', 'po@@', ',', 'e', 'la', 'mia', 'cosa', 'che', 'si', 'sono', 'un', 'po@@', ',', 'e', 'il', 'modo', 'che', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'sono', 'il', 'modo', 'che', 'si', 'si', 'sono', 'un', 'po@@', ',', 'e', 'il', 'modo', 'di', 'cui', 'si', 'si', 'si', 'sono', 'la', 'mia', 'cosa', 'che', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'sono', 'la', 'mia', 'cosa', 'che', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si', 'si']
2025-05-29 23:21:14,317 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:21:14,317 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:21:14,317 - INFO - joeynmt.training - 	Hypothesis: La mia cosa che si sono un po, e il nostro po, e la mia cosa che si si si si si sono un po, e il modo di un po, e il modo di un po, e la mia cosa che si sono un po, e il modo che si si si si si si si sono il modo che si si sono un po, e il modo di cui si si si sono la mia cosa che si si si si si si si si sono la mia cosa che si si si si si si si si si
2025-05-29 23:21:17,994 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     3.543820, Batch Acc: 0.166191, Tokens per Sec:    15703, Lr: 0.000300
2025-05-29 23:21:21,600 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     3.381431, Batch Acc: 0.166699, Tokens per Sec:    18491, Lr: 0.000300
2025-05-29 23:21:25,216 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     3.349018, Batch Acc: 0.170728, Tokens per Sec:    19230, Lr: 0.000300
2025-05-29 23:21:28,836 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     3.279128, Batch Acc: 0.176895, Tokens per Sec:    19038, Lr: 0.000300
2025-05-29 23:21:32,786 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     3.295831, Batch Acc: 0.184191, Tokens per Sec:    17862, Lr: 0.000300
2025-05-29 23:21:32,786 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:21:32,786 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:21:42,333 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.31, ppl:  27.26, acc:   0.19, generation: 9.5360[sec], evaluation: 0.0000[sec]
2025-05-29 23:21:42,333 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:21:42,860 - INFO - joeynmt.training - Example #0
2025-05-29 23:21:42,860 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:21:42,860 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:21:42,860 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'se', 'la', 'mia', 'gente', 'che', 'la', 'mia', 'cosa', 'che', 'la', 'gente', 'che', 'la', 'gente', 'che', 'la', 'gente', 'che', 'la', 'mia', 'loro', 'nostri', 'nostri', 'nostri', 'nostri', 'nostri', 'nostri', 'an@@', 'a@@', ',', 'e', 'la', 'gente', 'che', 'la', 'loro', 'loro', 'loro', 'loro', 'loro', 'nostri', 'an@@', 'o@@', ',', 'e', 'la', 'gente', 'che', 'la', 'gente', 'che', 'la', 'gente', 'che', 'la', 'gente', 'che', 'la', 'loro', 'nostri', 'nostri', 'nostri', 'nostri', 'nostri', 'nostri', 'an@@', 'a@@', '.', '</s>']
2025-05-29 23:21:42,861 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:21:42,861 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:21:42,861 - INFO - joeynmt.training - 	Hypothesis: E se la mia gente che la mia cosa che la gente che la gente che la gente che la mia loro nostri nostri nostri nostri nostri nostri ana, e la gente che la loro loro loro loro loro nostri ano, e la gente che la gente che la gente che la gente che la loro nostri nostri nostri nostri nostri nostri ana.
2025-05-29 23:21:42,861 - INFO - joeynmt.training - Example #1
2025-05-29 23:21:42,861 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:21:42,861 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:21:42,861 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'se', 'se', 'non', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>']
2025-05-29 23:21:42,862 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:21:42,862 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:21:42,862 - INFO - joeynmt.training - 	Hypothesis: Ma se se non è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk>
2025-05-29 23:21:42,862 - INFO - joeynmt.training - Example #2
2025-05-29 23:21:42,862 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:21:42,862 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:21:42,862 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'la', 'mia', 'mia', 'parte', 'di', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'modo', 'di', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>']
2025-05-29 23:21:42,863 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:21:42,863 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:21:42,863 - INFO - joeynmt.training - 	Hypothesis: E la mia mia parte di un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un modo di un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk>
2025-05-29 23:21:42,863 - INFO - joeynmt.training - Example #3
2025-05-29 23:21:42,863 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:21:42,863 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:21:42,863 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'il', 'mio', 'mio', 'mio', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', '.', '</s>']
2025-05-29 23:21:42,864 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:21:42,864 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:21:42,864 - INFO - joeynmt.training - 	Hypothesis: E il mio mio mio po<unk> è un po<unk> è un po<unk> .
2025-05-29 23:21:42,864 - INFO - joeynmt.training - Example #4
2025-05-29 23:21:42,864 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:21:42,864 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:21:42,864 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'se', 'la', 'mia', 'cosa', 'che', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'modo', 'di', 'un', 'modo', 'che', 'la', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'cosa', 'che', 'è', 'un', 'modo', 'di', 'un', 'po@@', '<unk>', 'è', 'un', 'modo', 'che', 'la', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'parte', 'di', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'modo', 'che', 'la', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'cosa', 'che', 'la', 'mia', 'mia', 'mia', 'cosa', 'che', 'è', 'un', 'è', 'un', 'è']
2025-05-29 23:21:42,865 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:21:42,865 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:21:42,865 - INFO - joeynmt.training - 	Hypothesis: E se la mia cosa che è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un modo di un modo che la mia mia mia mia mia mia mia mia mia mia mia cosa che è un modo di un po<unk> è un modo che la mia mia mia mia mia mia mia mia parte di un po<unk> è un po<unk> è un modo che la mia mia mia mia mia mia mia mia mia mia cosa che la mia mia mia cosa che è un è un è
2025-05-29 23:21:46,363 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     3.256157, Batch Acc: 0.190853, Tokens per Sec:    16612, Lr: 0.000300
2025-05-29 23:21:49,824 - INFO - joeynmt.training - Epoch   1, Step:     2700, Batch Loss:     3.359354, Batch Acc: 0.189796, Tokens per Sec:    19256, Lr: 0.000300
2025-05-29 23:21:53,299 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     3.148554, Batch Acc: 0.198617, Tokens per Sec:    19979, Lr: 0.000300
2025-05-29 23:21:56,788 - INFO - joeynmt.training - Epoch   1, Step:     2900, Batch Loss:     3.215324, Batch Acc: 0.202908, Tokens per Sec:    19896, Lr: 0.000300
2025-05-29 23:22:00,333 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     3.226784, Batch Acc: 0.208211, Tokens per Sec:    19140, Lr: 0.000300
2025-05-29 23:22:00,333 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:22:00,333 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:22:10,353 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.16, ppl:  23.63, acc:   0.21, generation: 10.0090[sec], evaluation: 0.0000[sec]
2025-05-29 23:22:10,353 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:22:11,106 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/500.ckpt
2025-05-29 23:22:11,132 - INFO - joeynmt.training - Example #0
2025-05-29 23:22:11,133 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:22:11,133 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:22:11,133 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'ho', 'fatto', 'un', 'po@@', '<unk>', ',', 'e', 'ho', 'fatto', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', ',', 'e', 'il', 'mondo', 'che', 'si', 'tratta', 'di', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', ',', 'e', 'il', 'mondo', 'che', 'si', 'tratta', 'di', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '.', '</s>']
2025-05-29 23:22:11,134 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:22:11,134 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:22:11,134 - INFO - joeynmt.training - 	Hypothesis: E ho fatto un po<unk> , e ho fatto un po<unk> è un po<unk> , e il mondo che si tratta di un po<unk> è un po<unk> è un po<unk> , e il mondo che si tratta di un po<unk> è un po<unk> è un po.
2025-05-29 23:22:11,134 - INFO - joeynmt.training - Example #1
2025-05-29 23:22:11,134 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:22:11,134 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:22:11,134 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', ',', 'e', 'non', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', ',', 'e', 'non', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', ',', 'e', 'non', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è']
2025-05-29 23:22:11,135 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:22:11,135 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:22:11,135 - INFO - joeynmt.training - 	Hypothesis: Ma non è un po<unk> è un po<unk> è un po<unk> , e non è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> , e non è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> , e non è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è un po<unk> è
2025-05-29 23:22:11,135 - INFO - joeynmt.training - Example #2
2025-05-29 23:22:11,135 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:22:11,135 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:22:11,135 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'questo', 'è', 'un', 'altro', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', '.', '</s>']
2025-05-29 23:22:11,136 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:22:11,136 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:22:11,136 - INFO - joeynmt.training - 	Hypothesis: In questo è un altro è un po<unk> è un po<unk> è un po<unk> è un po<unk> .
2025-05-29 23:22:11,136 - INFO - joeynmt.training - Example #3
2025-05-29 23:22:11,136 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:22:11,136 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:22:11,136 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'ho', 'fatto', 'un', 'po@@', '<unk>', '.', '</s>']
2025-05-29 23:22:11,136 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:22:11,137 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:22:11,137 - INFO - joeynmt.training - 	Hypothesis: E ho fatto un po<unk> .
2025-05-29 23:22:11,137 - INFO - joeynmt.training - Example #4
2025-05-29 23:22:11,137 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:22:11,137 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:22:11,137 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'mia', 'cosa', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'è', 'un', 'po@@', '<unk>', '.', '</s>']
2025-05-29 23:22:11,137 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:22:11,137 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:22:11,137 - INFO - joeynmt.training - 	Hypothesis: La mia cosa è un po<unk> è un po<unk> è un po<unk> .
2025-05-29 23:22:14,766 - INFO - joeynmt.training - Epoch   1, Step:     3100, Batch Loss:     3.210046, Batch Acc: 0.211071, Tokens per Sec:    15670, Lr: 0.000300
2025-05-29 23:22:18,357 - INFO - joeynmt.training - Epoch   1, Step:     3200, Batch Loss:     3.156087, Batch Acc: 0.219234, Tokens per Sec:    19088, Lr: 0.000300
2025-05-29 23:22:21,954 - INFO - joeynmt.training - Epoch   1, Step:     3300, Batch Loss:     3.082243, Batch Acc: 0.219909, Tokens per Sec:    19278, Lr: 0.000300
2025-05-29 23:22:25,536 - INFO - joeynmt.training - Epoch   1, Step:     3400, Batch Loss:     3.139864, Batch Acc: 0.222934, Tokens per Sec:    18865, Lr: 0.000300
2025-05-29 23:22:29,125 - INFO - joeynmt.training - Epoch   1, Step:     3500, Batch Loss:     3.087033, Batch Acc: 0.228452, Tokens per Sec:    19071, Lr: 0.000300
2025-05-29 23:22:29,126 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:22:29,126 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:22:38,221 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.03, ppl:  20.75, acc:   0.23, generation: 9.0849[sec], evaluation: 0.0000[sec]
2025-05-29 23:22:38,222 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:22:38,850 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/1000.ckpt
2025-05-29 23:22:38,879 - INFO - joeynmt.training - Example #0
2025-05-29 23:22:38,880 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:22:38,880 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:22:38,880 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'ho', 'fatto', 'che', 'ho', 'fatto', 'che', 'ho', 'fatto', 'che', 'ho', 'fatto', 'che', 'ho', 'fatto', 'che', 'si', 'può', 'essere', 'un', 'po@@', '<unk>', '.', '</s>']
2025-05-29 23:22:38,880 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:22:38,880 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:22:38,881 - INFO - joeynmt.training - 	Hypothesis: L<unk> ho fatto che ho fatto che ho fatto che ho fatto che ho fatto che si può essere un po<unk> .
2025-05-29 23:22:38,881 - INFO - joeynmt.training - Example #1
2025-05-29 23:22:38,881 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:22:38,881 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:22:38,881 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'un', 'paio', 'di', 'un', 'paio', 'di', 'un', 'm@@', 'e@@', ',', 'ma', 'non', 'è', 'che', 'la', 'gente', 'è', 'che', 'non', 'è', 'un', 'paio', 'di', 'un', 'm@@', 'om@@', 'ent@@', 'o@@', '.', '</s>']
2025-05-29 23:22:38,881 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:22:38,881 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:22:38,882 - INFO - joeynmt.training - 	Hypothesis: Ma non è un paio di un paio di un me, ma non è che la gente è che non è un paio di un momento.
2025-05-29 23:22:38,882 - INFO - joeynmt.training - Example #2
2025-05-29 23:22:38,882 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:22:38,882 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:22:38,882 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'questo', 'è', 'un', 'paio', 'di', 'una', 'storia', 'di', 'una', 'delle', 'person@@', 'e@@', ',', 'che', 'è', 'un', 'problema', 'di', 'una', 'delle', 'person@@', 'e@@', '.', '</s>']
2025-05-29 23:22:38,882 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:22:38,882 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:22:38,882 - INFO - joeynmt.training - 	Hypothesis: In questo è un paio di una storia di una delle persone, che è un problema di una delle persone.
2025-05-29 23:22:38,883 - INFO - joeynmt.training - Example #3
2025-05-29 23:22:38,883 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:22:38,883 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:22:38,883 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'ho', 'detto', 'che', 'la', 'mia', 'b@@', 'ella', 'di', 'un', 'b@@', 'oc@@', 'chi@@', 'a@@', '.', '</s>']
2025-05-29 23:22:38,883 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:22:38,883 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:22:38,883 - INFO - joeynmt.training - 	Hypothesis: L<unk> ho detto che la mia bella di un bocchia.
2025-05-29 23:22:38,883 - INFO - joeynmt.training - Example #4
2025-05-29 23:22:38,884 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:22:38,884 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:22:38,884 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'mio', 'padre', 'è', 'che', 'ho', 'fatto', 'che', 'ho', 'fatto', 'che', 'ho', 'fatto', 'che', 'ho', 'fatto', 'che', 'ho', 'fatto', 'che', 'ho', 'fatto', 'che', 'ho', 'fatto', 'che', 'ho', 'fatto', 'un', 'certo', 'è', 'un', 'certo', 'è', 'un', 'certo', 'punto', 'di', 'una', 'cosa', 'che', 'ho', 'fatto', 'che', 'ho', 'fatto', 'che', 'ho', 'fatto', 'che', 'ho', 'fatto', 'che', 'ho', 'fatto', 'che', 'si', 'può', 'essere', 'un', 'paio', 'di', 'una', 'cosa', 'che', 'ho', 'fatto', 'che', 'l@@', '<unk>', 'ho', 'fatto', 'che', 'ho', 'fatto', 'che', 'ho', 'fatto', 'che', 'ho', 'fatto', 'un', 'certo', 'è', 'un', 'po@@', '<unk>', 'in@@', 'o@@', ',', 'e', 'l@@', '<unk>', 'in@@', 'in@@', 'in@@', 'in@@', 'in@@']
2025-05-29 23:22:38,884 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:22:38,884 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:22:38,884 - INFO - joeynmt.training - 	Hypothesis: Il mio mio mio mio mio mio mio mio mio mio mio padre è che ho fatto che ho fatto che ho fatto che ho fatto che ho fatto che ho fatto che ho fatto che ho fatto un certo è un certo è un certo punto di una cosa che ho fatto che ho fatto che ho fatto che ho fatto che ho fatto che si può essere un paio di una cosa che ho fatto che l<unk> ho fatto che ho fatto che ho fatto che ho fatto un certo è un po<unk> ino, e l<unk> ininininin
2025-05-29 23:22:42,495 - INFO - joeynmt.training - Epoch   1, Step:     3600, Batch Loss:     3.210082, Batch Acc: 0.233362, Tokens per Sec:    15863, Lr: 0.000300
2025-05-29 23:22:46,086 - INFO - joeynmt.training - Epoch   1, Step:     3700, Batch Loss:     2.878808, Batch Acc: 0.234662, Tokens per Sec:    19015, Lr: 0.000300
2025-05-29 23:22:49,697 - INFO - joeynmt.training - Epoch   1, Step:     3800, Batch Loss:     3.031024, Batch Acc: 0.237604, Tokens per Sec:    19348, Lr: 0.000300
2025-05-29 23:22:53,275 - INFO - joeynmt.training - Epoch   1, Step:     3900, Batch Loss:     2.934030, Batch Acc: 0.238310, Tokens per Sec:    18807, Lr: 0.000300
2025-05-29 23:22:56,891 - INFO - joeynmt.training - Epoch   1, Step:     4000, Batch Loss:     2.864020, Batch Acc: 0.247487, Tokens per Sec:    19650, Lr: 0.000300
2025-05-29 23:22:56,891 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:22:56,891 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:23:07,019 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.93, ppl:  18.69, acc:   0.25, generation: 10.1070[sec], evaluation: 0.0000[sec]
2025-05-29 23:23:07,020 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:23:07,584 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/1500.ckpt
2025-05-29 23:23:07,610 - INFO - joeynmt.training - Example #0
2025-05-29 23:23:07,610 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:23:07,610 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:23:07,610 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'realtà', 'sono', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati']
2025-05-29 23:23:07,611 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:23:07,611 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:23:07,611 - INFO - joeynmt.training - 	Hypothesis: In realtà sono stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati
2025-05-29 23:23:07,611 - INFO - joeynmt.training - Example #1
2025-05-29 23:23:07,611 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:23:07,611 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:23:07,611 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'un', 'altro', 'che', 'non', 'è', 'un', 'altro', 'che', 'non', 'è', 'un', 'altro', 'modo', 'di', 'essere', 'in', 'cui', 'non', 'è', 'un', 'altro', 'altro', 'che', 'non', 'è', 'un', 'altro', 'che', 'non', 'è', 'un', 'altro', 'che', 'non', 'è', 'il', 'nostro', 'modo', 'di', 'essere', 'più', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande']
2025-05-29 23:23:07,612 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:23:07,612 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:23:07,612 - INFO - joeynmt.training - 	Hypothesis: Ma non è un altro che non è un altro che non è un altro modo di essere in cui non è un altro altro che non è un altro che non è un altro che non è il nostro modo di essere più grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande
2025-05-29 23:23:07,612 - INFO - joeynmt.training - Example #2
2025-05-29 23:23:07,612 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:23:07,612 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:23:07,612 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'che', 'è', 'il', 'nostro', 'modo', 'di', 'cui', 'il', 'nostro', 'modo', 'in', 'cui', 'il', 'nostro', 'modo', 'di', 'cui', 'il', 'nostro', 'modo', 'di', 'cui', 'il', 'nostro', 'modo', 'di', 'cui', 'il', 'nostro', 'modo', 'di', 'cui', 'il', 'nostro', 'modo', 'di', 'cui', 'è', 'il', 'nostro', 'modo', 'di', 'cui', 'si', 'può', 'essere', 'un', 'altro', 'altro', 'è', 'il', 'mon@@', 'do@@', '.', '</s>']
2025-05-29 23:23:07,612 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:23:07,612 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:23:07,613 - INFO - joeynmt.training - 	Hypothesis: In un altro altro altro altro altro altro che è il nostro modo di cui il nostro modo in cui il nostro modo di cui il nostro modo di cui il nostro modo di cui il nostro modo di cui il nostro modo di cui è il nostro modo di cui si può essere un altro altro è il mondo.
2025-05-29 23:23:07,613 - INFO - joeynmt.training - Example #3
2025-05-29 23:23:07,613 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:23:07,613 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:23:07,613 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Sono', 'stati', 'stati', 'stati', 'stati', 'i', 'miei', 'miei', 'genitori', 'e', 'e', 'i', 'miei', 'genitori', 'e', 'e', 'i', 'miei', 'miei', 'an@@', 'i@@', '.', '</s>']
2025-05-29 23:23:07,613 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:23:07,613 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:23:07,613 - INFO - joeynmt.training - 	Hypothesis: Sono stati stati stati stati i miei miei genitori e e i miei genitori e e i miei miei ani.
2025-05-29 23:23:07,613 - INFO - joeynmt.training - Example #4
2025-05-29 23:23:07,614 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:23:07,614 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:23:07,614 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'mio', 'giorno', 'è', 'che', 'ho', 'visto', 'che', 'ho', 'visto', 'un', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'che', 'è', 'un', 'altro', 'altro', 'altro', 'altro', 'altro', 'è', 'un', 'altro', 'che', 'è', 'un', 'altro', 'altro', 'che', 'ho', 'visto', 'che', 'è', 'un', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'è', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'un', 'altro', 'altro', 'che', 'è', 'un', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'altro', 'che', 'è', 'stato', 'stato', 'stato', 'stato', 'stato']
2025-05-29 23:23:07,614 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:23:07,614 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:23:07,614 - INFO - joeynmt.training - 	Hypothesis: Il mio giorno è che ho visto che ho visto un altro altro altro altro altro altro altro altro altro altro altro altro altro altro altro altro altro altro altro altro altro altro altro altro altro che è un altro altro altro altro altro è un altro che è un altro altro che ho visto che è un altro altro altro altro altro altro altro altro altro altro altro altro altro è stato stato stato stato stato stato stato stato stato un altro altro che è un altro altro altro altro altro altro altro altro altro altro altro altro altro altro che è stato stato stato stato stato
2025-05-29 23:23:11,080 - INFO - joeynmt.training - Epoch   1, Step:     4100, Batch Loss:     2.948192, Batch Acc: 0.244880, Tokens per Sec:    17183, Lr: 0.000300
2025-05-29 23:23:14,503 - INFO - joeynmt.training - Epoch   1, Step:     4200, Batch Loss:     2.912635, Batch Acc: 0.251757, Tokens per Sec:    20126, Lr: 0.000300
2025-05-29 23:23:17,919 - INFO - joeynmt.training - Epoch   1, Step:     4300, Batch Loss:     2.882407, Batch Acc: 0.250847, Tokens per Sec:    20390, Lr: 0.000300
2025-05-29 23:23:21,330 - INFO - joeynmt.training - Epoch   1, Step:     4400, Batch Loss:     2.816354, Batch Acc: 0.252660, Tokens per Sec:    19870, Lr: 0.000300
2025-05-29 23:23:24,729 - INFO - joeynmt.training - Epoch   1, Step:     4500, Batch Loss:     2.791478, Batch Acc: 0.257047, Tokens per Sec:    20191, Lr: 0.000300
2025-05-29 23:23:24,729 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:23:24,729 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:23:34,160 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.85, ppl:  17.33, acc:   0.26, generation: 9.4214[sec], evaluation: 0.0000[sec]
2025-05-29 23:23:34,160 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:23:34,702 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/2000.ckpt
2025-05-29 23:23:34,726 - INFO - joeynmt.training - Example #0
2025-05-29 23:23:34,727 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:23:34,727 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:23:34,727 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'questo', 'tipo', 'di', 'persone', 'che', 'ho', 'visto', 'il', 'mon@@', 'do@@', ',', 'la', 'mia', 'vit@@', 'a@@', ',', 'la', 'prima', 'volta', 'che', 'la', 'prima', 'volta', 'che', 'la', 'prima', 'volta', 'che', 'la', 'prima', 'volta', 'che', 'la', 'prima', 'volta', 'che', 'si', 'può', 'essere', 'in', 'cui', 'il', 'mon@@', 'do@@', ',', 'che', 'la', 'prima', 'volta', 'che', 'la', 'gente', 'a', 'cui', 'il', '1@@', '1@@', '1@@', ',', 'il', '1@@', '1@@', '1@@', ',', 'che', 'sono', 'stati', 'in', 'cui', 'il', '1@@', '1@@', '1@@', ',', 'il', '1@@', '0@@', '0@@', '0@@', '0@@', '0@@', ',', 'e', 'di', 'voi', 'è', 'stata', 'la', 'prima', 'volta', 'che', 'la', 'prima', 'volta', 'che', 'la', 'loro', 'm@@', 'assa', 'di', 'una', 'piccola', 'parte', 'del', '1@@', '1@@', '1@@']
2025-05-29 23:23:34,727 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:23:34,727 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:23:34,727 - INFO - joeynmt.training - 	Hypothesis: In questo tipo di persone che ho visto il mondo, la mia vita, la prima volta che la prima volta che la prima volta che la prima volta che la prima volta che si può essere in cui il mondo, che la prima volta che la gente a cui il 111, il 111, che sono stati in cui il 111, il 100000, e di voi è stata la prima volta che la prima volta che la loro massa di una piccola parte del 111
2025-05-29 23:23:34,727 - INFO - joeynmt.training - Example #1
2025-05-29 23:23:34,728 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:23:34,728 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:23:34,728 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'che', 'non', 'è', 'stata', 'la', 'gente', 'non', 'è', 'stato', 'stato', 'un', 'po@@', '<unk>', 'di', 'un', 'po@@', '<unk>', 'di', 'più', 'grande', 'che', 'non', 'è', 'stato', 'un', 'po@@', '<unk>', 'di', 'più', 'grande', 'che', 'non', 'è', 'stata', 'la', 'gente', 'che', 'non', 'è', 'stato', 'più', 'più', 'grande', 'che', 'non', 'è', 'stata', 'la', 'gente', 'che', 'non', 'è', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'un', 'po@@', '<unk>', ',', 'non', 'è', 'la', 'gente', 'che', 'non', 'si', 'può', 'essere', 'un', 'po@@', '<unk>', 'di', 'un', 'po@@', '<unk>', 'di', 'essere', 'essere', 'un', 'po@@', '<unk>', 'di', 'essere', 'un', 'po@@', '<unk>', 'di', 'un', 'po@@', '<unk>']
2025-05-29 23:23:34,728 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:23:34,728 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:23:34,728 - INFO - joeynmt.training - 	Hypothesis: Ma non è che non è stata la gente non è stato stato un po<unk> di un po<unk> di più grande che non è stato un po<unk> di più grande che non è stata la gente che non è stato più più grande che non è stata la gente che non è stato stato stato stato stato stato stato stato stato stato stato stato stato stato stato stato stato stato un po<unk> , non è la gente che non si può essere un po<unk> di un po<unk> di essere essere un po<unk> di essere un po<unk> di un po<unk>
2025-05-29 23:23:34,728 - INFO - joeynmt.training - Example #2
2025-05-29 23:23:34,728 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:23:34,728 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:23:34,728 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'questo', 'tipo', 'di', 'persone', 'che', 'è', 'la', 'gente', 'che', 'è', 'il', 'mon@@', 'do@@', ',', 'è', 'la', 'nostra', 'vit@@', 'a@@', '.', '</s>']
2025-05-29 23:23:34,729 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:23:34,729 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:23:34,729 - INFO - joeynmt.training - 	Hypothesis: In questo tipo di persone che è la gente che è il mondo, è la nostra vita.
2025-05-29 23:23:34,729 - INFO - joeynmt.training - Example #3
2025-05-29 23:23:34,729 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:23:34,729 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:23:34,729 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'chiama', '<unk>', 'S@@', 'S@@', 'an', 'e', 'e', 'e', 'la', 'b@@', 'oc@@', 'oc@@', 'a@@', '.', '</s>']
2025-05-29 23:23:34,729 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:23:34,729 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:23:34,730 - INFO - joeynmt.training - 	Hypothesis: Si chiama <unk> SSan e e e la bococa.
2025-05-29 23:23:34,730 - INFO - joeynmt.training - Example #4
2025-05-29 23:23:34,730 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:23:34,730 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:23:34,730 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'mia', 'cosa', 'è', 'che', 'ho', 'fatto', 'che', 'ho', 'fatto', 'una', 'volta', 'che', 'la', 'gente', 'è', 'una', 'volta', 'che', 'è', 'stata', 'una', 'volta', 'che', 'è', 'stata', 'una', 'volta', 'che', 'è', 'stata', 'una', 'volta', 'che', 'è', 'stata', 'una', 'volta', 'che', 'il', 'mon@@', 'do@@', '.', '</s>']
2025-05-29 23:23:34,730 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:23:34,730 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:23:34,730 - INFO - joeynmt.training - 	Hypothesis: La mia cosa è che ho fatto che ho fatto una volta che la gente è una volta che è stata una volta che è stata una volta che è stata una volta che è stata una volta che il mondo.
2025-05-29 23:23:38,213 - INFO - joeynmt.training - Epoch   1, Step:     4600, Batch Loss:     2.886149, Batch Acc: 0.259245, Tokens per Sec:    17196, Lr: 0.000300
2025-05-29 23:23:41,668 - INFO - joeynmt.training - Epoch   1, Step:     4700, Batch Loss:     2.872529, Batch Acc: 0.265417, Tokens per Sec:    19775, Lr: 0.000300
2025-05-29 23:23:45,123 - INFO - joeynmt.training - Epoch   1, Step:     4800, Batch Loss:     2.772117, Batch Acc: 0.265633, Tokens per Sec:    19604, Lr: 0.000300
2025-05-29 23:23:48,585 - INFO - joeynmt.training - Epoch   1, Step:     4900, Batch Loss:     2.742269, Batch Acc: 0.262412, Tokens per Sec:    20206, Lr: 0.000300
2025-05-29 23:23:52,029 - INFO - joeynmt.training - Epoch   1, Step:     5000, Batch Loss:     2.900985, Batch Acc: 0.268454, Tokens per Sec:    20077, Lr: 0.000300
2025-05-29 23:23:52,030 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:23:52,030 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:24:02,081 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.78, ppl:  16.09, acc:   0.28, generation: 10.0354[sec], evaluation: 0.0000[sec]
2025-05-29 23:24:02,081 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:24:02,634 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/2500.ckpt
2025-05-29 23:24:02,656 - INFO - joeynmt.training - Example #0
2025-05-29 23:24:02,656 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:24:02,657 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:24:02,657 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'bambini', 'sono', 'due', 'anni', 'fa@@', ',', 'e', 'i', 'bambini', 'sono', 'stati', 'in', 'cui', 'i', 'bambini', 'sono', 'stati', 'in', 'cui', 'i', 'bambini', 'che', 'i', 'bambini', 'che', 'i', 'bambini', 'che', 'i', 'bambini', 'che', 'i', 'bambini', 'sono', 'stati', 'in', 'cui', 'i', 'bambini', 'sono', 'stati', 'in', 'cui', 'i', 'bambini', 'che', 'i', 'bambini', 'sono', 'stati', 'in', 'cui', 'i', 'bambini', 'sono', 'stati', 'in', 'cui', 'i', 'bambini', 'sono', 'stati', 'in', 'cui', 'i', 'bambini', 'sono', 'stati', 'in', 'cui', 'i', '1@@', '000', 'anni', 'fa@@', '.', '</s>']
2025-05-29 23:24:02,657 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:24:02,657 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:24:02,657 - INFO - joeynmt.training - 	Hypothesis: I bambini sono due anni fa, e i bambini sono stati in cui i bambini sono stati in cui i bambini che i bambini che i bambini che i bambini che i bambini sono stati in cui i bambini sono stati in cui i bambini che i bambini sono stati in cui i bambini sono stati in cui i bambini sono stati in cui i bambini sono stati in cui i 1000 anni fa.
2025-05-29 23:24:02,657 - INFO - joeynmt.training - Example #1
2025-05-29 23:24:02,657 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:24:02,657 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:24:02,658 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'un', 'po@@', '<unk>', 'di', 'più', 'più', 'grande', 'di', 'più', 'grande', 'di', 'una', 'cosa', 'che', 'non', 'è', 'stato', 'un', 'po@@', '<unk>', 'di', 'più', 'grande', 'di', 'un', 'po@@', '<unk>', 'di', 'c@@', '<unk>', 'è', 'la', 'sua', 'vit@@', 'a@@', '.', '</s>']
2025-05-29 23:24:02,658 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:24:02,658 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:24:02,658 - INFO - joeynmt.training - 	Hypothesis: Ma non è un po<unk> di più più grande di più grande di una cosa che non è stato un po<unk> di più grande di un po<unk> di c<unk> è la sua vita.
2025-05-29 23:24:02,658 - INFO - joeynmt.training - Example #2
2025-05-29 23:24:02,658 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:24:02,658 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:24:02,658 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'po@@', '<unk>', 'di', 'un', 'po@@', '<unk>', 'di', 'un', 'po@@', '<unk>', 'di', 'un', 'po@@', '<unk>', 'di', 'più', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'grande', 'di', 'un', 'po@@', '<unk>', 'di', 'una', 'c@@', 'att@@', 'a@@', '.', '</s>']
2025-05-29 23:24:02,659 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:24:02,659 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:24:02,659 - INFO - joeynmt.training - 	Hypothesis: In un po<unk> di un po<unk> di un po<unk> di un po<unk> di più grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande grande di un po<unk> di una catta.
2025-05-29 23:24:02,659 - INFO - joeynmt.training - Example #3
2025-05-29 23:24:02,659 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:24:02,659 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:24:02,659 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'olo', 'e', 'la', 'sua', 's@@', 'edia', 'e', 'la', 'sua', 'C@@', 'er@@', 'ra@@', '.', '</s>']
2025-05-29 23:24:02,659 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:24:02,659 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:24:02,660 - INFO - joeynmt.training - 	Hypothesis: Solo e la sua sedia e la sua Cerra.
2025-05-29 23:24:02,660 - INFO - joeynmt.training - Example #4
2025-05-29 23:24:02,660 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:24:02,660 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:24:02,660 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'mia', 'cosa', 'che', 'vi', 'sono', 'più', 'più', 'di', 'più', 'di', 'più', 'di', 'più', 'di', 'più', 'di', 'più', 'più', 'di', 'più', 'più', 'più', 'più', 'più', 'più', 'più', 'di', 'più', 'più', 'più', 'più', 'più', 'più', 'più', 'più', 'di', 'più', 'più', 'di', 'più', 'di', 'più', 'più', 'più', 'più', 'di', 'più', 'di', 'più', 'di', 'più', 'di', 'più', 'di', 'più', 'più', 'più', 'di', 'più', 'di', 'più', 'di', 'più', 'di', 'più', 'di', 'più', 'di', 'più', 'più', 'di', 'più', 'di', 'più', 'di', 'più', 'più', 'più', 'più', 'di', 'più', 'di', 'più', 'di', 'più', 'di', 'più', 'di', 'più', 'più', 'più', 'più', 'più', 'più', 'più', 'più', 'più', 'di', 'più', 'di', 'più', 'più', 'più', 'più', 'più', 'più', 'più']
2025-05-29 23:24:02,660 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:24:02,660 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:24:02,660 - INFO - joeynmt.training - 	Hypothesis: La mia cosa che vi sono più più di più di più di più di più di più più di più più più più più più più di più più più più più più più più di più più di più di più più più più di più di più di più di più di più più più di più di più di più di più di più di più più di più di più di più più più più di più di più di più di più di più più più più più più più più più di più di più più più più più più più
2025-05-29 23:24:06,224 - INFO - joeynmt.training - Epoch   1, Step:     5100, Batch Loss:     2.902628, Batch Acc: 0.268511, Tokens per Sec:    15398, Lr: 0.000300
2025-05-29 23:24:09,822 - INFO - joeynmt.training - Epoch   1, Step:     5200, Batch Loss:     2.778404, Batch Acc: 0.275068, Tokens per Sec:    18828, Lr: 0.000300
2025-05-29 23:24:13,412 - INFO - joeynmt.training - Epoch   1, Step:     5300, Batch Loss:     2.980018, Batch Acc: 0.271987, Tokens per Sec:    18992, Lr: 0.000300
2025-05-29 23:24:16,983 - INFO - joeynmt.training - Epoch   1, Step:     5400, Batch Loss:     2.833136, Batch Acc: 0.277707, Tokens per Sec:    19481, Lr: 0.000300
2025-05-29 23:24:20,528 - INFO - joeynmt.training - Epoch   1, Step:     5500, Batch Loss:     2.662166, Batch Acc: 0.279348, Tokens per Sec:    18205, Lr: 0.000300
2025-05-29 23:24:20,528 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:24:20,528 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:24:30,207 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.73, ppl:  15.35, acc:   0.29, generation: 9.6642[sec], evaluation: 0.0000[sec]
2025-05-29 23:24:30,207 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:24:30,800 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/3000.ckpt
2025-05-29 23:24:30,828 - INFO - joeynmt.training - Example #0
2025-05-29 23:24:30,828 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:24:30,828 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:24:30,828 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'in', 'questo', 'mod@@', 'o@@', ',', 'ho', 'fatto', 'che', 'ho', 'fatto', 'che', 'la', 'maggior', 'parte', 'di', 'cui', 'si', 'può', 'essere', 'in', 'modo', 'in', 'cui', 'i', 'nostri', 'nostri', 'an@@', 'ni@@', ',', 'i', 'nostri', 'anni', 'fa@@', ',', 'e', 'poi', 'abbiamo', 'fatto', 'il', '19@@', '8@@', '0@@', ',', 'per', 'cento', 'di', 'questi', 'ultimi', 'anni', 'fa@@', ',', '1@@', ',', '1@@', ',', '1@@', '1@@', ',', '1@@', '5@@', ',', 'per', 'esem@@', 'pi@@', 'o@@', '.', '</s>']
2025-05-29 23:24:30,829 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:24:30,829 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:24:30,829 - INFO - joeynmt.training - 	Hypothesis: E in questo modo, ho fatto che ho fatto che la maggior parte di cui si può essere in modo in cui i nostri nostri anni, i nostri anni fa, e poi abbiamo fatto il 1980, per cento di questi ultimi anni fa, 1, 1, 11, 15, per esempio.
2025-05-29 23:24:30,829 - INFO - joeynmt.training - Example #1
2025-05-29 23:24:30,829 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:24:30,829 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:24:30,830 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'stato', 'più', 'importante', 'che', 'non', 'è', 'la', 'maggior', 'parte', 'del', 'nostro', 'modo', 'in', 'cui', 'non', 'è', 'il', 'modo', 'in', 'cui', 'non', 'è', 'stato', 'il', 'modo', 'in', 'cui', 'non', 'è', 'stato', 'il', 'modo', 'in', 'cui', 'il', 'modo', 'in', 'cui', 'il', 'modo', 'in', 'cui', 'non', 'è', 'stato', 'il', 'nostro', 'modo', 'in', 'cui', 'non', 'è', 'stato', 'il', 'nostro', 'modo', 'in', 'cui', 'non', 'è', 'stato', 'il', 'nostro', 'modo', 'in', 'cui', 'non', 'è', 'è', 'stato', 'il', 'nostro', 'modo', 'in', 'cui', 'il', 'nostro', 'modo', 'in', 'cui', 'non', 'è', 'stato', 'il', 'modo', 'in', 'cui', 'la', 'gente', 'non', 'è', 'stato', 'stato', 'stato', 'più', 'più', 'importante', 'per', 'la', 'nostra', 'parte', 'del', 'nostro', 'modo', 'in']
2025-05-29 23:24:30,830 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:24:30,830 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:24:30,830 - INFO - joeynmt.training - 	Hypothesis: Ma non è stato più importante che non è la maggior parte del nostro modo in cui non è il modo in cui non è stato il modo in cui non è stato il modo in cui il modo in cui il modo in cui non è stato il nostro modo in cui non è stato il nostro modo in cui non è stato il nostro modo in cui non è è stato il nostro modo in cui il nostro modo in cui non è stato il modo in cui la gente non è stato stato stato più più importante per la nostra parte del nostro modo in
2025-05-29 23:24:30,830 - INFO - joeynmt.training - Example #2
2025-05-29 23:24:30,830 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:24:30,830 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:24:30,830 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'po@@', '<unk>', 'di', 'un', 'po@@', '<unk>', 'di', 'c@@', '<unk>', 'è', 'la', 'nostra', 'nostra', 'nostra', 'nostra', 'nostra', 'nostra', 'nostra', 'nostra', 'nostra', 'nostra', 'nostra', 'nostra', 'nostra', 'nostra', 'nostra', 'nostra', 'nostra', 'nostra', 'vit@@', 'a@@', '.', '</s>']
2025-05-29 23:24:30,831 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:24:30,831 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:24:30,831 - INFO - joeynmt.training - 	Hypothesis: In un po<unk> di un po<unk> di c<unk> è la nostra nostra nostra nostra nostra nostra nostra nostra nostra nostra nostra nostra nostra nostra nostra nostra nostra nostra vita.
2025-05-29 23:24:30,831 - INFO - joeynmt.training - Example #3
2025-05-29 23:24:30,831 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:24:30,831 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:24:30,831 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'poi', 'si', 'sono', 'in', 'grado', 'di', 'ri@@', 'vel@@', 'o@@', '.', '</s>']
2025-05-29 23:24:30,832 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:24:30,832 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:24:30,832 - INFO - joeynmt.training - 	Hypothesis: E poi si sono in grado di rivelo.
2025-05-29 23:24:30,832 - INFO - joeynmt.training - Example #4
2025-05-29 23:24:30,832 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:24:30,832 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:24:30,832 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'volta', 'che', 'vi', 'mostr@@', 'erò', 'la', 'prima', 'volta', 'che', 'è', 'una', 'parte', 'di', 'ciò', 'che', 'è', 'stato', 'stato', 'stato', 'stato', 'in', 'questo', 'mod@@', 'o@@', '.', '</s>']
2025-05-29 23:24:30,832 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:24:30,832 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:24:30,832 - INFO - joeynmt.training - 	Hypothesis: La prima volta che vi mostrerò la prima volta che è una parte di ciò che è stato stato stato stato in questo modo.
2025-05-29 23:24:34,423 - INFO - joeynmt.training - Epoch   1, Step:     5600, Batch Loss:     2.746880, Batch Acc: 0.277822, Tokens per Sec:    15865, Lr: 0.000300
2025-05-29 23:24:37,989 - INFO - joeynmt.training - Epoch   1, Step:     5700, Batch Loss:     2.721489, Batch Acc: 0.285220, Tokens per Sec:    19141, Lr: 0.000300
2025-05-29 23:24:41,558 - INFO - joeynmt.training - Epoch   1, Step:     5800, Batch Loss:     2.640929, Batch Acc: 0.284974, Tokens per Sec:    19146, Lr: 0.000300
2025-05-29 23:24:45,116 - INFO - joeynmt.training - Epoch   1, Step:     5900, Batch Loss:     2.744639, Batch Acc: 0.288900, Tokens per Sec:    18849, Lr: 0.000300
2025-05-29 23:24:48,676 - INFO - joeynmt.training - Epoch   1, Step:     6000, Batch Loss:     2.703722, Batch Acc: 0.290807, Tokens per Sec:    18979, Lr: 0.000300
2025-05-29 23:24:48,676 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:24:48,677 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:24:58,005 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.68, ppl:  14.57, acc:   0.30, generation: 9.3153[sec], evaluation: 0.0000[sec]
2025-05-29 23:24:58,005 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:24:58,597 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/3500.ckpt
2025-05-29 23:24:58,625 - INFO - joeynmt.training - Example #0
2025-05-29 23:24:58,626 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:24:58,626 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:24:58,626 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'ho', 'visto', 'questa', 'storia', 'che', 'ho', 'visto', 'questa', 'prima', 'di', 'questa', 'storia', 'che', 'abbiamo', 'visto', 'la', 'prima', 'volta', 'che', 'la', 'prima', 'volta', 'che', 'la', 'prima', 'di', 'una', 'delle', 'persone', 'che', 'hanno', 'visto', 'la', 'prima', 'volta', 'che', 'le', 'tre', 'anni', 'fa@@', ',', 'per', 'la', 'prima', 'di', 'tre', 'anni', 'fa@@', ',', 'che', 'ho', 'visto', 'il', '1@@', '5@@', '%', 'di', 'persone', 'che', 'sono', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 's@@', 'ed@@', 'uti', 'in', 'un', 'anno', 'di', 'm@@', 'om@@', 'ent@@', 'o@@', '.', '</s>']
2025-05-29 23:24:58,627 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:24:58,627 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:24:58,627 - INFO - joeynmt.training - 	Hypothesis: E ho visto questa storia che ho visto questa prima di questa storia che abbiamo visto la prima volta che la prima volta che la prima di una delle persone che hanno visto la prima volta che le tre anni fa, per la prima di tre anni fa, che ho visto il 15% di persone che sono stati stati stati stati stati stati stati stati stati seduti in un anno di momento.
2025-05-29 23:24:58,627 - INFO - joeynmt.training - Example #1
2025-05-29 23:24:58,627 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:24:58,628 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:24:58,628 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'si', 'può', 'essere', 'più', 'più', 'di', 'una', 'nuova', 'e', 'non', 'si', 'può', 'essere', 'in', 'questo', 'mod@@', 'o@@', ',', 'non', 'si', 'è', 'stato', 'un', 'problema', 'di', 'pi@@', 'ù@@', ',', 'non', 'si', 'può', 'essere', 'in', 'modo', 'di', 'ri@@', 'ma@@', '.', '</s>']
2025-05-29 23:24:58,628 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:24:58,628 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:24:58,628 - INFO - joeynmt.training - 	Hypothesis: Ma non si può essere più più di una nuova e non si può essere in questo modo, non si è stato un problema di più, non si può essere in modo di rima.
2025-05-29 23:24:58,628 - INFO - joeynmt.training - Example #2
2025-05-29 23:24:58,628 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:24:58,628 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:24:58,629 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'questo', 'è', 'il', 'nostro', 'sistema', 'di', 's@@', 'essu@@', 'ale', 'è', 'il', 'nostro', 'sistema', 'di', 'c@@', 'ris@@', 'i', 'di', 'c@@', 'lin@@', 'ica', 'di', 'un', 'modo', 'di', 'ri@@', 'pet@@', 'o', 'di', 'essere', 'un', 'modo', 'di', 'ri@@', 'pet@@', 'o@@', '.', '</s>']
2025-05-29 23:24:58,629 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:24:58,629 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:24:58,629 - INFO - joeynmt.training - 	Hypothesis: In questo è il nostro sistema di sessuale è il nostro sistema di crisi di clinica di un modo di ripeto di essere un modo di ripeto.
2025-05-29 23:24:58,629 - INFO - joeynmt.training - Example #3
2025-05-29 23:24:58,629 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:24:58,629 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:24:58,629 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'tratta', 'di', 'una', 'delle', 's@@', 'quad@@', 'ra@@', ',', 'e', 'il', 't@@', 'om@@', 'om@@', 'om@@', 'om@@', 'i@@', '.', '</s>']
2025-05-29 23:24:58,630 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:24:58,630 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:24:58,630 - INFO - joeynmt.training - 	Hypothesis: Si tratta di una delle squadra, e il tomomomomi.
2025-05-29 23:24:58,630 - INFO - joeynmt.training - Example #4
2025-05-29 23:24:58,630 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:24:58,630 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:24:58,630 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'volta', 'che', 'vi', 'ho', 'mostr@@', 'ato', 'la', 'prima', 'volta', 'che', 'è', 'una', 'piccola', 'piccola', 'piccola', 'cosa', 'che', 'è', 'che', 'è', 'stato', 'un', 'paio', 'di', 'anni', 'fa@@', '.', '</s>']
2025-05-29 23:24:58,630 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:24:58,631 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:24:58,631 - INFO - joeynmt.training - 	Hypothesis: La prima volta che vi ho mostrato la prima volta che è una piccola piccola piccola cosa che è che è stato un paio di anni fa.
2025-05-29 23:25:02,252 - INFO - joeynmt.training - Epoch   1, Step:     6100, Batch Loss:     2.781755, Batch Acc: 0.289726, Tokens per Sec:    16478, Lr: 0.000300
2025-05-29 23:25:05,856 - INFO - joeynmt.training - Epoch   1, Step:     6200, Batch Loss:     2.703268, Batch Acc: 0.291049, Tokens per Sec:    18910, Lr: 0.000300
2025-05-29 23:25:09,389 - INFO - joeynmt.training - Epoch   1, Step:     6300, Batch Loss:     2.600262, Batch Acc: 0.292248, Tokens per Sec:    19632, Lr: 0.000300
2025-05-29 23:25:13,295 - INFO - joeynmt.training - Epoch   1, Step:     6400, Batch Loss:     2.649277, Batch Acc: 0.298843, Tokens per Sec:    17859, Lr: 0.000300
2025-05-29 23:25:16,845 - INFO - joeynmt.training - Epoch   1, Step:     6500, Batch Loss:     2.607142, Batch Acc: 0.294915, Tokens per Sec:    19310, Lr: 0.000300
2025-05-29 23:25:16,846 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:25:16,846 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:25:27,061 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.63, ppl:  13.90, acc:   0.31, generation: 10.2033[sec], evaluation: 0.0000[sec]
2025-05-29 23:25:27,062 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:25:27,664 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/4000.ckpt
2025-05-29 23:25:27,693 - INFO - joeynmt.training - Example #0
2025-05-29 23:25:27,693 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:25:27,693 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:25:27,693 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'ho', 'iniziato', 'a', 'tre', 'anni', 'fa@@', ',', 'ho', 'iniziato', 'a', 'fare', 'con', 'la', 'prima', 'volta', 'che', 'si', 'sono', 'stati', 'in', 'grado', 'di', 'fare', 'la', 'di@@', 'stru@@', 'zione', 'di', 'tre', 'anni', 'fa@@', ',', 'e', 'la', 'prima', 'di', 'tre', 'anni', 'fa@@', ',', 'per', 'cento', 'di', 'anni', 'fa@@', '.', '</s>']
2025-05-29 23:25:27,694 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:25:27,694 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:25:27,694 - INFO - joeynmt.training - 	Hypothesis: E ho iniziato a tre anni fa, ho iniziato a fare con la prima volta che si sono stati in grado di fare la distruzione di tre anni fa, e la prima di tre anni fa, per cento di anni fa.
2025-05-29 23:25:27,694 - INFO - joeynmt.training - Example #1
2025-05-29 23:25:27,694 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:25:27,694 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:25:27,694 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'non', 'è', 'un', 'problema', 'che', 'non', 'è', 'che', 'la', 'gente', 'non', 'è', 'la', 'maggior', 'parte', 'di', 'questo', 'non', 'è', 'il', 'problema', 'che', 'non', 'è', 'che', 'non', 'è', 'la', 'c@@', 'ris@@', 'i', 'di', 'c@@', 'atti@@', 'vo@@', '.', '</s>']
2025-05-29 23:25:27,695 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:25:27,695 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:25:27,695 - INFO - joeynmt.training - 	Hypothesis: Ma non non è un problema che non è che la gente non è la maggior parte di questo non è il problema che non è che non è la crisi di cattivo.
2025-05-29 23:25:27,695 - INFO - joeynmt.training - Example #2
2025-05-29 23:25:27,695 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:25:27,695 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:25:27,695 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'sistema', 'di', 'c@@', 'ris@@', 'i', 'è', 'il', 'nostro', 'sistema', 'di', 'S@@', 'al@@', 'e@@', ',', 'il', 'nostro', 'sistema', 'di', 'c@@', 'ris@@', 'i', 'di', 'c@@', 'au@@', 'si@@', 'a@@', '.', '</s>']
2025-05-29 23:25:27,696 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:25:27,696 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:25:27,696 - INFO - joeynmt.training - 	Hypothesis: In un sistema di crisi è il nostro sistema di Sale, il nostro sistema di crisi di causia.
2025-05-29 23:25:27,696 - INFO - joeynmt.training - Example #3
2025-05-29 23:25:27,696 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:25:27,696 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:25:27,696 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'ho', 'fatto', 'in', 'cui', 'si', 'sono', 'in', 'grado', 'di', 'fare', 'in', 'grado', 'di', 'fare', 'in', 'grado', 'di', 'fare', 'in', 'un', 't@@', 'asso', 'di', 't@@', 'ale', 'e', 'la', 'sc@@', 'av@@', 'at@@', 'a@@', '.', '</s>']
2025-05-29 23:25:27,697 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:25:27,697 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:25:27,697 - INFO - joeynmt.training - 	Hypothesis: L<unk> ho fatto in cui si sono in grado di fare in grado di fare in grado di fare in un tasso di tale e la scavata.
2025-05-29 23:25:27,697 - INFO - joeynmt.training - Example #4
2025-05-29 23:25:27,697 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:25:27,697 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:25:27,697 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'la', 'prima', 'volta', 'che', 'vi', 'mostr@@', 'o', 'che', 'vi', 'mostr@@', 'o', 'che', 'è', 'una', 'cosa', 'che', 'è', 'che', 'è', 'stato', 'stato', 'in', 'cui', 'è', 'stato', 'un', 'paio', 'di', 'anni', 'fa@@', '.', '</s>']
2025-05-29 23:25:27,697 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:25:27,698 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:25:27,698 - INFO - joeynmt.training - 	Hypothesis: E la prima volta che vi mostro che vi mostro che è una cosa che è che è stato stato in cui è stato un paio di anni fa.
2025-05-29 23:25:31,301 - INFO - joeynmt.training - Epoch   1, Step:     6600, Batch Loss:     2.610754, Batch Acc: 0.299074, Tokens per Sec:    16108, Lr: 0.000300
2025-05-29 23:25:34,872 - INFO - joeynmt.training - Epoch   1, Step:     6700, Batch Loss:     2.618871, Batch Acc: 0.303446, Tokens per Sec:    19562, Lr: 0.000300
2025-05-29 23:25:38,440 - INFO - joeynmt.training - Epoch   1, Step:     6800, Batch Loss:     2.601450, Batch Acc: 0.300635, Tokens per Sec:    18881, Lr: 0.000300
2025-05-29 23:25:42,026 - INFO - joeynmt.training - Epoch   1, Step:     6900, Batch Loss:     2.567791, Batch Acc: 0.301680, Tokens per Sec:    19442, Lr: 0.000300
2025-05-29 23:25:45,567 - INFO - joeynmt.training - Epoch   1, Step:     7000, Batch Loss:     2.671000, Batch Acc: 0.304381, Tokens per Sec:    19010, Lr: 0.000300
2025-05-29 23:25:45,567 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:25:45,567 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:25:55,776 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.59, ppl:  13.30, acc:   0.32, generation: 10.1954[sec], evaluation: 0.0000[sec]
2025-05-29 23:25:55,776 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:25:56,386 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/4500.ckpt
2025-05-29 23:25:56,410 - INFO - joeynmt.training - Example #0
2025-05-29 23:25:56,411 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:25:56,411 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:25:56,411 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'ho', 'fatto', 'a', 'cui', 'ho', 'fatto', 'a', 'fare', 'il', 'mon@@', 'do@@', ',', 'che', 'le', 'due', 'di', 'cui', 'le', 'persone', 'si', 'sono', 'ri@@', 'vel@@', 'ati', 'per', 'la', 'stessa', 'cosa', 'che', 'si', 'trov@@', 'ano', 'a', 'tre', 'milioni', 'di', 'anni', 'per', 'i', 'tre', 'milioni', 'di', 'anni', 'fa@@', ',', 'per', 'la', 'prima', 'volta', 'che', 'la', 'prima', 'volta', 'che', 'è', 'stato', 'per', 'il', '2@@', '5@@', '%', 'di', 'vita', 'di', 'ogni', 'ann@@', 'o@@', '.', '</s>']
2025-05-29 23:25:56,411 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:25:56,411 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:25:56,411 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno ho fatto a cui ho fatto a fare il mondo, che le due di cui le persone si sono rivelati per la stessa cosa che si trovano a tre milioni di anni per i tre milioni di anni fa, per la prima volta che la prima volta che è stato per il 25% di vita di ogni anno.
2025-05-29 23:25:56,412 - INFO - joeynmt.training - Example #1
2025-05-29 23:25:56,412 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:25:56,412 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:25:56,412 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'più', 'import@@', 'ant@@', 'e@@', ',', 'non', 'è', 'più', 'import@@', 'ant@@', 'e@@', ',', 'non', 'è', 'il', 'probl@@', 'em@@', 'a@@', ',', 'non', 'è', 'la', 'nostra', 'società', 'che', 'non', 'è', 'la', 'stessa', 'cos@@', 'a@@', '.', '</s>']
2025-05-29 23:25:56,412 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:25:56,412 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:25:56,412 - INFO - joeynmt.training - 	Hypothesis: Ma non è più importante, non è più importante, non è il problema, non è la nostra società che non è la stessa cosa.
2025-05-29 23:25:56,413 - INFO - joeynmt.training - Example #2
2025-05-29 23:25:56,413 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:25:56,413 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:25:56,413 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'realtà', 'è', 'la', 'c@@', 'lasse', 'è', 'la', 'tecnologia', 'che', 'è', 'la', 'C@@', 'aliforni@@', 'a', 'è', 'la', 'nostra', 'società', 'di', 'un', 'sistema', 'di', 'energia', 'di', 'energia', 'di', 'un', 'sistema', 'di', 's@@', 'essu@@', 'ale', 'di', 'un', 'sistema', 'di', 's@@', 'essu@@', 'ale', 'di', 'un', 'sistema', 'di', 's@@', 'essu@@', 'to', 'di', '<unk>', 'E@@', '<unk>', 'E@@', '<unk>', 'E@@', '<unk>', ',', 'e', 'la', 'nostra', 'società', 'di', 'un', 'po@@', '<unk>', 'di', 'pi@@', 'u@@', '<unk>', ',', 'e', 'la', 'nostra', 'società', 'di', 'una', 'c@@', 'lin@@', 'ica', 'di', 'c@@', 'lin@@', 'ic@@', 'a@@', ',', 'e', 'la', 'tecnologia', 'di', 'una', 'delle', 'b@@', 'ella', 'della', 'nostra', 'società', 'di', 's@@', 'essu@@', 'ale', 'di', 's@@', 'com@@', 'in@@', 'fer@@', 'i@@', 'a@@', ',', 'e']
2025-05-29 23:25:56,413 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:25:56,413 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:25:56,413 - INFO - joeynmt.training - 	Hypothesis: In realtà è la classe è la tecnologia che è la California è la nostra società di un sistema di energia di energia di un sistema di sessuale di un sistema di sessuale di un sistema di sessuto di <unk> E<unk> E<unk> E<unk> , e la nostra società di un po<unk> di piu<unk> , e la nostra società di una clinica di clinica, e la tecnologia di una delle bella della nostra società di sessuale di scominferia, e
2025-05-29 23:25:56,413 - INFO - joeynmt.training - Example #3
2025-05-29 23:25:56,414 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:25:56,414 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:25:56,414 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'olo', 'in', 'modo', 'in', 'cui', 'si', 'trov@@', 'ano', 'in', 'un', 'p@@', 'om@@', 'o@@', '.', '</s>']
2025-05-29 23:25:56,414 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:25:56,414 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:25:56,414 - INFO - joeynmt.training - 	Hypothesis: Solo in modo in cui si trovano in un pomo.
2025-05-29 23:25:56,414 - INFO - joeynmt.training - Example #4
2025-05-29 23:25:56,414 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:25:56,415 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:25:56,415 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'è', 'una', 'volta', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'delle', 'cose', 'che', 'è', 'una', 'cosa', 'che', 'è', 'un', 'po@@', '<unk>', 'di', 'circa', 'il', '50', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:25:56,415 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:25:56,415 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:25:56,415 - INFO - joeynmt.training - 	Hypothesis: La prossima è una volta che vi mostrerò è una delle cose che è una cosa che è un po<unk> di circa il 50 anni.
2025-05-29 23:26:00,010 - INFO - joeynmt.training - Epoch   1, Step:     7100, Batch Loss:     2.725680, Batch Acc: 0.305638, Tokens per Sec:    16344, Lr: 0.000300
2025-05-29 23:26:03,600 - INFO - joeynmt.training - Epoch   1, Step:     7200, Batch Loss:     2.610862, Batch Acc: 0.307914, Tokens per Sec:    19379, Lr: 0.000300
2025-05-29 23:26:07,200 - INFO - joeynmt.training - Epoch   1, Step:     7300, Batch Loss:     2.592621, Batch Acc: 0.313295, Tokens per Sec:    19507, Lr: 0.000300
2025-05-29 23:26:10,771 - INFO - joeynmt.training - Epoch   1, Step:     7400, Batch Loss:     2.602327, Batch Acc: 0.313004, Tokens per Sec:    19523, Lr: 0.000300
2025-05-29 23:26:14,335 - INFO - joeynmt.training - Epoch   1, Step:     7500, Batch Loss:     2.478419, Batch Acc: 0.311532, Tokens per Sec:    19152, Lr: 0.000300
2025-05-29 23:26:14,335 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:26:14,335 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:26:22,037 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.55, ppl:  12.79, acc:   0.32, generation: 7.6902[sec], evaluation: 0.0000[sec]
2025-05-29 23:26:22,038 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:26:22,634 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/5000.ckpt
2025-05-29 23:26:22,663 - INFO - joeynmt.training - Example #0
2025-05-29 23:26:22,663 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:26:22,663 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:26:22,663 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'fatto', 'questa', 'idea', 'di', 'cui', 'ho', 'fatto', 'questa', 'cosa', 'che', 'è', 'stato', 'per', 'ri@@', 'durre', 'la', 'di@@', 't@@', 't@@', 't@@', 't@@', 't@@', 'a@@', ',', 'per', 'la', 'di@@', 'stru@@', 'zione', 'di', 'tre', 'anni', 'per', 'la', 'prima', 'volta', 'che', 'la', 'maggior', 'parte', 'delle', 'tre', 'anni', 'per', 'cento', 'di', '1@@', '5@@', '0.000', 'anni', 'per', 'cento', 'del', '1@@', '1@@', '5@@', '0@@', '.', '</s>']
2025-05-29 23:26:22,664 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:26:22,664 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:26:22,664 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho fatto questa idea di cui ho fatto questa cosa che è stato per ridurre la dittttta, per la distruzione di tre anni per la prima volta che la maggior parte delle tre anni per cento di 150.000 anni per cento del 1150.
2025-05-29 23:26:22,664 - INFO - joeynmt.training - Example #1
2025-05-29 23:26:22,664 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:26:22,664 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:26:22,664 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'stato', 'stato', 'un', 'problema', 'di', 'più', 'grande', 'di', 'questo', 'probl@@', 'em@@', 'a@@', ',', 'e', 'non', 'è', 'il', 'problema', 'della', 'N@@', 'er@@', 'ra@@', ',', 'non', 'è', 'stato', 'il', 'problema', 'della', 'C@@', 'E@@', 'E@@', '<unk>', '.', '</s>']
2025-05-29 23:26:22,665 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:26:22,665 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:26:22,665 - INFO - joeynmt.training - 	Hypothesis: Ma non è stato stato un problema di più grande di questo problema, e non è il problema della Nerra, non è stato il problema della CEE<unk> .
2025-05-29 23:26:22,665 - INFO - joeynmt.training - Example #2
2025-05-29 23:26:22,665 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:26:22,665 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:26:22,665 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', 'di', 'ri@@', 'ma@@', 'i@@', ',', 'è', 'la', 'grande', 'E@@', 'ver@@', 'd@@', 'entale', 'che', 'la', 'tecnologia', 'è', 'stato', 'il', 'nostro', 'sistema', 'di', 'un', 'sistema', 'di', 's@@', 'essu@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:26:22,666 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:26:22,666 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:26:22,666 - INFO - joeynmt.training - 	Hypothesis: In un certo senso di rimai, è la grande Everdentale che la tecnologia è stato il nostro sistema di un sistema di sessuale.
2025-05-29 23:26:22,666 - INFO - joeynmt.training - Example #3
2025-05-29 23:26:22,666 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:26:22,666 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:26:22,666 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'ho', 'fatto', 'il', 'suo', 's@@', 'ettore', 'e', 'la', 's@@', 'quad@@', 'r@@', 'ati@@', '.', '</s>']
2025-05-29 23:26:22,666 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:26:22,666 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:26:22,667 - INFO - joeynmt.training - 	Hypothesis: L<unk> ho fatto il suo settore e la squadrati.
2025-05-29 23:26:22,667 - INFO - joeynmt.training - Example #4
2025-05-29 23:26:22,667 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:26:22,667 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:26:22,667 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'prossi@@', 'mo', 'prossi@@', 'mo', 'è', 'un', 'po@@', '<unk>', 'di', 'di@@', 'stru@@', 'zione', 'di', 'un', 'altro', 'altro', 'altro', 'che', 'è', 'un', 'anno', 'in', 'cui', 'è', 'stato', 'stato', 'stato', 'un', 'paio', 'di', 'anni', 'fa@@', '.', '</s>']
2025-05-29 23:26:22,667 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:26:22,667 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:26:22,667 - INFO - joeynmt.training - 	Hypothesis: Il prossimo prossimo è un po<unk> di distruzione di un altro altro altro che è un anno in cui è stato stato stato un paio di anni fa.
2025-05-29 23:26:26,268 - INFO - joeynmt.training - Epoch   1, Step:     7600, Batch Loss:     2.589596, Batch Acc: 0.313816, Tokens per Sec:    16794, Lr: 0.000300
2025-05-29 23:26:29,807 - INFO - joeynmt.training - Epoch   1, Step:     7700, Batch Loss:     2.527247, Batch Acc: 0.315369, Tokens per Sec:    19372, Lr: 0.000300
2025-05-29 23:26:33,355 - INFO - joeynmt.training - Epoch   1, Step:     7800, Batch Loss:     2.629193, Batch Acc: 0.318043, Tokens per Sec:    18962, Lr: 0.000300
2025-05-29 23:26:36,905 - INFO - joeynmt.training - Epoch   1, Step:     7900, Batch Loss:     2.562103, Batch Acc: 0.318825, Tokens per Sec:    19236, Lr: 0.000300
2025-05-29 23:26:40,447 - INFO - joeynmt.training - Epoch   1, Step:     8000, Batch Loss:     2.668962, Batch Acc: 0.317588, Tokens per Sec:    19440, Lr: 0.000300
2025-05-29 23:26:40,448 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:26:40,448 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:26:49,886 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.54, ppl:  12.63, acc:   0.33, generation: 9.4274[sec], evaluation: 0.0000[sec]
2025-05-29 23:26:49,887 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:26:50,479 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/5500.ckpt
2025-05-29 23:26:50,508 - INFO - joeynmt.training - Example #0
2025-05-29 23:26:50,509 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:26:50,509 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:26:50,509 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'ho', 'fatto', 'queste', 'due', 'settimane', 'e', 'ho', 'fatto', 'che', 'i', 'due', 'anni', 'di', 'lavor@@', 'o@@', ',', 'e', 'per', 'la', 'mia', 'm@@', 'amma', 'di', 'di@@', 'stri@@', 'bu@@', 'ito', 'per', 'la', 'c@@', 'ris@@', 'i', 'di', 'tre', 'anni', 'di', 'ri@@', 'durre', 'i', 'dati', 'di', 'ri@@', 'stor@@', 'anti', 'di', '3@@', '5', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:26:50,510 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:26:50,510 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:26:50,510 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno ho fatto queste due settimane e ho fatto che i due anni di lavoro, e per la mia mamma di distribuito per la crisi di tre anni di ridurre i dati di ristoranti di 35 anni.
2025-05-29 23:26:50,510 - INFO - joeynmt.training - Example #1
2025-05-29 23:26:50,510 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:26:50,510 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:26:50,510 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'stato', 'stato', 'molto', 'tempo', 'per', 'la', 'più', 'grande', 'di', 'questo', 'è', 'il', 'problema', 'di', 'questo', 'probl@@', 'em@@', 'a@@', ',', 'ma', 'non', 'è', 'il', 'problema', 'di', 'questo', 'probl@@', 'em@@', 'a@@', '.', '</s>']
2025-05-29 23:26:50,511 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:26:50,511 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:26:50,511 - INFO - joeynmt.training - 	Hypothesis: Ma non è stato stato molto tempo per la più grande di questo è il problema di questo problema, ma non è il problema di questo problema.
2025-05-29 23:26:50,511 - INFO - joeynmt.training - Example #2
2025-05-29 23:26:50,511 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:26:50,511 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:26:50,511 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'è', 'la', 'cosa', 'più', 'grande', 'di', 'cui', 'il', 'suo', 'inter@@', 'vento', 'di', 'un', 'po@@', '<unk>', 'di', 's@@', 'é@@', '.', '</s>']
2025-05-29 23:26:50,512 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:26:50,512 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:26:50,512 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, è la cosa più grande di cui il suo intervento di un po<unk> di sé.
2025-05-29 23:26:50,512 - INFO - joeynmt.training - Example #3
2025-05-29 23:26:50,512 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:26:50,512 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:26:50,512 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'il', 'mio', 'padre', 'è', 'stato', 'in', 'grado', 'di', 'ri@@', 'durre', 'il', 't@@', 'asso', 'di', 's@@', 'é@@', '.', '</s>']
2025-05-29 23:26:50,513 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:26:50,513 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:26:50,513 - INFO - joeynmt.training - 	Hypothesis: E il mio padre è stato in grado di ridurre il tasso di sé.
2025-05-29 23:26:50,513 - INFO - joeynmt.training - Example #4
2025-05-29 23:26:50,513 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:26:50,513 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:26:50,513 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'è', 'un', 'po@@', '<unk>', ',', 'è', 'un', 'po@@', '<unk>', ',', 'è', 'un', 'po@@', '<unk>', 'di', 'quello', 'che', 'è', 'un', 'po@@', '<unk>', '.', '</s>']
2025-05-29 23:26:50,513 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:26:50,514 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:26:50,514 - INFO - joeynmt.training - 	Hypothesis: La prossima è un po<unk> , è un po<unk> , è un po<unk> di quello che è un po<unk> .
2025-05-29 23:26:54,129 - INFO - joeynmt.training - Epoch   1, Step:     8100, Batch Loss:     2.522619, Batch Acc: 0.318402, Tokens per Sec:    16337, Lr: 0.000300
2025-05-29 23:26:57,682 - INFO - joeynmt.training - Epoch   1, Step:     8200, Batch Loss:     2.465538, Batch Acc: 0.327310, Tokens per Sec:    19506, Lr: 0.000300
2025-05-29 23:27:01,260 - INFO - joeynmt.training - Epoch   1, Step:     8300, Batch Loss:     2.577558, Batch Acc: 0.317681, Tokens per Sec:    19067, Lr: 0.000300
2025-05-29 23:27:04,832 - INFO - joeynmt.training - Epoch   1, Step:     8400, Batch Loss:     2.489882, Batch Acc: 0.322277, Tokens per Sec:    19232, Lr: 0.000300
2025-05-29 23:27:08,379 - INFO - joeynmt.training - Epoch   1, Step:     8500, Batch Loss:     2.496803, Batch Acc: 0.327448, Tokens per Sec:    19026, Lr: 0.000300
2025-05-29 23:27:08,379 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:27:08,379 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:27:17,086 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.48, ppl:  11.89, acc:   0.34, generation: 8.6948[sec], evaluation: 0.0000[sec]
2025-05-29 23:27:17,087 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:27:17,682 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/6000.ckpt
2025-05-29 23:27:17,710 - INFO - joeynmt.training - Example #0
2025-05-29 23:27:17,710 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:27:17,711 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:27:17,711 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'ho', 'fatto', 'che', 'questa', 'è', 'la', 'due', 'volte', 'per', 'la', 'stessa', 'cosa', 'che', 'la', 'gente', 'è', 'che', 'la', 'gente', 'che', 'è', 'che', 'la', 'c@@', '<unk>', 'è', 'la', 'c@@', 'lasse', 'di', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'maggior', 'parte', 'di', 'tre', 'milioni', 'di', 'anni', 'fa', 'fa', 'è', 'il', '5@@', '0@@', '%', 'di', 'circa', '1@@', '5@@', '0@@', '%', 'di', 'un', 'mili@@', 'one', 'di', 'dollari', 'per', 'la', 'popolazione', 'mon@@', 'diale', 'di', 'un', 'mili@@', 'one', 'di', 'dollari', 'per', 'la', 'prima', 'volta', 'che', 'è', 'il', '5@@', '%', 'di', 'una', 'delle', 'due', 'milioni', 'di', 'anni', 'fa', 'fa', 'è', 'il', '5@@', '0@@', '%', 'di', 'un', 'po@@']
2025-05-29 23:27:17,711 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:27:17,711 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:27:17,711 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno ho fatto che questa è la due volte per la stessa cosa che la gente è che la gente che è che la c<unk> è la classe di tre milioni di anni per la tre milioni di anni per la maggior parte di tre milioni di anni fa fa è il 50% di circa 150% di un milione di dollari per la popolazione mondiale di un milione di dollari per la prima volta che è il 5% di una delle due milioni di anni fa fa è il 50% di un po
2025-05-29 23:27:17,711 - INFO - joeynmt.training - Example #1
2025-05-29 23:27:17,712 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:27:17,712 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:27:17,712 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'una', 'cosa', 'che', 'non', 'è', 'molto', 'più', 'importante', 'per', 'la', 'stessa', 'cosa', 'che', 'non', 'è', 'il', 'problema', 'che', 'non', 'è', 'la', 'stessa', 'cos@@', 'a@@', ',', 'non', 'è', 'il', 'problema', 'di', 'una', 'volta', 'che', 'non', 'è', 'la', 'stessa', 'cos@@', 'a@@', '.', '</s>']
2025-05-29 23:27:17,712 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:27:17,712 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:27:17,712 - INFO - joeynmt.training - 	Hypothesis: Ma non è una cosa che non è molto più importante per la stessa cosa che non è il problema che non è la stessa cosa, non è il problema di una volta che non è la stessa cosa.
2025-05-29 23:27:17,712 - INFO - joeynmt.training - Example #2
2025-05-29 23:27:17,713 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:27:17,713 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:27:17,713 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'è', 'la', 'struttura', 'di', 'E@@', 'ver@@', 'd@@', 'd@@', 'entale', 'è', 'la', 'stessa', 'cos@@', 'a@@', '.', '</s>']
2025-05-29 23:27:17,713 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:27:17,713 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:27:17,713 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, è la struttura di Everddentale è la stessa cosa.
2025-05-29 23:27:17,713 - INFO - joeynmt.training - Example #3
2025-05-29 23:27:17,714 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:27:17,714 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:27:17,714 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'olo', 'in', 'cui', 'si', 'è', 'in', 'grado', 'di', 'fare', 'in', 'S@@', 'om@@', 'om@@', 'om@@', 'om@@', 'ia', 'e', 'si', 'può', 'essere', 'in', 'grado', 'di', 'ri@@', 'fer@@', 'ire', 'in', 'un', 'modo', 'in', 'cui', 'non', 'si', 'può', 'ri@@', 'durre', 'il', 'suo', 's@@', 'ettore', 'di', 's@@', 'abbia', 'a', 'ri@@', 'durre', 'il', 'suo', 's@@', 'ettore', 'di', 'S@@', 'om@@', 'en@@', 'en@@', 'en@@', 'en@@', 'o@@', '.', '</s>']
2025-05-29 23:27:17,714 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:27:17,714 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:27:17,714 - INFO - joeynmt.training - 	Hypothesis: Solo in cui si è in grado di fare in Somomomomia e si può essere in grado di riferire in un modo in cui non si può ridurre il suo settore di sabbia a ridurre il suo settore di Someneneneno.
2025-05-29 23:27:17,714 - INFO - joeynmt.training - Example #4
2025-05-29 23:27:17,714 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:27:17,714 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:27:17,714 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'è', 'una', 'volta', 'che', 'vi', 'mostr@@', 'erò', 'un', 'po@@', '<unk>', 'di', 'quello', 'che', 'è', 'il', '20@@', '1@@', '1@@', '1@@', '1@@', '0.000', 'anni', 'fa', 'fa', 'fa', 'il', '20@@', '1@@', '1@@', '1@@', '.', '</s>']
2025-05-29 23:27:17,715 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:27:17,715 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:27:17,715 - INFO - joeynmt.training - 	Hypothesis: La prossima è una volta che vi mostrerò un po<unk> di quello che è il 2011110.000 anni fa fa fa il 20111.
2025-05-29 23:27:21,316 - INFO - joeynmt.training - Epoch   1, Step:     8600, Batch Loss:     2.392611, Batch Acc: 0.328353, Tokens per Sec:    16682, Lr: 0.000300
2025-05-29 23:27:24,866 - INFO - joeynmt.training - Epoch   1, Step:     8700, Batch Loss:     2.508157, Batch Acc: 0.331016, Tokens per Sec:    19735, Lr: 0.000300
2025-05-29 23:27:28,398 - INFO - joeynmt.training - Epoch   1, Step:     8800, Batch Loss:     2.696689, Batch Acc: 0.332813, Tokens per Sec:    19601, Lr: 0.000300
2025-05-29 23:27:31,951 - INFO - joeynmt.training - Epoch   1, Step:     8900, Batch Loss:     2.609958, Batch Acc: 0.336571, Tokens per Sec:    19796, Lr: 0.000300
2025-05-29 23:27:35,530 - INFO - joeynmt.training - Epoch   1, Step:     9000, Batch Loss:     2.353763, Batch Acc: 0.333176, Tokens per Sec:    19578, Lr: 0.000300
2025-05-29 23:27:35,531 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:27:35,531 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:27:44,002 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.44, ppl:  11.47, acc:   0.35, generation: 8.4595[sec], evaluation: 0.0000[sec]
2025-05-29 23:27:44,002 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:27:44,586 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/6500.ckpt
2025-05-29 23:27:44,612 - INFO - joeynmt.training - Example #0
2025-05-29 23:27:44,613 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:27:44,613 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:27:44,613 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'ho', 'fatto', 'questa', 'cosa', 'che', 'ho', 'fatto', 'per', 'fare', 'per', 'essere', 'un', 'po@@', '<unk>', 'di', 'ri@@', 'guard@@', 'o@@', ',', 'che', 'la', 'gente', 'è', 'stato', 'stato', 'per', 'la', 'b@@', 'ella', 'di', 'una', 'b@@', 'ella', 'di', 'tre', 'anni', 'fa@@', ',', 'per', 'la', 'maggior', 'parte', 'dei', '3@@', '0@@', '0@@', '0@@', '0@@', '0@@', '0@@', '.', '</s>']
2025-05-29 23:27:44,614 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:27:44,614 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:27:44,614 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno ho fatto questa cosa che ho fatto per fare per essere un po<unk> di riguardo, che la gente è stato stato per la bella di una bella di tre anni fa, per la maggior parte dei 3000000.
2025-05-29 23:27:44,614 - INFO - joeynmt.training - Example #1
2025-05-29 23:27:44,614 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:27:44,614 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:27:44,614 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'più', 'importante', 'per', 'la', 'prima', 'volta', 'che', 'non', 'è', 'stato', 'un', 'problema', 'di', 'probl@@', 'em@@', 'a@@', ',', 'non', 'è', 'la', 'stessa', 'cos@@', 'a@@', ',', 'non', 'è', 'la', 'stessa', 'cosa', 'che', 'si', 'è', 'ri@@', 'ma@@', 'zione', 'del', 'D@@', 'ic@@', 'o@@', '.', '</s>']
2025-05-29 23:27:44,615 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:27:44,615 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:27:44,615 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto più importante per la prima volta che non è stato un problema di problema, non è la stessa cosa, non è la stessa cosa che si è rimazione del Dico.
2025-05-29 23:27:44,615 - INFO - joeynmt.training - Example #2
2025-05-29 23:27:44,615 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:27:44,615 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:27:44,615 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'è', 'la', 'cosa', 'più', 'grande', 'grande', 'E@@', '<unk>', 'la', 'l@@', '<unk>', 'ar@@', 'te@@', ',', 'il', 'nostro', 'sistema', 'di', 'g@@', 'as', 'del', 'nostro', 'sistema', 'di', 'g@@', 'as', 'del', 'nostro', 'sistema', 'sistema', 'di', 'g@@', 'ent@@', 'e@@', '.', '</s>']
2025-05-29 23:27:44,615 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:27:44,616 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:27:44,616 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, è la cosa più grande grande E<unk> la l<unk> arte, il nostro sistema di gas del nostro sistema di gas del nostro sistema sistema di gente.
2025-05-29 23:27:44,616 - INFO - joeynmt.training - Example #3
2025-05-29 23:27:44,616 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:27:44,616 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:27:44,616 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'primo', 'è', 'il', 'loro', 'e', 'di', 'ri@@', 'fer@@', 'imento', 'e', 'di', 'essere', 'in', 'grado', 'di', 'ri@@', 'durre', 'il', 'S@@', 'om@@', 'om@@', 'om@@', 'om@@', 'om@@', 'o@@', '.', '</s>']
2025-05-29 23:27:44,616 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:27:44,616 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:27:44,616 - INFO - joeynmt.training - 	Hypothesis: Il primo è il loro e di riferimento e di essere in grado di ridurre il Somomomomomo.
2025-05-29 23:27:44,616 - INFO - joeynmt.training - Example #4
2025-05-29 23:27:44,617 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:27:44,617 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:27:44,617 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'prossi@@', 'mo', 'prossi@@', 'mo', 'è', 'un', 'po@@', '<unk>', 'di', 'un', 'po@@', '<unk>', 'di', 'cosa', 'è', 'che', 'è', 'stato', 'stato', 'il', 'fatto', 'che', 'è', 'stato', 'stato', 'il', '20@@', '1@@', '1@@', '7', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:27:44,617 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:27:44,617 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:27:44,617 - INFO - joeynmt.training - 	Hypothesis: Il prossimo prossimo è un po<unk> di un po<unk> di cosa è che è stato stato il fatto che è stato stato il 20117 anni.
2025-05-29 23:27:48,213 - INFO - joeynmt.training - Epoch   1, Step:     9100, Batch Loss:     2.343277, Batch Acc: 0.332635, Tokens per Sec:    16564, Lr: 0.000300
2025-05-29 23:27:51,775 - INFO - joeynmt.training - Epoch   1, Step:     9200, Batch Loss:     2.600153, Batch Acc: 0.341876, Tokens per Sec:    19460, Lr: 0.000300
2025-05-29 23:27:55,370 - INFO - joeynmt.training - Epoch   1, Step:     9300, Batch Loss:     2.441103, Batch Acc: 0.342135, Tokens per Sec:    19569, Lr: 0.000300
2025-05-29 23:27:58,937 - INFO - joeynmt.training - Epoch   1, Step:     9400, Batch Loss:     2.275759, Batch Acc: 0.340484, Tokens per Sec:    19674, Lr: 0.000300
2025-05-29 23:27:59,512 - INFO - joeynmt.training - Epoch   1: total training loss 28023.99
2025-05-29 23:27:59,512 - INFO - joeynmt.training - EPOCH 2
2025-05-29 23:28:02,413 - INFO - joeynmt.training - Epoch   2, Step:     9500, Batch Loss:     2.386768, Batch Acc: 0.347793, Tokens per Sec:    19701, Lr: 0.000300
2025-05-29 23:28:02,414 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:28:02,414 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:28:10,760 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.40, ppl:  11.07, acc:   0.36, generation: 8.3351[sec], evaluation: 0.0000[sec]
2025-05-29 23:28:10,760 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:28:11,281 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/7000.ckpt
2025-05-29 23:28:11,300 - INFO - joeynmt.training - Example #0
2025-05-29 23:28:11,301 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:28:11,301 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:28:11,301 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'ho', 'cominciato', 'a', 'vedere', 'queste', 'due', 'an@@', 'ni@@', ',', 'ho', 'visto', 'che', 'la', 'c@@', 'ris@@', 'i', 'di', 'c@@', 'ant@@', 'are', 'che', 'la', 'g@@', 'amma', 'di', 'E@@', 'g@@', 'as', 'per', 'la', 'l@@', '<unk>', 'ar@@', 'te@@', ',', 'per', 'i', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'c@@', 'ris@@', 'i', '4@@', '8', 'an@@', 'ni@@', ',', 'per', 'il', '50', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:28:11,301 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:28:11,301 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:28:11,301 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno ho cominciato a vedere queste due anni, ho visto che la crisi di cantare che la gamma di Egas per la l<unk> arte, per i tre milioni di anni per la crisi 48 anni, per il 50 anni.
2025-05-29 23:28:11,302 - INFO - joeynmt.training - Example #1
2025-05-29 23:28:11,302 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:28:11,302 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:28:11,302 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'tempo', 'che', 'non', 'è', 'abbastanza', 'tempo', 'che', 'non', 'è', 'abbastanza', 'tempo', 'che', 'non', 'è', 'la', 'ver@@', 'sione', 'di', 'un', 'problema', 'che', 'non', 'è', 'la', 'l@@', '<unk>', 'E@@', 'g@@', 'ur@@', 'ban@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:28:11,302 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:28:11,302 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:28:11,302 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza tempo che non è abbastanza tempo che non è abbastanza tempo che non è la versione di un problema che non è la l<unk> Egurbanale.
2025-05-29 23:28:11,303 - INFO - joeynmt.training - Example #2
2025-05-29 23:28:11,303 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:28:11,303 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:28:11,303 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'è', 'la', 'c@@', 'ris@@', 'i', 'di', 'E@@', 'g@@', 'as', 'che', 'la', 'c@@', 'ris@@', 'i', 'di', 'nostro', 'sistema', 'di', 'cui', 'il', 'nostro', 'sistema', 'di', 'cui', 'si', 'è', 'ri@@', 'vel@@', 'ato', 'di', 'un', 'c@@', 'ic@@', 'l@@', 'ett@@', 'o@@', '.', '</s>']
2025-05-29 23:28:11,303 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:28:11,303 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:28:11,303 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, è la crisi di Egas che la crisi di nostro sistema di cui il nostro sistema di cui si è rivelato di un cicletto.
2025-05-29 23:28:11,303 - INFO - joeynmt.training - Example #3
2025-05-29 23:28:11,304 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:28:11,304 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:28:11,304 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'ei', 'più', 'gran@@', 'de@@', ',', 'e', 'la', 'gente', 'si', 'è', 'in', 'S@@', 'om@@', 'om@@', 'om@@', 'om@@', 'om@@', 'an@@', 'o@@', '.', '</s>']
2025-05-29 23:28:11,304 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:28:11,304 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:28:11,304 - INFO - joeynmt.training - 	Hypothesis: Sei più grande, e la gente si è in Somomomomomano.
2025-05-29 23:28:11,304 - INFO - joeynmt.training - Example #4
2025-05-29 23:28:11,304 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:28:11,305 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:28:11,305 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'è', 'un', 'progetto', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 'progetto', 'di', 'un', 'di@@', 'seg@@', 'no', 'di', 'un', 'di@@', 'seg@@', 'no', 'di', 'circa', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:28:11,305 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:28:11,305 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:28:11,305 - INFO - joeynmt.training - 	Hypothesis: La prossima è un progetto che vi mostrerò è un progetto di un disegno di un disegno di circa 25 anni.
2025-05-29 23:28:14,793 - INFO - joeynmt.training - Epoch   2, Step:     9600, Batch Loss:     2.310744, Batch Acc: 0.348308, Tokens per Sec:    17495, Lr: 0.000300
2025-05-29 23:28:18,306 - INFO - joeynmt.training - Epoch   2, Step:     9700, Batch Loss:     2.373966, Batch Acc: 0.351367, Tokens per Sec:    19565, Lr: 0.000300
2025-05-29 23:28:21,776 - INFO - joeynmt.training - Epoch   2, Step:     9800, Batch Loss:     2.551581, Batch Acc: 0.345396, Tokens per Sec:    19671, Lr: 0.000300
2025-05-29 23:28:25,262 - INFO - joeynmt.training - Epoch   2, Step:     9900, Batch Loss:     2.453607, Batch Acc: 0.352240, Tokens per Sec:    20083, Lr: 0.000300
2025-05-29 23:28:28,760 - INFO - joeynmt.training - Epoch   2, Step:    10000, Batch Loss:     2.518662, Batch Acc: 0.353524, Tokens per Sec:    19727, Lr: 0.000300
2025-05-29 23:28:28,760 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:28:28,761 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:28:37,590 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.37, ppl:  10.75, acc:   0.36, generation: 8.8226[sec], evaluation: 0.0000[sec]
2025-05-29 23:28:37,591 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:28:38,138 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/7500.ckpt
2025-05-29 23:28:38,163 - INFO - joeynmt.training - Example #0
2025-05-29 23:28:38,163 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:28:38,163 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:28:38,164 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'anno', 'anno', 'anno', 'anno', 'questi', 'due', 'anni', 'per', 'far@@', 'l@@', 'o@@', ',', 'e', 'i', 'primi', 'primi', 'anni', 'che', 'la', 'ri@@', 'stor@@', 'ante', 'di', 'm@@', 'e@@', ',', 'per', 'i', 'tre', 'milioni', 'di', 'anni', 'e', 'tre', 'milioni', 'di', 'anni', 'e', 'tre', 'milioni', 'di', 'anni', '<unk>', '6@@', '0@@', ',', '8@@', '0@@', ',', 'per', 'il', '5@@', '0@@', '%', 'di', 'circa', '4@@', '00', 'milioni', 'di', 'anni', 'per', 'l@@', '<unk>', 'op@@', 'pi@@', 'a@@', '.', '</s>']
2025-05-29 23:28:38,164 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:28:38,164 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:28:38,164 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno anno anno anno anno questi due anni per farlo, e i primi primi anni che la ristorante di me, per i tre milioni di anni e tre milioni di anni e tre milioni di anni <unk> 60, 80, per il 50% di circa 400 milioni di anni per l<unk> oppia.
2025-05-29 23:28:38,164 - INFO - joeynmt.training - Example #1
2025-05-29 23:28:38,164 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:28:38,164 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:28:38,164 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'più', 'facile', 'da', 'sol@@', 'di@@', ',', 'e', 'non', 'è', 'molto', 'semplic@@', 'e@@', ',', 'non', 'è', 'la', 'stessa', 'cos@@', 'a@@', ',', 'non', 'è', 'la', 'stessa', 'cos@@', 'a@@', '.', '</s>']
2025-05-29 23:28:38,165 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:28:38,165 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:28:38,165 - INFO - joeynmt.training - 	Hypothesis: Ma non è più facile da soldi, e non è molto semplice, non è la stessa cosa, non è la stessa cosa.
2025-05-29 23:28:38,165 - INFO - joeynmt.training - Example #2
2025-05-29 23:28:38,165 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:28:38,165 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:28:38,165 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'è', 'l@@', '<unk>', 'ar@@', 'te@@', ',', 'il', 'suo', 'sistema', 'san@@', 'it@@', 'ario', 'del', 'nostro', 'sistema', 'san@@', 'it@@', 'ario', 'del', 'nostro', 'sistema', 'san@@', 'it@@', 'ari@@', 'o@@', '.', '</s>']
2025-05-29 23:28:38,166 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:28:38,166 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:28:38,166 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, è l<unk> arte, il suo sistema sanitario del nostro sistema sanitario del nostro sistema sanitario.
2025-05-29 23:28:38,166 - INFO - joeynmt.training - Example #3
2025-05-29 23:28:38,166 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:28:38,166 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:28:38,166 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'la', 'cosa', 'più', 'grande', 'e', 'di', 'tutto', 'il', 't@@', 'umore', 'in', 't@@', 'oc@@', 'ca@@', '.', '</s>']
2025-05-29 23:28:38,166 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:28:38,167 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:28:38,167 - INFO - joeynmt.training - 	Hypothesis: E la cosa più grande e di tutto il tumore in tocca.
2025-05-29 23:28:38,167 - INFO - joeynmt.training - Example #4
2025-05-29 23:28:38,167 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:28:38,167 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:28:38,167 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'è', 'un', 'progetto', 'che', 'vi', 'mostr@@', 'erò', 'un', 'di@@', 'pin@@', 'to', 'di', 'ciò', 'che', 'è', 'successo', 'in', 'questo', 'mod@@', 'o@@', '.', '</s>']
2025-05-29 23:28:38,167 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:28:38,167 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:28:38,167 - INFO - joeynmt.training - 	Hypothesis: La prossima è un progetto che vi mostrerò un dipinto di ciò che è successo in questo modo.
2025-05-29 23:28:41,557 - INFO - joeynmt.training - Epoch   2, Step:    10100, Batch Loss:     2.274007, Batch Acc: 0.351669, Tokens per Sec:    17136, Lr: 0.000300
2025-05-29 23:28:44,924 - INFO - joeynmt.training - Epoch   2, Step:    10200, Batch Loss:     2.359529, Batch Acc: 0.351081, Tokens per Sec:    20742, Lr: 0.000300
2025-05-29 23:28:48,254 - INFO - joeynmt.training - Epoch   2, Step:    10300, Batch Loss:     2.474549, Batch Acc: 0.353770, Tokens per Sec:    20689, Lr: 0.000300
2025-05-29 23:28:51,573 - INFO - joeynmt.training - Epoch   2, Step:    10400, Batch Loss:     2.352953, Batch Acc: 0.361428, Tokens per Sec:    20274, Lr: 0.000300
2025-05-29 23:28:54,913 - INFO - joeynmt.training - Epoch   2, Step:    10500, Batch Loss:     2.277613, Batch Acc: 0.360009, Tokens per Sec:    21559, Lr: 0.000300
2025-05-29 23:28:54,914 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:28:54,914 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:29:03,182 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.35, ppl:  10.50, acc:   0.37, generation: 8.2567[sec], evaluation: 0.0000[sec]
2025-05-29 23:29:03,183 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:29:03,723 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/8000.ckpt
2025-05-29 23:29:03,749 - INFO - joeynmt.training - Example #0
2025-05-29 23:29:03,749 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:29:03,749 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:29:03,749 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'fatto', 'che', 'la', 'gente', 'si', 'è', 'ri@@', 'ma@@', 'sto', 'per', 'ri@@', 'durre', 'il', '5@@', '0@@', '0@@', '%', 'di', 'cui', 'il', 'di@@', 'str@@', 'etto', 'di', 'tre', 'milioni', 'di', 'anni', 'che', 'hanno', 'fatto', 'per', 'la', 'ri@@', 'du@@', 'zione', 'di', 'tre', 'milioni', 'di', 'anni', 'fa@@', ',', '4@@', '8', 'an@@', 'ni@@', ',', '4@@', '8', 'an@@', 'ni@@', ',', 'circa', '4@@', '8', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:29:03,750 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:29:03,750 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:29:03,750 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho fatto che la gente si è rimasto per ridurre il 500% di cui il distretto di tre milioni di anni che hanno fatto per la riduzione di tre milioni di anni fa, 48 anni, 48 anni, circa 48 anni.
2025-05-29 23:29:03,750 - INFO - joeynmt.training - Example #1
2025-05-29 23:29:03,750 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:29:03,750 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:29:03,750 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'stato', 'un', 'problema', 'di', 'più', 'grande', 'della', 'nostra', 'vit@@', 'a@@', ',', 'non', 'è', 'la', 'stessa', 'cos@@', 'a@@', ',', 'non', 'è', 'la', 'stessa', 'cos@@', 'a@@', '.', '</s>']
2025-05-29 23:29:03,751 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:29:03,751 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:29:03,751 - INFO - joeynmt.training - 	Hypothesis: Ma non è stato un problema di più grande della nostra vita, non è la stessa cosa, non è la stessa cosa.
2025-05-29 23:29:03,751 - INFO - joeynmt.training - Example #2
2025-05-29 23:29:03,751 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:29:03,751 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:29:03,751 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'è', 'la', 'c@@', 'ris@@', 'i', 'e', 'la', 'ri@@', 'fer@@', 'ita', 'di', 'c@@', 'att@@', 'ur@@', 'are', 'il', 'nostro', 'sistema', 'san@@', 'it@@', 'ario', 'del', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:29:03,752 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:29:03,752 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:29:03,752 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, è la crisi e la riferita di catturare il nostro sistema sanitario del nostro sistema globale.
2025-05-29 23:29:03,752 - INFO - joeynmt.training - Example #3
2025-05-29 23:29:03,752 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:29:03,752 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:29:03,752 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'il', 'giorno', 'in', 'cui', 'si', 'è', 'ri@@', 'ma@@', 'sto', 'in', 'S@@', 'ig@@', 'no@@', '.', '</s>']
2025-05-29 23:29:03,752 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:29:03,752 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:29:03,753 - INFO - joeynmt.training - 	Hypothesis: Ma il giorno in cui si è rimasto in Signo.
2025-05-29 23:29:03,753 - INFO - joeynmt.training - Example #4
2025-05-29 23:29:03,753 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:29:03,753 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:29:03,753 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'è', 'un', 'progetto', 'che', 'vi', 'mostr@@', 'o', 'un', 'progetto', 'di', 'di@@', 'f@@', 'esa', 'di', 'quello', 'che', 'è', 'successo', 'nel', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:29:03,753 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:29:03,753 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:29:03,753 - INFO - joeynmt.training - 	Hypothesis: La prossima è un progetto che vi mostro un progetto di difesa di quello che è successo nel 25 anni.
2025-05-29 23:29:07,148 - INFO - joeynmt.training - Epoch   2, Step:    10600, Batch Loss:     2.290755, Batch Acc: 0.361871, Tokens per Sec:    17117, Lr: 0.000300
2025-05-29 23:29:10,493 - INFO - joeynmt.training - Epoch   2, Step:    10700, Batch Loss:     2.281665, Batch Acc: 0.359606, Tokens per Sec:    20176, Lr: 0.000300
2025-05-29 23:29:13,841 - INFO - joeynmt.training - Epoch   2, Step:    10800, Batch Loss:     2.817798, Batch Acc: 0.358224, Tokens per Sec:    20503, Lr: 0.000300
2025-05-29 23:29:17,173 - INFO - joeynmt.training - Epoch   2, Step:    10900, Batch Loss:     2.224534, Batch Acc: 0.362822, Tokens per Sec:    20692, Lr: 0.000300
2025-05-29 23:29:20,470 - INFO - joeynmt.training - Epoch   2, Step:    11000, Batch Loss:     2.435806, Batch Acc: 0.360232, Tokens per Sec:    20678, Lr: 0.000300
2025-05-29 23:29:20,470 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:29:20,470 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:29:28,384 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.33, ppl:  10.29, acc:   0.37, generation: 7.9033[sec], evaluation: 0.0000[sec]
2025-05-29 23:29:28,385 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:29:28,906 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/8500.ckpt
2025-05-29 23:29:28,931 - INFO - joeynmt.training - Example #0
2025-05-29 23:29:28,931 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:29:28,931 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:29:28,932 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'ho', 'fatto', 'questa', 'idea', 'di', 'fare', 'queste', 'due', 'due', 'due', 'an@@', 'ni@@', ',', 'che', 'la', 'gente', 'è', 'stato', 'un', 'anno', 'per', 'fare', 'la', 'c@@', 'lasse', 'di', 'un', 'anno', 'per', 'la', 'c@@', 'lasse', 'di', 'un', 'altro', 'di', '2@@', '8', 'an@@', 'ni@@', ',', 'per', 'la', 'maggior', 'parte', 'dei', '4@@', '8', 'an@@', 'ni@@', ',', 'per', 'il', '4@@', '8', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:29:28,932 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:29:28,932 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:29:28,932 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno ho fatto questa idea di fare queste due due due anni, che la gente è stato un anno per fare la classe di un anno per la classe di un altro di 28 anni, per la maggior parte dei 48 anni, per il 48 anni.
2025-05-29 23:29:28,932 - INFO - joeynmt.training - Example #1
2025-05-29 23:29:28,932 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:29:28,932 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:29:28,932 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'un', 'p@@', 'ò', 'di', 'pi@@', 'ù@@', ',', 'ma', 'non', 'è', 'la', 'cosa', 'che', 'non', 'è', 'la', 'cosa', 'che', 'non', 'è', 'la', 'stessa', 'cos@@', 'a@@', '.', '</s>']
2025-05-29 23:29:28,933 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:29:28,933 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:29:28,933 - INFO - joeynmt.training - 	Hypothesis: Ma non è un pò di più, ma non è la cosa che non è la cosa che non è la stessa cosa.
2025-05-29 23:29:28,933 - INFO - joeynmt.training - Example #2
2025-05-29 23:29:28,933 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:29:28,933 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:29:28,933 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'è', 'la', 'cosa', 'che', 'è', 'l@@', '<unk>', 'eff@@', 'etto', 'di', 'un', 'p@@', 'ap@@', 'o', 'di', 'un', 'p@@', 'ac@@', 'co', 'di', 'lavoro', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:29:28,934 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:29:28,934 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:29:28,934 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, è la cosa che è l<unk> effetto di un papo di un pacco di lavoro globale.
2025-05-29 23:29:28,934 - INFO - joeynmt.training - Example #3
2025-05-29 23:29:28,934 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:29:28,934 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:29:28,934 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'cosa', 'che', 'è', 'successo', 'nel', 's@@', 'ettore', 'di', 'un', 'p@@', 'om@@', 'p@@', 'a', 'di', 's@@', 'om@@', 'p@@', 'a', 'di', 's@@', 'om@@', 'p@@', 'a', 'di', 'un', 'p@@', 'ap@@', 'or@@', 'o@@', '.', '</s>']
2025-05-29 23:29:28,934 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:29:28,934 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:29:28,934 - INFO - joeynmt.training - 	Hypothesis: La prima cosa che è successo nel settore di un pompa di sompa di sompa di un paporo.
2025-05-29 23:29:28,935 - INFO - joeynmt.training - Example #4
2025-05-29 23:29:28,935 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:29:28,935 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:29:28,935 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'cosa', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'cosa', 'che', 'è', 'successo', 'in', 'questo', 'momento', 'in', 'cui', 'è', 'successo', 'in', 'questo', 'momento', 'in', 'cui', 'è', 'successo', 'in', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:29:28,935 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:29:28,935 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:29:28,935 - INFO - joeynmt.training - 	Hypothesis: La prossima cosa che vi mostrerò è una cosa che è successo in questo momento in cui è successo in questo momento in cui è successo in 25 anni.
2025-05-29 23:29:32,414 - INFO - joeynmt.training - Epoch   2, Step:    11100, Batch Loss:     2.303536, Batch Acc: 0.367328, Tokens per Sec:    17033, Lr: 0.000300
2025-05-29 23:29:35,859 - INFO - joeynmt.training - Epoch   2, Step:    11200, Batch Loss:     2.385305, Batch Acc: 0.363914, Tokens per Sec:    20152, Lr: 0.000300
2025-05-29 23:29:39,301 - INFO - joeynmt.training - Epoch   2, Step:    11300, Batch Loss:     2.620552, Batch Acc: 0.366963, Tokens per Sec:    19797, Lr: 0.000300
2025-05-29 23:29:42,737 - INFO - joeynmt.training - Epoch   2, Step:    11400, Batch Loss:     2.547207, Batch Acc: 0.362569, Tokens per Sec:    19997, Lr: 0.000300
2025-05-29 23:29:46,158 - INFO - joeynmt.training - Epoch   2, Step:    11500, Batch Loss:     2.411351, Batch Acc: 0.365420, Tokens per Sec:    19751, Lr: 0.000300
2025-05-29 23:29:46,158 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:29:46,158 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:29:55,653 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.30, ppl:   9.93, acc:   0.38, generation: 9.4823[sec], evaluation: 0.0000[sec]
2025-05-29 23:29:55,654 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:29:56,262 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/9000.ckpt
2025-05-29 23:29:56,287 - INFO - joeynmt.training - Example #0
2025-05-29 23:29:56,287 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:29:56,287 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:29:56,287 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'due', 'due', 'due', 'di', 'cui', 'si', 'è', 'mostr@@', 'are', 'la', 'l@@', '<unk>', 'et@@', 'ica', 'che', 'la', 'c@@', 'av@@', 'allo', 'di', 'tre', 'milioni', 'di', 'anni', 'di', 'c@@', 'att@@', 'ur@@', 'azione', 'di', 'tre', 'milioni', 'di', 'anni', 'di', 'persone', 'che', 'hanno', 'portato', 'a', '40', 'milioni', 'di', 'anni', 'fa@@', ',', 'per', 'il', '40', 'per', 'cento', 'di', 'un', 'po@@', '<unk>', 'di', '4@@', '8', 'milioni', 'di', 'anni', 'fa@@', ',', 'e', 'abbiamo', 'mostr@@', 'ato', 'il', 'suo', 'ann@@', 'o@@', '.', '</s>']
2025-05-29 23:29:56,288 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:29:56,288 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:29:56,288 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno ho mostrato queste due due due due di cui si è mostrare la l<unk> etica che la cavallo di tre milioni di anni di catturazione di tre milioni di anni di persone che hanno portato a 40 milioni di anni fa, per il 40 per cento di un po<unk> di 48 milioni di anni fa, e abbiamo mostrato il suo anno.
2025-05-29 23:29:56,288 - INFO - joeynmt.training - Example #1
2025-05-29 23:29:56,288 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:29:56,289 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:29:56,289 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'più', 'la', 'nostra', 'idea', 'di', 'questa', 'speci@@', 'ale', 'di', 'questa', 'speci@@', 'e@@', ',', 'non', 'è', 'la', 'l@@', '<unk>', 'E@@', '<unk>', 'la', 'D@@', 'ic@@', 'ic@@', 'l@@', 'is@@', 'c@@', 'e@@', '.', '</s>']
2025-05-29 23:29:56,289 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:29:56,289 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:29:56,289 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto più la nostra idea di questa speciale di questa specie, non è la l<unk> E<unk> la Diciclisce.
2025-05-29 23:29:56,289 - INFO - joeynmt.training - Example #2
2025-05-29 23:29:56,289 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:29:56,289 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:29:56,290 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'è', 'la', 'più', 'grande', 'c@@', 'op@@', 'pia', 'di', 'un', 'p@@', 'all@@', 'e@@', 'x', 'del', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', ',', 'il', 'P@@', 'ak@@', 'ist@@', 'an@@', '.', '</s>']
2025-05-29 23:29:56,290 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:29:56,290 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:29:56,290 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, è la più grande coppia di un pallex del nostro sistema globale, il Pakistan.
2025-05-29 23:29:56,290 - INFO - joeynmt.training - Example #3
2025-05-29 23:29:56,290 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:29:56,290 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:29:56,290 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'ei', 'più', 'grandi', 'e', 'di', 'un', 'po@@', '<unk>', 'di', 'di', 's@@', 'om@@', 'p@@', 'are', 'in', 'S@@', 'om@@', '.', '</s>']
2025-05-29 23:29:56,291 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:29:56,291 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:29:56,291 - INFO - joeynmt.training - 	Hypothesis: Sei più grandi e di un po<unk> di di sompare in Som.
2025-05-29 23:29:56,291 - INFO - joeynmt.training - Example #4
2025-05-29 23:29:56,291 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:29:56,291 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:29:56,291 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'è', 'un', 'progetto', 'che', 'vi', 'mostr@@', 'erò', 'un', 'di@@', 'seg@@', 'no', 'di', 'un', 'di@@', 'str@@', 'etto', 'in', '25', 'anni', 'fa@@', '.', '</s>']
2025-05-29 23:29:56,291 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:29:56,291 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:29:56,292 - INFO - joeynmt.training - 	Hypothesis: La prossima è un progetto che vi mostrerò un disegno di un distretto in 25 anni fa.
2025-05-29 23:29:59,796 - INFO - joeynmt.training - Epoch   2, Step:    11600, Batch Loss:     2.411710, Batch Acc: 0.364066, Tokens per Sec:    17119, Lr: 0.000300
2025-05-29 23:30:03,249 - INFO - joeynmt.training - Epoch   2, Step:    11700, Batch Loss:     2.370093, Batch Acc: 0.370157, Tokens per Sec:    19690, Lr: 0.000300
2025-05-29 23:30:06,689 - INFO - joeynmt.training - Epoch   2, Step:    11800, Batch Loss:     2.289894, Batch Acc: 0.364781, Tokens per Sec:    19809, Lr: 0.000300
2025-05-29 23:30:10,145 - INFO - joeynmt.training - Epoch   2, Step:    11900, Batch Loss:     2.284029, Batch Acc: 0.371616, Tokens per Sec:    19842, Lr: 0.000300
2025-05-29 23:30:13,589 - INFO - joeynmt.training - Epoch   2, Step:    12000, Batch Loss:     2.300500, Batch Acc: 0.371508, Tokens per Sec:    19752, Lr: 0.000300
2025-05-29 23:30:13,590 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:30:13,590 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:30:22,070 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.28, ppl:   9.82, acc:   0.38, generation: 8.4681[sec], evaluation: 0.0000[sec]
2025-05-29 23:30:22,071 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:30:22,693 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/9500.ckpt
2025-05-29 23:30:22,720 - INFO - joeynmt.training - Example #0
2025-05-29 23:30:22,721 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:30:22,721 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:30:22,721 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'ho', 'usato', 'questi', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'anni', 'per', 'fare', 'per', 'fare', 'che', 'i', 'ri@@', 'vel@@', 'ano', 'che', 'i', 'sol@@', 'i', 'tre', 'anni', 'di', 'c@@', 'att@@', 'ur@@', 'are', 'tre', 'milioni', 'di', 'anni', 'di', 'ri@@', 'durre', 'il', '5@@', '8', 'milioni', 'di', 'anni', 'per', 'il', '5@@', '8', 'per', 'cento', 'di', 'questo', 'tipo', 'di', 'el@@', 'ef@@', 'ante', 'per', 'il', '5@@', '0@@', '0@@', '.', '</s>']
2025-05-29 23:30:22,722 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:30:22,722 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:30:22,722 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno ho usato questi due due due due due due due due due anni per fare per fare che i rivelano che i soli tre anni di catturare tre milioni di anni di ridurre il 58 milioni di anni per il 58 per cento di questo tipo di elefante per il 500.
2025-05-29 23:30:22,722 - INFO - joeynmt.training - Example #1
2025-05-29 23:30:22,722 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:30:22,722 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:30:22,723 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'il', 'modo', 'di', 'essere', 'la', 'stessa', 'cos@@', 'a@@', ',', 'non', 'è', 'la', 'stessa', 'cos@@', 'a@@', ',', 'non', 'è', 'il', 'problema', 'che', 'non', 'è', 'il', 't@@', 'asso', 'di', 'ri@@', 'vel@@', 'o@@', '.', '</s>']
2025-05-29 23:30:22,723 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:30:22,723 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:30:22,723 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è il modo di essere la stessa cosa, non è la stessa cosa, non è il problema che non è il tasso di rivelo.
2025-05-29 23:30:22,723 - INFO - joeynmt.training - Example #2
2025-05-29 23:30:22,723 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:30:22,723 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:30:22,724 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'cosa', 'più', 'grande', 'c@@', 'ris@@', 'i', 'del', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', ',', 'il', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:30:22,724 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:30:22,724 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:30:22,724 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cosa più grande crisi del nostro sistema globale, il nostro sistema globale.
2025-05-29 23:30:22,724 - INFO - joeynmt.training - Example #3
2025-05-29 23:30:22,724 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:30:22,724 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:30:22,724 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'cosa', 'che', 'la', 'prima', 'cosa', 'del', 's@@', 'ettore', 'e', 'la', 'sua', 'vita', 'in', 'S@@', 'om@@', 'en@@', '.', '</s>']
2025-05-29 23:30:22,725 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:30:22,725 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:30:22,725 - INFO - joeynmt.training - 	Hypothesis: La prima cosa che la prima cosa del settore e la sua vita in Somen.
2025-05-29 23:30:22,725 - INFO - joeynmt.training - Example #4
2025-05-29 23:30:22,725 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:30:22,725 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:30:22,725 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'cosa', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 'seg@@', 'no', 'di', 'cui', 'è', 'stato', 'un', 'seg@@', 'no', 'di', 'cui', 'è', 'successo', 'in', 'questo', 'mod@@', 'o@@', '.', '</s>']
2025-05-29 23:30:22,726 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:30:22,726 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:30:22,726 - INFO - joeynmt.training - 	Hypothesis: La prossima cosa che vi mostrerò è un segno di cui è stato un segno di cui è successo in questo modo.
2025-05-29 23:30:26,197 - INFO - joeynmt.training - Epoch   2, Step:    12100, Batch Loss:     2.235956, Batch Acc: 0.369422, Tokens per Sec:    16369, Lr: 0.000300
2025-05-29 23:30:29,657 - INFO - joeynmt.training - Epoch   2, Step:    12200, Batch Loss:     2.248056, Batch Acc: 0.371801, Tokens per Sec:    19777, Lr: 0.000300
2025-05-29 23:30:33,106 - INFO - joeynmt.training - Epoch   2, Step:    12300, Batch Loss:     2.237208, Batch Acc: 0.374223, Tokens per Sec:    20197, Lr: 0.000300
2025-05-29 23:30:36,534 - INFO - joeynmt.training - Epoch   2, Step:    12400, Batch Loss:     2.289942, Batch Acc: 0.374084, Tokens per Sec:    19558, Lr: 0.000300
2025-05-29 23:30:39,972 - INFO - joeynmt.training - Epoch   2, Step:    12500, Batch Loss:     2.299764, Batch Acc: 0.373342, Tokens per Sec:    19388, Lr: 0.000300
2025-05-29 23:30:39,973 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:30:39,973 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:30:49,019 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.25, ppl:   9.53, acc:   0.39, generation: 9.0353[sec], evaluation: 0.0000[sec]
2025-05-29 23:30:49,020 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:30:49,629 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/10000.ckpt
2025-05-29 23:30:49,648 - INFO - joeynmt.training - Example #0
2025-05-29 23:30:49,649 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:30:49,649 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:30:49,649 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'ho', 'mostr@@', 'ato', 'questi', 'due', 'anni', 'ho', 'mostr@@', 'ato', 'questi', 'due', 'anni', 'per', 'ri@@', 'durre', 'il', '4@@', '8', 'milioni', 'di', 'anni', 'di', 'c@@', 'att@@', 'ur@@', 'are', 'che', 'l@@', '<unk>', 'E@@', 'E@@', 'E@@', '<unk>', 'il', '4@@', '8', 'milioni', 'di', 'anni', 'di', 'ri@@', 'durre', 'il', '40', 'milioni', 'di', 'anni', 'per', 'la', 'ri@@', 'du@@', 'zione', 'di', 'circa', '8@@', '0@@', '.', '</s>']
2025-05-29 23:30:49,650 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:30:49,650 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:30:49,650 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno ho mostrato questi due anni ho mostrato questi due anni per ridurre il 48 milioni di anni di catturare che l<unk> EEE<unk> il 48 milioni di anni di ridurre il 40 milioni di anni per la riduzione di circa 80.
2025-05-29 23:30:49,650 - INFO - joeynmt.training - Example #1
2025-05-29 23:30:49,650 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:30:49,650 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:30:49,650 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'il', 'fatto', 'che', 'la', 'cosa', 'più', 'importante', 'per', 'la', 'nostra', 'in@@', 'fer@@', 'mat@@', 'a@@', ',', 'non', 'è', 'la', 'cosa', 'speci@@', 'fica', 'che', 'non', 'è', 'il', 'D@@', 'ic@@', 'ic@@', 'o@@', '.', '</s>']
2025-05-29 23:30:49,651 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:30:49,651 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:30:49,651 - INFO - joeynmt.training - 	Hypothesis: Ma non è il fatto che la cosa più importante per la nostra infermata, non è la cosa specifica che non è il Dicico.
2025-05-29 23:30:49,651 - INFO - joeynmt.training - Example #2
2025-05-29 23:30:49,651 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:30:49,651 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:30:49,651 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'è', 'la', 'cosa', 'è', 'la', 'cosa', 'più', 'grande', 'della', 'nostra', 'c@@', 'ris@@', 'i', 'del', 'nostro', 'c@@', 'ic@@', 'l@@', 'ett@@', 'o@@', '.', '</s>']
2025-05-29 23:30:49,652 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:30:49,652 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:30:49,652 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, è la cosa è la cosa più grande della nostra crisi del nostro cicletto.
2025-05-29 23:30:49,652 - INFO - joeynmt.training - Example #3
2025-05-29 23:30:49,652 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:30:49,652 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:30:49,652 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'che', 'è', 'la', 'cosa', 'in', 'cui', 'si', 'è', 'ri@@', 'ma@@', 'sto', 'in', 'S@@', 'om@@', 'en@@', 'en@@', 'e@@', '.', '</s>']
2025-05-29 23:30:49,652 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:30:49,653 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:30:49,653 - INFO - joeynmt.training - 	Hypothesis: Il che è la cosa in cui si è rimasto in Somenene.
2025-05-29 23:30:49,653 - INFO - joeynmt.training - Example #4
2025-05-29 23:30:49,653 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:30:49,653 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:30:49,653 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'prossi@@', 'mo', 'è', 'un', 'di@@', 'seg@@', 'no', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'in', '25', 'anni', 'fa@@', '.', '</s>']
2025-05-29 23:30:49,653 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:30:49,653 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:30:49,654 - INFO - joeynmt.training - 	Hypothesis: Il prossimo è un disegno che vi mostro è un disegno di quello che è successo in 25 anni fa.
2025-05-29 23:30:53,125 - INFO - joeynmt.training - Epoch   2, Step:    12600, Batch Loss:     2.443746, Batch Acc: 0.373966, Tokens per Sec:    16464, Lr: 0.000300
2025-05-29 23:30:56,576 - INFO - joeynmt.training - Epoch   2, Step:    12700, Batch Loss:     2.325727, Batch Acc: 0.373061, Tokens per Sec:    19272, Lr: 0.000300
2025-05-29 23:31:00,045 - INFO - joeynmt.training - Epoch   2, Step:    12800, Batch Loss:     2.458668, Batch Acc: 0.375247, Tokens per Sec:    19999, Lr: 0.000300
2025-05-29 23:31:03,491 - INFO - joeynmt.training - Epoch   2, Step:    12900, Batch Loss:     2.353499, Batch Acc: 0.376465, Tokens per Sec:    20328, Lr: 0.000300
2025-05-29 23:31:06,918 - INFO - joeynmt.training - Epoch   2, Step:    13000, Batch Loss:     2.219310, Batch Acc: 0.377470, Tokens per Sec:    20033, Lr: 0.000300
2025-05-29 23:31:06,918 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:31:06,919 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:31:16,849 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.23, ppl:   9.32, acc:   0.39, generation: 9.9173[sec], evaluation: 0.0000[sec]
2025-05-29 23:31:16,850 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:31:17,473 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/10500.ckpt
2025-05-29 23:31:17,500 - INFO - joeynmt.training - Example #0
2025-05-29 23:31:17,501 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:31:17,501 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:31:17,501 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'ho', 'mostr@@', 'ato', 'questi', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'c@@', 'lin@@', 'ee', 'che', 'la', 'ri@@', 'forma', 'di', 'el@@', 'im@@', 'in@@', 'are', 'per', 'la', 'c@@', 'att@@', 'ur@@', 'azione', 'di', 'circa', '4@@', '8', 'milioni', 'di', 'anni', 'di', 'con@@', 'vers@@', 'azione', 'per', 'il', '4@@', '8', 'milioni', 'di', 'anni', 'per', 'il', '4@@', '8', 'milioni', 'di', 'anni', 'fa@@', '.', '</s>']
2025-05-29 23:31:17,502 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:31:17,502 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:31:17,502 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno ho mostrato questi due due due due due due due due due due clinee che la riforma di eliminare per la catturazione di circa 48 milioni di anni di conversazione per il 48 milioni di anni per il 48 milioni di anni fa.
2025-05-29 23:31:17,502 - INFO - joeynmt.training - Example #1
2025-05-29 23:31:17,503 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:31:17,503 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:31:17,503 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'il', 'modo', 'di', 'essere', 'abbastanza', 'per', 'la', 'nostra', 'vit@@', 'a@@', ',', 'non', 'è', 'il', 'tempo', 'che', 'non', 'è', 'il', 'D@@', 'ic@@', 'ke', 'non', 'è', 'il', 'D@@', 'ic@@', 'ke', 'per', 'l@@', '<unk>', 'E@@', 'is@@', 'c@@', 'e@@', '.', '</s>']
2025-05-29 23:31:17,503 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:31:17,503 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:31:17,503 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è il modo di essere abbastanza per la nostra vita, non è il tempo che non è il Dicke non è il Dicke per l<unk> Eisce.
2025-05-29 23:31:17,504 - INFO - joeynmt.training - Example #2
2025-05-29 23:31:17,504 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:31:17,504 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:31:17,504 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'è', 'il', 'più', 'grande', 'd@@', 'ell@@', '<unk>', 'E@@', '<unk>', 'il', 'cuore', 'del', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', ',', 'il', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:31:17,504 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:31:17,504 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:31:17,504 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, è il più grande dell<unk> E<unk> il cuore del nostro sistema globale, il nostro sistema globale.
2025-05-29 23:31:17,505 - INFO - joeynmt.training - Example #3
2025-05-29 23:31:17,505 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:31:17,505 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:31:17,505 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', 'en@@', 'gono', 'in', 'una', 'b@@', 'ella', 'e', 'la', 'sua', 'f@@', 'em@@', 'm@@', 'ina', 'e', 'la', 'sua', 'f@@', 'om@@', 'ma@@', '.', '</s>']
2025-05-29 23:31:17,505 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:31:17,505 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:31:17,505 - INFO - joeynmt.training - 	Hypothesis: Vengono in una bella e la sua femmina e la sua fomma.
2025-05-29 23:31:17,505 - INFO - joeynmt.training - Example #4
2025-05-29 23:31:17,506 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:31:17,506 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:31:17,506 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'di@@', 'ment@@', 'e@@', ',', 'il', 'primo', 'di@@', 'seg@@', 'no', 'è', 'un', 'seg@@', 'no', 'di', 'tempo', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:31:17,506 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:31:17,506 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:31:17,506 - INFO - joeynmt.training - 	Hypothesis: La prossima dimente, il primo disegno è un segno di tempo che è successo negli ultimi 25 anni.
2025-05-29 23:31:21,034 - INFO - joeynmt.training - Epoch   2, Step:    13100, Batch Loss:     2.197530, Batch Acc: 0.377888, Tokens per Sec:    16533, Lr: 0.000300
2025-05-29 23:31:24,527 - INFO - joeynmt.training - Epoch   2, Step:    13200, Batch Loss:     2.486897, Batch Acc: 0.383559, Tokens per Sec:    19225, Lr: 0.000300
2025-05-29 23:31:27,985 - INFO - joeynmt.training - Epoch   2, Step:    13300, Batch Loss:     2.213282, Batch Acc: 0.377206, Tokens per Sec:    18990, Lr: 0.000300
2025-05-29 23:31:31,437 - INFO - joeynmt.training - Epoch   2, Step:    13400, Batch Loss:     2.143268, Batch Acc: 0.378602, Tokens per Sec:    19681, Lr: 0.000300
2025-05-29 23:31:34,872 - INFO - joeynmt.training - Epoch   2, Step:    13500, Batch Loss:     2.273840, Batch Acc: 0.376805, Tokens per Sec:    19664, Lr: 0.000300
2025-05-29 23:31:34,872 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:31:34,873 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:31:42,993 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.22, ppl:   9.18, acc:   0.40, generation: 8.1095[sec], evaluation: 0.0000[sec]
2025-05-29 23:31:42,994 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:31:43,568 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/11000.ckpt
2025-05-29 23:31:43,596 - INFO - joeynmt.training - Example #0
2025-05-29 23:31:43,597 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:31:43,597 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:31:43,597 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'questi', 'due', 'due', 'due', 'due', 'anni', 'per', 'far@@', 'l@@', 'o@@', ',', 'che', 'l@@', '<unk>', 'E@@', 'is@@', 'k@@', ',', 'che', 'l@@', '<unk>', 'E@@', 'is@@', 'k@@', ',', 'per', 'la', 'l@@', '<unk>', 'ar@@', 'te@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'che', 'è', 'stato', 'il', '4@@', '8', 'per', 'cento', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:31:43,598 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:31:43,598 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:31:43,598 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato questi due due due due anni per farlo, che l<unk> Eisk, che l<unk> Eisk, per la l<unk> arte, per tre milioni di anni che è stato il 48 per cento anni.
2025-05-29 23:31:43,598 - INFO - joeynmt.training - Example #1
2025-05-29 23:31:43,598 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:31:43,598 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:31:43,598 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'la', 'cosa', 'più', 'grande', 'della', 'nostra', 'speci@@', 'alizz@@', 'azione', 'di', 'questo', 'problema', 'speci@@', 'ale', 'che', 'non', 'è', 'la', 'l@@', '<unk>', 'E@@', 'is@@', 'c@@', 'e@@', 'a@@', ',', 'non', 'l@@', '<unk>', 'E@@', 'is@@', 'c@@', 'e@@', '.', '</s>']
2025-05-29 23:31:43,599 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:31:43,599 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:31:43,599 - INFO - joeynmt.training - 	Hypothesis: Ma non è la cosa più grande della nostra specializzazione di questo problema speciale che non è la l<unk> Eiscea, non l<unk> Eisce.
2025-05-29 23:31:43,599 - INFO - joeynmt.training - Example #2
2025-05-29 23:31:43,599 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:31:43,599 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:31:43,599 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'è', 'la', 'cosa', 'più', 'grande', 'della', 'c@@', 'ris@@', 'i', 'del', 'nostro', 'sistema', 'globale', 'del', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:31:43,600 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:31:43,600 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:31:43,600 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, è la cosa più grande della crisi del nostro sistema globale del nostro sistema globale.
2025-05-29 23:31:43,600 - INFO - joeynmt.training - Example #3
2025-05-29 23:31:43,600 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:31:43,600 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:31:43,600 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'cosa', 'che', 'si', 'è', 'in', 'con@@', 'vers@@', 'azione', 'e', 'la', 'sua', 'fam@@', 'ig@@', 'li@@', 'a@@', '.', '</s>']
2025-05-29 23:31:43,601 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:31:43,601 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:31:43,601 - INFO - joeynmt.training - 	Hypothesis: La prima cosa che si è in conversazione e la sua famiglia.
2025-05-29 23:31:43,601 - INFO - joeynmt.training - Example #4
2025-05-29 23:31:43,601 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:31:43,601 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:31:43,601 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'prossi@@', 'ma', 'di@@', 'segn@@', 'are', 'il', 'di@@', 'seg@@', 'no', 'è', 'una', 'di@@', 'stri@@', 'bu@@', 'zione', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:31:43,602 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:31:43,602 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:31:43,602 - INFO - joeynmt.training - 	Hypothesis: La prossima prossima disegnare il disegno è una distribuzione di quello che è successo negli ultimi 25 anni.
2025-05-29 23:31:47,138 - INFO - joeynmt.training - Epoch   2, Step:    13600, Batch Loss:     2.267833, Batch Acc: 0.381928, Tokens per Sec:    16440, Lr: 0.000300
2025-05-29 23:31:50,641 - INFO - joeynmt.training - Epoch   2, Step:    13700, Batch Loss:     2.199787, Batch Acc: 0.382642, Tokens per Sec:    19509, Lr: 0.000300
2025-05-29 23:31:54,074 - INFO - joeynmt.training - Epoch   2, Step:    13800, Batch Loss:     2.379667, Batch Acc: 0.382126, Tokens per Sec:    19924, Lr: 0.000300
2025-05-29 23:31:57,525 - INFO - joeynmt.training - Epoch   2, Step:    13900, Batch Loss:     2.264868, Batch Acc: 0.385190, Tokens per Sec:    20063, Lr: 0.000300
2025-05-29 23:32:00,971 - INFO - joeynmt.training - Epoch   2, Step:    14000, Batch Loss:     2.357137, Batch Acc: 0.384925, Tokens per Sec:    19742, Lr: 0.000300
2025-05-29 23:32:00,972 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:32:00,972 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:32:08,592 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.20, ppl:   9.05, acc:   0.40, generation: 7.6121[sec], evaluation: 0.0000[sec]
2025-05-29 23:32:08,592 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:32:09,091 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/11500.ckpt
2025-05-29 23:32:09,119 - INFO - joeynmt.training - Example #0
2025-05-29 23:32:09,119 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:32:09,120 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:32:09,120 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'ho', 'fatto', 'questa', 'idea', 'di', 'cui', 'ho', 'mostr@@', 'ato', 'questa', 'idea', 'di', 'cui', 'la', 'gente', 'si', 'è', 'ri@@', 'ma@@', 'sto', 'per', 'vedere', 'che', 'la', 'gente', 'che', 'si', 'è', 'ri@@', 'ma@@', 'sto', 'per', 'tre', 'milioni', 'di', 'anni', 'di', '<unk>', '4@@', '8', 'milioni', 'di', 'anni', 'per', 'i', 'li@@', 'velli', 'di', '4@@', '8', 'milioni', 'di', 'anni', 'per', 'la', '4@@', '8', 'per', 'cento', 'per', 'cento', 'di', '4@@', '8', 'per', 'cento', 'per', 'cento', 'di', 'questo', 'ti@@', 'zio', 'è', 'il', '4@@', '8', 'per', 'cento', 'di', 'questo', 'tipo', 'di', 'di@@', 'stru@@', 'g@@', 'gere', 'il', '40', 'per', 'cento', 'di', 'questi', 'due', 'anni', 'fa@@', '.', '</s>']
2025-05-29 23:32:09,120 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:32:09,120 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:32:09,120 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno ho fatto questa idea di cui ho mostrato questa idea di cui la gente si è rimasto per vedere che la gente che si è rimasto per tre milioni di anni di <unk> 48 milioni di anni per i livelli di 48 milioni di anni per la 48 per cento per cento di 48 per cento per cento di questo tizio è il 48 per cento di questo tipo di distruggere il 40 per cento di questi due anni fa.
2025-05-29 23:32:09,120 - INFO - joeynmt.training - Example #1
2025-05-29 23:32:09,120 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:32:09,121 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:32:09,121 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'un', 'problema', 'di', 'in@@', 'fer@@', 'i@@', 'ore', 'di', 'questa', 'speci@@', 'fica', 'è', 'la', 'giu@@', 'st@@', 'izia', 'che', 'non', 'è', 'il', 'D@@', 'ic@@', 'ke', 'non', 'è', 'il', 'b@@', 'us@@', 'in@@', 'ess', 'di', 'E@@', 'is@@', 'mo@@', '.', '</s>']
2025-05-29 23:32:09,121 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:32:09,121 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:32:09,121 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è un problema di inferiore di questa specifica è la giustizia che non è il Dicke non è il business di Eismo.
2025-05-29 23:32:09,121 - INFO - joeynmt.training - Example #2
2025-05-29 23:32:09,121 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:32:09,122 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:32:09,122 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'è', 'il', 'modo', 'in', 'cui', 'il', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:32:09,122 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:32:09,122 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:32:09,122 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, in un certo senso, è il modo in cui il nostro sistema globale.
2025-05-29 23:32:09,122 - INFO - joeynmt.training - Example #3
2025-05-29 23:32:09,122 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:32:09,122 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:32:09,122 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'om@@', 'mer@@', 'à', 'in', 'acqua', 'e', 'la', 'sua', 'mis@@', 'sione', 'e', 'di', 'un', 'po@@', '<unk>', 'di', 'l@@', 'or@@', 'o@@', '.', '</s>']
2025-05-29 23:32:09,123 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:32:09,123 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:32:09,123 - INFO - joeynmt.training - 	Hypothesis: Sommerà in acqua e la sua missione e di un po<unk> di loro.
2025-05-29 23:32:09,123 - INFO - joeynmt.training - Example #4
2025-05-29 23:32:09,123 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:32:09,123 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:32:09,123 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'prima', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'volta', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', 'fa@@', '.', '</s>']
2025-05-29 23:32:09,124 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:32:09,124 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:32:09,124 - INFO - joeynmt.training - 	Hypothesis: La prossima prima che vi mostrerò è una volta che vi mostrerò è un segno di quello che è successo negli ultimi 25 anni fa.
2025-05-29 23:32:12,435 - INFO - joeynmt.training - Epoch   2, Step:    14100, Batch Loss:     2.195989, Batch Acc: 0.389073, Tokens per Sec:    18015, Lr: 0.000300
2025-05-29 23:32:15,733 - INFO - joeynmt.training - Epoch   2, Step:    14200, Batch Loss:     2.309073, Batch Acc: 0.389857, Tokens per Sec:    20533, Lr: 0.000300
2025-05-29 23:32:19,003 - INFO - joeynmt.training - Epoch   2, Step:    14300, Batch Loss:     2.126607, Batch Acc: 0.384140, Tokens per Sec:    20722, Lr: 0.000300
2025-05-29 23:32:22,271 - INFO - joeynmt.training - Epoch   2, Step:    14400, Batch Loss:     2.255326, Batch Acc: 0.385608, Tokens per Sec:    21334, Lr: 0.000300
2025-05-29 23:32:25,547 - INFO - joeynmt.training - Epoch   2, Step:    14500, Batch Loss:     2.281611, Batch Acc: 0.385954, Tokens per Sec:    20993, Lr: 0.000300
2025-05-29 23:32:25,547 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:32:25,547 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:32:34,934 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.18, ppl:   8.86, acc:   0.41, generation: 9.3750[sec], evaluation: 0.0000[sec]
2025-05-29 23:32:34,935 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:32:35,517 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/12000.ckpt
2025-05-29 23:32:35,546 - INFO - joeynmt.training - Example #0
2025-05-29 23:32:35,546 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:32:35,546 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:32:35,546 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'ho', 'mostr@@', 'ato', 'questa', 'due', 'due', 'due', 'milioni', 'di', 'anni', 'per', 'creare', 'che', 'la', 'gente', 'che', 'ha', 'fatto', 'la', 'ri@@', 'du@@', 'zione', 'di', 'una', 'c@@', 'ris@@', 'i', 'per', 'i', 'tre', 'milioni', 'di', 'anni', 'di', '3@@', '0@@', ',', 'tre', 'milioni', 'di', 'anni', 'di', '4@@', '8', 'milioni', 'di', 'anni', 'di', '4@@', '8', 'milioni', 'di', 'anni', 'per', 'il', '4@@', '8', 'per', 'cento', 'di', '3@@', '0@@', '%', 'per', 'cento', 'di', 'questi', 'due', 'anni', 'di', '4@@', '8', 'milioni', 'di', 'dollari', 'per', 'cento', 'di', 'tre', 'milioni', 'di', 'anni', 'di', 'persone', 'che', 'hanno', 'mostr@@', 'ato', 'il', '40', 'per', 'cento', 'di', 'tre', 'milioni', 'di', 'anni', 'di', 'ri@@', 'durre', 'il', '40', 'per', 'cento', 'di']
2025-05-29 23:32:35,547 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:32:35,547 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:32:35,547 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno ho mostrato questa due due due milioni di anni per creare che la gente che ha fatto la riduzione di una crisi per i tre milioni di anni di 30, tre milioni di anni di 48 milioni di anni di 48 milioni di anni per il 48 per cento di 30% per cento di questi due anni di 48 milioni di dollari per cento di tre milioni di anni di persone che hanno mostrato il 40 per cento di tre milioni di anni di ridurre il 40 per cento di
2025-05-29 23:32:35,547 - INFO - joeynmt.training - Example #1
2025-05-29 23:32:35,548 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:32:35,548 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:32:35,548 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'una', 'cosa', 'che', 'è', 'stato', 'abbastanza', 'l@@', '<unk>', 'au@@', 'mento', 'di', 'questo', 'problema', 'di', 'cui', 'non', 'è', 'la', 'di@@', 'stri@@', 'bu@@', 'zione', 'di', 'una', 'b@@', 'ella', 'di', 'E@@', 'is@@', 'ca', 'che', 'il', 'D@@', 'ic@@', 'ke', 'E@@', 'is@@', '.', '</s>']
2025-05-29 23:32:35,548 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:32:35,548 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:32:35,548 - INFO - joeynmt.training - 	Hypothesis: Ma non è una cosa che è stato abbastanza l<unk> aumento di questo problema di cui non è la distribuzione di una bella di Eisca che il Dicke Eis.
2025-05-29 23:32:35,548 - INFO - joeynmt.training - Example #2
2025-05-29 23:32:35,549 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:32:35,549 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:32:35,549 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'qualche', 'mod@@', 'o@@', ',', 'la', 'cosa', 'più', 'grande', 'è', 'la', 'cosa', 'più', 'grande', 'della', 'nostra', 'tecnologia', 'del', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:32:35,549 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:32:35,549 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:32:35,549 - INFO - joeynmt.training - 	Hypothesis: In qualche modo, la cosa più grande è la cosa più grande della nostra tecnologia del nostro sistema globale.
2025-05-29 23:32:35,549 - INFO - joeynmt.training - Example #3
2025-05-29 23:32:35,550 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:32:35,550 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:32:35,550 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'è', 'che', 'il', 'suo', 'inter@@', 'no@@', ',', 'e', 'la', 'ri@@', 'du@@', 'zione', 'di', 's@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:32:35,550 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:32:35,550 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:32:35,550 - INFO - joeynmt.training - 	Hypothesis: La prima è che il suo interno, e la riduzione di sommer.
2025-05-29 23:32:35,550 - INFO - joeynmt.training - Example #4
2025-05-29 23:32:35,550 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:32:35,550 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:32:35,550 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'prossi@@', 'mo', 'prossi@@', 'mo', 'è', 'un', 'seg@@', 'no', 'di', 'un', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:32:35,551 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:32:35,551 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:32:35,551 - INFO - joeynmt.training - 	Hypothesis: Il prossimo prossimo è un segno di un segno di quello che è successo negli ultimi 25 anni.
2025-05-29 23:32:39,010 - INFO - joeynmt.training - Epoch   2, Step:    14600, Batch Loss:     2.130046, Batch Acc: 0.387831, Tokens per Sec:    16604, Lr: 0.000300
2025-05-29 23:32:42,451 - INFO - joeynmt.training - Epoch   2, Step:    14700, Batch Loss:     2.111122, Batch Acc: 0.392688, Tokens per Sec:    20434, Lr: 0.000300
2025-05-29 23:32:45,855 - INFO - joeynmt.training - Epoch   2, Step:    14800, Batch Loss:     2.513541, Batch Acc: 0.388090, Tokens per Sec:    19426, Lr: 0.000300
2025-05-29 23:32:49,259 - INFO - joeynmt.training - Epoch   2, Step:    14900, Batch Loss:     2.114976, Batch Acc: 0.386307, Tokens per Sec:    19963, Lr: 0.000300
2025-05-29 23:32:52,657 - INFO - joeynmt.training - Epoch   2, Step:    15000, Batch Loss:     2.443502, Batch Acc: 0.386918, Tokens per Sec:    19465, Lr: 0.000300
2025-05-29 23:32:52,657 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:32:52,657 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:33:02,215 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.17, ppl:   8.72, acc:   0.41, generation: 9.5454[sec], evaluation: 0.0000[sec]
2025-05-29 23:33:02,216 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:33:02,798 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/12500.ckpt
2025-05-29 23:33:02,828 - INFO - joeynmt.training - Example #0
2025-05-29 23:33:02,828 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:33:02,828 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:33:02,828 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'due', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'la', 'con@@', 'serv@@', 'azione', 'di', 'un', 'l@@', '<unk>', 'E@@', 'is@@', 'k@@', 'i@@', ',', 'che', 'la', 'gente', 'si', 'è', 'ri@@', 'ma@@', 'sto', 'per', 'tre', 'anni', 'di', 'circa', 'tre', 'milioni', 'di', 'anni', 'di', '4@@', '8', 'milioni', 'di', 'anni', 'di', '4@@', '8', 'milioni', 'di', 'anni', 'fa@@', '.', '</s>']
2025-05-29 23:33:02,829 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:33:02,829 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:33:02,829 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due due due diapositive che la conservazione di un l<unk> Eiski, che la gente si è rimasto per tre anni di circa tre milioni di anni di 48 milioni di anni di 48 milioni di anni fa.
2025-05-29 23:33:02,829 - INFO - joeynmt.training - Example #1
2025-05-29 23:33:02,830 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:33:02,830 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:33:02,830 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'un', 'po@@', '<unk>', 'più', 'grande', 'della', 'nostra', 'vit@@', 'a@@', ',', 'che', 'non', 'è', 'la', 'giu@@', 'sta', 'che', 'non', 'è', 'la', 'b@@', 'ella', 'di', 'una', 'b@@', 'ella', 'speci@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:33:02,830 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:33:02,830 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:33:02,830 - INFO - joeynmt.training - 	Hypothesis: Ma non è un po<unk> più grande della nostra vita, che non è la giusta che non è la bella di una bella speciale.
2025-05-29 23:33:02,831 - INFO - joeynmt.training - Example #2
2025-05-29 23:33:02,831 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:33:02,831 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:33:02,831 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'qualche', 'modo', 'c@@', '<unk>', 'è', 'la', 'sin@@', 'cer@@', 'a', 'è', 'la', 'più', 'grande', 'c@@', 'ris@@', 'i', 'del', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:33:02,831 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:33:02,831 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:33:02,831 - INFO - joeynmt.training - 	Hypothesis: In qualche modo c<unk> è la sincera è la più grande crisi del nostro sistema globale.
2025-05-29 23:33:02,832 - INFO - joeynmt.training - Example #3
2025-05-29 23:33:02,832 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:33:02,832 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:33:02,832 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'cosa', 'che', 'si', 'trova', 'nel', 'vent@@', 'o@@', ',', 'e', 'la', 'pi@@', 'att@@', 'a@@', '.', '</s>']
2025-05-29 23:33:02,832 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:33:02,832 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:33:02,832 - INFO - joeynmt.training - 	Hypothesis: La prima cosa che si trova nel vento, e la piatta.
2025-05-29 23:33:02,832 - INFO - joeynmt.training - Example #4
2025-05-29 23:33:02,833 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:33:02,833 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:33:02,833 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'prossi@@', 'mo', 'di@@', 'pin@@', 'to', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 'seg@@', 'no', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', 'fa@@', '.', '</s>']
2025-05-29 23:33:02,833 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:33:02,833 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:33:02,833 - INFO - joeynmt.training - 	Hypothesis: Il prossimo dipinto che vi mostrerò è un segno di ciò che è successo negli ultimi 25 anni fa.
2025-05-29 23:33:06,306 - INFO - joeynmt.training - Epoch   2, Step:    15100, Batch Loss:     2.314896, Batch Acc: 0.394445, Tokens per Sec:    16868, Lr: 0.000300
2025-05-29 23:33:09,713 - INFO - joeynmt.training - Epoch   2, Step:    15200, Batch Loss:     2.455643, Batch Acc: 0.393309, Tokens per Sec:    20016, Lr: 0.000300
2025-05-29 23:33:13,117 - INFO - joeynmt.training - Epoch   2, Step:    15300, Batch Loss:     2.157795, Batch Acc: 0.396509, Tokens per Sec:    20336, Lr: 0.000300
2025-05-29 23:33:16,509 - INFO - joeynmt.training - Epoch   2, Step:    15400, Batch Loss:     2.385933, Batch Acc: 0.392349, Tokens per Sec:    19938, Lr: 0.000300
2025-05-29 23:33:19,914 - INFO - joeynmt.training - Epoch   2, Step:    15500, Batch Loss:     2.167273, Batch Acc: 0.396469, Tokens per Sec:    20178, Lr: 0.000300
2025-05-29 23:33:19,915 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:33:19,915 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:33:29,048 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.15, ppl:   8.56, acc:   0.41, generation: 9.1215[sec], evaluation: 0.0000[sec]
2025-05-29 23:33:29,049 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:33:29,593 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/13000.ckpt
2025-05-29 23:33:29,622 - INFO - joeynmt.training - Example #0
2025-05-29 23:33:29,623 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:33:29,623 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:33:29,623 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'fatto', 'queste', 'due', 'due', 'due', 'due', 'an@@', 'ni@@', ',', 'ho', 'mostr@@', 'ato', 'che', 'la', 'con@@', 'vers@@', 'azione', 'di', 'tre', 'milioni', 'di', 'anni', 'che', 'la', 'Gr@@', 'azi@@', 'e@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'anni', 'di', 'ri@@', 'durre', 'la', 'gr@@', 'ave', 'di', 'circa', '4@@', '8', 'milioni', 'di', 'anni', 'di', 'c@@', 'ant@@', 'are', 'il', '40', 'per', 'cento', 'di', 'anni', 'per', 'il', '40', 'per', 'cento', 'di', 'anni', 'di', '4@@', '8', 'milioni', 'di', 'anni', 'fa@@', '.', '</s>']
2025-05-29 23:33:29,624 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:33:29,624 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:33:29,624 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho fatto queste due due due due anni, ho mostrato che la conversazione di tre milioni di anni che la Grazie, per tre milioni di anni di anni di ridurre la grave di circa 48 milioni di anni di cantare il 40 per cento di anni per il 40 per cento di anni di 48 milioni di anni fa.
2025-05-29 23:33:29,624 - INFO - joeynmt.training - Example #1
2025-05-29 23:33:29,624 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:33:29,624 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:33:29,624 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'più', 'pot@@', 'ente', 'di', 'quanto', 'la', 'più', 'grande', 'c@@', 'av@@', 'ità', 'di', 'questo', 'probl@@', 'em@@', 'a@@', ',', 'non', 'è', 'la', 'di@@', 'eta', 'di', 'E@@', 'is@@', 'c@@', 'e@@', ',', 'non', 'è', 'la', 'dimen@@', 'sione', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-29 23:33:29,625 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:33:29,625 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:33:29,625 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto più potente di quanto la più grande cavità di questo problema, non è la dieta di Eisce, non è la dimensione del ghiaccio.
2025-05-29 23:33:29,625 - INFO - joeynmt.training - Example #2
2025-05-29 23:33:29,625 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:33:29,625 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:33:29,626 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'cosa', 'più', 'grande', 'è', 'la', 'più', 'grande', 'c@@', 'ris@@', 'i', 'glob@@', 'ali', 'del', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:33:29,626 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:33:29,626 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:33:29,626 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cosa più grande è la più grande crisi globali del nostro sistema globale.
2025-05-29 23:33:29,626 - INFO - joeynmt.training - Example #3
2025-05-29 23:33:29,626 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:33:29,626 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:33:29,626 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:33:29,627 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:33:29,627 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:33:29,627 - INFO - joeynmt.training - 	Hypothesis: Sommer.
2025-05-29 23:33:29,627 - INFO - joeynmt.training - Example #4
2025-05-29 23:33:29,627 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:33:29,627 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:33:29,627 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'di@@', 'mostra', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'succ@@', 'ess@@', 'o@@', ',', 'è', 'succ@@', 'ess@@', 'o@@', ',', 'è', 'succ@@', 'ess@@', 'o@@', ',', 'è', 'succ@@', 'ess@@', 'o@@', ',', 'e', 'la', 'cosa', 'che', 'è', 'succ@@', 'ess@@', 'o@@', ',', 'e', 'la', 'prima', 'volta', 'che', 'vi', 'mostr@@', 'i', 'i', 'loro', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:33:29,628 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:33:29,628 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:33:29,628 - INFO - joeynmt.training - 	Hypothesis: La prossima dimostra che vi mostrerò è un disegno di quello che è successo, è successo, è successo, è successo, e la cosa che è successo, e la prima volta che vi mostri i loro anni.
2025-05-29 23:33:33,111 - INFO - joeynmt.training - Epoch   2, Step:    15600, Batch Loss:     2.137760, Batch Acc: 0.389513, Tokens per Sec:    17304, Lr: 0.000300
2025-05-29 23:33:36,536 - INFO - joeynmt.training - Epoch   2, Step:    15700, Batch Loss:     2.231490, Batch Acc: 0.390018, Tokens per Sec:    20652, Lr: 0.000300
2025-05-29 23:33:39,947 - INFO - joeynmt.training - Epoch   2, Step:    15800, Batch Loss:     2.280042, Batch Acc: 0.394788, Tokens per Sec:    20198, Lr: 0.000300
2025-05-29 23:33:43,415 - INFO - joeynmt.training - Epoch   2, Step:    15900, Batch Loss:     2.182354, Batch Acc: 0.400083, Tokens per Sec:    20226, Lr: 0.000300
2025-05-29 23:33:46,894 - INFO - joeynmt.training - Epoch   2, Step:    16000, Batch Loss:     2.248893, Batch Acc: 0.392969, Tokens per Sec:    20026, Lr: 0.000300
2025-05-29 23:33:46,895 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:33:46,895 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:33:55,505 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.13, ppl:   8.44, acc:   0.42, generation: 8.6033[sec], evaluation: 0.0000[sec]
2025-05-29 23:33:55,506 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:33:56,083 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/13500.ckpt
2025-05-29 23:33:56,110 - INFO - joeynmt.training - Example #0
2025-05-29 23:33:56,110 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:33:56,110 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:33:56,110 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'fatto', 'queste', 'due', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'la', 'sua', 'ri@@', 'vista', 'per', 'per', 'la', 'ri@@', 'vista', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'k@@', 'ap@@', 'ap@@', 'i@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'anni', 'per', 'l@@', '<unk>', 'in@@', 'ven@@', 'zione', 'di', 'tre', 'anni', 'per', 'la', 'gr@@', 'av@@', 'it@@', 'à@@', '.', '</s>']
2025-05-29 23:33:56,111 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:33:56,111 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:33:56,111 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho fatto queste due due diapositive che la sua rivista per per la rivista dell<unk> Eiskapapi, per tre milioni di anni di anni per l<unk> invenzione di tre anni per la gravità.
2025-05-29 23:33:56,111 - INFO - joeynmt.training - Example #1
2025-05-29 23:33:56,111 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:33:56,111 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:33:56,112 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'una', 'cosa', 'che', 'la', 'più', 'grande', 'grande', 'la', 'prima', 'vol@@', 't@@', 'a@@', ',', 'in', 'questo', 'probl@@', 'em@@', 'a@@', ',', 'non', 'è', 'la', 'ri@@', 'vista', 'di', 'E@@', 'is@@', 'l@@', 'at@@', 'a@@', '.', '</s>']
2025-05-29 23:33:56,112 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:33:56,112 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:33:56,112 - INFO - joeynmt.training - 	Hypothesis: Ma non è una cosa che la più grande grande la prima volta, in questo problema, non è la rivista di Eislata.
2025-05-29 23:33:56,112 - INFO - joeynmt.training - Example #2
2025-05-29 23:33:56,112 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:33:56,113 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:33:56,113 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'sin@@', 'ne', 'è', 'la', 'co@@', 'per@@', 't@@', 'ina', 'del', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:33:56,113 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:33:56,113 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:33:56,113 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la sinne è la copertina del nostro sistema globale.
2025-05-29 23:33:56,113 - INFO - joeynmt.training - Example #3
2025-05-29 23:33:56,113 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:33:56,113 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:33:56,113 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'cosa', 'che', 'si', 'è', 'in', 'grado', 'di', 's@@', 'om@@', 'mer@@', 'it@@', 'o@@', '.', '</s>']
2025-05-29 23:33:56,114 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:33:56,114 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:33:56,114 - INFO - joeynmt.training - 	Hypothesis: La prima cosa che si è in grado di sommerito.
2025-05-29 23:33:56,114 - INFO - joeynmt.training - Example #4
2025-05-29 23:33:56,114 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:33:56,114 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:33:56,114 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'di@@', 'stri@@', 'bu@@', 'zione', 'è', 'una', 'cosa', 'che', 'vi', 'mostr@@', 'i', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:33:56,115 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:33:56,115 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:33:56,115 - INFO - joeynmt.training - 	Hypothesis: La prossima distribuzione è una cosa che vi mostri che è successo negli ultimi 25 anni.
2025-05-29 23:33:59,650 - INFO - joeynmt.training - Epoch   2, Step:    16100, Batch Loss:     2.139904, Batch Acc: 0.396691, Tokens per Sec:    16365, Lr: 0.000300
2025-05-29 23:34:03,203 - INFO - joeynmt.training - Epoch   2, Step:    16200, Batch Loss:     2.120134, Batch Acc: 0.398505, Tokens per Sec:    19392, Lr: 0.000300
2025-05-29 23:34:06,716 - INFO - joeynmt.training - Epoch   2, Step:    16300, Batch Loss:     2.102561, Batch Acc: 0.401084, Tokens per Sec:    19975, Lr: 0.000300
2025-05-29 23:34:10,225 - INFO - joeynmt.training - Epoch   2, Step:    16400, Batch Loss:     2.087745, Batch Acc: 0.397379, Tokens per Sec:    19343, Lr: 0.000300
2025-05-29 23:34:13,716 - INFO - joeynmt.training - Epoch   2, Step:    16500, Batch Loss:     2.057139, Batch Acc: 0.399554, Tokens per Sec:    19659, Lr: 0.000300
2025-05-29 23:34:13,717 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:34:13,717 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:34:21,600 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.13, ppl:   8.38, acc:   0.42, generation: 7.8758[sec], evaluation: 0.0000[sec]
2025-05-29 23:34:21,600 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:34:22,178 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/14000.ckpt
2025-05-29 23:34:22,206 - INFO - joeynmt.training - Example #0
2025-05-29 23:34:22,207 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:34:22,207 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:34:22,207 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'fatto', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've@@', ',', 'che', 'le', 'cose', 'sono', 'state', 'mostr@@', 'ando', 'che', 'le', 'em@@', 'issi@@', 'oni', 'che', 'si', 'chiama', '<unk>', 'E@@', 'is@@', 'c@@', 'ic@@', 'a@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'tre', 'anni', 'di', 'circa', 'il', '40', 'per', 'cento', 'di', 'anni', 'di', '4@@', '8', 'per', 'cento', 'per', 'cento', 'per', 'i', '4@@', '8', 'per', 'cento', 'per', 'cento', 'per', 'cento', 'per', 'cento', 'per', 'il', '40', 'per', 'cento', 'per', 'il', '40', 'per', 'cento', 'per', 'le', 'persone', 'che', 'hanno', 'fatto', 'che', 'le', 'persone', 'sono', 'state', 'ri@@', 'fer@@', 'iti', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'ri@@', 'durre', 'i', 'gruppi', 'di', 'persone', 'che', 'si']
2025-05-29 23:34:22,207 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:34:22,207 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:34:22,207 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho fatto queste due diapositive, che le cose sono state mostrando che le emissioni che si chiama <unk> Eiscica, per tre milioni di anni di tre anni di circa il 40 per cento di anni di 48 per cento per cento per i 48 per cento per cento per cento per cento per il 40 per cento per il 40 per cento per le persone che hanno fatto che le persone sono state riferiti per tre milioni di anni di ridurre i gruppi di persone che si
2025-05-29 23:34:22,207 - INFO - joeynmt.training - Example #1
2025-05-29 23:34:22,208 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:34:22,208 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:34:22,208 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'più', 'importante', 'che', 'non', 'è', 'una', 'cosa', 'molto', 'molto', 'più', 'importante', 'che', 'non', 'è', 'il', 'D@@', 'ic@@', 'ke', 'E@@', 'is@@', 'l@@', 'and@@', 'es@@', 'i@@', '.', '</s>']
2025-05-29 23:34:22,208 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:34:22,208 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:34:22,208 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto più importante che non è una cosa molto molto più importante che non è il Dicke Eislandesi.
2025-05-29 23:34:22,208 - INFO - joeynmt.training - Example #2
2025-05-29 23:34:22,209 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:34:22,209 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:34:22,209 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'cosa', 'più', 'importante', 'che', 'la', 'gente', 'si', 'è', 'ri@@', 'd@@', 'otto', 'il', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:34:22,209 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:34:22,209 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:34:22,209 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cosa più importante che la gente si è ridotto il nostro sistema globale.
2025-05-29 23:34:22,209 - INFO - joeynmt.training - Example #3
2025-05-29 23:34:22,209 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:34:22,209 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:34:22,209 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Pri@@', 'ma', 'di', 'tut@@', 'to@@', ',', 'la', 'gente', 'si', 'è', 'ri@@', 'd@@', 'otto', 'in', 'S@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:34:22,210 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:34:22,210 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:34:22,210 - INFO - joeynmt.training - 	Hypothesis: Prima di tutto, la gente si è ridotto in Sommer.
2025-05-29 23:34:22,210 - INFO - joeynmt.training - Example #4
2025-05-29 23:34:22,210 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:34:22,210 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:34:22,210 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'di@@', 'stri@@', 'bu@@', 'zione', 'che', 'vi', 'mostr@@', 'o', 'che', 'vi', 'mostr@@', 'o', 'cosa', 'succede', 'negli', 'ultimi', '25', 'anni', 'è', 'succ@@', 'ess@@', 'o@@', '.', '</s>']
2025-05-29 23:34:22,211 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:34:22,211 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:34:22,211 - INFO - joeynmt.training - 	Hypothesis: La prossima distribuzione che vi mostro che vi mostro cosa succede negli ultimi 25 anni è successo.
2025-05-29 23:34:25,692 - INFO - joeynmt.training - Epoch   2, Step:    16600, Batch Loss:     2.248356, Batch Acc: 0.397006, Tokens per Sec:    16804, Lr: 0.000300
2025-05-29 23:34:29,115 - INFO - joeynmt.training - Epoch   2, Step:    16700, Batch Loss:     2.256157, Batch Acc: 0.403024, Tokens per Sec:    20083, Lr: 0.000300
2025-05-29 23:34:32,512 - INFO - joeynmt.training - Epoch   2, Step:    16800, Batch Loss:     2.155164, Batch Acc: 0.398081, Tokens per Sec:    20104, Lr: 0.000300
2025-05-29 23:34:35,934 - INFO - joeynmt.training - Epoch   2, Step:    16900, Batch Loss:     2.196950, Batch Acc: 0.396013, Tokens per Sec:    19690, Lr: 0.000300
2025-05-29 23:34:39,437 - INFO - joeynmt.training - Epoch   2, Step:    17000, Batch Loss:     2.320304, Batch Acc: 0.404560, Tokens per Sec:    19339, Lr: 0.000300
2025-05-29 23:34:39,437 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:34:39,437 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:34:47,956 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.12, ppl:   8.32, acc:   0.42, generation: 8.5122[sec], evaluation: 0.0000[sec]
2025-05-29 23:34:47,957 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:34:48,520 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/14500.ckpt
2025-05-29 23:34:48,547 - INFO - joeynmt.training - Example #0
2025-05-29 23:34:48,547 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:34:48,547 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:34:48,547 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'è', 'stato', 'per', 'fare', 'che', 'la', 'quantità', 'di', 'ghi@@', 'acci@@', 'o@@', ',', 'che', 'è', 'stato', 'in', 'grado', 'di', 'vedere', 'i', 'Gr@@', 'u@@', 'ov@@', 'a', 'e', 'tre', 'milioni', 'di', 'anni', 'di', '4@@', '8', 'per', 'cento', 'anni', 'di', '4@@', '8', 'per', 'cento', 'anni', 'di', '4@@', '8', 'per', 'cento', 'anni', 'di', '4@@', '8', 'per', 'cento', 'di', '4@@', '8', 'per', 'cento', 'di', '4@@', '8', 'per', 'cento', 'di', '4@@', '8', 'per', 'cento', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:34:48,548 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:34:48,548 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:34:48,548 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive che è stato per fare che la quantità di ghiaccio, che è stato in grado di vedere i Gruova e tre milioni di anni di 48 per cento anni di 48 per cento anni di 48 per cento anni di 48 per cento di 48 per cento di 48 per cento di 48 per cento anni.
2025-05-29 23:34:48,548 - INFO - joeynmt.training - Example #1
2025-05-29 23:34:48,548 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:34:48,548 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:34:48,548 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'più', 'difficile', 'da', 'fare', 'la', 'nostra', 'capacità', 'di', 'ri@@', 'durre', 'la', 'nostra', 'capacità', 'di', 'ri@@', 'durre', 'il', 't@@', 'asso', 'di', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-29 23:34:48,549 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:34:48,549 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:34:48,549 - INFO - joeynmt.training - 	Hypothesis: Ma non è più difficile da fare la nostra capacità di ridurre la nostra capacità di ridurre il tasso di ghiaccio.
2025-05-29 23:34:48,549 - INFO - joeynmt.training - Example #2
2025-05-29 23:34:48,549 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:34:48,549 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:34:48,549 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'cosa', 'più', 'grande', 'di', 'quanto', 'sia', 'il', 'cuore', 'del', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:34:48,549 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:34:48,550 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:34:48,550 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cosa più grande di quanto sia il cuore del nostro sistema globale.
2025-05-29 23:34:48,550 - INFO - joeynmt.training - Example #3
2025-05-29 23:34:48,550 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:34:48,550 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:34:48,550 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'la', 'prima', 'cosa', 'che', 'si', 'tratta', 'di', 'un', 'vent@@', 'o@@', ',', 'e', 'si', 'può', 'essere', 'in', 'grado', 'di', 'fare', 'con', 'il', 'suo', 'inter@@', 'no@@', '.', '</s>']
2025-05-29 23:34:48,550 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:34:48,550 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:34:48,550 - INFO - joeynmt.training - 	Hypothesis: E la prima cosa che si tratta di un vento, e si può essere in grado di fare con il suo interno.
2025-05-29 23:34:48,550 - INFO - joeynmt.training - Example #4
2025-05-29 23:34:48,551 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:34:48,551 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:34:48,551 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'che', 'vi', 'mostr@@', 'o', 'è', 'una', 'sor@@', 'ta', 'di', 'di@@', 'stri@@', 'bu@@', 'zione', 'che', 'è', 'succ@@', 'ess@@', 'o@@', '.', '</s>']
2025-05-29 23:34:48,551 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:34:48,551 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:34:48,551 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva che vi mostro è una sorta di distribuzione che è successo.
2025-05-29 23:34:52,074 - INFO - joeynmt.training - Epoch   2, Step:    17100, Batch Loss:     2.313520, Batch Acc: 0.400895, Tokens per Sec:    16828, Lr: 0.000300
2025-05-29 23:34:55,544 - INFO - joeynmt.training - Epoch   2, Step:    17200, Batch Loss:     2.085694, Batch Acc: 0.397847, Tokens per Sec:    20030, Lr: 0.000300
2025-05-29 23:34:59,069 - INFO - joeynmt.training - Epoch   2, Step:    17300, Batch Loss:     2.398964, Batch Acc: 0.400431, Tokens per Sec:    19473, Lr: 0.000300
2025-05-29 23:35:02,573 - INFO - joeynmt.training - Epoch   2, Step:    17400, Batch Loss:     2.125792, Batch Acc: 0.405556, Tokens per Sec:    19781, Lr: 0.000300
2025-05-29 23:35:06,077 - INFO - joeynmt.training - Epoch   2, Step:    17500, Batch Loss:     2.180751, Batch Acc: 0.401862, Tokens per Sec:    19897, Lr: 0.000300
2025-05-29 23:35:06,077 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:35:06,078 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:35:15,193 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.10, ppl:   8.20, acc:   0.42, generation: 9.1038[sec], evaluation: 0.0000[sec]
2025-05-29 23:35:15,194 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:35:15,850 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/15000.ckpt
2025-05-29 23:35:15,881 - INFO - joeynmt.training - Example #0
2025-05-29 23:35:15,882 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:35:15,882 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:35:15,882 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'f@@', 'enti', 'che', 'si', 'è', 'ri@@', 'ma@@', 'sto', 'per', 'vedere', 'che', 'il', 'mondo', 'è', 'stato', 'd@@', '<unk>', 'accor@@', 'do', 'che', 'il', 'mondo', 'è', 'stato', 'in', 'cui', 'i', 'Gr@@', 'an', 'B@@', 'ret@@', 'ag@@', 'na@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'di', '4@@', '8', 'negli', 'Stati', 'Un@@', 'iti@@', '.', '</s>']
2025-05-29 23:35:15,882 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:35:15,883 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:35:15,883 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due difenti che si è rimasto per vedere che il mondo è stato d<unk> accordo che il mondo è stato in cui i Gran Bretagna, per tre milioni di anni di 48 negli Stati Uniti.
2025-05-29 23:35:15,883 - INFO - joeynmt.training - Example #1
2025-05-29 23:35:15,883 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:35:15,883 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:35:15,883 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'stato', 'abbastanza', 'la', 'prima', 'cosa', 'che', 'non', 'è', 'più', 'facile', 'da', 'fare', 'il', 'problema', 'di', 'questo', 'probl@@', 'em@@', 'a@@', ',', 'ma', 'non', 'è', 'il', 'D@@', 'ic@@', 'ke', 'E@@', 'is@@', 'c@@', 'e@@', '.', '</s>']
2025-05-29 23:35:15,883 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:35:15,884 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:35:15,884 - INFO - joeynmt.training - 	Hypothesis: Ma non è stato abbastanza la prima cosa che non è più facile da fare il problema di questo problema, ma non è il Dicke Eisce.
2025-05-29 23:35:15,884 - INFO - joeynmt.training - Example #2
2025-05-29 23:35:15,884 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:35:15,884 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:35:15,884 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'è', 'il', 'cuore', 'di', 'un', 'sistema', 'di', 'em@@', 'is@@', 'c@@', 'e@@', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:35:15,884 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:35:15,885 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:35:15,885 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, è il cuore di un sistema di emisce, il cuore del nostro sistema globale.
2025-05-29 23:35:15,885 - INFO - joeynmt.training - Example #3
2025-05-29 23:35:15,885 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:35:15,885 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:35:15,885 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'cosa', 'che', 'è', 'il', 'vent@@', 'o@@', ',', 'e', 'la', 's@@', 'om@@', 'mer@@', 'idi@@', 'on@@', 'e@@', '.', '</s>']
2025-05-29 23:35:15,885 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:35:15,885 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:35:15,885 - INFO - joeynmt.training - 	Hypothesis: La prima cosa che è il vento, e la sommeridione.
2025-05-29 23:35:15,886 - INFO - joeynmt.training - Example #4
2025-05-29 23:35:15,886 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:35:15,886 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:35:15,886 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'di@@', 'v@@', 'a@@', ',', 'la', 'prossi@@', 'ma', 'di@@', 'stri@@', 'bu@@', 'zione', 'è', 'un', 'di@@', 'seg@@', 'no', 'che', 'è', 'succ@@', 'ess@@', 'o@@', ',', 'è', 'succ@@', 'ess@@', 'o@@', ',', 'è', 'succ@@', 'ess@@', 'o@@', ',', 'è', 'succ@@', 'ess@@', 'o@@', ',', 'il', 'mio', 'lavoro', 'è', 'succ@@', 'ess@@', 'o@@', ',', 'e', 'il', 'mio', 'lavoro', 'è', 'succ@@', 'ess@@', 'o@@', ',', 'e', 'il', 'mio', 'lavoro', 'è', 'successo', 'in', 'questo', 'm@@', 'om@@', 'ent@@', 'o@@', ',', 'e', 'il', 'mio', 'lavoro', 'è', 'stato', 'in', 'grado', 'di', 'ri@@', 'pro@@', 'durre', 'il', 'mon@@', 'do@@', ',', 'e', 'il', 'mio', 'lavoro', 'è', 'stato', 'il', 'di@@', 'v@@', 'ario', 'che', 'è', 'succ@@', 'ess@@', 'o@@', ',', 'e', 'il', 'mio', 'lavoro', 'è', 'successo']
2025-05-29 23:35:15,886 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:35:15,886 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:35:15,886 - INFO - joeynmt.training - 	Hypothesis: La prossima diva, la prossima distribuzione è un disegno che è successo, è successo, è successo, è successo, il mio lavoro è successo, e il mio lavoro è successo, e il mio lavoro è successo in questo momento, e il mio lavoro è stato in grado di riprodurre il mondo, e il mio lavoro è stato il divario che è successo, e il mio lavoro è successo
2025-05-29 23:35:19,424 - INFO - joeynmt.training - Epoch   2, Step:    17600, Batch Loss:     2.179756, Batch Acc: 0.402383, Tokens per Sec:    15972, Lr: 0.000300
2025-05-29 23:35:22,919 - INFO - joeynmt.training - Epoch   2, Step:    17700, Batch Loss:     2.196064, Batch Acc: 0.402534, Tokens per Sec:    19541, Lr: 0.000300
2025-05-29 23:35:26,419 - INFO - joeynmt.training - Epoch   2, Step:    17800, Batch Loss:     2.367436, Batch Acc: 0.405963, Tokens per Sec:    19421, Lr: 0.000300
2025-05-29 23:35:29,910 - INFO - joeynmt.training - Epoch   2, Step:    17900, Batch Loss:     2.232842, Batch Acc: 0.404278, Tokens per Sec:    19624, Lr: 0.000300
2025-05-29 23:35:33,391 - INFO - joeynmt.training - Epoch   2, Step:    18000, Batch Loss:     2.279263, Batch Acc: 0.403323, Tokens per Sec:    19613, Lr: 0.000300
2025-05-29 23:35:33,391 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:35:33,391 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:35:41,472 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.09, ppl:   8.10, acc:   0.43, generation: 8.0729[sec], evaluation: 0.0000[sec]
2025-05-29 23:35:41,472 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:35:42,045 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/15500.ckpt
2025-05-29 23:35:42,072 - INFO - joeynmt.training - Example #0
2025-05-29 23:35:42,073 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:35:42,073 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:35:42,073 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'l@@', '<unk>', 'anno', 'per', 'vedere', 'che', 'la', 'l@@', '<unk>', 'E@@', 'is@@', 'ca', 'che', 'l@@', '<unk>', 'E@@', 'is@@', 'ca', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'maggior', 'parte', 'dei', 'li@@', 'velli', 'di', 'gr@@', 'ave', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'maggior', 'parte', 'dei', '4@@', '8', 'per', 'cento', 'di', 'questi', 'di@@', 'seg@@', 'ni', 'di', 's@@', 'fr@@', 'utt@@', 'a@@', '.', '</s>']
2025-05-29 23:35:42,073 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:35:42,074 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:35:42,074 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive che l<unk> anno per vedere che la l<unk> Eisca che l<unk> Eisca per tre milioni di anni per la maggior parte dei livelli di grave per tre milioni di anni per la maggior parte dei 48 per cento di questi disegni di sfrutta.
2025-05-29 23:35:42,074 - INFO - joeynmt.training - Example #1
2025-05-29 23:35:42,074 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:35:42,074 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:35:42,074 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'stato', 'un', 'problema', 'di', 's@@', 'for@@', 'zo', 'di', 'questo', 'probl@@', 'em@@', 'a@@', ',', 'che', 'non', 'è', 'la', 'parte', 'del', 'problema', 'di', 'ghi@@', 'acci@@', 'o@@', ',', 'non', 'è', 'il', 'D@@', 'ic@@', 'ke', 'E@@', 'is@@', 'c@@', 'e@@', '.', '</s>']
2025-05-29 23:35:42,074 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:35:42,075 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:35:42,075 - INFO - joeynmt.training - 	Hypothesis: Ma non è stato un problema di sforzo di questo problema, che non è la parte del problema di ghiaccio, non è il Dicke Eisce.
2025-05-29 23:35:42,075 - INFO - joeynmt.training - Example #2
2025-05-29 23:35:42,075 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:35:42,075 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:35:42,075 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'di@@', 'apos@@', 'iti@@', 'va', 'di', 'ghi@@', 'acci@@', 'o@@', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:35:42,075 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:35:42,076 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:35:42,076 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la diapositiva di ghiaccio, il cuore del nostro sistema globale.
2025-05-29 23:35:42,076 - INFO - joeynmt.training - Example #3
2025-05-29 23:35:42,076 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:35:42,076 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:35:42,076 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'la', 'prima', 'volta', 'in', 'un', 'vent@@', 'o@@', ',', 'e', 's@@', 'fr@@', 'utt@@', 'at@@', 'a@@', '.', '</s>']
2025-05-29 23:35:42,076 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:35:42,076 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:35:42,076 - INFO - joeynmt.training - 	Hypothesis: E la prima volta in un vento, e sfruttata.
2025-05-29 23:35:42,076 - INFO - joeynmt.training - Example #4
2025-05-29 23:35:42,077 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:35:42,077 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:35:42,077 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'prossi@@', 'mo', 'di@@', 'apos@@', 'iti@@', 'vo', 'è', 'un', 'di@@', 'seg@@', 'no', 'è', 'un', 'di@@', 'seg@@', 'no', 'che', 'è', 'acca@@', 'duto', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:35:42,077 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:35:42,077 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:35:42,077 - INFO - joeynmt.training - 	Hypothesis: Il prossimo diapositivo è un disegno è un disegno che è accaduto negli ultimi 25 anni.
2025-05-29 23:35:45,511 - INFO - joeynmt.training - Epoch   2, Step:    18100, Batch Loss:     2.190306, Batch Acc: 0.403947, Tokens per Sec:    17507, Lr: 0.000300
2025-05-29 23:35:48,918 - INFO - joeynmt.training - Epoch   2, Step:    18200, Batch Loss:     2.180886, Batch Acc: 0.409288, Tokens per Sec:    20074, Lr: 0.000300
2025-05-29 23:35:52,324 - INFO - joeynmt.training - Epoch   2, Step:    18300, Batch Loss:     2.241316, Batch Acc: 0.406295, Tokens per Sec:    20049, Lr: 0.000300
2025-05-29 23:35:55,731 - INFO - joeynmt.training - Epoch   2, Step:    18400, Batch Loss:     2.077004, Batch Acc: 0.407156, Tokens per Sec:    20370, Lr: 0.000300
2025-05-29 23:35:59,181 - INFO - joeynmt.training - Epoch   2, Step:    18500, Batch Loss:     2.147120, Batch Acc: 0.405111, Tokens per Sec:    20301, Lr: 0.000300
2025-05-29 23:35:59,182 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:35:59,182 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:36:06,876 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.08, ppl:   8.00, acc:   0.43, generation: 7.6867[sec], evaluation: 0.0000[sec]
2025-05-29 23:36:06,876 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:36:07,421 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/16000.ckpt
2025-05-29 23:36:07,448 - INFO - joeynmt.training - Example #0
2025-05-29 23:36:07,449 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:36:07,449 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:36:07,449 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'è', 'la', 'di@@', 'apos@@', 'iti@@', 'va', 'che', 'che', 'la', 'pover@@', 't@@', 'à@@', ',', 'che', 'i', 'nostri', 'an@@', 'ni@@', ',', 'per', 'i', 'tre', 'milioni', 'di', 'anni', 'di', 'anni', 'per', 'la', 'ter@@', 'ra@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'ri@@', 'fer@@', 'imento', 'per', 'il', '40', 'per', 'cento', 'di', 'questi', 'due', 'anni', 'per', 'raggi@@', 'ung@@', 'er@@', 'e@@', '.', '</s>']
2025-05-29 23:36:07,450 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:36:07,450 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:36:07,450 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive che è la diapositiva che che la povertà, che i nostri anni, per i tre milioni di anni di anni per la terra, per tre milioni di anni di riferimento per il 40 per cento di questi due anni per raggiungere.
2025-05-29 23:36:07,450 - INFO - joeynmt.training - Example #1
2025-05-29 23:36:07,450 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:36:07,450 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:36:07,450 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'la', 'prima', 'cosa', 'che', 'la', 'vita', 'è', 'stata', 'la', 'prima', 'cosa', 'che', 'non', 'è', 'la', 'parte', 'di', 'questo', 'probl@@', 'em@@', 'a@@', ',', 'non', 'è', 'la', 'parte', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-29 23:36:07,451 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:36:07,451 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:36:07,451 - INFO - joeynmt.training - 	Hypothesis: Ma non è la prima cosa che la vita è stata la prima cosa che non è la parte di questo problema, non è la parte del ghiaccio.
2025-05-29 23:36:07,451 - INFO - joeynmt.training - Example #2
2025-05-29 23:36:07,451 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:36:07,451 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:36:07,451 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'è', 'l@@', '<unk>', 'ar@@', 't@@', 'a@@', ',', 'il', 'che', 'è', 'il', 'suo', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:36:07,452 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:36:07,452 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:36:07,452 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, è l<unk> arta, il che è il suo sistema globale.
2025-05-29 23:36:07,452 - INFO - joeynmt.training - Example #3
2025-05-29 23:36:07,452 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:36:07,452 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:36:07,452 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', 'è', 'il', 'vent@@', 'o@@', ',', 'e', 'la', 'sal@@', 'a', 'e', 'sc@@', 'ambi@@', 'o', 'e', 'di', 's@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:36:07,452 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:36:07,452 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:36:07,452 - INFO - joeynmt.training - 	Hypothesis: C<unk> è il vento, e la sala e scambio e di sommer.
2025-05-29 23:36:07,453 - INFO - joeynmt.training - Example #4
2025-05-29 23:36:07,453 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:36:07,453 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:36:07,453 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 'seg@@', 'no', 'di', 'di@@', 'eci', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:36:07,453 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:36:07,453 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:36:07,453 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva che vi mostrerò è un segno di dieci anni.
2025-05-29 23:36:10,944 - INFO - joeynmt.training - Epoch   2, Step:    18600, Batch Loss:     2.384236, Batch Acc: 0.405883, Tokens per Sec:    17444, Lr: 0.000300
2025-05-29 23:36:14,409 - INFO - joeynmt.training - Epoch   2, Step:    18700, Batch Loss:     2.113770, Batch Acc: 0.405306, Tokens per Sec:    19545, Lr: 0.000300
2025-05-29 23:36:17,848 - INFO - joeynmt.training - Epoch   2, Step:    18800, Batch Loss:     2.226737, Batch Acc: 0.411869, Tokens per Sec:    19789, Lr: 0.000300
2025-05-29 23:36:19,549 - INFO - joeynmt.training - Epoch   2: total training loss 21435.44
2025-05-29 23:36:19,549 - INFO - joeynmt.training - EPOCH 3
2025-05-29 23:36:21,312 - INFO - joeynmt.training - Epoch   3, Step:    18900, Batch Loss:     2.032343, Batch Acc: 0.421773, Tokens per Sec:    20076, Lr: 0.000300
2025-05-29 23:36:24,788 - INFO - joeynmt.training - Epoch   3, Step:    19000, Batch Loss:     2.170213, Batch Acc: 0.419094, Tokens per Sec:    20186, Lr: 0.000300
2025-05-29 23:36:24,788 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:36:24,789 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:36:32,250 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.07, ppl:   7.89, acc:   0.44, generation: 7.4493[sec], evaluation: 0.0000[sec]
2025-05-29 23:36:32,250 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:36:32,822 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/16500.ckpt
2025-05-29 23:36:32,848 - INFO - joeynmt.training - Example #0
2025-05-29 23:36:32,848 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:36:32,848 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:36:32,848 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'sono', 'stati', 'in', 'grado', 'di', 'vedere', 'che', 'il', 'sistema', 'ar@@', 'ic@@', 'o@@', ',', 'che', 'il', 'sistema', 'ar@@', 'ico', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'gr@@', 'ave', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'gr@@', 'ave', 'per', 'il', '4@@', '8', 'stati', 'stati', 'per', 'cento', 'di', 'questi', 'cam@@', 'pi@@', '.', '</s>']
2025-05-29 23:36:32,849 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:36:32,849 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:36:32,849 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive che sono stati in grado di vedere che il sistema arico, che il sistema arico per tre milioni di anni per la grave per tre milioni di anni per la grave per il 48 stati stati per cento di questi campi.
2025-05-29 23:36:32,849 - INFO - joeynmt.training - Example #1
2025-05-29 23:36:32,849 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:36:32,849 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:36:32,849 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'il', 'modo', 'di', 'ri@@', 'fer@@', 'imento', 'di', 'questo', 'probl@@', 'em@@', 'a@@', ',', 'ma', 'non', 'è', 'il', 'D@@', 'ic@@', 'ke', 'che', 'non', 'è', 'il', 'D@@', 'ic@@', 'ke', 'E@@', 'is@@', '.', '</s>']
2025-05-29 23:36:32,849 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:36:32,850 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:36:32,850 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è il modo di riferimento di questo problema, ma non è il Dicke che non è il Dicke Eis.
2025-05-29 23:36:32,850 - INFO - joeynmt.training - Example #2
2025-05-29 23:36:32,850 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:36:32,850 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:36:32,850 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'cosa', 'che', 'si', 'è', 'ri@@', 'vel@@', 'a', 'il', 'cuore', 'del', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:36:32,850 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:36:32,850 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:36:32,850 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cosa che si è rivela il cuore del nostro sistema globale.
2025-05-29 23:36:32,850 - INFO - joeynmt.training - Example #3
2025-05-29 23:36:32,851 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:36:32,851 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:36:32,851 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'volta', 'che', 'si', 'trova', 'nel', 'vent@@', 'o@@', ',', 'e', 'sc@@', 'en@@', 'dere', 'in', 'un', 's@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:36:32,851 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:36:32,851 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:36:32,851 - INFO - joeynmt.training - 	Hypothesis: La prima volta che si trova nel vento, e scendere in un sommer.
2025-05-29 23:36:32,851 - INFO - joeynmt.training - Example #4
2025-05-29 23:36:32,852 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:36:32,852 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:36:32,852 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'di@@', 'stri@@', 'bu@@', 'ito', 'che', 'vi', 'mostr@@', 'o', 'è', 'una', 'di@@', 'stri@@', 'bu@@', 'zione', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:36:32,852 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:36:32,852 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:36:32,852 - INFO - joeynmt.training - 	Hypothesis: La prossima distribuito che vi mostro è una distribuzione negli ultimi 25 anni.
2025-05-29 23:36:36,276 - INFO - joeynmt.training - Epoch   3, Step:    19100, Batch Loss:     2.322293, Batch Acc: 0.418462, Tokens per Sec:    17045, Lr: 0.000300
2025-05-29 23:36:39,738 - INFO - joeynmt.training - Epoch   3, Step:    19200, Batch Loss:     2.256105, Batch Acc: 0.420844, Tokens per Sec:    20124, Lr: 0.000300
2025-05-29 23:36:43,195 - INFO - joeynmt.training - Epoch   3, Step:    19300, Batch Loss:     2.089509, Batch Acc: 0.417421, Tokens per Sec:    19843, Lr: 0.000300
2025-05-29 23:36:46,665 - INFO - joeynmt.training - Epoch   3, Step:    19400, Batch Loss:     1.988411, Batch Acc: 0.419654, Tokens per Sec:    20492, Lr: 0.000300
2025-05-29 23:36:50,118 - INFO - joeynmt.training - Epoch   3, Step:    19500, Batch Loss:     2.153079, Batch Acc: 0.420800, Tokens per Sec:    19953, Lr: 0.000300
2025-05-29 23:36:50,118 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:36:50,118 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:36:58,283 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.07, ppl:   7.93, acc:   0.43, generation: 8.1576[sec], evaluation: 0.0000[sec]
2025-05-29 23:36:58,663 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/17000.ckpt
2025-05-29 23:36:58,689 - INFO - joeynmt.training - Example #0
2025-05-29 23:36:58,689 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:36:58,689 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:36:58,689 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'è', 'stato', 'il', '4@@', '8@@', '0@@', '%', 'di', 'ghi@@', 'accio', 'che', 'è', 'la', 'b@@', 'ella', 'che', 'è', 'stata', 'la', 'gr@@', 'ave', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'ghi@@', 'accio', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'ghi@@', 'accio', 'per', 'i', '4@@', '8', 'stati', 'per', 'cento', 'di', 'anni', 'per', 'la', 'maggior', 'parte', 'dei', '4@@', '8', 'stati', 'per', 'cento', 'dei', '4@@', '8', 'stati', 'per', 'cento', 'di', 'queste', 'due', 'dimen@@', 'sioni', 'per', 'tre', 'anni', 'fa@@', '.', '</s>']
2025-05-29 23:36:58,690 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:36:58,690 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:36:58,690 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive che è stato il 480% di ghiaccio che è la bella che è stata la grave per tre milioni di anni di ghiaccio per tre milioni di anni di ghiaccio per i 48 stati per cento di anni per la maggior parte dei 48 stati per cento dei 48 stati per cento di queste due dimensioni per tre anni fa.
2025-05-29 23:36:58,690 - INFO - joeynmt.training - Example #1
2025-05-29 23:36:58,690 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:36:58,690 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:36:58,690 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'il', 'più', 'grande', 'modo', 'di', 'essere', 'abbastanza', 'l@@', '<unk>', 'import@@', 'anza', 'di', 'queste', 'cos@@', 'e@@', ',', 'perché', 'non', 'è', 'il', 'vi@@', 'll@@', 'aggio', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-29 23:36:58,691 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:36:58,691 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:36:58,691 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è il più grande modo di essere abbastanza l<unk> importanza di queste cose, perché non è il villaggio del ghiaccio.
2025-05-29 23:36:58,691 - INFO - joeynmt.training - Example #2
2025-05-29 23:36:58,691 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:36:58,691 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:36:58,691 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'mat@@', 'eria', 'ar@@', 'ica', 'di', 'E@@', 'is@@', 'co@@', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'globale', 'globale', 'del', 'nostro', 'sistema', 'globale', 'globale', 'globale', 'del', 'nostro', 'sistema', 'globale', 'globale', 'globale', 'del', 'nostro', 'sistema', 'globale', 'globale', 'di', 'c@@', 'ic@@', 'lo', 'di', 'un', 'sistema', 'globale', 'globale', 'di', 'ri@@', 'fer@@', 'im@@', 'ent@@', 'o@@', '.', '</s>']
2025-05-29 23:36:58,692 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:36:58,692 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:36:58,692 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la materia arica di Eisco, il cuore del nostro sistema globale globale del nostro sistema globale globale globale del nostro sistema globale globale globale del nostro sistema globale globale di ciclo di un sistema globale globale di riferimento.
2025-05-29 23:36:58,692 - INFO - joeynmt.training - Example #3
2025-05-29 23:36:58,692 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:36:58,692 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:36:58,692 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'è', 'che', 'la', 'prima', 'cosa', 'che', 'si', 'trova', 'nel', 's@@', 'om@@', 'mer@@', 'a@@', '.', '</s>']
2025-05-29 23:36:58,692 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:36:58,692 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:36:58,693 - INFO - joeynmt.training - 	Hypothesis: La prima è che la prima cosa che si trova nel sommera.
2025-05-29 23:36:58,693 - INFO - joeynmt.training - Example #4
2025-05-29 23:36:58,693 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:36:58,693 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:36:58,693 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'v@@', 'a@@', ',', 'è', 'una', 'cosa', 'che', 'vi', 'mostr@@', 'o', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:36:58,693 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:36:58,693 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:36:58,693 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva, è una cosa che vi mostro è successo negli ultimi 25 anni.
2025-05-29 23:37:02,138 - INFO - joeynmt.training - Epoch   3, Step:    19600, Batch Loss:     2.085586, Batch Acc: 0.424771, Tokens per Sec:    18129, Lr: 0.000300
2025-05-29 23:37:05,570 - INFO - joeynmt.training - Epoch   3, Step:    19700, Batch Loss:     2.199725, Batch Acc: 0.415695, Tokens per Sec:    20294, Lr: 0.000300
2025-05-29 23:37:08,993 - INFO - joeynmt.training - Epoch   3, Step:    19800, Batch Loss:     2.121186, Batch Acc: 0.427572, Tokens per Sec:    19637, Lr: 0.000300
2025-05-29 23:37:12,410 - INFO - joeynmt.training - Epoch   3, Step:    19900, Batch Loss:     2.281179, Batch Acc: 0.413755, Tokens per Sec:    19692, Lr: 0.000300
2025-05-29 23:37:15,828 - INFO - joeynmt.training - Epoch   3, Step:    20000, Batch Loss:     2.031783, Batch Acc: 0.418028, Tokens per Sec:    19819, Lr: 0.000300
2025-05-29 23:37:15,828 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:37:15,828 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:37:20,808 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.06, ppl:   7.84, acc:   0.43, generation: 4.9742[sec], evaluation: 0.0000[sec]
2025-05-29 23:37:20,809 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:37:21,376 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/17500.ckpt
2025-05-29 23:37:21,397 - INFO - joeynmt.training - Example #0
2025-05-29 23:37:21,397 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:37:21,397 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:37:21,398 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'questa', 'di@@', 'apos@@', 'iti@@', 'va', 'che', 'è', 'stato', 'fatto', 'per', 'vedere', 'che', 'il', 'problema', 'd@@', 'ell@@', '<unk>', 'ar@@', 'ic@@', 'a@@', ',', 'che', 'è', 'stato', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'il', 'numero', 'di', 'anni', 'per', 'il', '4@@', '8', 'per', 'cento', 'di', '4@@', '8', 'per', 'cento', 'di', '4@@', '8', 'per', 'cento', 'per', 'cento', 'del', '4@@', '8@@', '0@@', '.', '</s>']
2025-05-29 23:37:21,398 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:37:21,398 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:37:21,398 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato questa diapositiva che è stato fatto per vedere che il problema dell<unk> arica, che è stato fatto per tre milioni di anni che ha fatto per tre milioni di anni il numero di anni per il 48 per cento di 48 per cento di 48 per cento per cento del 480.
2025-05-29 23:37:21,398 - INFO - joeynmt.training - Example #1
2025-05-29 23:37:21,398 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:37:21,399 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:37:21,399 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'la', 'più', 'grande', 'quantità', 'di', 'dati', 'che', 'si', 'è', 'ri@@', 'ma@@', 'sto', 'in', 'modo', 'che', 'non', 'è', 'il', 'bu@@', 'co', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'c@@', 'e@@', '.', '</s>']
2025-05-29 23:37:21,399 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:37:21,399 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:37:21,399 - INFO - joeynmt.training - 	Hypothesis: Ma non è la più grande quantità di dati che si è rimasto in modo che non è il buco dell<unk> Eisce.
2025-05-29 23:37:21,399 - INFO - joeynmt.training - Example #2
2025-05-29 23:37:21,399 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:37:21,399 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:37:21,400 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'cosa', 'che', 'si', 'può', 'essere', 'in', 'grado', 'di', 'ri@@', 'fer@@', 'imento', 'della', 'nostra', 'c@@', 'av@@', 'a@@', '.', '</s>']
2025-05-29 23:37:21,400 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:37:21,400 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:37:21,400 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cosa che si può essere in grado di riferimento della nostra cava.
2025-05-29 23:37:21,400 - INFO - joeynmt.training - Example #3
2025-05-29 23:37:21,400 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:37:21,400 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:37:21,400 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ei', 'è', 'stato', 'il', 'vent@@', 'o@@', ',', 'e', 'la', 'pi@@', 'att@@', 'a@@', '.', '</s>']
2025-05-29 23:37:21,401 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:37:21,401 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:37:21,401 - INFO - joeynmt.training - 	Hypothesis: Lei è stato il vento, e la piatta.
2025-05-29 23:37:21,401 - INFO - joeynmt.training - Example #4
2025-05-29 23:37:21,401 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:37:21,401 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:37:21,401 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'v@@', 'a@@', ',', 'è', 'un', 'seg@@', 'no', 'di', 'un', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:37:21,402 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:37:21,402 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:37:21,402 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva, è un segno di un segno di quello che è successo negli ultimi 25 anni.
2025-05-29 23:37:24,889 - INFO - joeynmt.training - Epoch   3, Step:    20100, Batch Loss:     2.169744, Batch Acc: 0.419223, Tokens per Sec:    16739, Lr: 0.000300
2025-05-29 23:37:28,396 - INFO - joeynmt.training - Epoch   3, Step:    20200, Batch Loss:     2.027960, Batch Acc: 0.420522, Tokens per Sec:    19984, Lr: 0.000300
2025-05-29 23:37:31,891 - INFO - joeynmt.training - Epoch   3, Step:    20300, Batch Loss:     2.257680, Batch Acc: 0.420498, Tokens per Sec:    20029, Lr: 0.000300
2025-05-29 23:37:35,371 - INFO - joeynmt.training - Epoch   3, Step:    20400, Batch Loss:     2.297987, Batch Acc: 0.414212, Tokens per Sec:    19844, Lr: 0.000300
2025-05-29 23:37:38,847 - INFO - joeynmt.training - Epoch   3, Step:    20500, Batch Loss:     2.191042, Batch Acc: 0.416384, Tokens per Sec:    19594, Lr: 0.000300
2025-05-29 23:37:38,848 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:37:38,848 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:37:46,784 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.05, ppl:   7.79, acc:   0.44, generation: 7.9256[sec], evaluation: 0.0000[sec]
2025-05-29 23:37:46,785 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:37:47,436 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/18000.ckpt
2025-05-29 23:37:47,465 - INFO - joeynmt.training - Example #0
2025-05-29 23:37:47,466 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:37:47,466 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:37:47,466 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'questa', 'due', 'di@@', 'apos@@', 'iti@@', 'va', 'che', 'è', 'stato', 'un', 'em@@', 'is@@', 'fer@@', 'o', 'che', 'il', 't@@', 'asso', 'di', 'ghi@@', 'acci@@', 'o@@', ',', 'che', 'è', 'stato', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'c@@', 'att@@', 'ur@@', 'are', 'il', '4@@', '8', 'stati', 'dei', 'primi', 'per', 'cento', 'di', 'circa', '4@@', '8', 'stati', 'dei', 'paesi', 'in', 'via', 'di', 'svilupp@@', 'o@@', '.', '</s>']
2025-05-29 23:37:47,467 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:37:47,467 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:37:47,467 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato questa due diapositiva che è stato un emisfero che il tasso di ghiaccio, che è stato fatto per tre milioni di anni di catturare il 48 stati dei primi per cento di circa 48 stati dei paesi in via di sviluppo.
2025-05-29 23:37:47,467 - INFO - joeynmt.training - Example #1
2025-05-29 23:37:47,467 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:37:47,467 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:37:47,467 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'stato', 'abbastanza', 'l@@', '<unk>', 'in@@', 'for@@', 'mat@@', 'ica', 'di', 'questo', 'probl@@', 'em@@', 'a@@', ',', 'perché', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'c@@', 'e@@', '.', '</s>']
2025-05-29 23:37:47,468 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:37:47,468 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:37:47,468 - INFO - joeynmt.training - 	Hypothesis: Ma non è stato abbastanza l<unk> informatica di questo problema, perché non è il dell<unk> Eisce.
2025-05-29 23:37:47,468 - INFO - joeynmt.training - Example #2
2025-05-29 23:37:47,468 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:37:47,468 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:37:47,468 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'ris@@', 'i', 'di', 'ghi@@', 'acci@@', 'o@@', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:37:47,469 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:37:47,469 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:37:47,469 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la crisi di ghiaccio, il cuore del nostro sistema globale.
2025-05-29 23:37:47,469 - INFO - joeynmt.training - Example #3
2025-05-29 23:37:47,469 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:37:47,469 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:37:47,469 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', 'è', 'una', 'volta', 'che', 'si', 'è', 'in', 'vent@@', 'o@@', ',', 'e', 's@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:37:47,470 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:37:47,470 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:37:47,470 - INFO - joeynmt.training - 	Hypothesis: C<unk> è una volta che si è in vento, e sommer.
2025-05-29 23:37:47,470 - INFO - joeynmt.training - Example #4
2025-05-29 23:37:47,470 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:37:47,470 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:37:47,470 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'seg@@', 'no', 'di', 'tem@@', 'po@@', '.', '</s>']
2025-05-29 23:37:47,470 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:37:47,471 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:37:47,471 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva che vi mostro è un segno di tempo.
2025-05-29 23:37:50,977 - INFO - joeynmt.training - Epoch   3, Step:    20600, Batch Loss:     2.076160, Batch Acc: 0.414470, Tokens per Sec:    16599, Lr: 0.000300
2025-05-29 23:37:54,460 - INFO - joeynmt.training - Epoch   3, Step:    20700, Batch Loss:     2.072145, Batch Acc: 0.419811, Tokens per Sec:    20251, Lr: 0.000300
2025-05-29 23:37:57,938 - INFO - joeynmt.training - Epoch   3, Step:    20800, Batch Loss:     2.123833, Batch Acc: 0.420931, Tokens per Sec:    19362, Lr: 0.000300
2025-05-29 23:38:01,417 - INFO - joeynmt.training - Epoch   3, Step:    20900, Batch Loss:     2.195838, Batch Acc: 0.420246, Tokens per Sec:    19705, Lr: 0.000300
2025-05-29 23:38:04,856 - INFO - joeynmt.training - Epoch   3, Step:    21000, Batch Loss:     1.981110, Batch Acc: 0.423478, Tokens per Sec:    19922, Lr: 0.000300
2025-05-29 23:38:04,856 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:38:04,856 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:38:13,334 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.04, ppl:   7.73, acc:   0.44, generation: 8.4674[sec], evaluation: 0.0000[sec]
2025-05-29 23:38:13,334 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:38:14,010 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/18500.ckpt
2025-05-29 23:38:14,039 - INFO - joeynmt.training - Example #0
2025-05-29 23:38:14,039 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:38:14,039 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:38:14,039 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'l@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'che', 'gli', 'ar@@', 't@@', 'ori', 'ar@@', 't@@', 'ic@@', 'ani', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'c@@', 'att@@', 'ur@@', 'are', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'popolazione', 'd@@', 'ell@@', '<unk>', 'in@@', 'fan@@', 'zia', 'per', 'la', 'ri@@', 'du@@', 'zione', 'di', '4@@', '8', 'Pa@@', 'es@@', 'e@@', '.', '</s>']
2025-05-29 23:38:14,040 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:38:14,040 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:38:14,040 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due due diapositive che l<unk> anno scorso, che gli artori articani che ha fatto per tre milioni di anni di catturare per tre milioni di anni per la popolazione dell<unk> infanzia per la riduzione di 48 Paese.
2025-05-29 23:38:14,040 - INFO - joeynmt.training - Example #1
2025-05-29 23:38:14,040 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:38:14,040 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:38:14,040 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'la', 'più', 'grande', 'di', 'quanto', 'la', 'ter@@', 'za', 'è', 'la', 'più', 'grande', 'di', 'questo', 'problema', 'che', 'non', 'è', 'il', 'doc@@', 'um@@', 'ent@@', 'ario', 'di', 'questo', 'problema', 'è', 'che', 'è', 'il', 'D@@', 'ic@@', 'ke', 'E@@', 'is@@', 'l@@', 'and@@', 'es@@', '.', '</s>']
2025-05-29 23:38:14,041 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:38:14,041 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:38:14,041 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza la più grande di quanto la terza è la più grande di questo problema che non è il documentario di questo problema è che è il Dicke Eislandes.
2025-05-29 23:38:14,041 - INFO - joeynmt.training - Example #2
2025-05-29 23:38:14,041 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:38:14,041 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:38:14,041 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'chi@@', 'ave', 'd@@', 'ell@@', '<unk>', 'epi@@', 'de@@', 'mia', 'di', 'c@@', 'att@@', 'ur@@', 'are', 'il', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:38:14,042 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:38:14,042 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:38:14,042 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la chiave dell<unk> epidemia di catturare il nostro sistema globale.
2025-05-29 23:38:14,042 - INFO - joeynmt.training - Example #3
2025-05-29 23:38:14,042 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:38:14,042 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:38:14,042 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', 'er@@', 'to@@', ',', 'si', 'è', 'ri@@', 'ma@@', 'sta', 'nel', 's@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:38:14,043 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:38:14,043 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:38:14,043 - INFO - joeynmt.training - 	Hypothesis: Certo, si è rimasta nel sommer.
2025-05-29 23:38:14,043 - INFO - joeynmt.training - Example #4
2025-05-29 23:38:14,043 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:38:14,043 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:38:14,043 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'è', 'una', 'di@@', 'apos@@', 'iti@@', 'va', 'che', 'è', 'un', 'seg@@', 'no', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:38:14,044 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:38:14,044 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:38:14,044 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza è una diapositiva che è un segno di ciò che è successo negli ultimi 25 anni.
2025-05-29 23:38:17,543 - INFO - joeynmt.training - Epoch   3, Step:    21100, Batch Loss:     2.022889, Batch Acc: 0.423859, Tokens per Sec:    15993, Lr: 0.000300
2025-05-29 23:38:21,036 - INFO - joeynmt.training - Epoch   3, Step:    21200, Batch Loss:     2.259435, Batch Acc: 0.426441, Tokens per Sec:    19854, Lr: 0.000300
2025-05-29 23:38:24,508 - INFO - joeynmt.training - Epoch   3, Step:    21300, Batch Loss:     2.214401, Batch Acc: 0.418014, Tokens per Sec:    19625, Lr: 0.000300
2025-05-29 23:38:27,959 - INFO - joeynmt.training - Epoch   3, Step:    21400, Batch Loss:     2.001973, Batch Acc: 0.420363, Tokens per Sec:    19345, Lr: 0.000300
2025-05-29 23:38:31,412 - INFO - joeynmt.training - Epoch   3, Step:    21500, Batch Loss:     2.071631, Batch Acc: 0.422402, Tokens per Sec:    19129, Lr: 0.000300
2025-05-29 23:38:31,413 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:38:31,413 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:38:38,801 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.04, ppl:   7.68, acc:   0.44, generation: 7.3779[sec], evaluation: 0.0000[sec]
2025-05-29 23:38:38,801 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:38:39,441 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/19500.ckpt
2025-05-29 23:38:39,463 - INFO - joeynmt.training - Example #0
2025-05-29 23:38:39,463 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:38:39,464 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:38:39,464 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'questi', 'due', 'di@@', 'apos@@', 'iti@@', 'vi', 'che', 'è', 'stato', 'fatto', 'per', 'essere', 'in', 'grado', 'di', 'vedere', 'che', 'la', 'mat@@', 'eria', 'ar@@', 'ic@@', 'a@@', ',', 'per', 'tre', 'anni', 'di', 's@@', 'é@@', ',', 'per', 'tre', 'anni', 'per', 'cento', 'anni', 'i', 'gruppi', 'di', '4@@', '8', 'stati', 'per', 'cento', 'anni', 'per', 'l@@', '<unk>', 'ep@@', 'oca', 'di', '4@@', '8', 'stati', 'per', 'cento', 'anni', 'per', 'la', 'maggior', 'parte', 'dei', 'nostri', 'figli', 'hanno', 'fatto', 'in', 'grado', 'di', 'vedere', 'il', '40', 'per', 'cento', 'anni', 'di', 'ri@@', 'fer@@', 'imento', 'di', 'un', 'po@@', '<unk>', 'di', 'm@@', 'e@@', '.', '</s>']
2025-05-29 23:38:39,464 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:38:39,464 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:38:39,464 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato questi due diapositivi che è stato fatto per essere in grado di vedere che la materia arica, per tre anni di sé, per tre anni per cento anni i gruppi di 48 stati per cento anni per l<unk> epoca di 48 stati per cento anni per la maggior parte dei nostri figli hanno fatto in grado di vedere il 40 per cento anni di riferimento di un po<unk> di me.
2025-05-29 23:38:39,464 - INFO - joeynmt.training - Example #1
2025-05-29 23:38:39,464 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:38:39,465 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:38:39,465 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'stato', 'un', 'modo', 'in', 'cui', 'non', 'è', 'stato', 'un', 'problema', 'molto', 'speci@@', 'al@@', 'e@@', ',', 'perché', 'non', 'è', 'il', 'D@@', 'ic@@', 'ke', 'E@@', 'is@@', 'es', 'che', 'mostr@@', 'ano', 'la', 'D@@', 'ic@@', 'ke', 'E@@', 'is@@', 'es', 'che', 'mostr@@', 'ano', 'la', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'c@@', 'e@@', '.', '</s>']
2025-05-29 23:38:39,465 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:38:39,465 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:38:39,465 - INFO - joeynmt.training - 	Hypothesis: Ma non è stato un modo in cui non è stato un problema molto speciale, perché non è il Dicke Eises che mostrano la Dicke Eises che mostrano la dell<unk> Eisce.
2025-05-29 23:38:39,465 - INFO - joeynmt.training - Example #2
2025-05-29 23:38:39,465 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:38:39,465 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:38:39,465 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'co@@', 'per@@', 't@@', 'ic@@', 'a@@', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'sistema', 'di', 'cal@@', 'col@@', 'o@@', '.', '</s>']
2025-05-29 23:38:39,466 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:38:39,466 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:38:39,466 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la copertica, il cuore del nostro sistema sistema di calcolo.
2025-05-29 23:38:39,466 - INFO - joeynmt.training - Example #3
2025-05-29 23:38:39,466 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:38:39,466 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:38:39,466 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', 'er@@', 'to@@', ',', 'in', 'sal@@', 'a', 'e', 'si', 'è', 'ri@@', 'ma@@', 'sta', 'in', 'sal@@', 'a@@', '.', '</s>']
2025-05-29 23:38:39,466 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:38:39,467 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:38:39,467 - INFO - joeynmt.training - 	Hypothesis: Certo, in sala e si è rimasta in sala.
2025-05-29 23:38:39,467 - INFO - joeynmt.training - Example #4
2025-05-29 23:38:39,467 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:38:39,467 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:38:39,467 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'prossi@@', 'mo', 'di@@', 'mostra', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 'seg@@', 'no', 'di', 'un', 'seg@@', 'no', 'di', 'un', 'di@@', 'seg@@', 'no', 'di', 'ciò', 'che', 'è', 'acca@@', 'duto', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:38:39,467 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:38:39,467 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:38:39,467 - INFO - joeynmt.training - 	Hypothesis: Il prossimo dimostra che vi mostrerò è un segno di un segno di un disegno di ciò che è accaduto negli ultimi 25 anni.
2025-05-29 23:38:42,962 - INFO - joeynmt.training - Epoch   3, Step:    21600, Batch Loss:     2.081535, Batch Acc: 0.419865, Tokens per Sec:    16208, Lr: 0.000300
2025-05-29 23:38:46,467 - INFO - joeynmt.training - Epoch   3, Step:    21700, Batch Loss:     2.090650, Batch Acc: 0.422327, Tokens per Sec:    20462, Lr: 0.000300
2025-05-29 23:38:49,926 - INFO - joeynmt.training - Epoch   3, Step:    21800, Batch Loss:     2.345902, Batch Acc: 0.417517, Tokens per Sec:    19366, Lr: 0.000300
2025-05-29 23:38:53,394 - INFO - joeynmt.training - Epoch   3, Step:    21900, Batch Loss:     2.106031, Batch Acc: 0.421884, Tokens per Sec:    19494, Lr: 0.000300
2025-05-29 23:38:56,868 - INFO - joeynmt.training - Epoch   3, Step:    22000, Batch Loss:     2.132623, Batch Acc: 0.422144, Tokens per Sec:    19878, Lr: 0.000300
2025-05-29 23:38:56,869 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:38:56,869 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:39:05,266 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.02, ppl:   7.57, acc:   0.45, generation: 8.3868[sec], evaluation: 0.0000[sec]
2025-05-29 23:39:05,266 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:39:05,930 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/19000.ckpt
2025-05-29 23:39:05,959 - INFO - joeynmt.training - Example #0
2025-05-29 23:39:05,959 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:39:05,959 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:39:05,959 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'l@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'che', 'i', 'pover@@', 't@@', 'à@@', ',', 'che', 'i', 'pover@@', 'i', 'che', 'hanno', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'c@@', 'accia', 'di', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'maggior', 'parte', 'dei', '4@@', '8', 'stati', 'per', 'cento', 'di', 'anni', 'per', 'il', '40', 'per', 'cento', 'di', 'anni', 'per', 'cento', 'di', 'questi', 'due', 'anni', 'di', 'ri@@', 'vel@@', 'are', 'il', '40', 'per', 'cento', 'di', 'questi', 'due', 'anni', 'di', 'ri@@', 'vel@@', 'are', 'il', '40', 'per', 'cento', 'di', 'questi', 'due', 'anni', 'fa@@', '.', '</s>']
2025-05-29 23:39:05,960 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:39:05,960 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:39:05,960 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive che l<unk> anno scorso, che i povertà, che i poveri che hanno fatto per tre milioni di anni di caccia di tre milioni di anni per la maggior parte dei 48 stati per cento di anni per il 40 per cento di anni per cento di questi due anni di rivelare il 40 per cento di questi due anni di rivelare il 40 per cento di questi due anni fa.
2025-05-29 23:39:05,960 - INFO - joeynmt.training - Example #1
2025-05-29 23:39:05,960 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:39:05,960 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:39:05,960 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'la', 'più', 'grande', 'giu@@', 'st@@', 'izia', 'che', 'non', 'è', 'che', 'non', 'è', 'la', 'possibilità', 'di', 'questo', 'probl@@', 'em@@', 'a@@', ',', 'perché', 'non', 'è', 'la', 'b@@', 'ella', 'E@@', 'is@@', 'c@@', 'ci@@', 'a@@', '.', '</s>']
2025-05-29 23:39:05,961 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:39:05,961 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:39:05,961 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza la più grande giustizia che non è che non è la possibilità di questo problema, perché non è la bella Eisccia.
2025-05-29 23:39:05,961 - INFO - joeynmt.training - Example #2
2025-05-29 23:39:05,961 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:39:05,961 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:39:05,961 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'for@@', 'esta', 'ar@@', 't@@', 'ic@@', 'ale', 'di', 'E@@', 'is@@', 'k@@', 'ist@@', 'a@@', '.', '</s>']
2025-05-29 23:39:05,962 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:39:05,962 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:39:05,962 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la foresta articale di Eiskista.
2025-05-29 23:39:05,962 - INFO - joeynmt.training - Example #3
2025-05-29 23:39:05,962 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:39:05,962 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:39:05,962 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', 'è', 'una', 'cosa', 'che', 'si', 'trova', 'nel', 's@@', 'om@@', 'mer@@', 'it@@', 'o@@', '.', '</s>']
2025-05-29 23:39:05,963 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:39:05,963 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:39:05,963 - INFO - joeynmt.training - 	Hypothesis: C<unk> è una cosa che si trova nel sommerito.
2025-05-29 23:39:05,963 - INFO - joeynmt.training - Example #4
2025-05-29 23:39:05,963 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:39:05,963 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:39:05,963 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'di@@', 'stri@@', 'bu@@', 'zione', 'che', 'vi', 'mostr@@', 'o', 'è', 'una', 'cosa', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:39:05,963 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:39:05,964 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:39:05,964 - INFO - joeynmt.training - 	Hypothesis: La prossima distribuzione che vi mostro è una cosa che è successo negli ultimi 25 anni.
2025-05-29 23:39:09,399 - INFO - joeynmt.training - Epoch   3, Step:    22100, Batch Loss:     2.121847, Batch Acc: 0.421555, Tokens per Sec:    16528, Lr: 0.000300
2025-05-29 23:39:12,878 - INFO - joeynmt.training - Epoch   3, Step:    22200, Batch Loss:     2.090503, Batch Acc: 0.419579, Tokens per Sec:    19713, Lr: 0.000300
2025-05-29 23:39:16,334 - INFO - joeynmt.training - Epoch   3, Step:    22300, Batch Loss:     2.103024, Batch Acc: 0.423249, Tokens per Sec:    19233, Lr: 0.000300
2025-05-29 23:39:19,796 - INFO - joeynmt.training - Epoch   3, Step:    22400, Batch Loss:     1.951998, Batch Acc: 0.425939, Tokens per Sec:    19370, Lr: 0.000300
2025-05-29 23:39:23,296 - INFO - joeynmt.training - Epoch   3, Step:    22500, Batch Loss:     2.050211, Batch Acc: 0.423291, Tokens per Sec:    19833, Lr: 0.000300
2025-05-29 23:39:23,297 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:39:23,297 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:39:30,632 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.02, ppl:   7.53, acc:   0.45, generation: 7.3277[sec], evaluation: 0.0000[sec]
2025-05-29 23:39:30,633 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:39:31,174 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/20000.ckpt
2025-05-29 23:39:31,200 - INFO - joeynmt.training - Example #0
2025-05-29 23:39:31,201 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:39:31,201 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:39:31,201 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'vedere', 'che', 'le', 'cose', 'che', 'hanno', 'fatto', 'il', 'livello', 'di', 'E@@', 'is@@', 'k@@', 'e@@', ',', 'che', 'i', 'con@@', 'fin@@', 'i', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'pes@@', 'o', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'l@@', '<unk>', 'in@@', 'fer@@', 'i@@', 'ore', 'di', 'circa', '4@@', '8', 'stati', 'per', 'cento', 'di', 'anni', 'per', 'il', '4@@', '8', 'stati', 'per', 'cento', 'di', 'questi', 'sono', 'stati', 'stati', 'in', 'grado', 'di', 'vedere', 'il', '4@@', '8', 'stati', 'in', 'grado', 'di', 'fare', 'il', '4@@', '8', 'stati', 'in', 'grado', 'di', 'vedere', 'il', '4@@', '8', 'stati', 'in', 'grado', 'di', 'vedere', 'il', 'mon@@', 'do@@', '.', '</s>']
2025-05-29 23:39:31,202 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:39:31,202 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:39:31,202 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per vedere che le cose che hanno fatto il livello di Eiske, che i confini per tre milioni di anni di peso per tre milioni di anni per l<unk> inferiore di circa 48 stati per cento di anni per il 48 stati per cento di questi sono stati stati in grado di vedere il 48 stati in grado di fare il 48 stati in grado di vedere il 48 stati in grado di vedere il mondo.
2025-05-29 23:39:31,202 - INFO - joeynmt.training - Example #1
2025-05-29 23:39:31,202 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:39:31,202 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:39:31,202 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'la', 'possibilità', 'di', 'essere', 'la', 'possibilità', 'di', 'questa', 'particolare', 'probl@@', 'em@@', 'a@@', ',', 'perché', 'non', 'lo', 'far@@', 'à', 'la', 'b@@', 'ella', 'cos@@', 'a@@', '.', '</s>']
2025-05-29 23:39:31,203 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:39:31,203 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:39:31,203 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza la possibilità di essere la possibilità di questa particolare problema, perché non lo farà la bella cosa.
2025-05-29 23:39:31,203 - INFO - joeynmt.training - Example #2
2025-05-29 23:39:31,203 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:39:31,203 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:39:31,203 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'mat@@', 'eria', 'ar@@', 'ica', 'ar@@', 'ric@@', 'ch@@', 'etta', 'il', 'cuore', 'del', 'nostro', 'sistema', 'sistema', 'di', 'c@@', 'lin@@', 'ic@@', 'o@@', '.', '</s>']
2025-05-29 23:39:31,204 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:39:31,204 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:39:31,204 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la materia arica arricchetta il cuore del nostro sistema sistema di clinico.
2025-05-29 23:39:31,204 - INFO - joeynmt.training - Example #3
2025-05-29 23:39:31,204 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:39:31,204 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:39:31,204 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'di', 's@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:39:31,204 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:39:31,204 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:39:31,204 - INFO - joeynmt.training - 	Hypothesis: C<unk> è un po<unk> di sommer.
2025-05-29 23:39:31,204 - INFO - joeynmt.training - Example #4
2025-05-29 23:39:31,205 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:39:31,205 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:39:31,205 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enz@@', 'a@@', ',', 'la', 'prossi@@', 'ma', 'di@@', 'stri@@', 'bu@@', 'ito', 'a', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:39:31,205 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:39:31,205 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:39:31,205 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza, la prossima distribuito a quello che è successo negli ultimi 25 anni.
2025-05-29 23:39:34,615 - INFO - joeynmt.training - Epoch   3, Step:    22600, Batch Loss:     2.046533, Batch Acc: 0.429835, Tokens per Sec:    16520, Lr: 0.000300
2025-05-29 23:39:38,091 - INFO - joeynmt.training - Epoch   3, Step:    22700, Batch Loss:     1.998157, Batch Acc: 0.421402, Tokens per Sec:    19774, Lr: 0.000300
2025-05-29 23:39:41,560 - INFO - joeynmt.training - Epoch   3, Step:    22800, Batch Loss:     2.008850, Batch Acc: 0.427785, Tokens per Sec:    20113, Lr: 0.000300
2025-05-29 23:39:45,035 - INFO - joeynmt.training - Epoch   3, Step:    22900, Batch Loss:     2.195333, Batch Acc: 0.424277, Tokens per Sec:    20120, Lr: 0.000300
2025-05-29 23:39:48,500 - INFO - joeynmt.training - Epoch   3, Step:    23000, Batch Loss:     2.118927, Batch Acc: 0.426254, Tokens per Sec:    20248, Lr: 0.000300
2025-05-29 23:39:48,501 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:39:48,501 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:39:56,391 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.02, ppl:   7.52, acc:   0.44, generation: 7.8805[sec], evaluation: 0.0000[sec]
2025-05-29 23:39:56,392 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:39:57,033 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/20500.ckpt
2025-05-29 23:39:57,061 - INFO - joeynmt.training - Example #0
2025-05-29 23:39:57,061 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:39:57,061 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:39:57,061 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'che', 'i', 'pover@@', 'i', 'che', 'i', 'pover@@', 'i', 'che', 'i', 'pover@@', 'i', 'che', 'hanno', 'l@@', '<unk>', 'epi@@', 'de@@', 'mia', 'che', 'hanno', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'pes@@', 'o', 'di', 'circa', 'tre', 'milioni', 'di', 'anni', 'di', 'c@@', 'att@@', 'ur@@', 'are', 'il', '40', 'per', 'cento', 'di', 'questi', 'due', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:39:57,062 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:39:57,062 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:39:57,062 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive che che i poveri che i poveri che i poveri che hanno l<unk> epidemia che hanno fatto per tre milioni di anni di peso di circa tre milioni di anni di catturare il 40 per cento di questi due anni.
2025-05-29 23:39:57,062 - INFO - joeynmt.training - Example #1
2025-05-29 23:39:57,062 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:39:57,062 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:39:57,063 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'la', 'cosa', 'più', 'importante', 'è', 'che', 'la', 'possibilità', 'di', 'essere', 'la', 'possibilità', 'di', 'essere', 'la', 'possibilità', 'di', 'non', 'è', 'la', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'a@@', '.', '</s>']
2025-05-29 23:39:57,063 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:39:57,063 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:39:57,063 - INFO - joeynmt.training - 	Hypothesis: Ma non è la cosa più importante è che la possibilità di essere la possibilità di essere la possibilità di non è la dell<unk> Eisla.
2025-05-29 23:39:57,063 - INFO - joeynmt.training - Example #2
2025-05-29 23:39:57,063 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:39:57,064 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:39:57,064 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'è', 'l@@', '<unk>', 'epi@@', 'de@@', 'mia', 'di', 'l@@', '<unk>', 'au@@', 'tor@@', 'ità', 'del', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:39:57,064 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:39:57,064 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:39:57,064 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cuore è l<unk> epidemia di l<unk> autorità del nostro sistema globale.
2025-05-29 23:39:57,064 - INFO - joeynmt.training - Example #3
2025-05-29 23:39:57,064 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:39:57,064 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:39:57,064 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', 'è', 'il', 'vento', 'e', 'poi', 'si', 'è', 'ri@@', 'ma@@', 'sta', 'in', 'sal@@', 'a@@', '.', '</s>']
2025-05-29 23:39:57,065 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:39:57,065 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:39:57,065 - INFO - joeynmt.training - 	Hypothesis: C<unk> è il vento e poi si è rimasta in sala.
2025-05-29 23:39:57,065 - INFO - joeynmt.training - Example #4
2025-05-29 23:39:57,065 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:39:57,065 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:39:57,065 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'di@@', 'seg@@', 'ret@@', 'a@@', ',', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:39:57,066 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:39:57,066 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:39:57,066 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva che vi mostrerò è una disegreta, che è successo negli ultimi 25 anni.
2025-05-29 23:40:00,552 - INFO - joeynmt.training - Epoch   3, Step:    23100, Batch Loss:     2.003149, Batch Acc: 0.424974, Tokens per Sec:    16116, Lr: 0.000300
2025-05-29 23:40:03,949 - INFO - joeynmt.training - Epoch   3, Step:    23200, Batch Loss:     2.152537, Batch Acc: 0.422702, Tokens per Sec:    20811, Lr: 0.000300
2025-05-29 23:40:07,306 - INFO - joeynmt.training - Epoch   3, Step:    23300, Batch Loss:     2.217481, Batch Acc: 0.425864, Tokens per Sec:    21025, Lr: 0.000300
2025-05-29 23:40:10,622 - INFO - joeynmt.training - Epoch   3, Step:    23400, Batch Loss:     2.237909, Batch Acc: 0.427461, Tokens per Sec:    20094, Lr: 0.000300
2025-05-29 23:40:13,937 - INFO - joeynmt.training - Epoch   3, Step:    23500, Batch Loss:     2.222865, Batch Acc: 0.427364, Tokens per Sec:    20419, Lr: 0.000300
2025-05-29 23:40:13,938 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:40:13,938 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:40:20,756 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.00, ppl:   7.42, acc:   0.45, generation: 6.8125[sec], evaluation: 0.0000[sec]
2025-05-29 23:40:20,757 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:40:21,312 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/21000.ckpt
2025-05-29 23:40:21,332 - INFO - joeynmt.training - Example #0
2025-05-29 23:40:21,332 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:40:21,332 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:40:21,332 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'l@@', '<unk>', 'E@@', 'is@@', 'k@@', 'ap@@', 'i@@', 'a@@', ',', 'che', 'la', 'ghi@@', 'acci@@', 'a@@', ',', 'che', 'è', 'stato', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'pes@@', 'ce', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'di', '4@@', '8', 'stati', 'per', 'cento', 'di', 'anni', 'di', '4@@', '8', 'negli', 'Stati', 'Un@@', 'iti@@', '.', '</s>']
2025-05-29 23:40:21,333 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:40:21,333 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:40:21,333 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive che l<unk> Eiskapia, che la ghiaccia, che è stato fatto per tre milioni di anni di pesce che ha fatto per tre milioni di anni di 48 stati per cento di anni di 48 negli Stati Uniti.
2025-05-29 23:40:21,333 - INFO - joeynmt.training - Example #1
2025-05-29 23:40:21,333 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:40:21,333 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:40:21,333 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'da', 'l@@', '<unk>', 'in@@', 'giu@@', 'st@@', 'izia', 'che', 'non', 'è', 'la', 'sol@@', 'u@@', 'zion@@', 'e@@', ',', 'perché', 'non', 'c@@', '<unk>', 'è', 'la', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'a@@', '.', '</s>']
2025-05-29 23:40:21,334 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:40:21,334 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:40:21,334 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza da l<unk> ingiustizia che non è la soluzione, perché non c<unk> è la dell<unk> Eisla.
2025-05-29 23:40:21,334 - INFO - joeynmt.training - Example #2
2025-05-29 23:40:21,334 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:40:21,334 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:40:21,334 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'qualche', 'certo', 'sen@@', 'so@@', ',', 'la', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'il', 'cuore', 'di', 'questo', 'sistema', 'ar@@', 'co', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:40:21,334 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:40:21,335 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:40:21,335 - INFO - joeynmt.training - 	Hypothesis: In qualche certo senso, la Eiskappe il cuore di questo sistema arco globale.
2025-05-29 23:40:21,335 - INFO - joeynmt.training - Example #3
2025-05-29 23:40:21,335 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:40:21,335 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:40:21,335 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'è', 'ri@@', 'ma@@', 'sta', 'in', 'sal@@', 'a@@', '.', '</s>']
2025-05-29 23:40:21,335 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:40:21,335 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:40:21,336 - INFO - joeynmt.training - 	Hypothesis: Si è rimasta in sala.
2025-05-29 23:40:21,336 - INFO - joeynmt.training - Example #4
2025-05-29 23:40:21,336 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:40:21,336 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:40:21,336 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'che', 'vi', 'mostr@@', 'i', 'è', 'un', 'seg@@', 'no', 'di', 'quello', 'che', 'succede', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:40:21,336 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:40:21,336 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:40:21,336 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva che vi mostri è un segno di quello che succede negli ultimi 25 anni.
2025-05-29 23:40:24,765 - INFO - joeynmt.training - Epoch   3, Step:    23600, Batch Loss:     2.101895, Batch Acc: 0.418704, Tokens per Sec:    17225, Lr: 0.000300
2025-05-29 23:40:28,202 - INFO - joeynmt.training - Epoch   3, Step:    23700, Batch Loss:     2.302329, Batch Acc: 0.424152, Tokens per Sec:    19001, Lr: 0.000300
2025-05-29 23:40:31,653 - INFO - joeynmt.training - Epoch   3, Step:    23800, Batch Loss:     1.898668, Batch Acc: 0.428341, Tokens per Sec:    19759, Lr: 0.000300
2025-05-29 23:40:35,107 - INFO - joeynmt.training - Epoch   3, Step:    23900, Batch Loss:     2.218864, Batch Acc: 0.422475, Tokens per Sec:    19422, Lr: 0.000300
2025-05-29 23:40:38,559 - INFO - joeynmt.training - Epoch   3, Step:    24000, Batch Loss:     2.196173, Batch Acc: 0.428998, Tokens per Sec:    19586, Lr: 0.000300
2025-05-29 23:40:38,559 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:40:38,559 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:40:46,970 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.01, ppl:   7.45, acc:   0.45, generation: 8.4002[sec], evaluation: 0.0000[sec]
2025-05-29 23:40:47,410 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/21500.ckpt
2025-05-29 23:40:47,438 - INFO - joeynmt.training - Example #0
2025-05-29 23:40:47,438 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:40:47,438 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:40:47,438 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'vedere', 'che', 'i', 'li@@', 'velli', 'ar@@', 'res@@', 'o', 'conto', 'che', 'i', 'pover@@', 'i', 'che', 'i', 'li@@', 'velli', 'ar@@', 'ic@@', 'ati', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'pes@@', 'ca', 'di', 'circa', '4@@', '8', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:40:47,439 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:40:47,439 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:40:47,439 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per vedere che i livelli arreso conto che i poveri che i livelli aricati per tre milioni di anni di pesca di circa 48 anni.
2025-05-29 23:40:47,439 - INFO - joeynmt.training - Example #1
2025-05-29 23:40:47,439 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:40:47,439 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:40:47,439 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'stato', 'abbastanza', 'difficile', 'da', 'fare', 'un', 'problema', 'di', 'queste', 'speci@@', 'fi@@', 'che', 'non', 'è', 'il', 'problema', 'che', 'non', 'è', 'il', 'D@@', 'ic@@', 'ke', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'and@@', 'es@@', '.', '</s>']
2025-05-29 23:40:47,440 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:40:47,440 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:40:47,440 - INFO - joeynmt.training - 	Hypothesis: Ma non è stato abbastanza difficile da fare un problema di queste specifiche non è il problema che non è il Dicke dell<unk> Eislandes.
2025-05-29 23:40:47,440 - INFO - joeynmt.training - Example #2
2025-05-29 23:40:47,440 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:40:47,440 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:40:47,440 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'qualche', 'modo', 'c@@', '<unk>', 'è', 'la', 'realtà', 'ar@@', 'ica', 'ar@@', 'ric@@', 'ca', 'di', 'ghi@@', 'accio', 'ar@@', 'ic@@', 'o@@', '.', '</s>']
2025-05-29 23:40:47,441 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:40:47,441 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:40:47,441 - INFO - joeynmt.training - 	Hypothesis: In qualche modo c<unk> è la realtà arica arricca di ghiaccio arico.
2025-05-29 23:40:47,441 - INFO - joeynmt.training - Example #3
2025-05-29 23:40:47,441 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:40:47,441 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:40:47,441 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'di', 'pi@@', 'ù@@', ',', 'e', 'poi', 'si', 'è', 'ri@@', 'ma@@', 'sta', 'in', 'S@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:40:47,442 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:40:47,442 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:40:47,442 - INFO - joeynmt.training - 	Hypothesis: C<unk> è un po<unk> di più, e poi si è rimasta in Sommer.
2025-05-29 23:40:47,442 - INFO - joeynmt.training - Example #4
2025-05-29 23:40:47,442 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:40:47,442 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:40:47,442 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:40:47,442 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:40:47,442 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:40:47,443 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva che vi mostro è un disegno di quello che è successo negli ultimi 25 anni.
2025-05-29 23:40:50,921 - INFO - joeynmt.training - Epoch   3, Step:    24100, Batch Loss:     2.116271, Batch Acc: 0.427043, Tokens per Sec:    17319, Lr: 0.000300
2025-05-29 23:40:54,390 - INFO - joeynmt.training - Epoch   3, Step:    24200, Batch Loss:     2.336655, Batch Acc: 0.423581, Tokens per Sec:    19823, Lr: 0.000300
2025-05-29 23:40:57,860 - INFO - joeynmt.training - Epoch   3, Step:    24300, Batch Loss:     1.897036, Batch Acc: 0.421964, Tokens per Sec:    19942, Lr: 0.000300
2025-05-29 23:41:01,312 - INFO - joeynmt.training - Epoch   3, Step:    24400, Batch Loss:     2.083261, Batch Acc: 0.424957, Tokens per Sec:    19521, Lr: 0.000300
2025-05-29 23:41:04,768 - INFO - joeynmt.training - Epoch   3, Step:    24500, Batch Loss:     2.215986, Batch Acc: 0.426826, Tokens per Sec:    20259, Lr: 0.000300
2025-05-29 23:41:04,768 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:41:04,768 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:41:12,166 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.99, ppl:   7.32, acc:   0.45, generation: 7.3917[sec], evaluation: 0.0000[sec]
2025-05-29 23:41:12,167 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:41:12,708 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/22000.ckpt
2025-05-29 23:41:12,734 - INFO - joeynmt.training - Example #0
2025-05-29 23:41:12,734 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:41:12,735 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:41:12,735 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'l@@', '<unk>', 'ar@@', 'te@@', ',', 'che', 'l@@', '<unk>', 'ar@@', 'ic@@', 'a@@', ',', 'che', 'l@@', '<unk>', 'ar@@', 'ric@@', 'o@@', 'stru@@', 'zione', 'ar@@', 'ic@@', 'a@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'maggior', 'parte', 'degli', 'studenti', 'di', '4@@', '8', 'Stati', 'Uniti', 'per', 'cento', 'di', 'questi', 'ultimi', '4@@', '8', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:41:12,735 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:41:12,735 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:41:12,735 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive che l<unk> arte, che l<unk> arica, che l<unk> arricostruzione arica, per tre milioni di anni per la maggior parte degli studenti di 48 Stati Uniti per cento di questi ultimi 48 anni.
2025-05-29 23:41:12,735 - INFO - joeynmt.training - Example #1
2025-05-29 23:41:12,735 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:41:12,735 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:41:12,735 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'più', 'prob@@', 'abile', 'che', 'non', 'è', 'abbastanza', 'la', 'più', 'importante', 'di', 'questo', 'problema', 'speci@@', 'ale', 'che', 'non', 'è', 'il', 'D@@', 'ic@@', 'ke', 'del', 'ghi@@', 'accio', 'del', 'ghi@@', 'accio', 'del', 'ghi@@', 'accio', 'del', 'ghi@@', 'accio', 'del', 'ghi@@', 'accio', 'del', 'ghi@@', 'accio', 'del', 'ghi@@', 'accio', 'che', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'tutto', 'il', 'problema', 'della', 'nostra', 'vita', 'è', 'un', 'problema', 'di', 'un', 'problema', 'di', 'ri@@', 'fer@@', 'imento', 'di', 'un', 'problema', 'di', 'ri@@', 'fer@@', 'imento', 'di', 'un', 'problema', 'di', 'ri@@', 'fer@@', 'imento', 'di', 'un', 'problema', 'di', 'ri@@', 'fu@@', 'gi@@', 're', 'e', 'la', 'sua', 'vita', 'e', 'la', 'sua', 'vita', 'di', 'un', 'problema', 'di', 'vita', 'che', 'non', 'è', 'il']
2025-05-29 23:41:12,736 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:41:12,736 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:41:12,736 - INFO - joeynmt.training - 	Hypothesis: Ma non è più probabile che non è abbastanza la più importante di questo problema speciale che non è il Dicke del ghiaccio del ghiaccio del ghiaccio del ghiaccio del ghiaccio del ghiaccio del ghiaccio che è il dibattito del tutto il problema della nostra vita è un problema di un problema di riferimento di un problema di riferimento di un problema di riferimento di un problema di rifugire e la sua vita e la sua vita di un problema di vita che non è il
2025-05-29 23:41:12,736 - INFO - joeynmt.training - Example #2
2025-05-29 23:41:12,736 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:41:12,736 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:41:12,736 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'sistema', 'ar@@', 'ico', 'ar@@', 'ric@@', 'ci@@', 'mento', 'di', 'ghi@@', 'accio', 'ar@@', 'ic@@', 'o@@', ',', 'il', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:41:12,737 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:41:12,737 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:41:12,737 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il sistema arico arriccimento di ghiaccio arico, il sistema globale.
2025-05-29 23:41:12,737 - INFO - joeynmt.training - Example #3
2025-05-29 23:41:12,737 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:41:12,737 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:41:12,737 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'cosa', 'che', 'si', 'è', 'fer@@', 'mato', 'in', 'sal@@', 'a@@', '.', '</s>']
2025-05-29 23:41:12,738 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:41:12,738 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:41:12,738 - INFO - joeynmt.training - 	Hypothesis: La prima cosa che si è fermato in sala.
2025-05-29 23:41:12,738 - INFO - joeynmt.training - Example #4
2025-05-29 23:41:12,738 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:41:12,738 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:41:12,738 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'di@@', 'stri@@', 'bu@@', 'zione', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:41:12,738 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:41:12,739 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:41:12,739 - INFO - joeynmt.training - 	Hypothesis: La prossima distribuzione che vi mostro è un disegno di quello che è successo negli ultimi 25 anni.
2025-05-29 23:41:16,206 - INFO - joeynmt.training - Epoch   3, Step:    24600, Batch Loss:     2.032833, Batch Acc: 0.427258, Tokens per Sec:    17443, Lr: 0.000300
2025-05-29 23:41:19,658 - INFO - joeynmt.training - Epoch   3, Step:    24700, Batch Loss:     2.107884, Batch Acc: 0.423563, Tokens per Sec:    19106, Lr: 0.000300
2025-05-29 23:41:23,080 - INFO - joeynmt.training - Epoch   3, Step:    24800, Batch Loss:     2.113219, Batch Acc: 0.425188, Tokens per Sec:    19972, Lr: 0.000300
2025-05-29 23:41:26,549 - INFO - joeynmt.training - Epoch   3, Step:    24900, Batch Loss:     2.110186, Batch Acc: 0.426063, Tokens per Sec:    19916, Lr: 0.000300
2025-05-29 23:41:30,014 - INFO - joeynmt.training - Epoch   3, Step:    25000, Batch Loss:     2.017536, Batch Acc: 0.427631, Tokens per Sec:    19389, Lr: 0.000300
2025-05-29 23:41:30,015 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:41:30,015 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:41:38,718 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.00, ppl:   7.36, acc:   0.45, generation: 8.6964[sec], evaluation: 0.0000[sec]
2025-05-29 23:41:39,068 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/22500.ckpt
2025-05-29 23:41:39,094 - INFO - joeynmt.training - Example #0
2025-05-29 23:41:39,094 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:41:39,094 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:41:39,095 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'vedere', 'che', 'il', '4@@', '8', 'stati', 'in', 'grado', 'di', 'vedere', 'che', 'i', 'paesi', 'pover@@', 'i', 'che', 'hanno', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'c@@', 'ant@@', 'are', 'i', 'con@@', 'fin@@', 'i', 'per', 'i', 'paesi', 'in', 'via', 'di', 'sviluppo', 'e', 'l@@', '<unk>', '4@@', '8', 'stati', 'per', 'cento', 'dei', 'paesi', 'in', 'via', 'di', 'svilupp@@', 'o@@', '.', '</s>']
2025-05-29 23:41:39,095 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:41:39,095 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:41:39,095 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per vedere che il 48 stati in grado di vedere che i paesi poveri che hanno fatto per tre milioni di anni di cantare i confini per i paesi in via di sviluppo e l<unk> 48 stati per cento dei paesi in via di sviluppo.
2025-05-29 23:41:39,095 - INFO - joeynmt.training - Example #1
2025-05-29 23:41:39,095 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:41:39,095 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:41:39,095 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'la', 'più', 'grande', 'quantità', 'di', 'cose', 'che', 'non', 'è', 'il', 'problema', 'di', 'questo', 'problema', 'speci@@', 'ale', 'che', 'non', 'lo', 'fa', 'il', 'D@@', 'ic@@', 'ke', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'a@@', '.', '</s>']
2025-05-29 23:41:39,096 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:41:39,096 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:41:39,096 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto la più grande quantità di cose che non è il problema di questo problema speciale che non lo fa il Dicke dell<unk> Eisla.
2025-05-29 23:41:39,096 - INFO - joeynmt.training - Example #2
2025-05-29 23:41:39,096 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:41:39,096 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:41:39,096 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'di', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'il', 'cuore', 'del', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:41:39,097 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:41:39,097 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:41:39,097 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la Eiskappe di Eiskappe il cuore del nostro sistema globale.
2025-05-29 23:41:39,097 - INFO - joeynmt.training - Example #3
2025-05-29 23:41:39,097 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:41:39,097 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:41:39,097 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', 'è', 'una', 'cosa', 'che', 'si', 'è', 'in', 'sal@@', 'a@@', '.', '</s>']
2025-05-29 23:41:39,098 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:41:39,098 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:41:39,098 - INFO - joeynmt.training - 	Hypothesis: C<unk> è una cosa che si è in sala.
2025-05-29 23:41:39,098 - INFO - joeynmt.training - Example #4
2025-05-29 23:41:39,098 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:41:39,098 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:41:39,098 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:41:39,098 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:41:39,099 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:41:39,099 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva che vi mostro è un disegno di quello che è successo negli ultimi 25 anni.
2025-05-29 23:41:42,566 - INFO - joeynmt.training - Epoch   3, Step:    25100, Batch Loss:     2.080744, Batch Acc: 0.429159, Tokens per Sec:    18195, Lr: 0.000300
2025-05-29 23:41:46,048 - INFO - joeynmt.training - Epoch   3, Step:    25200, Batch Loss:     2.098598, Batch Acc: 0.427633, Tokens per Sec:    20293, Lr: 0.000300
2025-05-29 23:41:49,506 - INFO - joeynmt.training - Epoch   3, Step:    25300, Batch Loss:     2.122397, Batch Acc: 0.424846, Tokens per Sec:    18935, Lr: 0.000300
2025-05-29 23:41:52,971 - INFO - joeynmt.training - Epoch   3, Step:    25400, Batch Loss:     1.899125, Batch Acc: 0.436050, Tokens per Sec:    19826, Lr: 0.000300
2025-05-29 23:41:56,446 - INFO - joeynmt.training - Epoch   3, Step:    25500, Batch Loss:     2.106807, Batch Acc: 0.429181, Tokens per Sec:    20306, Lr: 0.000300
2025-05-29 23:41:56,446 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:41:56,446 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:42:04,919 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.99, ppl:   7.34, acc:   0.45, generation: 8.4622[sec], evaluation: 0.0000[sec]
2025-05-29 23:42:05,299 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/23000.ckpt
2025-05-29 23:42:05,324 - INFO - joeynmt.training - Example #0
2025-05-29 23:42:05,324 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:42:05,324 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:42:05,324 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'è', 'stata', 'l@@', '<unk>', 'E@@', 'is@@', 'co@@', ',', 'che', 'la', 'mat@@', 'eria', 'ar@@', 't@@', 'ica', 'ar@@', 'ica', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'k@@', 'e@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'i', 'primi', '4@@', '8', 'stati', 'per', 'cento', 'dei', 'li@@', 'velli', 'di', 'più', 'gran@@', 'de@@', '.', '</s>']
2025-05-29 23:42:05,325 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:42:05,325 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:42:05,325 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive che è stata l<unk> Eisco, che la materia artica arica dell<unk> Eiske, per tre milioni di anni per i primi 48 stati per cento dei livelli di più grande.
2025-05-29 23:42:05,325 - INFO - joeynmt.training - Example #1
2025-05-29 23:42:05,325 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:42:05,325 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:42:05,325 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'il', 'punto', 'di', 'vista', 'abbastanza', 'la', 'più', 'grande', 'parte', 'di', 'questo', 'speci@@', 'fico', 'che', 'non', 'c@@', '<unk>', 'è', 'il', 'D@@', 'ic@@', 'ke', 'E@@', 'is@@', 'es', 'che', 'non', 'è', 'il', 'D@@', 'ic@@', 'ke', 'E@@', 'is@@', 'es', 'che', 'è', 'il', 'D@@', 'ic@@', 'ke', 'E@@', 'is@@', 'es', 'che', 'è', 'il', 'ter@@', 'ren@@', 'o', 'del', 'ghi@@', 'accio', 'che', 'si', 'conto', 'di', 'come', 'il', 'ghi@@', 'accio', 'è', 'stato', 'fatto', 'a', 'un', 'po@@', '<unk>', 'di', 'un', 'po@@', '<unk>', 'di', 'queste', 'cos@@', 'e@@', '.', '</s>']
2025-05-29 23:42:05,325 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:42:05,325 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:42:05,326 - INFO - joeynmt.training - 	Hypothesis: Ma non è il punto di vista abbastanza la più grande parte di questo specifico che non c<unk> è il Dicke Eises che non è il Dicke Eises che è il Dicke Eises che è il terreno del ghiaccio che si conto di come il ghiaccio è stato fatto a un po<unk> di un po<unk> di queste cose.
2025-05-29 23:42:05,326 - INFO - joeynmt.training - Example #2
2025-05-29 23:42:05,326 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:42:05,326 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:42:05,326 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'di', 'un', 'c@@', 'aff@@', 'è', 'l@@', '<unk>', 'ar@@', 'co', 'di', 'un', 'cuore', 'di', 'c@@', 'aff@@', 'è', 'il', 'cuore', 'del', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:42:05,326 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:42:05,326 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:42:05,326 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cuore di un caffè l<unk> arco di un cuore di caffè il cuore del nostro sistema globale.
2025-05-29 23:42:05,326 - INFO - joeynmt.training - Example #3
2025-05-29 23:42:05,327 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:42:05,327 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:42:05,327 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'or@@', 'a@@', ',', 'e', 'poi', 'si', 'è', 'ri@@', 'ma@@', 'sta', 'in', 'S@@', 'om@@', '.', '</s>']
2025-05-29 23:42:05,327 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:42:05,327 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:42:05,327 - INFO - joeynmt.training - 	Hypothesis: L<unk> ora, e poi si è rimasta in Som.
2025-05-29 23:42:05,327 - INFO - joeynmt.training - Example #4
2025-05-29 23:42:05,328 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:42:05,328 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:42:05,328 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:42:05,328 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:42:05,328 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:42:05,328 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva che vi mostrerò è un disegno di quello che è successo negli ultimi 25 anni.
2025-05-29 23:42:08,771 - INFO - joeynmt.training - Epoch   3, Step:    25600, Batch Loss:     2.027997, Batch Acc: 0.424817, Tokens per Sec:    17997, Lr: 0.000300
2025-05-29 23:42:12,188 - INFO - joeynmt.training - Epoch   3, Step:    25700, Batch Loss:     2.176764, Batch Acc: 0.421843, Tokens per Sec:    19835, Lr: 0.000300
2025-05-29 23:42:15,603 - INFO - joeynmt.training - Epoch   3, Step:    25800, Batch Loss:     1.985069, Batch Acc: 0.425573, Tokens per Sec:    20387, Lr: 0.000300
2025-05-29 23:42:19,012 - INFO - joeynmt.training - Epoch   3, Step:    25900, Batch Loss:     2.154996, Batch Acc: 0.431205, Tokens per Sec:    19752, Lr: 0.000300
2025-05-29 23:42:22,437 - INFO - joeynmt.training - Epoch   3, Step:    26000, Batch Loss:     2.228350, Batch Acc: 0.428801, Tokens per Sec:    19853, Lr: 0.000300
2025-05-29 23:42:22,437 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:42:22,437 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:42:30,516 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.98, ppl:   7.27, acc:   0.45, generation: 8.0710[sec], evaluation: 0.0000[sec]
2025-05-29 23:42:30,516 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:42:31,052 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/24000.ckpt
2025-05-29 23:42:31,077 - INFO - joeynmt.training - Example #0
2025-05-29 23:42:31,077 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:42:31,077 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:42:31,077 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'ho', 'mostr@@', 'ato', 'che', 'i', 'l@@', '<unk>', 'E@@', 'is@@', 'co@@', ',', 'che', 'i', 'con@@', 'fron@@', 'ti', 'che', 'i', 'con@@', 'fron@@', 'to', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'pes@@', 'ca', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'pes@@', 'ca', 'che', 'ha', '4@@', '8', 'stati', 'in', 'grado', 'di', 'fare', 'il', '4@@', '8', 'stati', 'per', 'cento', 'di', 'questi', 'gra@@', 'di', 'sono', 'ri@@', 'ma@@', 'sto', 'per', 'la', 'maggior', 'parte', 'dei', 'nostri', 's@@', 'for@@', 'zi', 'sono', 'in', 'grado', 'di', 'ri@@', 'fu@@', 'gi@@', 're', 'il', '4@@', '8', 'stati', 'in', 'grado', 'di', 'in@@', 'fer@@', 'i@@', 'ore', 'di', 'un', 'po@@', '<unk>', 'di', 'l@@']
2025-05-29 23:42:31,077 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:42:31,077 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:42:31,077 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive che ho mostrato che i l<unk> Eisco, che i confronti che i confronto per tre milioni di anni di pesca per tre milioni di anni di pesca che ha 48 stati in grado di fare il 48 stati per cento di questi gradi sono rimasto per la maggior parte dei nostri sforzi sono in grado di rifugire il 48 stati in grado di inferiore di un po<unk> di l
2025-05-29 23:42:31,078 - INFO - joeynmt.training - Example #1
2025-05-29 23:42:31,078 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:42:31,078 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:42:31,078 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'la', 'più', 'grande', 'grande', 'è', 'la', 'più', 'grande', 'grande', 'sost@@', 'eni@@', 'bilità', 'di', 'questo', 'probl@@', 'em@@', 'a@@', ',', 'non', 'è', 'il', 'd@@', 'ic@@', 'ke', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'a@@', '.', '</s>']
2025-05-29 23:42:31,078 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:42:31,078 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:42:31,078 - INFO - joeynmt.training - 	Hypothesis: Ma non è la più grande grande è la più grande grande sostenibilità di questo problema, non è il dicke dell<unk> Eisla.
2025-05-29 23:42:31,078 - INFO - joeynmt.training - Example #2
2025-05-29 23:42:31,078 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:42:31,078 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:42:31,078 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'ad@@', 'uta', 'ar@@', 't@@', 'ica', 'è', 'la', 'c@@', 'att@@', 'ica', 'del', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:42:31,079 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:42:31,079 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:42:31,079 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la caduta artica è la cattica del nostro sistema globale.
2025-05-29 23:42:31,079 - INFO - joeynmt.training - Example #3
2025-05-29 23:42:31,079 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:42:31,079 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:42:31,079 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'altra', 'cosa', 'che', 'si', 'trova', 'nel', 's@@', 'om@@', 'in@@', 'o@@', ',', 'e', 'la', 'sua', 's@@', 'om@@', 'ba', 'e', 'la', 'sua', 's@@', 'om@@', 'ba', 'e', 'la', 'sua', 's@@', 'om@@', 'ba', 'e', 'la', 'sua', 'm@@', 'am@@', 'ma@@', '.', '</s>']
2025-05-29 23:42:31,079 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:42:31,080 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:42:31,080 - INFO - joeynmt.training - 	Hypothesis: L<unk> altra cosa che si trova nel somino, e la sua somba e la sua somba e la sua somba e la sua mamma.
2025-05-29 23:42:31,080 - INFO - joeynmt.training - Example #4
2025-05-29 23:42:31,080 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:42:31,080 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:42:31,080 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'che', 'vi', 'mostr@@', 'o', 'è', 'una', 'cosa', 'che', 'vi', 'mostr@@', 'o', 'è', 'che', 'il', 'lavoro', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:42:31,080 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:42:31,080 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:42:31,080 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza che vi mostro è una cosa che vi mostro è che il lavoro negli ultimi 25 anni.
2025-05-29 23:42:34,479 - INFO - joeynmt.training - Epoch   3, Step:    26100, Batch Loss:     2.269937, Batch Acc: 0.429685, Tokens per Sec:    17166, Lr: 0.000300
2025-05-29 23:42:37,873 - INFO - joeynmt.training - Epoch   3, Step:    26200, Batch Loss:     2.076581, Batch Acc: 0.440754, Tokens per Sec:    20723, Lr: 0.000300
2025-05-29 23:42:41,260 - INFO - joeynmt.training - Epoch   3, Step:    26300, Batch Loss:     1.849898, Batch Acc: 0.432363, Tokens per Sec:    21161, Lr: 0.000300
2025-05-29 23:42:44,603 - INFO - joeynmt.training - Epoch   3, Step:    26400, Batch Loss:     1.929646, Batch Acc: 0.427924, Tokens per Sec:    19810, Lr: 0.000300
2025-05-29 23:42:47,914 - INFO - joeynmt.training - Epoch   3, Step:    26500, Batch Loss:     2.138871, Batch Acc: 0.430538, Tokens per Sec:    19685, Lr: 0.000300
2025-05-29 23:42:47,914 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:42:47,914 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:42:54,960 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.98, ppl:   7.27, acc:   0.45, generation: 7.0391[sec], evaluation: 0.0000[sec]
2025-05-29 23:42:55,321 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/23500.ckpt
2025-05-29 23:42:55,348 - INFO - joeynmt.training - Example #0
2025-05-29 23:42:55,349 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:42:55,349 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:42:55,349 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'la', 'c@@', 'ad@@', 'uta', 'ar@@', 'ar@@', 't@@', 'a@@', ',', 'che', 'la', 'mat@@', 'eria', 'ar@@', 't@@', 'ica', 'che', 'i', 'primi', 'anni', 'di', '4@@', '8', 'stati', 'per', 'cento', 'di', 'questi', 'sono', 'stati', 'in', 'grado', 'di', 'vedere', 'il', '4@@', '8', 'stati', 'per', 'cento', 'di', 'questi', '4@@', '8', 'stati', 'per', 'cento', 'per', 'la', 'maggior', 'parte', 'del', '4@@', '8', 'stati', 'per', 'cento', 'di', 'questi', 'sono', 'stati', 'in', 'grado', 'di', 'vedere', 'il', '4@@', '8', 'stati', 'in', 'grado', 'di', 'vedere', 'il', '4@@', '8', 'stati', 'per', 'cento', 'di', 'questi', 'sono', 'stati', 'in', 'grado', 'di', 'vedere', 'il', 'suo', 's@@', 'ov@@', 'rap@@', 'porto', 'di']
2025-05-29 23:42:55,349 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:42:55,349 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:42:55,349 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive che la caduta ararta, che la materia artica che i primi anni di 48 stati per cento di questi sono stati in grado di vedere il 48 stati per cento di questi 48 stati per cento per la maggior parte del 48 stati per cento di questi sono stati in grado di vedere il 48 stati in grado di vedere il 48 stati per cento di questi sono stati in grado di vedere il suo sovrapporto di
2025-05-29 23:42:55,349 - INFO - joeynmt.training - Example #1
2025-05-29 23:42:55,349 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:42:55,350 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:42:55,350 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'più', 'for@@', 'te@@', ',', 'che', 'non', 'è', 'la', 'più', 'grande', 'di', 'questo', 'probl@@', 'em@@', 'a@@', ',', 'perché', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'a@@', '.', '</s>']
2025-05-29 23:42:55,350 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:42:55,350 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:42:55,350 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto più forte, che non è la più grande di questo problema, perché non è il dell<unk> Eisla.
2025-05-29 23:42:55,350 - INFO - joeynmt.training - Example #2
2025-05-29 23:42:55,350 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:42:55,350 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:42:55,350 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'b@@', 'ella', 'è', 'la', 'c@@', 'op@@', 'pia', 'di', 'cal@@', 'col@@', 'o@@', ',', 'la', 'c@@', 'op@@', 'pia', 'di', 'un', 'sistema', 'di', 'c@@', 'lin@@', 'ic@@', 'a@@', '.', '</s>']
2025-05-29 23:42:55,351 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:42:55,351 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:42:55,351 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la bella è la coppia di calcolo, la coppia di un sistema di clinica.
2025-05-29 23:42:55,351 - INFO - joeynmt.training - Example #3
2025-05-29 23:42:55,351 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:42:55,351 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:42:55,351 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'in@@', 'ver@@', 'no@@', ',', 'e', 'si', 'è', 'ri@@', 'ma@@', 'sta', 'in', 's@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:42:55,352 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:42:55,352 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:42:55,352 - INFO - joeynmt.training - 	Hypothesis: L<unk> inverno, e si è rimasta in sommer.
2025-05-29 23:42:55,352 - INFO - joeynmt.training - Example #4
2025-05-29 23:42:55,352 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:42:55,352 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:42:55,352 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 'seg@@', 'no', 'di', 'un', 'seg@@', 'no', 'di', 'un', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:42:55,353 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:42:55,353 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:42:55,353 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva che vi mostrerò è un segno di un segno di un disegno di quello che è successo negli ultimi 25 anni.
2025-05-29 23:42:58,741 - INFO - joeynmt.training - Epoch   3, Step:    26600, Batch Loss:     2.126970, Batch Acc: 0.429155, Tokens per Sec:    18602, Lr: 0.000300
2025-05-29 23:43:02,117 - INFO - joeynmt.training - Epoch   3, Step:    26700, Batch Loss:     2.050609, Batch Acc: 0.430744, Tokens per Sec:    20401, Lr: 0.000300
2025-05-29 23:43:05,496 - INFO - joeynmt.training - Epoch   3, Step:    26800, Batch Loss:     2.008025, Batch Acc: 0.435410, Tokens per Sec:    20594, Lr: 0.000300
2025-05-29 23:43:08,835 - INFO - joeynmt.training - Epoch   3, Step:    26900, Batch Loss:     2.201657, Batch Acc: 0.431333, Tokens per Sec:    20178, Lr: 0.000300
2025-05-29 23:43:12,188 - INFO - joeynmt.training - Epoch   3, Step:    27000, Batch Loss:     2.064308, Batch Acc: 0.428869, Tokens per Sec:    20785, Lr: 0.000300
2025-05-29 23:43:12,188 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:43:12,189 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:43:18,807 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.97, ppl:   7.14, acc:   0.46, generation: 6.6120[sec], evaluation: 0.0000[sec]
2025-05-29 23:43:18,808 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:43:19,346 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/25000.ckpt
2025-05-29 23:43:19,367 - INFO - joeynmt.training - Example #0
2025-05-29 23:43:19,368 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:43:19,368 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:43:19,368 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'questa', 'due', 'consegu@@', 'enze', 'di', 'queste', 'due', 'consegu@@', 'enze', 'ar@@', 'ch@@', 'e@@', ',', 'per', 'vedere', 'che', 'il', 'ghi@@', 'acci@@', 'aio', 'di', 'anni', 'di', 'pes@@', 'ca', 'che', 'è', 'stata', 'la', 'ver@@', 'sione', 'd@@', 'ell@@', '<unk>', 'in@@', 'ver@@', 'no', 'di', 'di', 'circa', '4@@', '8', 'stati', 'per', 'cento', 'milioni', 'di', 'anni', 'per', 'raggi@@', 'ungere', 'il', '40', 'per', 'cento', 'di', 'anni', 'per', 'il', '40', 'per', 'cento', 'di', 'questi', 'sono', 'stati', 'stati', 'in', 'grado', 'di', 'fare', 'il', '4@@', '8', 'stati', 'in', 'grado', 'di', 'fare', 'il', 'nostro', 'sistema', 'di', 's@@', 'quad@@', 'ra', 'di', 'un', 'di@@', 'stri@@', 'bu@@', 'ito', 'a', 'che', 'è', 'stata', 'sc@@', 'att@@', 'ata', 'per', 'il']
2025-05-29 23:43:19,368 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:43:19,369 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:43:19,369 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato questa due conseguenze di queste due conseguenze arche, per vedere che il ghiacciaio di anni di pesca che è stata la versione dell<unk> inverno di di circa 48 stati per cento milioni di anni per raggiungere il 40 per cento di anni per il 40 per cento di questi sono stati stati in grado di fare il 48 stati in grado di fare il nostro sistema di squadra di un distribuito a che è stata scattata per il
2025-05-29 23:43:19,369 - INFO - joeynmt.training - Example #1
2025-05-29 23:43:19,369 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:43:19,369 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:43:19,369 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'questo', 'è', 'il', 'fatto', 'che', 'non', 'è', 'più', 'for@@', 'te@@', ',', 'ma', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'and@@', 'o@@', '.', '</s>']
2025-05-29 23:43:19,369 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:43:19,370 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:43:19,370 - INFO - joeynmt.training - 	Hypothesis: Ma non questo è il fatto che non è più forte, ma non è il dell<unk> Eislando.
2025-05-29 23:43:19,370 - INFO - joeynmt.training - Example #2
2025-05-29 23:43:19,370 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:43:19,370 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:43:19,370 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'ar@@', 'co', 'ar@@', 'ric@@', 'co', 'ar@@', 'ic@@', 'o@@', ',', 'il', 'cuore', 'della', 'nostra', 'c@@', 'lasse', 'di', 'c@@', 'lin@@', 'ic@@', 'a@@', '.', '</s>']
2025-05-29 23:43:19,370 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:43:19,370 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:43:19,371 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cuore arco arricco arico, il cuore della nostra classe di clinica.
2025-05-29 23:43:19,371 - INFO - joeynmt.training - Example #3
2025-05-29 23:43:19,371 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:43:19,371 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:43:19,371 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', 'è', 'una', 'cosa', 'che', 'si', 'è', 'ri@@', 'ma@@', 'sto', 'in@@', 'gen@@', 'do', 'e', 's@@', 'ì@@', '.', '</s>']
2025-05-29 23:43:19,371 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:43:19,371 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:43:19,371 - INFO - joeynmt.training - 	Hypothesis: C<unk> è una cosa che si è rimasto ingendo e sì.
2025-05-29 23:43:19,371 - INFO - joeynmt.training - Example #4
2025-05-29 23:43:19,372 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:43:19,372 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:43:19,372 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'è', 'una', 'cosa', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 'seg@@', 'ret@@', 'o', 'che', 'è', 'succ@@', 'ess@@', 'o@@', '.', '</s>']
2025-05-29 23:43:19,372 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:43:19,372 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:43:19,372 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza è una cosa che vi mostrerò è un segreto che è successo.
2025-05-29 23:43:22,778 - INFO - joeynmt.training - Epoch   3, Step:    27100, Batch Loss:     1.954242, Batch Acc: 0.440903, Tokens per Sec:    17386, Lr: 0.000300
2025-05-29 23:43:26,135 - INFO - joeynmt.training - Epoch   3, Step:    27200, Batch Loss:     1.996295, Batch Acc: 0.426644, Tokens per Sec:    20811, Lr: 0.000300
2025-05-29 23:43:29,536 - INFO - joeynmt.training - Epoch   3, Step:    27300, Batch Loss:     1.890183, Batch Acc: 0.434041, Tokens per Sec:    20010, Lr: 0.000300
2025-05-29 23:43:32,948 - INFO - joeynmt.training - Epoch   3, Step:    27400, Batch Loss:     2.222405, Batch Acc: 0.426347, Tokens per Sec:    19318, Lr: 0.000300
2025-05-29 23:43:36,407 - INFO - joeynmt.training - Epoch   3, Step:    27500, Batch Loss:     2.050694, Batch Acc: 0.432099, Tokens per Sec:    20307, Lr: 0.000300
2025-05-29 23:43:36,408 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:43:36,408 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:43:45,353 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.97, ppl:   7.17, acc:   0.46, generation: 8.9385[sec], evaluation: 0.0000[sec]
2025-05-29 23:43:45,707 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/25500.ckpt
2025-05-29 23:43:45,732 - INFO - joeynmt.training - Example #0
2025-05-29 23:43:45,733 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:43:45,733 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:43:45,733 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'due', 'due', 'consegu@@', 'enze', 'di', 'questo', 'tipo', 'di', 'el@@', 'abor@@', 'azione', 'ar@@', 'ric@@', 'o@@', 'stru@@', 'ire', 'il', 'ghi@@', 'acci@@', 'o@@', ',', 'che', 'è', 'stato', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'i', 'li@@', 'velli', 'di', '4@@', '8', 'stati', 'per', 'cento', 'anni', 'i', 'con@@', 'fl@@', 'it@@', 'ti', 'd@@', 'ell@@', '<unk>', '4@@', '8', 'stati', 'per', 'cento', 'dei', 'li@@', 'velli', 'di', '4@@', '8', 'stati', 'per', 'cento', 'di', 'questi', 'due', 's@@', 'ov@@', 'rap@@', 'por@@', 'ti', 'e', 'il', '40', 'per', 'cento', 'di', 'questi', 'due', 's@@', 'ov@@', 'ran@@', 'i@@', ',', 'e', 'il', '4@@', '8', 'per', 'cento', 'di', 'questi', 'due', 's@@', 'ov@@', 'rap@@', 'por@@', 'ti', 'per', 'il']
2025-05-29 23:43:45,733 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:43:45,733 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:43:45,733 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due due due conseguenze di questo tipo di elaborazione arricostruire il ghiaccio, che è stato fatto per tre milioni di anni i livelli di 48 stati per cento anni i conflitti dell<unk> 48 stati per cento dei livelli di 48 stati per cento di questi due sovrapporti e il 40 per cento di questi due sovrani, e il 48 per cento di questi due sovrapporti per il
2025-05-29 23:43:45,734 - INFO - joeynmt.training - Example #1
2025-05-29 23:43:45,734 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:43:45,734 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:43:45,734 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'più', 'di', 'quanto', 'la', 'ter@@', 'za', 'cosa', 'più', 'importante', 'per', 'la', 'prima', 'volta', 'che', 'non', 'lo', 'ha', 'fatto', 'di', 'non', 'è', 'il', 't@@', 'asso', 'di', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-29 23:43:45,734 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:43:45,734 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:43:45,734 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto più di quanto la terza cosa più importante per la prima volta che non lo ha fatto di non è il tasso di ghiaccio.
2025-05-29 23:43:45,734 - INFO - joeynmt.training - Example #2
2025-05-29 23:43:45,735 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:43:45,735 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:43:45,735 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cal@@', 'colo', 'della', 'c@@', 'ass@@', 'etta', 'di', 'ghi@@', 'accio', 'che', 'si', 'chiama', 'il', 'cuore', 'del', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:43:45,735 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:43:45,735 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:43:45,735 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il calcolo della cassetta di ghiaccio che si chiama il cuore del nostro sistema globale.
2025-05-29 23:43:45,735 - INFO - joeynmt.training - Example #3
2025-05-29 23:43:45,736 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:43:45,736 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:43:45,736 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'abbiamo', 'fatto', 'di', 'sc@@', 'al@@', 'a@@', ',', 'e', 'si', 'ri@@', 'du@@', 'ce', 'n@@', '<unk>', 'è', 'un', 't@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:43:45,736 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:43:45,736 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:43:45,736 - INFO - joeynmt.training - 	Hypothesis: L<unk> abbiamo fatto di scala, e si riduce n<unk> è un tommer.
2025-05-29 23:43:45,736 - INFO - joeynmt.training - Example #4
2025-05-29 23:43:45,736 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:43:45,737 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:43:45,737 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'è', 'una', 'cosa', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:43:45,737 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:43:45,737 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:43:45,737 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza è una cosa che vi mostro è un disegno di quello che negli ultimi 25 anni.
2025-05-29 23:43:49,180 - INFO - joeynmt.training - Epoch   3, Step:    27600, Batch Loss:     2.002824, Batch Acc: 0.433190, Tokens per Sec:    18316, Lr: 0.000300
2025-05-29 23:43:52,641 - INFO - joeynmt.training - Epoch   3, Step:    27700, Batch Loss:     2.218372, Batch Acc: 0.437149, Tokens per Sec:    20070, Lr: 0.000300
2025-05-29 23:43:56,091 - INFO - joeynmt.training - Epoch   3, Step:    27800, Batch Loss:     1.940484, Batch Acc: 0.431362, Tokens per Sec:    19623, Lr: 0.000300
2025-05-29 23:43:59,576 - INFO - joeynmt.training - Epoch   3, Step:    27900, Batch Loss:     2.028042, Batch Acc: 0.436642, Tokens per Sec:    20033, Lr: 0.000300
2025-05-29 23:44:03,052 - INFO - joeynmt.training - Epoch   3, Step:    28000, Batch Loss:     2.064234, Batch Acc: 0.430258, Tokens per Sec:    20037, Lr: 0.000300
2025-05-29 23:44:03,052 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:44:03,052 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:44:12,613 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.95, ppl:   7.04, acc:   0.46, generation: 9.5507[sec], evaluation: 0.0000[sec]
2025-05-29 23:44:12,614 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:44:13,235 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/24500.ckpt
2025-05-29 23:44:13,263 - INFO - joeynmt.training - Example #0
2025-05-29 23:44:13,264 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:44:13,264 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:44:13,264 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'consegu@@', 'enze', 'per', 'vedere', 'che', 'le', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'di', 'tre', 'milioni', 'di', 'anni', 'di', 'c@@', 'att@@', 'ur@@', 'are', 'le', 'n@@', 'azioni', 'ar@@', 'ch@@', 'e@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'per', '4@@', '8', 'stati', 'degli', 'anni', '4@@', '8', 'stati', 'per', 'cento', 'di', '4@@', '8', 'stati', 'per', 'cento', 'di', '4@@', '8', 'stati', 'per', 'cento', 'di', 'qu@@', 'arti@@', 'eri', 'per', 'il', '40', 'per', 'cento', 'di', 'questi', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'due']
2025-05-29 23:44:13,265 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:44:13,265 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:44:13,265 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due conseguenze per vedere che le due due due due due due due due due due di tre milioni di anni di catturare le nazioni arche, per tre milioni di anni per 48 stati degli anni 48 stati per cento di 48 stati per cento di 48 stati per cento di quartieri per il 40 per cento di questi due due due due due due due due due due due due due due due due due due due due due due due due due due due
2025-05-29 23:44:13,265 - INFO - joeynmt.training - Example #1
2025-05-29 23:44:13,265 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:44:13,265 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:44:13,265 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'il', 'problema', 'di', 'quanto', 'la', 'più', 'grande', 'grande', 'quantità', 'di', 'problemi', 'speci@@', 'ali@@', ',', 'perché', 'non', 'lo', 'ha', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'a@@', '.', '</s>']
2025-05-29 23:44:13,266 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:44:13,266 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:44:13,266 - INFO - joeynmt.training - 	Hypothesis: Ma non è il problema di quanto la più grande grande quantità di problemi speciali, perché non lo ha il dell<unk> Eisla.
2025-05-29 23:44:13,266 - INFO - joeynmt.training - Example #2
2025-05-29 23:44:13,266 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:44:13,266 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:44:13,266 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'della', 'C@@', 'aliforni@@', 'a', 'è', 'il', 'nostro', 'cuore', 'di', 'un', 'sistema', 'di', 'car@@', 'ico', 'del', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:44:13,267 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:44:13,267 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:44:13,267 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cuore della California è il nostro cuore di un sistema di carico del nostro sistema globale.
2025-05-29 23:44:13,267 - INFO - joeynmt.training - Example #3
2025-05-29 23:44:13,267 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:44:13,267 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:44:13,267 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Pri@@', 'ma', 'di', 'tut@@', 'to@@', ',', 'e', 'poi', 'si', 'è', 's@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:44:13,267 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:44:13,268 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:44:13,268 - INFO - joeynmt.training - 	Hypothesis: Prima di tutto, e poi si è sommer.
2025-05-29 23:44:13,268 - INFO - joeynmt.training - Example #4
2025-05-29 23:44:13,268 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:44:13,268 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:44:13,268 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'è', 'una', 'cosa', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:44:13,268 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:44:13,268 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:44:13,268 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza è una cosa che vi mostro è un disegno di quello che negli ultimi 25 anni.
2025-05-29 23:44:16,762 - INFO - joeynmt.training - Epoch   3, Step:    28100, Batch Loss:     2.013903, Batch Acc: 0.430278, Tokens per Sec:    16872, Lr: 0.000300
2025-05-29 23:44:20,224 - INFO - joeynmt.training - Epoch   3, Step:    28200, Batch Loss:     2.142612, Batch Acc: 0.434382, Tokens per Sec:    20545, Lr: 0.000300
2025-05-29 23:44:22,780 - INFO - joeynmt.training - Epoch   3: total training loss 19668.57
2025-05-29 23:44:22,781 - INFO - joeynmt.training - EPOCH 4
2025-05-29 23:44:23,678 - INFO - joeynmt.training - Epoch   4, Step:    28300, Batch Loss:     2.196463, Batch Acc: 0.448793, Tokens per Sec:    19373, Lr: 0.000300
2025-05-29 23:44:27,112 - INFO - joeynmt.training - Epoch   4, Step:    28400, Batch Loss:     1.842168, Batch Acc: 0.445096, Tokens per Sec:    19329, Lr: 0.000300
2025-05-29 23:44:30,525 - INFO - joeynmt.training - Epoch   4, Step:    28500, Batch Loss:     1.890079, Batch Acc: 0.446636, Tokens per Sec:    19979, Lr: 0.000300
2025-05-29 23:44:30,526 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:44:30,526 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:44:39,914 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.96, ppl:   7.11, acc:   0.46, generation: 9.3751[sec], evaluation: 0.0000[sec]
2025-05-29 23:44:40,364 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/26500.ckpt
2025-05-29 23:44:40,393 - INFO - joeynmt.training - Example #0
2025-05-29 23:44:40,393 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:44:40,394 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:44:40,394 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'l@@', '<unk>', 'el@@', 'ef@@', 'ante', 'per', 'vedere', 'che', 'l@@', '<unk>', 'el@@', 'ef@@', 'ante', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'ri@@', 'fer@@', 'imento', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'gli', 'anni', 'di', '4@@', '8', 'stati', 'dei', '4@@', '8', 'stati', 'dei', '4@@', '8', 'per', 'cento', 'di', 'circa', '4@@', '8', 'per', 'cento', 'per', 'cento', 'di', 'questi', 'sono', 'stati', 'in', 'grado', 'di', 'ri@@', 'fer@@', 'imento', 'per', 'il', '40', 'per', 'cento', 'dei', 'li@@', 'velli', 'di', 'ri@@', 'fer@@', 'imento', 'di', 'un', 'po@@', '<unk>', '.', '</s>']
2025-05-29 23:44:40,394 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:44:40,394 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:44:40,394 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive che l<unk> elefante per vedere che l<unk> elefante per tre milioni di anni di riferimento che ha fatto per tre milioni di anni gli anni di 48 stati dei 48 stati dei 48 per cento di circa 48 per cento per cento di questi sono stati in grado di riferimento per il 40 per cento dei livelli di riferimento di un po<unk> .
2025-05-29 23:44:40,394 - INFO - joeynmt.training - Example #1
2025-05-29 23:44:40,395 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:44:40,395 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:44:40,395 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'più', 'for@@', 'te@@', ',', 'non', 'è', 'più', 'for@@', 'te@@', ',', 'ma', 'non', 'è', 'la', 'cosa', 'più', 'importante', 'per', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'amento', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-29 23:44:40,395 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:44:40,395 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:44:40,395 - INFO - joeynmt.training - 	Hypothesis: Ma non è più forte, non è più forte, ma non è la cosa più importante per la dimensione dell<unk> Eislamento del ghiaccio.
2025-05-29 23:44:40,395 - INFO - joeynmt.training - Example #2
2025-05-29 23:44:40,396 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:44:40,396 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:44:40,396 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'att@@', 'ur@@', 'azione', 'ar@@', 'ric@@', 'e', 'di', 'un', 'grande', 'sistema', 'di', 'ri@@', 'fer@@', 'imento', 'di', 'un', 'sistema', 'globale', 'del', 'nostro', 'sistema', 'globale', 'globale', 'del', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:44:40,396 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:44:40,396 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:44:40,396 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la catturazione arrice di un grande sistema di riferimento di un sistema globale del nostro sistema globale globale del nostro sistema globale.
2025-05-29 23:44:40,396 - INFO - joeynmt.training - Example #3
2025-05-29 23:44:40,397 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:44:40,397 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:44:40,397 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', 'è', 'un', 't@@', 'asso', 'di', 's@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:44:40,397 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:44:40,397 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:44:40,397 - INFO - joeynmt.training - 	Hypothesis: C<unk> è un tasso di sommer.
2025-05-29 23:44:40,397 - INFO - joeynmt.training - Example #4
2025-05-29 23:44:40,397 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:44:40,397 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:44:40,397 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 'seg@@', 'ret@@', 'o', 'di', 'quello', 'che', 'succede', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:44:40,398 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:44:40,398 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:44:40,398 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è un segreto di quello che succede negli ultimi 25 anni.
2025-05-29 23:44:43,870 - INFO - joeynmt.training - Epoch   4, Step:    28600, Batch Loss:     2.142239, Batch Acc: 0.450268, Tokens per Sec:    17274, Lr: 0.000300
2025-05-29 23:44:47,335 - INFO - joeynmt.training - Epoch   4, Step:    28700, Batch Loss:     2.124935, Batch Acc: 0.449376, Tokens per Sec:    19789, Lr: 0.000300
2025-05-29 23:44:50,789 - INFO - joeynmt.training - Epoch   4, Step:    28800, Batch Loss:     1.905303, Batch Acc: 0.444865, Tokens per Sec:    20358, Lr: 0.000300
2025-05-29 23:44:54,239 - INFO - joeynmt.training - Epoch   4, Step:    28900, Batch Loss:     2.188875, Batch Acc: 0.440723, Tokens per Sec:    20186, Lr: 0.000300
2025-05-29 23:44:57,696 - INFO - joeynmt.training - Epoch   4, Step:    29000, Batch Loss:     1.866377, Batch Acc: 0.446375, Tokens per Sec:    19902, Lr: 0.000300
2025-05-29 23:44:57,696 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:44:57,696 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:45:05,838 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.96, ppl:   7.09, acc:   0.46, generation: 8.1310[sec], evaluation: 0.0000[sec]
2025-05-29 23:45:06,297 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/26000.ckpt
2025-05-29 23:45:06,325 - INFO - joeynmt.training - Example #0
2025-05-29 23:45:06,325 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:45:06,325 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:45:06,326 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'vedere', 'che', 'le', 'cose', 'che', 'hanno', 'fatto', 'per', 'la', 'ri@@', 'du@@', 'zione', 'd@@', 'ell@@', '<unk>', 'acqua', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'l@@', '<unk>', 'anno', '4@@', '8', 'stati', 'per', 'cento', 'milioni', 'di', 'anni', 'per', 'ri@@', 'durre', 'il', '40', 'per', 'cento', 'di', 'anni', 'per', 'ri@@', 'durre', 'il', '40', 'per', 'cento', 'di', 'questi', 'due', 'due', 'due', 'anni', 'per', 'ri@@', 'durre', 'il', '40', 'per', 'cento', 'di', 'questi', 's@@', 'for@@', 'zi', 'sono', 'stati', 'in', 'grado', 'di', 'ri@@', 'durre', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'la', 'loro', 's@@', 'li@@', 'de', 'di', 'ri@@', 'durre', 'queste', 'due']
2025-05-29 23:45:06,326 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:45:06,326 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:45:06,326 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per vedere che le cose che hanno fatto per la riduzione dell<unk> acqua che ha fatto per tre milioni di anni per l<unk> anno 48 stati per cento milioni di anni per ridurre il 40 per cento di anni per ridurre il 40 per cento di questi due due due anni per ridurre il 40 per cento di questi sforzi sono stati in grado di ridurre queste due diapositive per la loro slide di ridurre queste due
2025-05-29 23:45:06,326 - INFO - joeynmt.training - Example #1
2025-05-29 23:45:06,327 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:45:06,327 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:45:06,327 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'il', 'più', 'grande', 'probl@@', 'em@@', 'a@@', ',', 'ma', 'non', 'è', 'il', 'problema', 'che', 'non', 'è', 'il', 'd@@', 'ic@@', 'o@@', ',', 'perché', 'non', 'è', 'il', 'd@@', 'ic@@', 'o@@', '.', '</s>']
2025-05-29 23:45:06,327 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:45:06,327 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:45:06,327 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è il più grande problema, ma non è il problema che non è il dico, perché non è il dico.
2025-05-29 23:45:06,327 - INFO - joeynmt.training - Example #2
2025-05-29 23:45:06,327 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:45:06,328 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:45:06,328 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'b@@', 'ella', 'mat@@', 'eria', 'ar@@', 'ica', 'di', 'ghi@@', 'acci@@', 'aio', 'del', 'nostro', 'sistema', 'c@@', 'lim@@', 'pi@@', 'a@@', '.', '</s>']
2025-05-29 23:45:06,328 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:45:06,328 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:45:06,328 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la bella materia arica di ghiacciaio del nostro sistema climpia.
2025-05-29 23:45:06,328 - INFO - joeynmt.training - Example #3
2025-05-29 23:45:06,328 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:45:06,328 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:45:06,328 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'di', 's@@', 'om@@', 'in@@', 'o@@', '.', '</s>']
2025-05-29 23:45:06,329 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:45:06,329 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:45:06,329 - INFO - joeynmt.training - 	Hypothesis: C<unk> è un po<unk> di somino.
2025-05-29 23:45:06,329 - INFO - joeynmt.training - Example #4
2025-05-29 23:45:06,329 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:45:06,329 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:45:06,329 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'v@@', 'a@@', ',', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'di@@', 'apos@@', 'iti@@', 'v@@', 'a@@', ',', 'che', 'è', 'acca@@', 'duto', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:45:06,330 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:45:06,330 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:45:06,330 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva, che vi mostrerò è una diapositiva, che è accaduto negli ultimi 25 anni.
2025-05-29 23:45:09,804 - INFO - joeynmt.training - Epoch   4, Step:    29100, Batch Loss:     1.874176, Batch Acc: 0.448485, Tokens per Sec:    17240, Lr: 0.000300
2025-05-29 23:45:13,264 - INFO - joeynmt.training - Epoch   4, Step:    29200, Batch Loss:     2.044049, Batch Acc: 0.440538, Tokens per Sec:    18974, Lr: 0.000300
2025-05-29 23:45:16,716 - INFO - joeynmt.training - Epoch   4, Step:    29300, Batch Loss:     1.848001, Batch Acc: 0.448741, Tokens per Sec:    19842, Lr: 0.000300
2025-05-29 23:45:20,169 - INFO - joeynmt.training - Epoch   4, Step:    29400, Batch Loss:     1.946771, Batch Acc: 0.446508, Tokens per Sec:    19632, Lr: 0.000300
2025-05-29 23:45:23,633 - INFO - joeynmt.training - Epoch   4, Step:    29500, Batch Loss:     1.978141, Batch Acc: 0.442510, Tokens per Sec:    20763, Lr: 0.000300
2025-05-29 23:45:23,633 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:45:23,633 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:45:31,406 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.95, ppl:   7.03, acc:   0.46, generation: 7.7618[sec], evaluation: 0.0000[sec]
2025-05-29 23:45:31,406 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:45:32,051 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/27500.ckpt
2025-05-29 23:45:32,072 - INFO - joeynmt.training - Example #0
2025-05-29 23:45:32,073 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:45:32,073 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:45:32,073 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'consegu@@', 'enze', 'per', 'vedere', 'che', 'l@@', '<unk>', 'et@@', 'i@@', 'ch@@', 'etta', 'che', 'l@@', '<unk>', 'et@@', 'i@@', 'ch@@', 'etta', 'che', 'l@@', '<unk>', 'et@@', 'i@@', 'ch@@', 'etta', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'l@@', '<unk>', 'in@@', 'dag@@', 'ine', 'di', 'di', 'circa', '4@@', '8', 'stati', 'per', 'cento', 'anni', 'per', 'il', '4@@', '8@@', '0@@', ',', 'per', 'il', '4@@', '8', 'stati', 'per', 'cento', 'di', 'anni', 'per', 'ri@@', 'durre', 'il', '4@@', '8', 'stati', 'per', 'cento', 'di', 'questi', 'gruppi', 'di', 'persone', 'che', 'hanno', 'fatto', 'il', '4@@', '8', 'stati', 'per', 'cento', 'di', 'questi', 'due', 's@@', 'ett@@', 'ori@@', '.', '</s>']
2025-05-29 23:45:32,074 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:45:32,074 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:45:32,074 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due conseguenze per vedere che l<unk> etichetta che l<unk> etichetta che l<unk> etichetta per tre milioni di anni per l<unk> indagine di di circa 48 stati per cento anni per il 480, per il 48 stati per cento di anni per ridurre il 48 stati per cento di questi gruppi di persone che hanno fatto il 48 stati per cento di questi due settori.
2025-05-29 23:45:32,074 - INFO - joeynmt.training - Example #1
2025-05-29 23:45:32,074 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:45:32,074 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:45:32,074 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'più', 'for@@', 'ti', 'che', 'la', 'più', 'grande', 'cosa', 'che', 'non', 'è', 'più', 'for@@', 'ti', 'di', 'questo', 'probl@@', 'em@@', 'a@@', ',', 'perché', 'non', 'è', 'il', 'seg@@', 'ret@@', 'o', 'del', 'ghi@@', 'accio', 'd@@', 'ell@@', '<unk>', 'em@@', 'a@@', '.', '</s>']
2025-05-29 23:45:32,075 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:45:32,075 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:45:32,075 - INFO - joeynmt.training - 	Hypothesis: Ma non è più forti che la più grande cosa che non è più forti di questo problema, perché non è il segreto del ghiaccio dell<unk> ema.
2025-05-29 23:45:32,075 - INFO - joeynmt.training - Example #2
2025-05-29 23:45:32,075 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:45:32,075 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:45:32,075 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'cosa', 'più', 'c@@', 'att@@', 'ur@@', 'a@@', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'di', 'c@@', 'lim@@', 'pi@@', 'a@@', '.', '</s>']
2025-05-29 23:45:32,076 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:45:32,076 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:45:32,076 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cosa più cattura, il cuore del nostro sistema di climpia.
2025-05-29 23:45:32,076 - INFO - joeynmt.training - Example #3
2025-05-29 23:45:32,076 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:45:32,076 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:45:32,076 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'cosa', 'che', 'si', 'fa', 'in', 'sal@@', 'a@@', ',', 'e', 'si', 'è', 'ri@@', 'ma@@', 'sta', 'in', 's@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:45:32,077 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:45:32,077 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:45:32,077 - INFO - joeynmt.training - 	Hypothesis: La prima cosa che si fa in sala, e si è rimasta in sommer.
2025-05-29 23:45:32,077 - INFO - joeynmt.training - Example #4
2025-05-29 23:45:32,077 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:45:32,077 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:45:32,077 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'cosa', 'che', 'vi', 'mostr@@', 'erò', 'è', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:45:32,077 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:45:32,077 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:45:32,078 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza che vi mostrerò è una cosa che vi mostrerò è che è successo negli ultimi 25 anni.
2025-05-29 23:45:35,509 - INFO - joeynmt.training - Epoch   4, Step:    29600, Batch Loss:     2.071946, Batch Acc: 0.445063, Tokens per Sec:    16784, Lr: 0.000300
2025-05-29 23:45:38,959 - INFO - joeynmt.training - Epoch   4, Step:    29700, Batch Loss:     1.977356, Batch Acc: 0.446075, Tokens per Sec:    19905, Lr: 0.000300
2025-05-29 23:45:42,390 - INFO - joeynmt.training - Epoch   4, Step:    29800, Batch Loss:     1.817001, Batch Acc: 0.446458, Tokens per Sec:    20157, Lr: 0.000300
2025-05-29 23:45:45,820 - INFO - joeynmt.training - Epoch   4, Step:    29900, Batch Loss:     1.824857, Batch Acc: 0.451874, Tokens per Sec:    20234, Lr: 0.000300
2025-05-29 23:45:49,251 - INFO - joeynmt.training - Epoch   4, Step:    30000, Batch Loss:     2.100948, Batch Acc: 0.444622, Tokens per Sec:    19565, Lr: 0.000300
2025-05-29 23:45:49,251 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:45:49,251 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:45:57,970 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.95, ppl:   7.02, acc:   0.46, generation: 8.7080[sec], evaluation: 0.0000[sec]
2025-05-29 23:45:57,971 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:45:58,636 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/27000.ckpt
2025-05-29 23:45:58,666 - INFO - joeynmt.training - Example #0
2025-05-29 23:45:58,666 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:45:58,666 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:45:58,666 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'consegu@@', 'enze', 'per', 'vedere', 'che', 'i', 'li@@', 'velli', 'pover@@', 'i', 'che', 'i', 'mer@@', 'c@@', 'ati', 'che', 'i', 'li@@', 'velli', 'ar@@', 'ric@@', 'chi', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'i', 'gruppi', 'di', 'anni', 'per', 'l@@', '<unk>', '4@@', '8', 'stati', 'per', 'cento', 'di', 'anni', 'i', 'li@@', 'velli', 'di', '4@@', '8', 'stati', 'per', 'cento', 'per', 'i', 'primi', 'per', 'cento', 'di', 'anni', 'di', '4@@', '8', 'stati', 'per', 'cento', 'di', 'questi', 'di@@', 'stri@@', 'bu@@', 'it@@', 'o@@', '.', '</s>']
2025-05-29 23:45:58,667 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:45:58,667 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:45:58,667 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due conseguenze per vedere che i livelli poveri che i mercati che i livelli arricchi che ha fatto per tre milioni di anni i gruppi di anni per l<unk> 48 stati per cento di anni i livelli di 48 stati per cento per i primi per cento di anni di 48 stati per cento di questi distribuito.
2025-05-29 23:45:58,667 - INFO - joeynmt.training - Example #1
2025-05-29 23:45:58,667 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:45:58,668 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:45:58,668 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'più', 'for@@', 'ti', 'la', 'prima', 'cosa', 'che', 'si', 'chiama', '<unk>', 'E@@', '<unk>', 'la', 'mia', 'vita', 'che', 'non', 'c@@', '<unk>', 'è', 'il', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-29 23:45:58,668 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:45:58,668 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:45:58,668 - INFO - joeynmt.training - 	Hypothesis: Ma non è più forti la prima cosa che si chiama <unk> E<unk> la mia vita che non c<unk> è il ghiaccio.
2025-05-29 23:45:58,668 - INFO - joeynmt.training - Example #2
2025-05-29 23:45:58,668 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:45:58,668 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:45:58,669 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'mat@@', 'eria', 'ar@@', 'ica', 'di', 'ghi@@', 'accio', 'che', 'si', 'chiama', 'il', 'cuore', 'del', 'nostro', 'sistema', 'di', 'c@@', 'lim@@', 'pi@@', 'da', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:45:58,669 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:45:58,669 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:45:58,669 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la materia arica di ghiaccio che si chiama il cuore del nostro sistema di climpida globale.
2025-05-29 23:45:58,669 - INFO - joeynmt.training - Example #3
2025-05-29 23:45:58,669 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:45:58,669 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:45:58,669 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Pri@@', 'ma', 'di', 'tut@@', 'to@@', ',', 'la', 'prima', 'cosa', 'che', 'si', 'può', 'fare', 'con', 'il', 's@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:45:58,670 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:45:58,670 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:45:58,670 - INFO - joeynmt.training - 	Hypothesis: Prima di tutto, la prima cosa che si può fare con il sommer.
2025-05-29 23:45:58,670 - INFO - joeynmt.training - Example #4
2025-05-29 23:45:58,670 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:45:58,670 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:45:58,670 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:45:58,671 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:45:58,671 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:45:58,671 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza che vi mostro è un disegno che è successo negli ultimi 25 anni.
2025-05-29 23:46:02,151 - INFO - joeynmt.training - Epoch   4, Step:    30100, Batch Loss:     2.162553, Batch Acc: 0.446753, Tokens per Sec:    16719, Lr: 0.000300
2025-05-29 23:46:05,598 - INFO - joeynmt.training - Epoch   4, Step:    30200, Batch Loss:     1.984095, Batch Acc: 0.440165, Tokens per Sec:    19680, Lr: 0.000300
2025-05-29 23:46:09,038 - INFO - joeynmt.training - Epoch   4, Step:    30300, Batch Loss:     1.862171, Batch Acc: 0.440928, Tokens per Sec:    19811, Lr: 0.000300
2025-05-29 23:46:12,496 - INFO - joeynmt.training - Epoch   4, Step:    30400, Batch Loss:     1.750170, Batch Acc: 0.446459, Tokens per Sec:    20006, Lr: 0.000300
2025-05-29 23:46:15,939 - INFO - joeynmt.training - Epoch   4, Step:    30500, Batch Loss:     2.291414, Batch Acc: 0.445333, Tokens per Sec:    20056, Lr: 0.000300
2025-05-29 23:46:15,939 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:46:15,939 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:46:24,741 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.94, ppl:   6.98, acc:   0.46, generation: 8.7919[sec], evaluation: 0.0000[sec]
2025-05-29 23:46:24,742 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:46:25,373 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/28500.ckpt
2025-05-29 23:46:25,397 - INFO - joeynmt.training - Example #0
2025-05-29 23:46:25,397 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:46:25,397 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:46:25,397 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'vedere', 'che', 'la', 'gente', 'ha', 'fatto', 'che', 'la', 'g@@', 'am@@', 'ma@@', 'zione', 'ar@@', 'ic@@', 'a@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'che', 'ha', 'fatto', 'per', 'i', 'gruppi', 'di', '4@@', '8', 'stati', 'per', 'cento', 'anni', 'i', 'gruppi', 'di', '4@@', '8', 'stati', 'per', 'cento', 'anni', 'per', 'raggi@@', 'ungere', 'il', '40', 'per', 'cento', 'per', 'cento', 'anni', 'per', 'ri@@', 'fu@@', 'gi@@', '.', '</s>']
2025-05-29 23:46:25,398 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:46:25,398 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:46:25,398 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per vedere che la gente ha fatto che la gammazione arica, per tre milioni di anni che ha fatto per i gruppi di 48 stati per cento anni i gruppi di 48 stati per cento anni per raggiungere il 40 per cento per cento anni per rifugi.
2025-05-29 23:46:25,398 - INFO - joeynmt.training - Example #1
2025-05-29 23:46:25,398 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:46:25,398 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:46:25,398 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'più', 'for@@', 'te@@', ',', 'non', 'è', 'abbastanza', 'la', 'più', 'grande', 'di', 'questo', 'probl@@', 'em@@', 'a@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'l@@', 'ett@@', 'o@@', '.', '</s>']
2025-05-29 23:46:25,399 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:46:25,399 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:46:25,399 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto più forte, non è abbastanza la più grande di questo problema, perché non è il diletto.
2025-05-29 23:46:25,399 - INFO - joeynmt.training - Example #2
2025-05-29 23:46:25,399 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:46:25,399 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:46:25,399 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'è', 'la', 'c@@', 'ru@@', 'ci@@', 'ale', 'di', 'ghi@@', 'accio', 'che', 'si', 'è', 'ri@@', 'ma@@', 'sto', 'in', 'modo', 'che', 'il', 'cuore', 'del', 'nostro', 'sistema', 'di', 'K@@', 'lim@@', 'a@@', '.', '</s>']
2025-05-29 23:46:25,400 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:46:25,400 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:46:25,400 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, è la cruciale di ghiaccio che si è rimasto in modo che il cuore del nostro sistema di Klima.
2025-05-29 23:46:25,400 - INFO - joeynmt.training - Example #3
2025-05-29 23:46:25,400 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:46:25,400 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:46:25,400 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'poi', 'si', 'è', 'ri@@', 'ma@@', 'sto', 'in', 'modo', 'che', 'il', 'suo', 's@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:46:25,401 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:46:25,401 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:46:25,401 - INFO - joeynmt.training - 	Hypothesis: E poi si è rimasto in modo che il suo sommer.
2025-05-29 23:46:25,401 - INFO - joeynmt.training - Example #4
2025-05-29 23:46:25,401 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:46:25,401 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:46:25,401 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'v@@', 'a@@', ',', 'è', 'un', 'di@@', 'seg@@', 'no', 'che', 'è', 'un', 'di@@', 'seg@@', 'no', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:46:25,401 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:46:25,402 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:46:25,402 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva, è un disegno che è un disegno che è successo negli ultimi 25 anni.
2025-05-29 23:46:28,884 - INFO - joeynmt.training - Epoch   4, Step:    30600, Batch Loss:     2.118714, Batch Acc: 0.439753, Tokens per Sec:    16463, Lr: 0.000300
2025-05-29 23:46:32,347 - INFO - joeynmt.training - Epoch   4, Step:    30700, Batch Loss:     2.129001, Batch Acc: 0.441909, Tokens per Sec:    19877, Lr: 0.000300
2025-05-29 23:46:35,785 - INFO - joeynmt.training - Epoch   4, Step:    30800, Batch Loss:     1.994390, Batch Acc: 0.440413, Tokens per Sec:    19684, Lr: 0.000300
2025-05-29 23:46:39,211 - INFO - joeynmt.training - Epoch   4, Step:    30900, Batch Loss:     2.055196, Batch Acc: 0.441596, Tokens per Sec:    19985, Lr: 0.000300
2025-05-29 23:46:42,656 - INFO - joeynmt.training - Epoch   4, Step:    31000, Batch Loss:     1.855868, Batch Acc: 0.443832, Tokens per Sec:    19857, Lr: 0.000300
2025-05-29 23:46:42,656 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:46:42,656 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:46:50,571 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.94, ppl:   6.93, acc:   0.46, generation: 7.9044[sec], evaluation: 0.0000[sec]
2025-05-29 23:46:50,571 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:46:51,214 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/29000.ckpt
2025-05-29 23:46:51,241 - INFO - joeynmt.training - Example #0
2025-05-29 23:46:51,242 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:46:51,242 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:46:51,242 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'consegu@@', 'enze', 'di', 'questo', 'ti@@', 'po@@', ',', 'ho', 'mostr@@', 'ato', 'che', 'le', 'cose', 'che', 'le', 'c@@', 'ass@@', 'e@@', ',', 'che', 'le', 'persone', 'hanno', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'i', 'li@@', 'velli', 'di', 'anni', 'per', 'la', 'quantità', 'di', 'li@@', 'sta', 'per', 'la', 'ter@@', 'ra@@', '.', '</s>']
2025-05-29 23:46:51,243 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:46:51,243 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:46:51,243 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due conseguenze di questo tipo, ho mostrato che le cose che le casse, che le persone hanno fatto per tre milioni di anni per i livelli di anni per la quantità di lista per la terra.
2025-05-29 23:46:51,243 - INFO - joeynmt.training - Example #1
2025-05-29 23:46:51,243 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:46:51,243 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:46:51,243 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'più', 'for@@', 'te@@', ',', 'la', 'cosa', 'più', 'importante', 'per', 'la', 'ter@@', 'ri@@', 'bile', 'di', 'questo', 'problema', 'speci@@', 'al@@', 'e@@', ',', 'perché', 'non', 'c@@', '<unk>', 'è', 'il', 'c@@', 'chio', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'a@@', '.', '</s>']
2025-05-29 23:46:51,244 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:46:51,244 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:46:51,244 - INFO - joeynmt.training - 	Hypothesis: Ma non è più forte, la cosa più importante per la terribile di questo problema speciale, perché non c<unk> è il cchio dell<unk> Eisla.
2025-05-29 23:46:51,244 - INFO - joeynmt.training - Example #2
2025-05-29 23:46:51,244 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:46:51,244 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:46:51,244 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'cosa', 'che', 'si', 'chiama', '<unk>', 'E@@', 'is@@', 'co@@', 'pi@@', 'a@@', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:46:51,245 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:46:51,245 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:46:51,245 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cosa che si chiama <unk> Eiscopia, il cuore del nostro sistema globale.
2025-05-29 23:46:51,245 - INFO - joeynmt.training - Example #3
2025-05-29 23:46:51,245 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:46:51,245 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:46:51,245 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'prima', 'di', 'andare', 'al', 'vento', 'e', 'sc@@', 'r@@', 'um@@', 'p@@', 'are', 'in', 's@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:46:51,245 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:46:51,245 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:46:51,246 - INFO - joeynmt.training - 	Hypothesis: E prima di andare al vento e scrumpare in sommer.
2025-05-29 23:46:51,246 - INFO - joeynmt.training - Example #4
2025-05-29 23:46:51,246 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:46:51,246 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:46:51,246 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'una', 'foto', 'di', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:46:51,246 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:46:51,246 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:46:51,246 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è una foto di disegno di quello che è successo negli ultimi 25 anni.
2025-05-29 23:46:54,725 - INFO - joeynmt.training - Epoch   4, Step:    31100, Batch Loss:     2.407736, Batch Acc: 0.440415, Tokens per Sec:    16773, Lr: 0.000300
2025-05-29 23:46:58,168 - INFO - joeynmt.training - Epoch   4, Step:    31200, Batch Loss:     1.894307, Batch Acc: 0.444608, Tokens per Sec:    19386, Lr: 0.000300
2025-05-29 23:47:01,594 - INFO - joeynmt.training - Epoch   4, Step:    31300, Batch Loss:     1.929585, Batch Acc: 0.443169, Tokens per Sec:    19550, Lr: 0.000300
2025-05-29 23:47:05,040 - INFO - joeynmt.training - Epoch   4, Step:    31400, Batch Loss:     2.144953, Batch Acc: 0.440708, Tokens per Sec:    20210, Lr: 0.000300
2025-05-29 23:47:08,472 - INFO - joeynmt.training - Epoch   4, Step:    31500, Batch Loss:     2.088273, Batch Acc: 0.443349, Tokens per Sec:    20100, Lr: 0.000300
2025-05-29 23:47:08,472 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:47:08,472 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:47:16,720 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.93, ppl:   6.90, acc:   0.47, generation: 8.2379[sec], evaluation: 0.0000[sec]
2025-05-29 23:47:16,721 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:47:17,350 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/28000.ckpt
2025-05-29 23:47:17,378 - INFO - joeynmt.training - Example #0
2025-05-29 23:47:17,379 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:47:17,379 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:47:17,379 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'per@@', 'dere', 'che', 'la', 'c@@', 'ad@@', 'uta', 'di', 'ghi@@', 'acci@@', 'o@@', ',', 'che', 'la', 'c@@', 'ad@@', 'uta', 'di', 'tre', 'milioni', 'di', 'anni', 'di', 'con@@', 'ver@@', 't@@', 'ic@@', 'ale', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'l@@', '<unk>', 'in@@', 'fer@@', 'i@@', 'ore', 'di', 'circa', '4@@', '8', 'stati', 'per', 'cento', 'di', 'anni', 'per', 'l@@', '<unk>', 'in@@', 'fer@@', 'i@@', 'ore', 'di', 'circa', 'il', '4@@', '8', 'stati', 'per', 'cento', 'di', 'anni', 'di', 'tutto', 'il', 'rest@@', 'o@@', '.', '</s>']
2025-05-29 23:47:17,380 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:47:17,380 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:47:17,380 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per perdere che la caduta di ghiaccio, che la caduta di tre milioni di anni di converticale che ha fatto per tre milioni di anni per l<unk> inferiore di circa 48 stati per cento di anni per l<unk> inferiore di circa il 48 stati per cento di anni di tutto il resto.
2025-05-29 23:47:17,380 - INFO - joeynmt.training - Example #1
2025-05-29 23:47:17,380 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:47:17,380 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:47:17,380 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'così', 'che', 'non', 'è', 'stata', 'la', 'ter@@', 'ri@@', 'bile', 'che', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es', 'che', 'mostra', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es', 'che', 'mostra', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es', 'che', 'mostra', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es', 'che', 'mostra', 'il', 't@@', 'asso', 'di', 'di@@', 'stri@@', 'bu@@', 'zione', 'di', 'questo', 'probl@@', 'em@@', 'a@@', '.', '</s>']
2025-05-29 23:47:17,381 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:47:17,381 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:47:17,381 - INFO - joeynmt.training - 	Hypothesis: Ma non è così che non è stata la terribile che non è il dell<unk> Eises che mostra il dell<unk> Eises che mostra il dell<unk> Eises che mostra il dell<unk> Eises che mostra il tasso di distribuzione di questo problema.
2025-05-29 23:47:17,381 - INFO - joeynmt.training - Example #2
2025-05-29 23:47:17,381 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:47:17,381 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:47:17,381 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'att@@', 'ur@@', 'azione', 'ar@@', 'ric@@', 'e', 'il', 'cuore', 'del', 'nostro', 'sistema', 'di', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.', '</s>']
2025-05-29 23:47:17,382 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:47:17,382 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:47:17,382 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la catturazione arrice il cuore del nostro sistema di Klimasystems.
2025-05-29 23:47:17,382 - INFO - joeynmt.training - Example #3
2025-05-29 23:47:17,382 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:47:17,382 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:47:17,382 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'prima', 'di', 'tutto', 'nel', 'vent@@', 'o@@', ',', 'e', 'si', 'è', 'ri@@', 'ma@@', 'sta', 'in', 's@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:47:17,382 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:47:17,383 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:47:17,383 - INFO - joeynmt.training - 	Hypothesis: E prima di tutto nel vento, e si è rimasta in sommer.
2025-05-29 23:47:17,383 - INFO - joeynmt.training - Example #4
2025-05-29 23:47:17,383 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:47:17,383 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:47:17,383 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 'seg@@', 'ret@@', 'o', 'di', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:47:17,383 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:47:17,383 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:47:17,383 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza che vi mostrerò è un segreto di disegno di quello che è successo negli ultimi 25 anni.
2025-05-29 23:47:20,840 - INFO - joeynmt.training - Epoch   4, Step:    31600, Batch Loss:     1.964350, Batch Acc: 0.445963, Tokens per Sec:    16420, Lr: 0.000300
2025-05-29 23:47:24,294 - INFO - joeynmt.training - Epoch   4, Step:    31700, Batch Loss:     2.125140, Batch Acc: 0.448967, Tokens per Sec:    20347, Lr: 0.000300
2025-05-29 23:47:27,743 - INFO - joeynmt.training - Epoch   4, Step:    31800, Batch Loss:     2.004793, Batch Acc: 0.446717, Tokens per Sec:    19933, Lr: 0.000300
2025-05-29 23:47:31,185 - INFO - joeynmt.training - Epoch   4, Step:    31900, Batch Loss:     1.881704, Batch Acc: 0.446347, Tokens per Sec:    20076, Lr: 0.000300
2025-05-29 23:47:34,626 - INFO - joeynmt.training - Epoch   4, Step:    32000, Batch Loss:     1.897412, Batch Acc: 0.444641, Tokens per Sec:    19517, Lr: 0.000300
2025-05-29 23:47:34,627 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:47:34,627 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:47:43,339 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.93, ppl:   6.91, acc:   0.47, generation: 8.7013[sec], evaluation: 0.0000[sec]
2025-05-29 23:47:43,783 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/29500.ckpt
2025-05-29 23:47:43,812 - INFO - joeynmt.training - Example #0
2025-05-29 23:47:43,812 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:47:43,813 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:47:43,813 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'vedere', 'che', 'la', 'maggior', 'parte', 'delle', 'persone', 'che', 'hanno', 'fatto', 'per', 'i', 'nostri', 'an@@', 'ni@@', ',', 'che', 'i', 'gr@@', 'avi', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'il', '4@@', '8', 'stati', 'dei', 'nostri', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:47:43,813 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:47:43,813 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:47:43,813 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per vedere che la maggior parte delle persone che hanno fatto per i nostri anni, che i gravi per tre milioni di anni per il 48 stati dei nostri anni.
2025-05-29 23:47:43,813 - INFO - joeynmt.training - Example #1
2025-05-29 23:47:43,814 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:47:43,814 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:47:43,814 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'più', 'for@@', 'te@@', ',', 'non', 'è', 'così', 'che', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'er@@', 'o@@', '<unk>', 'di', 'questo', 'problema', 'è', 'che', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'at@@', 'o@@', '.', '</s>']
2025-05-29 23:47:43,814 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:47:43,814 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:47:43,814 - INFO - joeynmt.training - 	Hypothesis: Ma non è più forte, non è così che non è il dell<unk> ero<unk> di questo problema è che non è il dell<unk> Eislato.
2025-05-29 23:47:43,814 - INFO - joeynmt.training - Example #2
2025-05-29 23:47:43,814 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:47:43,815 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:47:43,815 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'di', 'E@@', 'is@@', 'k@@', 'app@@', 'a', 'il', 'cuore', 'del', 'nostro', 'sistema', 'di', 'cal@@', 'col@@', 'o@@', '.', '</s>']
2025-05-29 23:47:43,815 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:47:43,815 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:47:43,815 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cuore di Eiskappa il cuore del nostro sistema di calcolo.
2025-05-29 23:47:43,815 - INFO - joeynmt.training - Example #3
2025-05-29 23:47:43,815 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:47:43,815 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:47:43,815 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'prima', 'che', 'si', 'è', 'ri@@', 'ma@@', 'sto', 'per', 's@@', 'fr@@', 'utt@@', 'are', 'il', 's@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:47:43,816 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:47:43,816 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:47:43,816 - INFO - joeynmt.training - 	Hypothesis: E prima che si è rimasto per sfruttare il sommer.
2025-05-29 23:47:43,816 - INFO - joeynmt.training - Example #4
2025-05-29 23:47:43,816 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:47:43,816 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:47:43,816 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'prossi@@', 'mo', 'di@@', 'stri@@', 'bu@@', 'ito', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'seg@@', 'no', 'di', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:47:43,817 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:47:43,817 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:47:43,817 - INFO - joeynmt.training - 	Hypothesis: Il prossimo distribuito che vi mostro è un segno di disegno di quello che è successo negli ultimi 25 anni.
2025-05-29 23:47:47,287 - INFO - joeynmt.training - Epoch   4, Step:    32100, Batch Loss:     2.068137, Batch Acc: 0.435837, Tokens per Sec:    17429, Lr: 0.000300
2025-05-29 23:47:50,741 - INFO - joeynmt.training - Epoch   4, Step:    32200, Batch Loss:     2.105849, Batch Acc: 0.450025, Tokens per Sec:    19565, Lr: 0.000300
2025-05-29 23:47:54,197 - INFO - joeynmt.training - Epoch   4, Step:    32300, Batch Loss:     2.117563, Batch Acc: 0.444867, Tokens per Sec:    20085, Lr: 0.000300
2025-05-29 23:47:57,648 - INFO - joeynmt.training - Epoch   4, Step:    32400, Batch Loss:     1.848740, Batch Acc: 0.445777, Tokens per Sec:    19672, Lr: 0.000300
2025-05-29 23:48:01,093 - INFO - joeynmt.training - Epoch   4, Step:    32500, Batch Loss:     1.983006, Batch Acc: 0.453001, Tokens per Sec:    20361, Lr: 0.000300
2025-05-29 23:48:01,093 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:48:01,093 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:48:09,635 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.92, ppl:   6.84, acc:   0.47, generation: 8.5312[sec], evaluation: 0.0000[sec]
2025-05-29 23:48:09,635 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:48:10,212 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/30000.ckpt
2025-05-29 23:48:10,235 - INFO - joeynmt.training - Example #0
2025-05-29 23:48:10,236 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:48:10,236 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:48:10,236 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'consegu@@', 'enze', 'per', 'vedere', 'che', 'la', 'gente', 'ha', 'mostr@@', 'ato', 'che', 'la', 'li@@', 'sta', 'di', 'ghi@@', 'accio', 'che', 'gli', 'im@@', 'pieg@@', 'ati', 'per', 'tre', 'milioni', 'di', 'anni', 'i', 'li@@', 'velli', 'di', 'pes@@', 'o', 'di', 'circa', '4@@', '8', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', 'vol@@', 't@@', 'a@@', '.', '</s>']
2025-05-29 23:48:10,237 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:48:10,237 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:48:10,237 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due conseguenze per vedere che la gente ha mostrato che la lista di ghiaccio che gli impiegati per tre milioni di anni i livelli di peso di circa 48 milioni di anni per la più grande volta.
2025-05-29 23:48:10,237 - INFO - joeynmt.training - Example #1
2025-05-29 23:48:10,237 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:48:10,237 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:48:10,237 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'più', 'for@@', 'ti', 'di', 'questo', 'problema', 'è', 'che', 'la', 'vita', 'di', 'questo', 'problema', 'è', 'che', 'non', 'è', 'il', 'd@@', 'ic@@', 'ke', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'a@@', '.', '</s>']
2025-05-29 23:48:10,238 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:48:10,238 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:48:10,238 - INFO - joeynmt.training - 	Hypothesis: Ma non è più forti di questo problema è che la vita di questo problema è che non è il dicke dell<unk> Eisla.
2025-05-29 23:48:10,238 - INFO - joeynmt.training - Example #2
2025-05-29 23:48:10,238 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:48:10,238 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:48:10,238 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'att@@', 'ura', 'ar@@', 't@@', 'ic@@', 'a@@', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'di', 'K@@', 'ef@@', 'al@@', '.', '</s>']
2025-05-29 23:48:10,238 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:48:10,239 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:48:10,239 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cattura artica, il cuore del nostro sistema di Kefal.
2025-05-29 23:48:10,239 - INFO - joeynmt.training - Example #3
2025-05-29 23:48:10,239 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:48:10,239 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:48:10,239 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', 'er@@', 'c@@', 'ano', 'e', 'sc@@', 'ambi@@', 'o', 'di', 's@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:48:10,239 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:48:10,239 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:48:10,239 - INFO - joeynmt.training - 	Hypothesis: Cercano e scambio di sommer.
2025-05-29 23:48:10,240 - INFO - joeynmt.training - Example #4
2025-05-29 23:48:10,240 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:48:10,240 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:48:10,240 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'cosa', 'che', 'è', 'succ@@', 'ess@@', 'o@@', ',', 'che', 'è', 'acca@@', 'duto', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:48:10,240 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:48:10,240 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:48:10,240 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza che vi mostrerò è una cosa che è successo, che è accaduto negli ultimi 25 anni.
2025-05-29 23:48:13,718 - INFO - joeynmt.training - Epoch   4, Step:    32600, Batch Loss:     2.117797, Batch Acc: 0.450189, Tokens per Sec:    16945, Lr: 0.000300
2025-05-29 23:48:17,187 - INFO - joeynmt.training - Epoch   4, Step:    32700, Batch Loss:     1.912336, Batch Acc: 0.447716, Tokens per Sec:    20128, Lr: 0.000300
2025-05-29 23:48:20,651 - INFO - joeynmt.training - Epoch   4, Step:    32800, Batch Loss:     1.949685, Batch Acc: 0.446049, Tokens per Sec:    19462, Lr: 0.000300
2025-05-29 23:48:24,096 - INFO - joeynmt.training - Epoch   4, Step:    32900, Batch Loss:     1.883817, Batch Acc: 0.443218, Tokens per Sec:    19885, Lr: 0.000300
2025-05-29 23:48:27,542 - INFO - joeynmt.training - Epoch   4, Step:    33000, Batch Loss:     1.967698, Batch Acc: 0.436620, Tokens per Sec:    19862, Lr: 0.000300
2025-05-29 23:48:27,542 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:48:27,542 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:48:35,462 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.93, ppl:   6.86, acc:   0.47, generation: 7.9095[sec], evaluation: 0.0000[sec]
2025-05-29 23:48:35,903 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/30500.ckpt
2025-05-29 23:48:35,932 - INFO - joeynmt.training - Example #0
2025-05-29 23:48:35,933 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:48:35,933 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:48:35,933 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'l@@', '<unk>', 'E@@', 'is@@', 'co@@', ',', 'che', 'l@@', '<unk>', 'el@@', 'ef@@', 'ante', 'è', 'che', 'l@@', '<unk>', 'el@@', 'ef@@', 'ante', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'i', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'grande', 'grande', 'quantità', 'di', '4@@', '8', 'stati', 'per', 'cento', 'per', 'i', 'li@@', 'velli', 'di', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cento', 'dei', 'li@@', 'velli', 'di', '4@@', '8', 'per', 'cento', 'di', 'questi', 'due', 'di@@', 'apos@@', 'iti@@', 'vi@@', '.', '</s>']
2025-05-29 23:48:35,934 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:48:35,934 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:48:35,934 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive che l<unk> Eisco, che l<unk> elefante è che l<unk> elefante per tre milioni di anni per i tre milioni di anni per la grande grande quantità di 48 stati per cento per i livelli di 48 stati per cento per cento dei livelli di 48 per cento di questi due diapositivi.
2025-05-29 23:48:35,934 - INFO - joeynmt.training - Example #1
2025-05-29 23:48:35,934 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:48:35,934 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:48:35,934 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'così', 'difficile', 'da', 'fare', 'la', 'più', 'grande', 'grande', 'quantità', 'di', 'problemi', 'che', 'non', 'è', 'la', 'di@@', 'chiar@@', 'azione', 'di', 'questo', 'probl@@', 'em@@', 'a@@', '.', '</s>']
2025-05-29 23:48:35,935 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:48:35,935 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:48:35,935 - INFO - joeynmt.training - 	Hypothesis: Ma non è così difficile da fare la più grande grande quantità di problemi che non è la dichiarazione di questo problema.
2025-05-29 23:48:35,935 - INFO - joeynmt.training - Example #2
2025-05-29 23:48:35,935 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:48:35,935 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:48:35,935 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'lasse', 'ar@@', 'ic@@', 'a@@', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:48:35,936 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:48:35,936 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:48:35,936 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la classe arica, il cuore del nostro sistema globale.
2025-05-29 23:48:35,936 - INFO - joeynmt.training - Example #3
2025-05-29 23:48:35,936 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:48:35,936 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:48:35,936 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'poi', 'si', 'ri@@', 'du@@', 'ce', 'e', 's@@', 'om@@', 'p@@', 'a', 'e', 'si', 's@@', 'om@@', 'p@@', 'a', 'in', 's@@', 'om@@', 'od@@', 'o@@', '.', '</s>']
2025-05-29 23:48:35,937 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:48:35,937 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:48:35,937 - INFO - joeynmt.training - 	Hypothesis: E poi si riduce e sompa e si sompa in somodo.
2025-05-29 23:48:35,937 - INFO - joeynmt.training - Example #4
2025-05-29 23:48:35,937 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:48:35,937 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:48:35,937 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'cosa', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:48:35,937 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:48:35,938 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:48:35,938 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza che vi mostrerò è una cosa che è successo negli ultimi 25 anni.
2025-05-29 23:48:39,420 - INFO - joeynmt.training - Epoch   4, Step:    33100, Batch Loss:     2.134080, Batch Acc: 0.443968, Tokens per Sec:    17317, Lr: 0.000300
2025-05-29 23:48:42,880 - INFO - joeynmt.training - Epoch   4, Step:    33200, Batch Loss:     1.884351, Batch Acc: 0.443272, Tokens per Sec:    20293, Lr: 0.000300
2025-05-29 23:48:46,317 - INFO - joeynmt.training - Epoch   4, Step:    33300, Batch Loss:     2.171234, Batch Acc: 0.444586, Tokens per Sec:    19886, Lr: 0.000300
2025-05-29 23:48:49,785 - INFO - joeynmt.training - Epoch   4, Step:    33400, Batch Loss:     1.861141, Batch Acc: 0.445962, Tokens per Sec:    19982, Lr: 0.000300
2025-05-29 23:48:53,256 - INFO - joeynmt.training - Epoch   4, Step:    33500, Batch Loss:     1.900371, Batch Acc: 0.444311, Tokens per Sec:    20093, Lr: 0.000300
2025-05-29 23:48:53,256 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:48:53,257 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:49:01,756 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.92, ppl:   6.81, acc:   0.47, generation: 8.4881[sec], evaluation: 0.0000[sec]
2025-05-29 23:49:01,757 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:49:02,406 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/31000.ckpt
2025-05-29 23:49:02,433 - INFO - joeynmt.training - Example #0
2025-05-29 23:49:02,434 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:49:02,434 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:49:02,434 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'la', 'ter@@', 'ra@@', ',', 'che', 'la', 'c@@', 'ris@@', 'i', 'ar@@', 'c@@', 'ic@@', 'li', 'che', 'ha', 'ri@@', 'ma@@', 'sto', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'dimen@@', 'sione', 'di', 'circa', '4@@', '8', 'stati', 'per', 'cento', 'di', 'anni', 'per', 'ri@@', 'durre', 'il', '40', 'per', 'cento', 'di', 'questi', 'due', 'di@@', 'apos@@', 'iti@@', 'vi@@', '.', '</s>']
2025-05-29 23:49:02,434 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:49:02,434 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:49:02,435 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per osservare la terra, che la crisi arcicli che ha rimasto per tre milioni di anni per la dimensione di circa 48 stati per cento di anni per ridurre il 40 per cento di questi due diapositivi.
2025-05-29 23:49:02,435 - INFO - joeynmt.training - Example #1
2025-05-29 23:49:02,435 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:49:02,435 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:49:02,435 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'più', 'for@@', 'te@@', ',', 'la', 'più', 'grande', 'parte', 'di', 'questo', 'probl@@', 'em@@', 'a@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-29 23:49:02,435 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:49:02,435 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:49:02,436 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto più forte, la più grande parte di questo problema, perché non è il dibattito del ghiaccio.
2025-05-29 23:49:02,436 - INFO - joeynmt.training - Example #2
2025-05-29 23:49:02,436 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:49:02,436 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:49:02,436 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'ris@@', 'i', 'ar@@', 'c@@', 'atti@@', 'vi', 'che', 'il', 'cuore', 'del', 'nostro', 'sistema', 'sistema', 'sistema', 'di', 'cal@@', 'col@@', 'o@@', '.', '</s>']
2025-05-29 23:49:02,436 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:49:02,436 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:49:02,436 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la crisi arcattivi che il cuore del nostro sistema sistema sistema di calcolo.
2025-05-29 23:49:02,437 - INFO - joeynmt.training - Example #3
2025-05-29 23:49:02,437 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:49:02,437 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:49:02,437 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'ora', 'è', 'un', 'po@@', '<unk>', 'di', 's@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:49:02,437 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:49:02,437 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:49:02,437 - INFO - joeynmt.training - 	Hypothesis: L<unk> ora è un po<unk> di sommer.
2025-05-29 23:49:02,437 - INFO - joeynmt.training - Example #4
2025-05-29 23:49:02,438 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:49:02,438 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:49:02,438 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'di@@', 'stri@@', 'bu@@', 'zione', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:49:02,438 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:49:02,438 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:49:02,438 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una distribuzione di quello che è successo negli ultimi 25 anni.
2025-05-29 23:49:05,921 - INFO - joeynmt.training - Epoch   4, Step:    33600, Batch Loss:     1.715621, Batch Acc: 0.442479, Tokens per Sec:    16506, Lr: 0.000300
2025-05-29 23:49:09,369 - INFO - joeynmt.training - Epoch   4, Step:    33700, Batch Loss:     2.103158, Batch Acc: 0.448879, Tokens per Sec:    19444, Lr: 0.000300
2025-05-29 23:49:12,819 - INFO - joeynmt.training - Epoch   4, Step:    33800, Batch Loss:     2.080576, Batch Acc: 0.448929, Tokens per Sec:    20029, Lr: 0.000300
2025-05-29 23:49:16,263 - INFO - joeynmt.training - Epoch   4, Step:    33900, Batch Loss:     2.079082, Batch Acc: 0.443658, Tokens per Sec:    20138, Lr: 0.000300
2025-05-29 23:49:19,698 - INFO - joeynmt.training - Epoch   4, Step:    34000, Batch Loss:     2.117716, Batch Acc: 0.440114, Tokens per Sec:    19710, Lr: 0.000300
2025-05-29 23:49:19,699 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:49:19,699 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:49:27,133 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.92, ppl:   6.83, acc:   0.47, generation: 7.4236[sec], evaluation: 0.0000[sec]
2025-05-29 23:49:27,570 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/32000.ckpt
2025-05-29 23:49:27,598 - INFO - joeynmt.training - Example #0
2025-05-29 23:49:27,599 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:49:27,599 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:49:27,599 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'co', 'ar@@', 'ric@@', 'e', 'che', 'il', 'ghi@@', 'accio', 'd@@', 'ell@@', '<unk>', 'et@@', 'i@@', 'ch@@', 'ett@@', 'a@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'l@@', '<unk>', 'in@@', 'gr@@', 'esso', 'di', '4@@', '8', 'stati', 'per', 'cento', 'anni', 'per', 'ri@@', 'durre', 'il', '40', 'per', 'cento', 'di', 'questi', 'due', 'di@@', 'stri@@', 'bu@@', 'ire', 'a', '4@@', '8', 'stati', 'per', 'cento', 'di', 'questi', 'due', 'di@@', 'apos@@', 'iti@@', 'vi@@', '.', '</s>']
2025-05-29 23:49:27,599 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:49:27,599 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:49:27,599 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive che il ghiaccio arco arrice che il ghiaccio dell<unk> etichetta, per tre milioni di anni per l<unk> ingresso di 48 stati per cento anni per ridurre il 40 per cento di questi due distribuire a 48 stati per cento di questi due diapositivi.
2025-05-29 23:49:27,599 - INFO - joeynmt.training - Example #1
2025-05-29 23:49:27,600 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:49:27,600 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:49:27,600 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'più', 'for@@', 'ti', 'della', 'ter@@', 'za', 'capacità', 'di', 'essere', 'più', 'effic@@', 'a@@', 'ci@@', ',', 'perché', 'non', 'lo', 'fa', 'il', 't@@', 'asso', 'di', 'ghi@@', 'accio', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'a@@', '.', '</s>']
2025-05-29 23:49:27,600 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:49:27,600 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:49:27,600 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto più forti della terza capacità di essere più efficaci, perché non lo fa il tasso di ghiaccio dell<unk> Eisla.
2025-05-29 23:49:27,600 - INFO - joeynmt.training - Example #2
2025-05-29 23:49:27,601 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:49:27,601 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:49:27,601 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'att@@', 'ura', 'ar@@', 'ica', 'ar@@', 'ric@@', 'e', 'il', 'cuore', 'del', 'nostro', 'sistema', 'di', 'cal@@', 'col@@', 'o@@', '.', '</s>']
2025-05-29 23:49:27,601 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:49:27,601 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:49:27,601 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cattura arica arrice il cuore del nostro sistema di calcolo.
2025-05-29 23:49:27,601 - INFO - joeynmt.training - Example #3
2025-05-29 23:49:27,601 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:49:27,601 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:49:27,601 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', 'è', 'una', 'cosa', 'che', 'si', 'trova', 'in', 's@@', 'om@@', 'mer@@', 'e@@', '.', '</s>']
2025-05-29 23:49:27,602 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:49:27,602 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:49:27,602 - INFO - joeynmt.training - 	Hypothesis: C<unk> è una cosa che si trova in sommere.
2025-05-29 23:49:27,602 - INFO - joeynmt.training - Example #4
2025-05-29 23:49:27,602 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:49:27,602 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:49:27,602 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'i', 'è', 'una', 'cosa', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:49:27,603 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:49:27,603 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:49:27,603 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostri è una cosa che è successo negli ultimi 25 anni.
2025-05-29 23:49:31,086 - INFO - joeynmt.training - Epoch   4, Step:    34100, Batch Loss:     2.001069, Batch Acc: 0.446803, Tokens per Sec:    17260, Lr: 0.000300
2025-05-29 23:49:34,557 - INFO - joeynmt.training - Epoch   4, Step:    34200, Batch Loss:     2.019277, Batch Acc: 0.444406, Tokens per Sec:    19745, Lr: 0.000300
2025-05-29 23:49:38,036 - INFO - joeynmt.training - Epoch   4, Step:    34300, Batch Loss:     1.877646, Batch Acc: 0.445572, Tokens per Sec:    20310, Lr: 0.000300
2025-05-29 23:49:41,501 - INFO - joeynmt.training - Epoch   4, Step:    34400, Batch Loss:     2.043268, Batch Acc: 0.454598, Tokens per Sec:    19953, Lr: 0.000300
2025-05-29 23:49:44,975 - INFO - joeynmt.training - Epoch   4, Step:    34500, Batch Loss:     2.029111, Batch Acc: 0.446272, Tokens per Sec:    19942, Lr: 0.000300
2025-05-29 23:49:44,975 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:49:44,975 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:49:53,942 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.91, ppl:   6.77, acc:   0.47, generation: 8.9555[sec], evaluation: 0.0000[sec]
2025-05-29 23:49:53,943 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:49:54,760 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/31500.ckpt
2025-05-29 23:49:54,781 - INFO - joeynmt.training - Example #0
2025-05-29 23:49:54,781 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:49:54,781 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:49:54,781 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'l@@', '<unk>', 'et@@', 'i@@', 'ch@@', 'etta', 'di', 'ghi@@', 'accio', 'ar@@', 'ic@@', 'o@@', ',', 'che', 'l@@', '<unk>', 'et@@', 'i@@', 'ch@@', 'etta', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'gr@@', 'avi', 'che', 'ha', 'fatto', 'il', '4@@', '8', 'stati', 'd@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'ci@@', 'dent@@', 'e@@', '.', '</s>']
2025-05-29 23:49:54,782 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:49:54,782 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:49:54,782 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive che l<unk> etichetta di ghiaccio arico, che l<unk> etichetta per tre milioni di anni di gravi che ha fatto il 48 stati dell<unk> Occidente.
2025-05-29 23:49:54,782 - INFO - joeynmt.training - Example #1
2025-05-29 23:49:54,782 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:49:54,782 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:49:54,782 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'più', 'for@@', 'te@@', ',', 'la', 'più', 'grande', 'di', 'questo', 'problema', 'è', 'che', 'non', 'è', 'il', 'd@@', 'ic@@', 'o@@', ',', 'perché', 'non', 'lo', 'fa', 'il', 'd@@', 'ic@@', 'o@@', '.', '</s>']
2025-05-29 23:49:54,782 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:49:54,782 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:49:54,782 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto più forte, la più grande di questo problema è che non è il dico, perché non lo fa il dico.
2025-05-29 23:49:54,783 - INFO - joeynmt.training - Example #2
2025-05-29 23:49:54,783 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:49:54,783 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:49:54,783 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'lasse', 'ar@@', 'ic@@', 'ano', 'il', 'cuore', 'di', 'un', 'sistema', 'di', 'cal@@', 'col@@', 'o@@', '.', '</s>']
2025-05-29 23:49:54,783 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:49:54,783 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:49:54,783 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la classe aricano il cuore di un sistema di calcolo.
2025-05-29 23:49:54,783 - INFO - joeynmt.training - Example #3
2025-05-29 23:49:54,784 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:49:54,784 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:49:54,784 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'in@@', 'ver@@', 'no@@', ',', 'nel', 'vent@@', 'o@@', ',', 'e', 'si', 'è', 'ri@@', 'd@@', 'otto', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-29 23:49:54,784 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:49:54,784 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:49:54,784 - INFO - joeynmt.training - 	Hypothesis: L<unk> inverno, nel vento, e si è ridotto in estate.
2025-05-29 23:49:54,784 - INFO - joeynmt.training - Example #4
2025-05-29 23:49:54,784 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:49:54,784 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:49:54,784 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'i', 'è', 'una', 'cosa', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:49:54,785 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:49:54,785 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:49:54,785 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostri è una cosa che è successo negli ultimi 25 anni.
2025-05-29 23:49:58,240 - INFO - joeynmt.training - Epoch   4, Step:    34600, Batch Loss:     1.954577, Batch Acc: 0.449461, Tokens per Sec:    15583, Lr: 0.000300
2025-05-29 23:50:01,678 - INFO - joeynmt.training - Epoch   4, Step:    34700, Batch Loss:     1.834869, Batch Acc: 0.449615, Tokens per Sec:    19178, Lr: 0.000300
2025-05-29 23:50:04,991 - INFO - joeynmt.training - Epoch   4, Step:    34800, Batch Loss:     1.880514, Batch Acc: 0.448474, Tokens per Sec:    20499, Lr: 0.000300
2025-05-29 23:50:08,288 - INFO - joeynmt.training - Epoch   4, Step:    34900, Batch Loss:     2.086884, Batch Acc: 0.448568, Tokens per Sec:    20238, Lr: 0.000300
2025-05-29 23:50:11,595 - INFO - joeynmt.training - Epoch   4, Step:    35000, Batch Loss:     1.953411, Batch Acc: 0.448608, Tokens per Sec:    21260, Lr: 0.000300
2025-05-29 23:50:11,596 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:50:11,596 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:50:19,767 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.91, ppl:   6.73, acc:   0.47, generation: 8.1645[sec], evaluation: 0.0000[sec]
2025-05-29 23:50:19,768 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:50:20,301 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/33000.ckpt
2025-05-29 23:50:20,321 - INFO - joeynmt.training - Example #0
2025-05-29 23:50:20,321 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:50:20,321 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:50:20,322 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'consegu@@', 'enze', 'per', 'vedere', 'che', 'i', 'ghi@@', 'acci@@', 'ai', 'di', 'ghi@@', 'accio', 'ar@@', 'ic@@', 'o@@', ',', 'che', 'i', 'n@@', 'omi', 'ar@@', 'ric@@', 'chi', 'di', 'tre', 'milioni', 'di', 'anni', 'di', 'con@@', 'ver@@', 'ti@@', 're', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'maggior', 'parte', 'dei', '4@@', '8', 'stati', 'per', 'tre', 'milioni', 'di', 'anni', 'di', '4@@', '8', 'in', 'un', 'certo', 'sen@@', 'so@@', '.', '</s>']
2025-05-29 23:50:20,322 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:50:20,322 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:50:20,322 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due conseguenze per vedere che i ghiacciai di ghiaccio arico, che i nomi arricchi di tre milioni di anni di convertire per tre milioni di anni per la maggior parte dei 48 stati per tre milioni di anni di 48 in un certo senso.
2025-05-29 23:50:20,322 - INFO - joeynmt.training - Example #1
2025-05-29 23:50:20,323 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:50:20,323 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:50:20,323 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'la', 'maggior', 'parte', 'di', 'questi', 'problemi', 'che', 'non', 'è', 'il', 'd@@', 'ic@@', 'ke', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es', 'che', 'non', 'lo', 'fa', 'il', 'd@@', 'ic@@', 'ke', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es', 'che', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'co@@', '.', '</s>']
2025-05-29 23:50:20,323 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:50:20,323 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:50:20,323 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, la maggior parte di questi problemi che non è il dicke dell<unk> Eises che non lo fa il dicke dell<unk> Eises che non è il dell<unk> Eisco.
2025-05-29 23:50:20,323 - INFO - joeynmt.training - Example #2
2025-05-29 23:50:20,324 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:50:20,324 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:50:20,324 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'att@@', 'ica', 'ar@@', 't@@', 'ica', 'ar@@', 't@@', 'ica', 'di', 'un', 'cuore', 'di', 'un', 'sistema', 'di', 'ri@@', 'fer@@', 'imento', 'del', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:50:20,324 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:50:20,324 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:50:20,324 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cattica artica artica di un cuore di un sistema di riferimento del nostro sistema globale.
2025-05-29 23:50:20,324 - INFO - joeynmt.training - Example #3
2025-05-29 23:50:20,324 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:50:20,324 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:50:20,324 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'prima', 'di', 'andare', 'in', 'giro', 'in', 's@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:50:20,325 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:50:20,325 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:50:20,325 - INFO - joeynmt.training - 	Hypothesis: E prima di andare in giro in sommer.
2025-05-29 23:50:20,325 - INFO - joeynmt.training - Example #4
2025-05-29 23:50:20,325 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:50:20,325 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:50:20,325 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'di@@', 'stri@@', 'bu@@', 'zione', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:50:20,326 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:50:20,326 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:50:20,326 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una distribuzione di quello che è successo negli ultimi 25 anni.
2025-05-29 23:50:23,707 - INFO - joeynmt.training - Epoch   4, Step:    35100, Batch Loss:     2.078730, Batch Acc: 0.444196, Tokens per Sec:    16600, Lr: 0.000300
2025-05-29 23:50:27,101 - INFO - joeynmt.training - Epoch   4, Step:    35200, Batch Loss:     1.873176, Batch Acc: 0.447096, Tokens per Sec:    20635, Lr: 0.000300
2025-05-29 23:50:30,468 - INFO - joeynmt.training - Epoch   4, Step:    35300, Batch Loss:     1.867165, Batch Acc: 0.447558, Tokens per Sec:    20244, Lr: 0.000300
2025-05-29 23:50:33,909 - INFO - joeynmt.training - Epoch   4, Step:    35400, Batch Loss:     1.799332, Batch Acc: 0.449755, Tokens per Sec:    19844, Lr: 0.000300
2025-05-29 23:50:37,352 - INFO - joeynmt.training - Epoch   4, Step:    35500, Batch Loss:     1.926568, Batch Acc: 0.455240, Tokens per Sec:    19998, Lr: 0.000300
2025-05-29 23:50:37,352 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:50:37,352 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:50:45,108 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.90, ppl:   6.70, acc:   0.47, generation: 7.7460[sec], evaluation: 0.0000[sec]
2025-05-29 23:50:45,109 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:50:45,670 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/32500.ckpt
2025-05-29 23:50:45,692 - INFO - joeynmt.training - Example #0
2025-05-29 23:50:45,693 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:50:45,693 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:50:45,693 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'consegu@@', 'enze', 'per', 'vedere', 'che', 'l@@', '<unk>', 'et@@', 'à@@', ',', 'che', 'l@@', '<unk>', 'et@@', 'à@@', ',', 'che', 'l@@', '<unk>', 'et@@', 'i@@', 'ch@@', 'ett@@', 'a@@', ',', 'che', 'per', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'sc@@', 'att@@', 'ata', '4@@', '8', 'stati', 'per', 'cento', 'anni', 'di', '4@@', '8', 'stati', 'per', 'cento', 'anni', 'di', '4@@', '8', 'stati', 'per', 'cento', 'anni', 'di', 'tutto', 'il', 'mon@@', 'do@@', '.', '</s>']
2025-05-29 23:50:45,693 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:50:45,693 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:50:45,694 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due conseguenze per vedere che l<unk> età, che l<unk> età, che l<unk> etichetta, che per tre milioni di anni è stata scattata 48 stati per cento anni di 48 stati per cento anni di 48 stati per cento anni di tutto il mondo.
2025-05-29 23:50:45,694 - INFO - joeynmt.training - Example #1
2025-05-29 23:50:45,694 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:50:45,694 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:50:45,694 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'una', 'cosa', 'che', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'ma', 'non', 'lo', 'è@@', ',', 'non', 'lo', 'fa', 'è', 'il', 'd@@', 'ic@@', 'ke', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'at@@', 'o@@', '.', '</s>']
2025-05-29 23:50:45,694 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:50:45,694 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:50:45,695 - INFO - joeynmt.training - 	Hypothesis: Ma non è una cosa che non è molto forte, ma non lo è, non lo fa è il dicke dell<unk> Eislato.
2025-05-29 23:50:45,695 - INFO - joeynmt.training - Example #2
2025-05-29 23:50:45,695 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:50:45,695 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:50:45,695 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'att@@', 'ica', 'di', 'ghi@@', 'accio', 'ar@@', 'ic@@', 'o@@', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'sistema', 'sistema', 'sistema', 'sistema', 'sistema', 'di', 'cal@@', 'col@@', 'o@@', '.', '</s>']
2025-05-29 23:50:45,695 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:50:45,695 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:50:45,696 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cattica di ghiaccio arico, il cuore del nostro sistema sistema sistema sistema sistema sistema di calcolo.
2025-05-29 23:50:45,696 - INFO - joeynmt.training - Example #3
2025-05-29 23:50:45,696 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:50:45,696 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:50:45,696 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'di', 's@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:50:45,696 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:50:45,696 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:50:45,696 - INFO - joeynmt.training - 	Hypothesis: C<unk> è un po<unk> di sommer.
2025-05-29 23:50:45,696 - INFO - joeynmt.training - Example #4
2025-05-29 23:50:45,697 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:50:45,697 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:50:45,697 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'di@@', 'seg@@', 'na', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:50:45,697 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:50:45,697 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:50:45,697 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza che vi mostrerò è una disegna che è successo negli ultimi 25 anni.
2025-05-29 23:50:49,168 - INFO - joeynmt.training - Epoch   4, Step:    35600, Batch Loss:     2.188653, Batch Acc: 0.447373, Tokens per Sec:    16501, Lr: 0.000300
2025-05-29 23:50:52,587 - INFO - joeynmt.training - Epoch   4, Step:    35700, Batch Loss:     1.892873, Batch Acc: 0.448702, Tokens per Sec:    20207, Lr: 0.000300
2025-05-29 23:50:56,036 - INFO - joeynmt.training - Epoch   4, Step:    35800, Batch Loss:     2.014428, Batch Acc: 0.449748, Tokens per Sec:    19540, Lr: 0.000300
2025-05-29 23:50:59,475 - INFO - joeynmt.training - Epoch   4, Step:    35900, Batch Loss:     1.901047, Batch Acc: 0.448540, Tokens per Sec:    19449, Lr: 0.000300
2025-05-29 23:51:02,939 - INFO - joeynmt.training - Epoch   4, Step:    36000, Batch Loss:     2.026430, Batch Acc: 0.450784, Tokens per Sec:    19952, Lr: 0.000300
2025-05-29 23:51:02,939 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:51:02,939 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:51:11,293 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.90, ppl:   6.67, acc:   0.47, generation: 8.3432[sec], evaluation: 0.0000[sec]
2025-05-29 23:51:11,294 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:51:11,905 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/34000.ckpt
2025-05-29 23:51:11,933 - INFO - joeynmt.training - Example #0
2025-05-29 23:51:11,934 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:51:11,934 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:51:11,934 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'consegu@@', 'enze', 'di', 'questa', 'due', 'di@@', 'apos@@', 'iti@@', 'v@@', 'a@@', ',', 'che', 'gli', 'im@@', 'pieg@@', 'ati', 'ar@@', 'ric@@', 'ch@@', 'ezz@@', 'a@@', ',', 'che', 'è', 'stato', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'l@@', '<unk>', 'in@@', 'ver@@', 't@@', 'ic@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:51:11,934 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:51:11,934 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:51:11,935 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due conseguenze di questa due diapositiva, che gli impiegati arricchezza, che è stato fatto per tre milioni di anni per l<unk> inverticale.
2025-05-29 23:51:11,935 - INFO - joeynmt.training - Example #1
2025-05-29 23:51:11,935 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:51:11,935 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:51:11,935 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'ma', 'non', 'è', 'così', 'la', 'più', 'grande', 'parte', 'di', 'questo', 'probl@@', 'em@@', 'a@@', ',', 'perché', 'non', 'lo', 'fa', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'at@@', 'o@@', '.', '</s>']
2025-05-29 23:51:11,936 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:51:11,936 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:51:11,936 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, ma non è così la più grande parte di questo problema, perché non lo fa il dell<unk> Eislato.
2025-05-29 23:51:11,936 - INFO - joeynmt.training - Example #2
2025-05-29 23:51:11,936 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:51:11,936 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:51:11,936 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'd@@', 'ell@@', '<unk>', 'et@@', 'i@@', 'ch@@', 'etta', 'il', 'cuore', 'del', 'nostro', 'sistema', 'sistema', 'sistema', 'di', 'cal@@', 'col@@', 'o@@', '.', '</s>']
2025-05-29 23:51:11,936 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:51:11,937 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:51:11,937 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cuore dell<unk> etichetta il cuore del nostro sistema sistema sistema di calcolo.
2025-05-29 23:51:11,937 - INFO - joeynmt.training - Example #3
2025-05-29 23:51:11,937 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:51:11,937 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:51:11,937 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'in@@', 'ver@@', 'no', 'e', 'in@@', 'ver@@', 'ti@@', 're', 'in', 'S@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:51:11,937 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:51:11,937 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:51:11,937 - INFO - joeynmt.training - 	Hypothesis: L<unk> inverno e invertire in Sommer.
2025-05-29 23:51:11,937 - INFO - joeynmt.training - Example #4
2025-05-29 23:51:11,938 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:51:11,938 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:51:11,938 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'di@@', 'seg@@', 'nat@@', 'a@@', ',', 'che', 'è', 'acca@@', 'duto', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:51:11,938 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:51:11,938 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:51:11,938 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza che vi mostrerò è una disegnata, che è accaduto negli ultimi 25 anni.
2025-05-29 23:51:15,421 - INFO - joeynmt.training - Epoch   4, Step:    36100, Batch Loss:     1.833599, Batch Acc: 0.447422, Tokens per Sec:    16577, Lr: 0.000300
2025-05-29 23:51:18,866 - INFO - joeynmt.training - Epoch   4, Step:    36200, Batch Loss:     1.880980, Batch Acc: 0.447025, Tokens per Sec:    19788, Lr: 0.000300
2025-05-29 23:51:22,306 - INFO - joeynmt.training - Epoch   4, Step:    36300, Batch Loss:     1.945801, Batch Acc: 0.448826, Tokens per Sec:    19618, Lr: 0.000300
2025-05-29 23:51:25,780 - INFO - joeynmt.training - Epoch   4, Step:    36400, Batch Loss:     2.068172, Batch Acc: 0.449217, Tokens per Sec:    20653, Lr: 0.000300
2025-05-29 23:51:29,227 - INFO - joeynmt.training - Epoch   4, Step:    36500, Batch Loss:     1.938906, Batch Acc: 0.450624, Tokens per Sec:    20028, Lr: 0.000300
2025-05-29 23:51:29,227 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:51:29,227 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:51:37,795 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.89, ppl:   6.65, acc:   0.48, generation: 8.5575[sec], evaluation: 0.0000[sec]
2025-05-29 23:51:37,796 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:51:38,445 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/33500.ckpt
2025-05-29 23:51:38,475 - INFO - joeynmt.training - Example #0
2025-05-29 23:51:38,475 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:51:38,476 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:51:38,476 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'il', 'cal@@', 'am@@', 'pio', 'di', 'ghi@@', 'accio', 'che', 'l@@', '<unk>', 'ar@@', 'co', 'di', 'ghi@@', 'accio', 'che', 'per', 'tre', 'milioni', 'di', 'anni', 'gli', 'Stati', 'Uniti', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'più', 'grande', 'di', '4@@', '8', 'stati', 'per', 'cento', 'di', 'anni', 'per', 'l@@', '<unk>', 'anno', 'scor@@', 'r@@', 'ett@@', 'o@@', '.', '</s>']
2025-05-29 23:51:38,476 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:51:38,476 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:51:38,476 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive che il calampio di ghiaccio che l<unk> arco di ghiaccio che per tre milioni di anni gli Stati Uniti per tre milioni di anni di più grande di 48 stati per cento di anni per l<unk> anno scorretto.
2025-05-29 23:51:38,476 - INFO - joeynmt.training - Example #1
2025-05-29 23:51:38,477 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:51:38,477 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:51:38,477 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'più', 'for@@', 'te@@', ',', 'la', 'più', 'grande', 'è', 'la', 'più', 'grande', 'di', 'questo', 'problema', 'speci@@', 'ale', 'che', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'amento', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'a@@', '.', '</s>']
2025-05-29 23:51:38,477 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:51:38,477 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:51:38,477 - INFO - joeynmt.training - 	Hypothesis: Ma non è più forte, la più grande è la più grande di questo problema speciale che non è il dell<unk> Eislamento dell<unk> Eisla.
2025-05-29 23:51:38,477 - INFO - joeynmt.training - Example #2
2025-05-29 23:51:38,477 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:51:38,478 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:51:38,478 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'cosa', 'più', 'intelli@@', 'gente', 'è', 'il', 'cuore', 'del', 'nostro', 'sistema', 'sistema', 'di', 'c@@', 'lim@@', 'ati@@', 'vo@@', '.', '</s>']
2025-05-29 23:51:38,478 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:51:38,478 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:51:38,478 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cosa più intelligente è il cuore del nostro sistema sistema di climativo.
2025-05-29 23:51:38,478 - INFO - joeynmt.training - Example #3
2025-05-29 23:51:38,478 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:51:38,478 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:51:38,478 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'nel', 's@@', 'ettore', 'di', 'S@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:51:38,479 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:51:38,479 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:51:38,479 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo nel settore di Sommer.
2025-05-29 23:51:38,479 - INFO - joeynmt.training - Example #4
2025-05-29 23:51:38,479 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:51:38,479 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:51:38,479 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'cosa', 'che', 'vi', 'mostr@@', 'o', 'che', 'cosa', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:51:38,480 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:51:38,480 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:51:38,480 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza che vi mostrerò è una cosa che vi mostro che cosa è successo negli ultimi 25 anni.
2025-05-29 23:51:41,948 - INFO - joeynmt.training - Epoch   4, Step:    36600, Batch Loss:     2.162595, Batch Acc: 0.451934, Tokens per Sec:    16386, Lr: 0.000300
2025-05-29 23:51:45,396 - INFO - joeynmt.training - Epoch   4, Step:    36700, Batch Loss:     1.858211, Batch Acc: 0.450583, Tokens per Sec:    19777, Lr: 0.000300
2025-05-29 23:51:48,846 - INFO - joeynmt.training - Epoch   4, Step:    36800, Batch Loss:     1.844012, Batch Acc: 0.448091, Tokens per Sec:    20674, Lr: 0.000300
2025-05-29 23:51:52,286 - INFO - joeynmt.training - Epoch   4, Step:    36900, Batch Loss:     1.914780, Batch Acc: 0.449099, Tokens per Sec:    20235, Lr: 0.000300
2025-05-29 23:51:55,681 - INFO - joeynmt.training - Epoch   4, Step:    37000, Batch Loss:     1.726321, Batch Acc: 0.441508, Tokens per Sec:    20831, Lr: 0.000300
2025-05-29 23:51:55,682 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:51:55,682 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:52:03,802 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.89, ppl:   6.63, acc:   0.48, generation: 8.1098[sec], evaluation: 0.0000[sec]
2025-05-29 23:52:03,802 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:52:04,400 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/34500.ckpt
2025-05-29 23:52:04,428 - INFO - joeynmt.training - Example #0
2025-05-29 23:52:04,428 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:52:04,428 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:52:04,428 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'è', 'stato', 'un', 'ghi@@', 'acci@@', 'aio', 'che', 'l@@', '<unk>', 'E@@', 'is@@', 'co@@', ',', 'che', 'è', 'stato', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'il', '4@@', '8@@', '0@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'il', '4@@', '8@@', '0@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'raggi@@', 'ungere', 'il', '40', 'per', 'cento', 'per', 'cento', 'di', 'anni', 'per', 'raggi@@', 'ungere', 'il', '40', 'per', 'cento', 'di', 'anni', 'per', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'ci@@', 'dent@@', 'e@@', '.', '</s>']
2025-05-29 23:52:04,429 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:52:04,429 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:52:04,429 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive che è stato un ghiacciaio che l<unk> Eisco, che è stato fatto per tre milioni di anni il 480, per tre milioni di anni il 480, per tre milioni di anni per raggiungere il 40 per cento per cento di anni per raggiungere il 40 per cento di anni per la dimensione dell<unk> Occidente.
2025-05-29 23:52:04,429 - INFO - joeynmt.training - Example #1
2025-05-29 23:52:04,429 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:52:04,429 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:52:04,429 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'più', 'for@@', 'te@@', ',', 'la', 'cosa', 'più', 'grande', 'della', 'mia', 'vit@@', 'a@@', ',', 'perché', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'c@@', 'e@@', '.', '</s>']
2025-05-29 23:52:04,430 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:52:04,430 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:52:04,430 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto più forte, la cosa più grande della mia vita, perché non è il dell<unk> Eisce.
2025-05-29 23:52:04,430 - INFO - joeynmt.training - Example #2
2025-05-29 23:52:04,430 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:52:04,430 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:52:04,430 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'cosa', 'più', 'intelli@@', 'gente', 'è', 'il', 'cuore', 'del', 'nostro', 'sistema', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:52:04,431 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:52:04,431 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:52:04,431 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cosa più intelligente è il cuore del nostro sistema sistema globale.
2025-05-29 23:52:04,431 - INFO - joeynmt.training - Example #3
2025-05-29 23:52:04,431 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:52:04,431 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:52:04,431 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'prima', 'di', 'tut@@', 'to@@', ',', 'e', 's@@', 'ì@@', '.', '</s>']
2025-05-29 23:52:04,431 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:52:04,432 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:52:04,432 - INFO - joeynmt.training - 	Hypothesis: E prima di tutto, e sì.
2025-05-29 23:52:04,432 - INFO - joeynmt.training - Example #4
2025-05-29 23:52:04,432 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:52:04,432 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:52:04,432 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:52:04,432 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:52:04,432 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:52:04,433 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è un disegno che è successo negli ultimi 25 anni.
2025-05-29 23:52:07,906 - INFO - joeynmt.training - Epoch   4, Step:    37100, Batch Loss:     1.788328, Batch Acc: 0.445774, Tokens per Sec:    16212, Lr: 0.000300
2025-05-29 23:52:11,358 - INFO - joeynmt.training - Epoch   4, Step:    37200, Batch Loss:     2.080374, Batch Acc: 0.453843, Tokens per Sec:    19591, Lr: 0.000300
2025-05-29 23:52:14,846 - INFO - joeynmt.training - Epoch   4, Step:    37300, Batch Loss:     2.133952, Batch Acc: 0.444952, Tokens per Sec:    20638, Lr: 0.000300
2025-05-29 23:52:18,308 - INFO - joeynmt.training - Epoch   4, Step:    37400, Batch Loss:     2.015483, Batch Acc: 0.445188, Tokens per Sec:    19954, Lr: 0.000300
2025-05-29 23:52:21,761 - INFO - joeynmt.training - Epoch   4, Step:    37500, Batch Loss:     1.726338, Batch Acc: 0.458157, Tokens per Sec:    19717, Lr: 0.000300
2025-05-29 23:52:21,761 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:52:21,761 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:52:30,322 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.89, ppl:   6.63, acc:   0.48, generation: 8.5501[sec], evaluation: 0.0000[sec]
2025-05-29 23:52:30,323 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:52:30,978 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/35000.ckpt
2025-05-29 23:52:31,005 - INFO - joeynmt.training - Example #0
2025-05-29 23:52:31,005 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:52:31,006 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:52:31,006 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'questi', 'due', 's@@', 'ett@@', 'e@@', ',', 'per', 'vedere', 'che', 'il', 'ghi@@', 'acci@@', 'o@@', ',', 'che', 'gli', 'indi@@', 'c@@', 'ano', 'che', 'gli', 'el@@', 'ef@@', 'anti', 'di', 'ghi@@', 'accio', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'circa', '4@@', '8', 'stati', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'raggi@@', 'ungere', 'il', '4@@', '8', 'stati', 'per', 'cento', 'di', 'anni', 'per', 'il', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cento', 'di', 'stati', 'per', 'la', 'maggior', 'parte', 'dei', 'nostri', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:52:31,006 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:52:31,006 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:52:31,006 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato questi due sette, per vedere che il ghiaccio, che gli indicano che gli elefanti di ghiaccio per tre milioni di anni di circa 48 stati per tre milioni di anni per raggiungere il 48 stati per cento di anni per il 48 stati per cento per cento di stati per la maggior parte dei nostri anni.
2025-05-29 23:52:31,006 - INFO - joeynmt.training - Example #1
2025-05-29 23:52:31,007 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:52:31,007 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:52:31,007 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'la', 'cosa', 'più', 'importante', 'che', 'la', 'maggior', 'parte', 'di', 'questo', 'speci@@', 'ale', 'che', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-29 23:52:31,007 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:52:31,007 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:52:31,007 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, la cosa più importante che la maggior parte di questo speciale che non è il dell<unk> Eises.
2025-05-29 23:52:31,007 - INFO - joeynmt.training - Example #2
2025-05-29 23:52:31,008 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:52:31,008 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:52:31,008 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'lasse', 'ar@@', 't@@', 'ic@@', 'a@@', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'di', 'K@@', 'lim@@', 'e@@', '.', '</s>']
2025-05-29 23:52:31,008 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:52:31,008 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:52:31,008 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la classe artica, il cuore del nostro sistema di Klime.
2025-05-29 23:52:31,008 - INFO - joeynmt.training - Example #3
2025-05-29 23:52:31,008 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:52:31,009 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:52:31,009 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'cominci@@', 'a', 'a', 'a', 'a', 'mar@@', 'e@@', ',', 'e', 's@@', 'com@@', 'par@@', 'e@@', '.', '</s>']
2025-05-29 23:52:31,009 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:52:31,009 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:52:31,009 - INFO - joeynmt.training - 	Hypothesis: Si comincia a a a mare, e scompare.
2025-05-29 23:52:31,009 - INFO - joeynmt.training - Example #4
2025-05-29 23:52:31,009 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:52:31,009 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:52:31,009 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'che', 'vi', 'mostr@@', 'o', 'è', 'una', 'cosa', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:52:31,010 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:52:31,010 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:52:31,010 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza che vi mostro è una cosa che è successo negli ultimi 25 anni.
2025-05-29 23:52:34,503 - INFO - joeynmt.training - Epoch   4, Step:    37600, Batch Loss:     1.883478, Batch Acc: 0.449964, Tokens per Sec:    16168, Lr: 0.000300
2025-05-29 23:52:37,967 - INFO - joeynmt.training - Epoch   4, Step:    37700, Batch Loss:     2.057496, Batch Acc: 0.453223, Tokens per Sec:    19792, Lr: 0.000300
2025-05-29 23:52:38,214 - INFO - joeynmt.training - Epoch   4: total training loss 18836.08
2025-05-29 23:52:38,214 - INFO - joeynmt.training - EPOCH 5
2025-05-29 23:52:41,426 - INFO - joeynmt.training - Epoch   5, Step:    37800, Batch Loss:     1.698853, Batch Acc: 0.467902, Tokens per Sec:    19242, Lr: 0.000300
2025-05-29 23:52:44,890 - INFO - joeynmt.training - Epoch   5, Step:    37900, Batch Loss:     1.962013, Batch Acc: 0.465783, Tokens per Sec:    20221, Lr: 0.000300
2025-05-29 23:52:48,353 - INFO - joeynmt.training - Epoch   5, Step:    38000, Batch Loss:     1.844049, Batch Acc: 0.461520, Tokens per Sec:    19708, Lr: 0.000300
2025-05-29 23:52:48,353 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:52:48,353 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:52:55,568 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.89, ppl:   6.63, acc:   0.48, generation: 7.2010[sec], evaluation: 0.0000[sec]
2025-05-29 23:52:55,995 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/35500.ckpt
2025-05-29 23:52:56,023 - INFO - joeynmt.training - Example #0
2025-05-29 23:52:56,024 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:52:56,024 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:52:56,024 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 's@@', 'li@@', 'de', 'per', 'vedere', 'che', 'il', 'ghi@@', 'acci@@', 'o@@', ',', 'che', 'il', 'ghi@@', 'acci@@', 'o@@', ',', 'che', 'il', '4@@', '8', 'stati', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'c@@', 'att@@', 'ur@@', 'are', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'maggior', 'parte', 'delle', 'persone', 'che', 'hanno', 'fatto', 'in', 'grado', 'di', 'ri@@', 'vel@@', 'are', 'il', '40', 'per', 'cento', 'per', 'cento', 'di', 'questi', 'due', 'anni', 'per', 'cento', 'di', 'queste', 'dimen@@', 'sioni', 'per', 'i', 'nostri', 's@@', 'for@@', 'z@@', 'a@@', '.', '</s>']
2025-05-29 23:52:56,024 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:52:56,025 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:52:56,025 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due slide per vedere che il ghiaccio, che il ghiaccio, che il 48 stati per tre milioni di anni di catturare per tre milioni di anni per la maggior parte delle persone che hanno fatto in grado di rivelare il 40 per cento per cento di questi due anni per cento di queste dimensioni per i nostri sforza.
2025-05-29 23:52:56,025 - INFO - joeynmt.training - Example #1
2025-05-29 23:52:56,025 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:52:56,025 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:52:56,025 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'così', 'for@@', 'te@@', ',', 'la', 'più', 'grande', 'è', 'la', 'più', 'grande', 'cosa', 'che', 'non', 'è', 'il', 't@@', 'asso', 'di', 'di@@', 'f@@', 'ett@@', 'o@@', '.', '</s>']
2025-05-29 23:52:56,026 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:52:56,026 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:52:56,026 - INFO - joeynmt.training - 	Hypothesis: Ma non è così forte, la più grande è la più grande cosa che non è il tasso di difetto.
2025-05-29 23:52:56,026 - INFO - joeynmt.training - Example #2
2025-05-29 23:52:56,026 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:52:56,026 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:52:56,026 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'lasse', 'di', 'ghi@@', 'accio', 'ar@@', 't@@', 'ic@@', 'a@@', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'c@@', 'lim@@', 'ati@@', 'vo@@', '.', '</s>']
2025-05-29 23:52:56,026 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:52:56,027 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:52:56,027 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la classe di ghiaccio artica, il cuore del nostro sistema climativo.
2025-05-29 23:52:56,027 - INFO - joeynmt.training - Example #3
2025-05-29 23:52:56,027 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:52:56,027 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:52:56,027 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', 'er@@', 'to@@', ',', 'nel', 'vent@@', 'o@@', ',', 'e', 's@@', 'ì@@', '.', '</s>']
2025-05-29 23:52:56,027 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:52:56,027 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:52:56,027 - INFO - joeynmt.training - 	Hypothesis: Certo, nel vento, e sì.
2025-05-29 23:52:56,027 - INFO - joeynmt.training - Example #4
2025-05-29 23:52:56,028 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:52:56,028 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:52:56,028 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 'di@@', 'seg@@', 'no', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:52:56,028 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:52:56,028 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:52:56,028 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza che vi mostrerò è un disegno che è successo negli ultimi 25 anni.
2025-05-29 23:52:59,504 - INFO - joeynmt.training - Epoch   5, Step:    38100, Batch Loss:     1.999384, Batch Acc: 0.463424, Tokens per Sec:    17459, Lr: 0.000300
2025-05-29 23:53:02,928 - INFO - joeynmt.training - Epoch   5, Step:    38200, Batch Loss:     1.970226, Batch Acc: 0.465576, Tokens per Sec:    20053, Lr: 0.000300
2025-05-29 23:53:06,384 - INFO - joeynmt.training - Epoch   5, Step:    38300, Batch Loss:     1.789877, Batch Acc: 0.463225, Tokens per Sec:    19511, Lr: 0.000300
2025-05-29 23:53:09,841 - INFO - joeynmt.training - Epoch   5, Step:    38400, Batch Loss:     1.887070, Batch Acc: 0.461579, Tokens per Sec:    19845, Lr: 0.000300
2025-05-29 23:53:13,291 - INFO - joeynmt.training - Epoch   5, Step:    38500, Batch Loss:     1.831276, Batch Acc: 0.461341, Tokens per Sec:    19792, Lr: 0.000300
2025-05-29 23:53:13,292 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:53:13,292 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:53:21,483 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.89, ppl:   6.65, acc:   0.48, generation: 8.1803[sec], evaluation: 0.0000[sec]
2025-05-29 23:53:21,902 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/36000.ckpt
2025-05-29 23:53:21,927 - INFO - joeynmt.training - Example #0
2025-05-29 23:53:21,928 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:53:21,928 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:53:21,928 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 's@@', 'ett@@', 'e@@', ',', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'c@@', 'en@@', 'e@@', ',', 'che', 'il', 'ghi@@', 'acci@@', 'o@@', ',', 'che', 'è', 'stata', 'l@@', '<unk>', 'es@@', 'pressione', 'che', 'per', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'ri@@', 'vel@@', 'ata', '4@@', '8', 'stati', 'per', 'cento', 'milioni', 'di', 'anni', 'per', 'raggi@@', 'ungere', 'il', '40', 'per', 'cento', 'per', 'cento', 'di', 'questi', 'due', 'anni', 'per', 'raggi@@', 'ungere', 'il', '40', 'per', 'cento', 'del', '40', 'per', 'cento', 'del', '40', 'per', 'cento', 'del', '40', 'per', 'cento', 'del', '40', 'per', 'cento', 'di', 'questi', 'due', 'anni', 'fa@@', '.', '</s>']
2025-05-29 23:53:21,929 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:53:21,929 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:53:21,929 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due sette, per osservare che il ghiaccio arcene, che il ghiaccio, che è stata l<unk> espressione che per tre milioni di anni è stata rivelata 48 stati per cento milioni di anni per raggiungere il 40 per cento per cento di questi due anni per raggiungere il 40 per cento del 40 per cento del 40 per cento del 40 per cento del 40 per cento di questi due anni fa.
2025-05-29 23:53:21,929 - INFO - joeynmt.training - Example #1
2025-05-29 23:53:21,929 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:53:21,929 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:53:21,929 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'non', 'è', 'molto', 'più', 'la', 'più', 'grande', 'è', 'il', 'problema', 'che', 'non', 'è', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-29 23:53:21,930 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:53:21,930 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:53:21,930 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, non è molto più la più grande è il problema che non è che non è il dibattito del ghiaccio.
2025-05-29 23:53:21,930 - INFO - joeynmt.training - Example #2
2025-05-29 23:53:21,930 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:53:21,930 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:53:21,930 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'cosa', 'più', 'intelli@@', 'gente', 'è', 'il', 'cuore', 'del', 'nostro', 'sistema', 'c@@', 'lim@@', 'ativo', 'del', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:53:21,930 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:53:21,930 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:53:21,930 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cosa più intelligente è il cuore del nostro sistema climativo del nostro sistema globale.
2025-05-29 23:53:21,931 - INFO - joeynmt.training - Example #3
2025-05-29 23:53:21,931 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:53:21,931 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:53:21,931 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'prima', 'di', 'tutto', 'nel', 'senso', 'e', 's@@', 'fr@@', 'utt@@', 'are', 'il', 'vento', 'e', 's@@', 'ì@@', '.', '</s>']
2025-05-29 23:53:21,931 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:53:21,931 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:53:21,931 - INFO - joeynmt.training - 	Hypothesis: E prima di tutto nel senso e sfruttare il vento e sì.
2025-05-29 23:53:21,931 - INFO - joeynmt.training - Example #4
2025-05-29 23:53:21,931 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:53:21,932 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:53:21,932 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'che', 'vi', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'che', 'è', 'succ@@', 'ess@@', 'o@@', ',', 'che', 'è', 'succ@@', 'ess@@', 'o@@', '.', '</s>']
2025-05-29 23:53:21,932 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:53:21,932 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:53:21,932 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza che vi vi mostro è un disegno che è successo, che è successo.
2025-05-29 23:53:25,411 - INFO - joeynmt.training - Epoch   5, Step:    38600, Batch Loss:     2.046450, Batch Acc: 0.464847, Tokens per Sec:    17724, Lr: 0.000300
2025-05-29 23:53:28,872 - INFO - joeynmt.training - Epoch   5, Step:    38700, Batch Loss:     1.909158, Batch Acc: 0.455855, Tokens per Sec:    20111, Lr: 0.000300
2025-05-29 23:53:32,321 - INFO - joeynmt.training - Epoch   5, Step:    38800, Batch Loss:     1.763516, Batch Acc: 0.464932, Tokens per Sec:    19538, Lr: 0.000300
2025-05-29 23:53:35,797 - INFO - joeynmt.training - Epoch   5, Step:    38900, Batch Loss:     1.716266, Batch Acc: 0.460028, Tokens per Sec:    20303, Lr: 0.000300
2025-05-29 23:53:39,251 - INFO - joeynmt.training - Epoch   5, Step:    39000, Batch Loss:     1.891748, Batch Acc: 0.462082, Tokens per Sec:    19930, Lr: 0.000300
2025-05-29 23:53:39,252 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:53:39,252 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:53:46,010 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.89, ppl:   6.63, acc:   0.48, generation: 6.7512[sec], evaluation: 0.0000[sec]
2025-05-29 23:53:46,377 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/38500.ckpt
2025-05-29 23:53:46,403 - INFO - joeynmt.training - Example #0
2025-05-29 23:53:46,404 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:53:46,404 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:53:46,404 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'consegu@@', 'enze', 'per', 'con@@', 'ver@@', 't@@', 'ic@@', 'al@@', 'e@@', ',', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'ic@@', 'o@@', ',', 'che', 'è', 'stata', 'la', 'sua', 'dimen@@', 'sione', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'questa', 'dimen@@', 'sione', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'questa', 'è', 'stata', 'la', 'stessa', 'cos@@', 'a@@', '.', '</s>']
2025-05-29 23:53:46,404 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:53:46,405 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:53:46,405 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due conseguenze per converticale, che il ghiaccio arico, che è stata la sua dimensione per tre milioni di anni di questa dimensione per tre milioni di anni di questa è stata la stessa cosa.
2025-05-29 23:53:46,405 - INFO - joeynmt.training - Example #1
2025-05-29 23:53:46,405 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:53:46,405 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:53:46,405 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'la', 'più', 'grande', 'quantità', 'di', 'problemi', 'speci@@', 'ali@@', ',', 'perché', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'amento', 'di', 'ghi@@', 'accio', 'che', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'at@@', 'o@@', '.', '</s>']
2025-05-29 23:53:46,405 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:53:46,406 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:53:46,406 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, la più grande quantità di problemi speciali, perché non è il dell<unk> Eislamento di ghiaccio che non è il dell<unk> Eislato.
2025-05-29 23:53:46,406 - INFO - joeynmt.training - Example #2
2025-05-29 23:53:46,406 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:53:46,406 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:53:46,406 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'lasse', 'ar@@', 'ic@@', 'a@@', ',', 'la', 'c@@', 'atti@@', 'va', 'di', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ic@@', 'e@@', '.', '</s>']
2025-05-29 23:53:46,406 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:53:46,406 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:53:46,407 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la classe arica, la cattiva di ghiaccio arktice.
2025-05-29 23:53:46,407 - INFO - joeynmt.training - Example #3
2025-05-29 23:53:46,407 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:53:46,407 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:53:46,407 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'prima', 'di', 'tutto', 'nel', 'vent@@', 'o@@', ',', 'e', 'poi', 'si', 'è', 'ri@@', 'ma@@', 'sta', 'nel', 's@@', 'ettore', 's@@', 'om@@', 'ic@@', 'o@@', '.', '</s>']
2025-05-29 23:53:46,407 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:53:46,407 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:53:46,407 - INFO - joeynmt.training - 	Hypothesis: E prima di tutto nel vento, e poi si è rimasta nel settore somico.
2025-05-29 23:53:46,407 - INFO - joeynmt.training - Example #4
2025-05-29 23:53:46,408 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:53:46,408 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:53:46,408 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'cosa', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:53:46,408 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:53:46,408 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:53:46,408 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza che vi mostrerò è una cosa che è successo negli ultimi 25 anni.
2025-05-29 23:53:49,826 - INFO - joeynmt.training - Epoch   5, Step:    39100, Batch Loss:     1.861068, Batch Acc: 0.466089, Tokens per Sec:    18129, Lr: 0.000300
2025-05-29 23:53:53,284 - INFO - joeynmt.training - Epoch   5, Step:    39200, Batch Loss:     1.888387, Batch Acc: 0.459017, Tokens per Sec:    19678, Lr: 0.000300
2025-05-29 23:53:56,732 - INFO - joeynmt.training - Epoch   5, Step:    39300, Batch Loss:     2.003211, Batch Acc: 0.457634, Tokens per Sec:    19811, Lr: 0.000300
2025-05-29 23:54:00,185 - INFO - joeynmt.training - Epoch   5, Step:    39400, Batch Loss:     1.884213, Batch Acc: 0.462989, Tokens per Sec:    19334, Lr: 0.000300
2025-05-29 23:54:03,620 - INFO - joeynmt.training - Epoch   5, Step:    39500, Batch Loss:     2.270694, Batch Acc: 0.463493, Tokens per Sec:    19604, Lr: 0.000300
2025-05-29 23:54:03,620 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:54:03,620 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:54:11,501 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.89, ppl:   6.60, acc:   0.48, generation: 7.8712[sec], evaluation: 0.0000[sec]
2025-05-29 23:54:11,501 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:54:12,204 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/36500.ckpt
2025-05-29 23:54:12,230 - INFO - joeynmt.training - Example #0
2025-05-29 23:54:12,230 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:54:12,230 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:54:12,230 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'vedere', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'co', 'di', 'ghi@@', 'accio', 'ar@@', 'ch@@', 'e@@', ',', 'che', 'l@@', '<unk>', 'e@@', '<unk>', ',', 'che', 'è', 'stato', 'il', '4@@', '8', 'stati', 'd@@', '<unk>', 'acqua', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'l@@', '<unk>', 'età', 'di', '4@@', '8', 'stati', 'per', 'cento', 'anni', 'di', 'circa', 'il', '4@@', '8', 'per', 'cento', 'di', 'questi', 'con@@', 'fin@@', 'i@@', ',', 'il', '4@@', '8', 'per', 'cento', 'di', 'questi', 'con@@', 'fin@@', 'i', 'per', 'la', 'maggior', 'parte', 'di', 'no@@', 'i@@', ',', 'e', 'poi', 'ho', 'mostr@@', 'ato', 'la', 'più', 'grande', 'cos@@', 'a@@', '.', '</s>']
2025-05-29 23:54:12,231 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:54:12,231 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:54:12,231 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per vedere che il ghiaccio arco di ghiaccio arche, che l<unk> e<unk> , che è stato il 48 stati d<unk> acqua per tre milioni di anni per l<unk> età di 48 stati per cento anni di circa il 48 per cento di questi confini, il 48 per cento di questi confini per la maggior parte di noi, e poi ho mostrato la più grande cosa.
2025-05-29 23:54:12,231 - INFO - joeynmt.training - Example #1
2025-05-29 23:54:12,231 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:54:12,231 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:54:12,231 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'ma', 'non', 'è', 'molto', 'più', 'for@@', 'te@@', ',', 'che', 'non', 'è', 'il', 't@@', 'asso', 'di', 'di@@', 'sag@@', 'i@@', 'o@@', '.', '</s>']
2025-05-29 23:54:12,232 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:54:12,232 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:54:12,232 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, ma non è molto più forte, che non è il tasso di disagio.
2025-05-29 23:54:12,232 - INFO - joeynmt.training - Example #2
2025-05-29 23:54:12,232 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:54:12,232 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:54:12,232 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'di', 'ghi@@', 'accio', 'ar@@', 'ic@@', 'o@@', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'di', 'K@@', 'lim@@', 'as@@', 'sist@@', 'o@@', '.', '</s>']
2025-05-29 23:54:12,233 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:54:12,233 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:54:12,233 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cuore di ghiaccio arico, il cuore del nostro sistema di Klimassisto.
2025-05-29 23:54:12,233 - INFO - joeynmt.training - Example #3
2025-05-29 23:54:12,233 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:54:12,233 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:54:12,233 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'prima', 'di', 'tutto', 'nel', 'vent@@', 'o@@', ',', 'e', 'sc@@', 'ambi@@', 'ato', 'nel', 's@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:54:12,233 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:54:12,234 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:54:12,234 - INFO - joeynmt.training - 	Hypothesis: E prima di tutto nel vento, e scambiato nel sommer.
2025-05-29 23:54:12,234 - INFO - joeynmt.training - Example #4
2025-05-29 23:54:12,234 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:54:12,234 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:54:12,234 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'edia', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:54:12,234 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:54:12,234 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:54:12,234 - INFO - joeynmt.training - 	Hypothesis: La prossima sedia che vi mostro è un disegno di disegno di quello che negli ultimi 25 anni.
2025-05-29 23:54:15,706 - INFO - joeynmt.training - Epoch   5, Step:    39600, Batch Loss:     1.885144, Batch Acc: 0.458773, Tokens per Sec:    16715, Lr: 0.000300
2025-05-29 23:54:19,149 - INFO - joeynmt.training - Epoch   5, Step:    39700, Batch Loss:     1.697820, Batch Acc: 0.457453, Tokens per Sec:    19722, Lr: 0.000300
2025-05-29 23:54:22,590 - INFO - joeynmt.training - Epoch   5, Step:    39800, Batch Loss:     1.838547, Batch Acc: 0.463434, Tokens per Sec:    19983, Lr: 0.000300
2025-05-29 23:54:26,031 - INFO - joeynmt.training - Epoch   5, Step:    39900, Batch Loss:     1.803711, Batch Acc: 0.460670, Tokens per Sec:    19790, Lr: 0.000300
2025-05-29 23:54:29,470 - INFO - joeynmt.training - Epoch   5, Step:    40000, Batch Loss:     1.933818, Batch Acc: 0.460199, Tokens per Sec:    19805, Lr: 0.000300
2025-05-29 23:54:29,470 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:54:29,470 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:54:37,846 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.88, ppl:   6.58, acc:   0.48, generation: 8.3658[sec], evaluation: 0.0000[sec]
2025-05-29 23:54:37,847 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:54:38,533 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/39000.ckpt
2025-05-29 23:54:38,563 - INFO - joeynmt.training - Example #0
2025-05-29 23:54:38,563 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:54:38,563 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:54:38,563 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'l@@', '<unk>', 'E@@', 'str@@', 'an@@', 'a@@', ',', 'che', 'l@@', '<unk>', 'al@@', 'bero', 'd@@', 'ell@@', '<unk>', 'acqua', 'che', 'ha', 'fatto', 'l@@', '<unk>', 'al@@', 'te@@', 'zza', 'di', 'tre', 'milioni', 'di', 'anni', 'di', 'gr@@', 'av@@', 'it@@', 'à@@', ',', 'il', '4@@', '8', 'stati', 'per', 'cento', 'di', 'anni', 'di', '4@@', '8', 'stati', 'per', 'cento', 'di', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:54:38,564 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:54:38,564 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:54:38,564 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per osservare che l<unk> Estrana, che l<unk> albero dell<unk> acqua che ha fatto l<unk> altezza di tre milioni di anni di gravità, il 48 stati per cento di anni di 48 stati per cento di anni.
2025-05-29 23:54:38,564 - INFO - joeynmt.training - Example #1
2025-05-29 23:54:38,564 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:54:38,564 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:54:38,564 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'ma', 'non', 'è', 'molto', 'più', 'for@@', 'te@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-29 23:54:38,565 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:54:38,565 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:54:38,565 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, ma non è molto più forte, perché non è il dibattito del ghiaccio.
2025-05-29 23:54:38,565 - INFO - joeynmt.training - Example #2
2025-05-29 23:54:38,565 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:54:38,565 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:54:38,565 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'l@@', '<unk>', 'ar@@', 'te@@', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'c@@', 'atti@@', 'vo', 'del', 'nostro', 'sistema', 'c@@', 'atti@@', 'vo@@', '.', '</s>']
2025-05-29 23:54:38,566 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:54:38,566 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:54:38,566 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, l<unk> arte, il cuore del nostro sistema cattivo del nostro sistema cattivo.
2025-05-29 23:54:38,566 - INFO - joeynmt.training - Example #3
2025-05-29 23:54:38,566 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:54:38,566 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:54:38,566 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'in@@', 'ver@@', 'no@@', ',', 'e', 'dol@@', 'c@@', 'e@@', ',', 'e', 'dol@@', 'c@@', 'e@@', '.', '</s>']
2025-05-29 23:54:38,567 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:54:38,567 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:54:38,567 - INFO - joeynmt.training - 	Hypothesis: L<unk> inverno, e dolce, e dolce.
2025-05-29 23:54:38,567 - INFO - joeynmt.training - Example #4
2025-05-29 23:54:38,567 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:54:38,567 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:54:38,567 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:54:38,568 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:54:38,568 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:54:38,568 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva che vi mostro è un disegno di quello che è successo negli ultimi 25 anni.
2025-05-29 23:54:42,047 - INFO - joeynmt.training - Epoch   5, Step:    40100, Batch Loss:     1.998076, Batch Acc: 0.458957, Tokens per Sec:    16249, Lr: 0.000300
2025-05-29 23:54:45,510 - INFO - joeynmt.training - Epoch   5, Step:    40200, Batch Loss:     1.807555, Batch Acc: 0.460973, Tokens per Sec:    19458, Lr: 0.000300
2025-05-29 23:54:48,975 - INFO - joeynmt.training - Epoch   5, Step:    40300, Batch Loss:     1.853690, Batch Acc: 0.462904, Tokens per Sec:    19748, Lr: 0.000300
2025-05-29 23:54:52,401 - INFO - joeynmt.training - Epoch   5, Step:    40400, Batch Loss:     1.907857, Batch Acc: 0.455577, Tokens per Sec:    19459, Lr: 0.000300
2025-05-29 23:54:55,853 - INFO - joeynmt.training - Epoch   5, Step:    40500, Batch Loss:     1.873793, Batch Acc: 0.464695, Tokens per Sec:    19718, Lr: 0.000300
2025-05-29 23:54:55,854 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:54:55,854 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:55:04,047 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.88, ppl:   6.57, acc:   0.48, generation: 8.1834[sec], evaluation: 0.0000[sec]
2025-05-29 23:55:04,048 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:55:04,633 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/38000.ckpt
2025-05-29 23:55:04,662 - INFO - joeynmt.training - Example #0
2025-05-29 23:55:04,662 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:55:04,662 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:55:04,662 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 's@@', 'li@@', 'de', 'per', 'vedere', 'che', 'il', 'ghi@@', 'acci@@', 'o@@', ',', 'che', 'il', 'ghi@@', 'acci@@', 'aio', 'di', 'ghi@@', 'accio', 'che', 'è', 'stato', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'dimen@@', 'sione', 'del', '4@@', '8@@', '.', '</s>']
2025-05-29 23:55:04,663 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:55:04,663 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:55:04,663 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due slide per vedere che il ghiaccio, che il ghiacciaio di ghiaccio che è stato fatto per tre milioni di anni che ha fatto per tre milioni di anni per la dimensione del 48.
2025-05-29 23:55:04,663 - INFO - joeynmt.training - Example #1
2025-05-29 23:55:04,663 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:55:04,663 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:55:04,663 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'più', 'for@@', 'te@@', ',', 'la', 'ter@@', 'za', 'cosa', 'che', 'non', 'è', 'più', 'for@@', 'te@@', ',', 'perché', 'non', 'lo', 'di@@', 'seg@@', 'na', 'di', 'ghi@@', 'accio', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es', 'e', 'l@@', '<unk>', 'or@@', 'o@@', '.', '</s>']
2025-05-29 23:55:04,664 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:55:04,664 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:55:04,664 - INFO - joeynmt.training - 	Hypothesis: Ma non è più forte, la terza cosa che non è più forte, perché non lo disegna di ghiaccio dell<unk> Eises e l<unk> oro.
2025-05-29 23:55:04,664 - INFO - joeynmt.training - Example #2
2025-05-29 23:55:04,664 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:55:04,664 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:55:04,664 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'ru@@', 'ci@@', 'ale', 'che', 'si', 'trova', 'il', 'cuore', 'del', 'nostro', 'sistema', 'c@@', 'atti@@', 'vo', 'del', 'nostro', 'sistema', 'c@@', 'atti@@', 'vo', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:55:04,665 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:55:04,665 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:55:04,665 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cruciale che si trova il cuore del nostro sistema cattivo del nostro sistema cattivo globale.
2025-05-29 23:55:04,665 - INFO - joeynmt.training - Example #3
2025-05-29 23:55:04,665 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:55:04,665 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:55:04,665 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', 'è', 'un', 'altro', 'che', 'si', 'trova', 'nel', 's@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:55:04,666 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:55:04,666 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:55:04,666 - INFO - joeynmt.training - 	Hypothesis: C<unk> è un altro che si trova nel sommer.
2025-05-29 23:55:04,666 - INFO - joeynmt.training - Example #4
2025-05-29 23:55:04,666 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:55:04,666 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:55:04,666 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:55:04,666 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:55:04,666 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:55:04,666 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è un disegno di quello che è successo negli ultimi 25 anni.
2025-05-29 23:55:08,102 - INFO - joeynmt.training - Epoch   5, Step:    40600, Batch Loss:     2.141993, Batch Acc: 0.460894, Tokens per Sec:    16928, Lr: 0.000300
2025-05-29 23:55:11,544 - INFO - joeynmt.training - Epoch   5, Step:    40700, Batch Loss:     1.742774, Batch Acc: 0.463869, Tokens per Sec:    20122, Lr: 0.000300
2025-05-29 23:55:14,978 - INFO - joeynmt.training - Epoch   5, Step:    40800, Batch Loss:     1.812574, Batch Acc: 0.458200, Tokens per Sec:    19778, Lr: 0.000300
2025-05-29 23:55:18,429 - INFO - joeynmt.training - Epoch   5, Step:    40900, Batch Loss:     2.033779, Batch Acc: 0.464608, Tokens per Sec:    20636, Lr: 0.000300
2025-05-29 23:55:21,878 - INFO - joeynmt.training - Epoch   5, Step:    41000, Batch Loss:     2.142212, Batch Acc: 0.463736, Tokens per Sec:    20204, Lr: 0.000300
2025-05-29 23:55:21,878 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:55:21,878 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:55:29,306 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.88, ppl:   6.55, acc:   0.48, generation: 7.4183[sec], evaluation: 0.0000[sec]
2025-05-29 23:55:29,307 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:55:29,930 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/37000.ckpt
2025-05-29 23:55:29,958 - INFO - joeynmt.training - Example #0
2025-05-29 23:55:29,959 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:55:29,959 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:55:29,959 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'ho', 'mostr@@', 'ato', 'che', 'l@@', '<unk>', 'epi@@', 'de@@', 'mia', 'che', 'ha', 'fatto', 'l@@', '<unk>', 'el@@', 'abor@@', 'azione', 'di', 'ghi@@', 'accio', 'che', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', '4@@', '8', 'stati', 'per', 'cento', 'per', 'la', 'popolazione', 'd@@', 'ell@@', '<unk>', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cento', 'di', 'anni', 'per', 'raggi@@', 'ungere', 'il', '40', 'per', 'cento', 'di', 'questi', 'due', 'di@@', 'apos@@', 'iti@@', 'v@@', 'a@@', '.', '</s>']
2025-05-29 23:55:29,960 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:55:29,960 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:55:29,960 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive che ho mostrato che l<unk> epidemia che ha fatto l<unk> elaborazione di ghiaccio che per tre milioni di anni per la dimensione dell<unk> 48 stati per cento per la popolazione dell<unk> 48 stati per cento per cento di anni per raggiungere il 40 per cento di questi due diapositiva.
2025-05-29 23:55:29,960 - INFO - joeynmt.training - Example #1
2025-05-29 23:55:29,960 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:55:29,960 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:55:29,960 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'non', 'è', 'molto', 'più', 'for@@', 'te@@', ',', 'non', 'è', 'molto', 'più', 'for@@', 'te@@', ',', 'perché', 'non', 'è', 'il', 'D@@', 'ic@@', 'ke', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'a@@', '.', '</s>']
2025-05-29 23:55:29,961 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:55:29,961 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:55:29,961 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, non è molto più forte, non è molto più forte, perché non è il Dicke dell<unk> Eisla.
2025-05-29 23:55:29,961 - INFO - joeynmt.training - Example #2
2025-05-29 23:55:29,961 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:55:29,961 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:55:29,961 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'att@@', 'ica', 'di', 'ghi@@', 'accio', 'che', 'il', 'cuore', 'del', 'nostro', 'sistema', 'c@@', 'atti@@', 'vo@@', '.', '</s>']
2025-05-29 23:55:29,961 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:55:29,961 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:55:29,962 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cattica di ghiaccio che il cuore del nostro sistema cattivo.
2025-05-29 23:55:29,962 - INFO - joeynmt.training - Example #3
2025-05-29 23:55:29,962 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:55:29,962 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:55:29,962 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'prima', 'di', 'tut@@', 'to@@', ',', 'e', 's@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:55:29,962 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:55:29,962 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:55:29,962 - INFO - joeynmt.training - 	Hypothesis: E prima di tutto, e sommer.
2025-05-29 23:55:29,962 - INFO - joeynmt.training - Example #4
2025-05-29 23:55:29,963 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:55:29,963 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:55:29,963 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'che', 'vi', 'mostr@@', 'o', 'è', 'una', 'cosa', 'che', 'è', 'succ@@', 'ess@@', 'o@@', ',', 'che', 'è', 'acc@@', 'ad@@', 'ut@@', 'o@@', '.', '</s>']
2025-05-29 23:55:29,963 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:55:29,963 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:55:29,963 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva che vi mostro è una cosa che è successo, che è accaduto.
2025-05-29 23:55:33,441 - INFO - joeynmt.training - Epoch   5, Step:    41100, Batch Loss:     1.799037, Batch Acc: 0.459766, Tokens per Sec:    16673, Lr: 0.000300
2025-05-29 23:55:36,903 - INFO - joeynmt.training - Epoch   5, Step:    41200, Batch Loss:     2.027922, Batch Acc: 0.459116, Tokens per Sec:    20640, Lr: 0.000300
2025-05-29 23:55:40,369 - INFO - joeynmt.training - Epoch   5, Step:    41300, Batch Loss:     1.809838, Batch Acc: 0.462485, Tokens per Sec:    20167, Lr: 0.000300
2025-05-29 23:55:43,828 - INFO - joeynmt.training - Epoch   5, Step:    41400, Batch Loss:     1.854093, Batch Acc: 0.463424, Tokens per Sec:    20274, Lr: 0.000300
2025-05-29 23:55:47,279 - INFO - joeynmt.training - Epoch   5, Step:    41500, Batch Loss:     1.818190, Batch Acc: 0.463162, Tokens per Sec:    19953, Lr: 0.000300
2025-05-29 23:55:47,279 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:55:47,279 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:55:55,186 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.88, ppl:   6.52, acc:   0.48, generation: 7.8962[sec], evaluation: 0.0000[sec]
2025-05-29 23:55:55,186 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:55:55,863 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/37500.ckpt
2025-05-29 23:55:55,891 - INFO - joeynmt.training - Example #0
2025-05-29 23:55:55,891 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:55:55,891 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:55:55,891 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'l@@', '<unk>', 'et@@', 'à@@', ',', 'che', 'la', 'l@@', '<unk>', 'et@@', 'i@@', 'ch@@', 'ett@@', 'a@@', ',', 'che', 'l@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'u@@', 'mi@@', 'di@@', '.', '</s>']
2025-05-29 23:55:55,892 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:55:55,892 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:55:55,892 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive che l<unk> età, che la l<unk> etichetta, che l<unk> anno scorso, che ha fatto per tre milioni di anni di umidi.
2025-05-29 23:55:55,892 - INFO - joeynmt.training - Example #1
2025-05-29 23:55:55,892 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:55:55,892 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:55:55,892 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'una', 'cosa', 'che', 'è', 'la', 'prima', 'cosa', 'che', 'non', 'è', 'la', 'sost@@', 'eni@@', 'bilità', 'di', 'questo', 'speci@@', 'fico', 'speci@@', 'al@@', 'e@@', ',', 'perché', 'non', 'lo', 'fa', 'il', 'D@@', 'ic@@', 'ke', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-29 23:55:55,893 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:55:55,893 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:55:55,893 - INFO - joeynmt.training - 	Hypothesis: Ma non è una cosa che è la prima cosa che non è la sostenibilità di questo specifico speciale, perché non lo fa il Dicke dell<unk> Eises.
2025-05-29 23:55:55,893 - INFO - joeynmt.training - Example #2
2025-05-29 23:55:55,893 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:55:55,893 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:55:55,893 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'att@@', 'ura', 'di', 'ghi@@', 'accio', 'che', 'il', 'cuore', 'c@@', 'att@@', 'ur@@', 'ato', 'del', 'nostro', 'sistema', 'di', 'c@@', 'ru@@', 'del@@', 'e', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:55:55,894 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:55:55,894 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:55:55,894 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cattura di ghiaccio che il cuore catturato del nostro sistema di crudele globale.
2025-05-29 23:55:55,894 - INFO - joeynmt.training - Example #3
2025-05-29 23:55:55,894 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:55:55,894 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:55:55,894 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'in@@', 'ver@@', 'no@@', ',', 'e', 's@@', 'ì@@', ',', 'in', 'S@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:55:55,894 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:55:55,895 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:55:55,895 - INFO - joeynmt.training - 	Hypothesis: L<unk> inverno, e sì, in Sommer.
2025-05-29 23:55:55,895 - INFO - joeynmt.training - Example #4
2025-05-29 23:55:55,895 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:55:55,895 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:55:55,895 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'cosa', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:55:55,895 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:55:55,895 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:55:55,896 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una cosa che è successo negli ultimi 25 anni.
2025-05-29 23:55:59,388 - INFO - joeynmt.training - Epoch   5, Step:    41600, Batch Loss:     2.210072, Batch Acc: 0.454767, Tokens per Sec:    16523, Lr: 0.000300
2025-05-29 23:56:02,851 - INFO - joeynmt.training - Epoch   5, Step:    41700, Batch Loss:     1.961902, Batch Acc: 0.460391, Tokens per Sec:    19690, Lr: 0.000300
2025-05-29 23:56:06,307 - INFO - joeynmt.training - Epoch   5, Step:    41800, Batch Loss:     2.029566, Batch Acc: 0.463376, Tokens per Sec:    19658, Lr: 0.000300
2025-05-29 23:56:09,762 - INFO - joeynmt.training - Epoch   5, Step:    41900, Batch Loss:     1.899447, Batch Acc: 0.460760, Tokens per Sec:    19830, Lr: 0.000300
2025-05-29 23:56:13,207 - INFO - joeynmt.training - Epoch   5, Step:    42000, Batch Loss:     1.969520, Batch Acc: 0.456875, Tokens per Sec:    19931, Lr: 0.000300
2025-05-29 23:56:13,208 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:56:13,208 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:56:21,373 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.88, ppl:   6.52, acc:   0.48, generation: 8.1549[sec], evaluation: 0.0000[sec]
2025-05-29 23:56:21,373 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:56:22,170 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/39500.ckpt
2025-05-29 23:56:22,189 - INFO - joeynmt.training - Example #0
2025-05-29 23:56:22,189 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:56:22,189 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:56:22,189 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'consegu@@', 'enze', 'per', 'osserv@@', 'are', 'che', 'la', 'gente', 'ha', 'fatto', 'che', 'la', 'gente', 'che', 'ha', 'fatto', 'per', 'le', 'dimen@@', 'sioni', 'che', 'hanno', 'creato', 'per', 'i', 'tre', 'milioni', 'di', 'anni', 'di', 's@@', 'fr@@', 'utt@@', 'are', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'e@@', '<unk>', 'è', 'il', '4@@', '8@@', '0@@', '%', 'del', 'li@@', 'b@@', 'ic@@', 'chi@@', 'er@@', 'e@@', '.', '</s>']
2025-05-29 23:56:22,190 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:56:22,190 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:56:22,190 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due conseguenze per osservare che la gente ha fatto che la gente che ha fatto per le dimensioni che hanno creato per i tre milioni di anni di sfruttare la dimensione dell<unk> Oce<unk> è il 480% del libicchiere.
2025-05-29 23:56:22,190 - INFO - joeynmt.training - Example #1
2025-05-29 23:56:22,190 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:56:22,190 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:56:22,190 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'così', 'che', 'non', 'si', 'può', 'essere', 'abbastanza', 'più', 'pot@@', 'ente', 'da', 'questo', 'problema', 'che', 'non', 'lo', 'fa', 'in', 'questo', 'mod@@', 'o@@', ',', 'perché', 'non', 'lo', 'fa', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'amento', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'at@@', 'o@@', '.', '</s>']
2025-05-29 23:56:22,190 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:56:22,190 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:56:22,191 - INFO - joeynmt.training - 	Hypothesis: Ma non è così che non si può essere abbastanza più potente da questo problema che non lo fa in questo modo, perché non lo fa il dell<unk> Eislamento dell<unk> Eislato.
2025-05-29 23:56:22,191 - INFO - joeynmt.training - Example #2
2025-05-29 23:56:22,191 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:56:22,191 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:56:22,191 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'cosa', 'più', 'importante', 'è', 'che', 'il', 'cuore', 'del', 'nostro', 'sistema', 'c@@', 'atti@@', 'vo', 'di', 'c@@', 'au@@', 'se', 'del', 'nostro', 'sistema', 'c@@', 'lin@@', 'ic@@', 'o@@', '.', '</s>']
2025-05-29 23:56:22,191 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:56:22,191 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:56:22,191 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cosa più importante è che il cuore del nostro sistema cattivo di cause del nostro sistema clinico.
2025-05-29 23:56:22,191 - INFO - joeynmt.training - Example #3
2025-05-29 23:56:22,192 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:56:22,192 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:56:22,192 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'cosa', 'che', 'si', 'può', 'essere', 'in', 'grado', 'di', 'fare', 'con', 'il', 'suo', 's@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:56:22,192 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:56:22,192 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:56:22,192 - INFO - joeynmt.training - 	Hypothesis: La prima cosa che si può essere in grado di fare con il suo sommer.
2025-05-29 23:56:22,192 - INFO - joeynmt.training - Example #4
2025-05-29 23:56:22,192 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:56:22,192 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:56:22,193 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'una', 'cosa', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:56:22,193 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:56:22,193 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:56:22,193 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è una cosa che è successo negli ultimi 25 anni.
2025-05-29 23:56:25,619 - INFO - joeynmt.training - Epoch   5, Step:    42100, Batch Loss:     1.887857, Batch Acc: 0.458993, Tokens per Sec:    16366, Lr: 0.000300
2025-05-29 23:56:29,046 - INFO - joeynmt.training - Epoch   5, Step:    42200, Batch Loss:     1.945077, Batch Acc: 0.456102, Tokens per Sec:    19736, Lr: 0.000300
2025-05-29 23:56:32,474 - INFO - joeynmt.training - Epoch   5, Step:    42300, Batch Loss:     1.815253, Batch Acc: 0.460068, Tokens per Sec:    20104, Lr: 0.000300
2025-05-29 23:56:35,874 - INFO - joeynmt.training - Epoch   5, Step:    42400, Batch Loss:     1.781899, Batch Acc: 0.462616, Tokens per Sec:    19546, Lr: 0.000300
2025-05-29 23:56:39,301 - INFO - joeynmt.training - Epoch   5, Step:    42500, Batch Loss:     1.765972, Batch Acc: 0.460744, Tokens per Sec:    19784, Lr: 0.000300
2025-05-29 23:56:39,302 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:56:39,302 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:56:46,246 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.87, ppl:   6.46, acc:   0.48, generation: 6.9347[sec], evaluation: 0.0000[sec]
2025-05-29 23:56:46,247 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:56:46,781 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/40000.ckpt
2025-05-29 23:56:46,801 - INFO - joeynmt.training - Example #0
2025-05-29 23:56:46,801 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:56:46,801 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:56:46,801 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'consegu@@', 'enze', 'per', 'osserv@@', 'are', 'le', 'due', 'consegu@@', 'enze', 'd@@', 'ell@@', '<unk>', 'et@@', 'is@@', 'ca', 'che', 'l@@', '<unk>', 'et@@', 'i@@', 'ch@@', 'e@@', ',', 'che', 'l@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'e', 'l@@', '<unk>', 'anno', 'scorso', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'dimen@@', 'sione', 'del', '4@@', '8', 'stati', 'per', 'cento', 'per', 'la', 'dimen@@', 'sione', 'del', '4@@', '8', 'stati', 'per', 'cento', 'per', 'la', 'popolazione', 'è', 'ri@@', 'ma@@', 'sta', 'per', 'ri@@', 'durre', 'il', '4@@', '8', 'stati', 'per', 'cento', 'di', 'questi', 'due', 'p@@', 'ezzi', 'di', 'p@@', 'ec@@', 'or@@', 'e@@', '.', '</s>']
2025-05-29 23:56:46,802 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:56:46,802 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:56:46,802 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due conseguenze per osservare le due conseguenze dell<unk> etisca che l<unk> etiche, che l<unk> anno scorso, e l<unk> anno scorso per tre milioni di anni per la dimensione del 48 stati per cento per la dimensione del 48 stati per cento per la popolazione è rimasta per ridurre il 48 stati per cento di questi due pezzi di pecore.
2025-05-29 23:56:46,802 - INFO - joeynmt.training - Example #1
2025-05-29 23:56:46,802 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:56:46,802 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:56:46,802 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'la', 'più', 'grande', 'della', 'di@@', 'eta', 'di', 'questo', 'problema', 'perché', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'amento', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'am@@', 'ent@@', 'e@@', '.', '</s>']
2025-05-29 23:56:46,803 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:56:46,803 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:56:46,803 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, la più grande della dieta di questo problema perché non è il dell<unk> Eislamento dell<unk> Eislamente.
2025-05-29 23:56:46,803 - INFO - joeynmt.training - Example #2
2025-05-29 23:56:46,803 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:56:46,803 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:56:46,803 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'cosa', 'più', 'importante', 'è', 'la', 'c@@', 'att@@', 'ura', 'di', 'ghi@@', 'accio', 'che', 'il', 'cuore', 'del', 'nostro', 'sistema', 'sistema', 'sistema', 'sistema', 'cli@@', 'mat@@', 'ic@@', 'o@@', '.', '</s>']
2025-05-29 23:56:46,803 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:56:46,804 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:56:46,804 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cosa più importante è la cattura di ghiaccio che il cuore del nostro sistema sistema sistema sistema climatico.
2025-05-29 23:56:46,804 - INFO - joeynmt.training - Example #3
2025-05-29 23:56:46,804 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:56:46,804 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:56:46,804 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', 'è', 'un', 'po@@', '<unk>', 'di', 's@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:56:46,804 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:56:46,804 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:56:46,804 - INFO - joeynmt.training - 	Hypothesis: C<unk> è un po<unk> di sommer.
2025-05-29 23:56:46,805 - INFO - joeynmt.training - Example #4
2025-05-29 23:56:46,805 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:56:46,805 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:56:46,805 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'i', 'è', 'un', 'seg@@', 'gi@@', 'ol@@', 'in@@', 'e@@', ',', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:56:46,805 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:56:46,805 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:56:46,805 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostri è un seggioline, che è successo negli ultimi 25 anni.
2025-05-29 23:56:50,211 - INFO - joeynmt.training - Epoch   5, Step:    42600, Batch Loss:     2.086226, Batch Acc: 0.457953, Tokens per Sec:    17545, Lr: 0.000300
2025-05-29 23:56:53,589 - INFO - joeynmt.training - Epoch   5, Step:    42700, Batch Loss:     2.095486, Batch Acc: 0.456510, Tokens per Sec:    20109, Lr: 0.000300
2025-05-29 23:56:56,957 - INFO - joeynmt.training - Epoch   5, Step:    42800, Batch Loss:     1.858618, Batch Acc: 0.463502, Tokens per Sec:    19754, Lr: 0.000300
2025-05-29 23:57:00,348 - INFO - joeynmt.training - Epoch   5, Step:    42900, Batch Loss:     1.743452, Batch Acc: 0.455862, Tokens per Sec:    20079, Lr: 0.000300
2025-05-29 23:57:03,730 - INFO - joeynmt.training - Epoch   5, Step:    43000, Batch Loss:     1.920012, Batch Acc: 0.456224, Tokens per Sec:    20739, Lr: 0.000300
2025-05-29 23:57:03,731 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:57:03,731 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:57:11,763 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.87, ppl:   6.46, acc:   0.48, generation: 8.0254[sec], evaluation: 0.0000[sec]
2025-05-29 23:57:12,131 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/40500.ckpt
2025-05-29 23:57:12,156 - INFO - joeynmt.training - Example #0
2025-05-29 23:57:12,156 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:57:12,156 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:57:12,156 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'consegu@@', 'enze', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'acci@@', 'o@@', ',', 'che', 'il', 'ghi@@', 'acci@@', 'o@@', ',', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'i', 'gruppi', 'di', 'tre', 'milioni', 'di', 'anni', 'di', 'dimen@@', 'sione', 'di', 'circa', '4@@', '8', 'stati', 'per', 'cento', 'di', 'questi', 'due', 'anni', 'per', 'il', '40', 'per', 'cento', 'del', '2@@', '0@@', '.', '</s>']
2025-05-29 23:57:12,157 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:57:12,157 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:57:12,157 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due conseguenze per osservare che il ghiaccio, che il ghiaccio, che ha fatto per tre milioni di anni i gruppi di tre milioni di anni di dimensione di circa 48 stati per cento di questi due anni per il 40 per cento del 20.
2025-05-29 23:57:12,157 - INFO - joeynmt.training - Example #1
2025-05-29 23:57:12,157 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:57:12,157 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:57:12,157 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'che', 'non', 'è', 'abbastanza', 'la', 'nostra', 'st@@', 'abil@@', 'it@@', 'à@@', ',', 'ma', 'non', 'è', 'il', 'D@@', 'ic@@', 'ke', 'd@@', 'ell@@', '<unk>', 'E@@', '<unk>', '.', '</s>']
2025-05-29 23:57:12,158 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:57:12,158 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:57:12,158 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, che non è abbastanza la nostra stabilità, ma non è il Dicke dell<unk> E<unk> .
2025-05-29 23:57:12,158 - INFO - joeynmt.training - Example #2
2025-05-29 23:57:12,158 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:57:12,158 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:57:12,158 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'd@@', 'ell@@', '<unk>', 'et@@', 'er@@', 'na', 'ar@@', 'ram@@', 'pic@@', 'ata', 'd@@', 'ell@@', '<unk>', 'ar@@', 'te@@', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'c@@', 'atti@@', 'vo@@', '.', '</s>']
2025-05-29 23:57:12,159 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:57:12,159 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:57:12,159 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cuore dell<unk> eterna arrampicata dell<unk> arte, il cuore del nostro sistema cattivo.
2025-05-29 23:57:12,159 - INFO - joeynmt.training - Example #3
2025-05-29 23:57:12,159 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:57:12,159 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:57:12,159 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'prima', 'di', 'tutto', 'il', 'mon@@', 'do@@', ',', 'e', 's@@', 'ì@@', ',', 'in', 'est@@', 'at@@', 'e@@', ',', 'e', 'poi', 'si', 'è', 'ri@@', 'ma@@', 'sto', 'in', 'modo', 'che', 'non', 'è', 'stato', 'un', 'po@@', '<unk>', 'di', 'più', 'di', 'quello', 'che', 'è', 'successo', 'in', 'questo', 'mod@@', 'o@@', ',', 'e', 'poi', 'si', 'è', 'ri@@', 'ma@@', 'sto', 'per', 'far', 'sì', 'che', 'il', 'mondo', 'non', 'è', 'mai', 'stato', 'fatto', 'in', 'modo', 'che', 'il', 'mondo', 'non', 'è', 'mai', 'stato', 'fatto', 'in', 'modo', 'che', 'il', 'mondo', 'non', 'è', 'che', 'non', 'è', 'mai', 'stato', 'un', 'po@@', '<unk>', 'di', 'più', 'di', 'quanto', 'il', 'mondo', 'sia', 'stato', 'in', 'cui', 'non', 'è', 'mai', 'stato', 'fatto', 'in', 'modo', 'che', 'il', 'mondo']
2025-05-29 23:57:12,160 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:57:12,160 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:57:12,160 - INFO - joeynmt.training - 	Hypothesis: E prima di tutto il mondo, e sì, in estate, e poi si è rimasto in modo che non è stato un po<unk> di più di quello che è successo in questo modo, e poi si è rimasto per far sì che il mondo non è mai stato fatto in modo che il mondo non è mai stato fatto in modo che il mondo non è che non è mai stato un po<unk> di più di quanto il mondo sia stato in cui non è mai stato fatto in modo che il mondo
2025-05-29 23:57:12,160 - INFO - joeynmt.training - Example #4
2025-05-29 23:57:12,160 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:57:12,160 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:57:12,160 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'cosa', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:57:12,161 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:57:12,161 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:57:12,161 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza che vi mostrerò è una cosa che è successo negli ultimi 25 anni.
2025-05-29 23:57:15,493 - INFO - joeynmt.training - Epoch   5, Step:    43100, Batch Loss:     1.965997, Batch Acc: 0.457841, Tokens per Sec:    18454, Lr: 0.000300
2025-05-29 23:57:18,834 - INFO - joeynmt.training - Epoch   5, Step:    43200, Batch Loss:     1.886416, Batch Acc: 0.459595, Tokens per Sec:    20838, Lr: 0.000300
2025-05-29 23:57:22,239 - INFO - joeynmt.training - Epoch   5, Step:    43300, Batch Loss:     1.830610, Batch Acc: 0.461035, Tokens per Sec:    19496, Lr: 0.000300
2025-05-29 23:57:25,680 - INFO - joeynmt.training - Epoch   5, Step:    43400, Batch Loss:     2.086195, Batch Acc: 0.460219, Tokens per Sec:    20117, Lr: 0.000300
2025-05-29 23:57:29,134 - INFO - joeynmt.training - Epoch   5, Step:    43500, Batch Loss:     1.842374, Batch Acc: 0.465068, Tokens per Sec:    20195, Lr: 0.000300
2025-05-29 23:57:29,134 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:57:29,134 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:57:37,016 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.86, ppl:   6.43, acc:   0.48, generation: 7.8715[sec], evaluation: 0.0000[sec]
2025-05-29 23:57:37,016 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:57:37,689 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/41000.ckpt
2025-05-29 23:57:37,717 - INFO - joeynmt.training - Example #0
2025-05-29 23:57:37,718 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:57:37,718 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:57:37,718 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'consegu@@', 'enze', 'per', 'vedere', 'che', 'il', 'ghi@@', 'acci@@', 'o@@', ',', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'ch@@', 'e@@', ',', 'che', 'il', 'ghi@@', 'acci@@', 'o@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'con@@', 'ver@@', 't@@', 'ic@@', 'ale', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'maggior', 'parte', 'dei', 'nostri', 'figli', 'è', 'il', '4@@', '8', 'stati', 'per', 'cento', 'del', '4@@', '8', 'stati', 'per', 'cento', 'di', 'questi', 'due', 'di@@', 'pin@@', 'ti@@', '.', '</s>']
2025-05-29 23:57:37,718 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:57:37,719 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:57:37,719 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due conseguenze per vedere che il ghiaccio, che il ghiaccio arche, che il ghiaccio, per tre milioni di anni di converticale per tre milioni di anni per la maggior parte dei nostri figli è il 48 stati per cento del 48 stati per cento di questi due dipinti.
2025-05-29 23:57:37,719 - INFO - joeynmt.training - Example #1
2025-05-29 23:57:37,719 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:57:37,719 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:57:37,719 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'così', 'che', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'la', 'cosa', 'più', 'importante', 'è', 'che', 'non', 'c@@', '<unk>', 'è', 'il', 't@@', 'asso', 'di', 'ghi@@', 'accio', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-29 23:57:37,720 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:57:37,720 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:57:37,720 - INFO - joeynmt.training - 	Hypothesis: Ma non è così che non è abbastanza forte, la cosa più importante è che non c<unk> è il tasso di ghiaccio dell<unk> Eises.
2025-05-29 23:57:37,720 - INFO - joeynmt.training - Example #2
2025-05-29 23:57:37,720 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:57:37,720 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:57:37,720 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'di', 'ghi@@', 'accio', 'ar@@', 'ch@@', 'e@@', ',', 'il', 'cuore', 'c@@', 'atti@@', 'vo', 'del', 'nostro', 'sistema', 'c@@', 'atti@@', 'vo', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:57:37,720 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:57:37,721 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:57:37,721 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cuore di ghiaccio arche, il cuore cattivo del nostro sistema cattivo globale.
2025-05-29 23:57:37,721 - INFO - joeynmt.training - Example #3
2025-05-29 23:57:37,721 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:57:37,721 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:57:37,721 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'cominci@@', 'a', 'a', 'a', 'c@@', 'att@@', 'ur@@', 'are', 'il', 't@@', 'asso', 'di', 'di@@', 'stru@@', 'g@@', 'gere', 'il', 'suo', 's@@', 'om@@', 'o@@', '.', '</s>']
2025-05-29 23:57:37,721 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:57:37,721 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:57:37,721 - INFO - joeynmt.training - 	Hypothesis: E comincia a a catturare il tasso di distruggere il suo somo.
2025-05-29 23:57:37,722 - INFO - joeynmt.training - Example #4
2025-05-29 23:57:37,722 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:57:37,722 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:57:37,722 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 'seg@@', 'no', 'di', 'tem@@', 'po@@', ',', 'che', 'cosa', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:57:37,722 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:57:37,722 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:57:37,722 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è un segno di tempo, che cosa è successo negli ultimi 25 anni.
2025-05-29 23:57:41,206 - INFO - joeynmt.training - Epoch   5, Step:    43600, Batch Loss:     1.907681, Batch Acc: 0.455085, Tokens per Sec:    16434, Lr: 0.000300
2025-05-29 23:57:44,655 - INFO - joeynmt.training - Epoch   5, Step:    43700, Batch Loss:     1.943239, Batch Acc: 0.458596, Tokens per Sec:    20036, Lr: 0.000300
2025-05-29 23:57:48,119 - INFO - joeynmt.training - Epoch   5, Step:    43800, Batch Loss:     2.018781, Batch Acc: 0.456765, Tokens per Sec:    20241, Lr: 0.000300
2025-05-29 23:57:51,569 - INFO - joeynmt.training - Epoch   5, Step:    43900, Batch Loss:     1.785029, Batch Acc: 0.459658, Tokens per Sec:    20167, Lr: 0.000300
2025-05-29 23:57:55,034 - INFO - joeynmt.training - Epoch   5, Step:    44000, Batch Loss:     1.896560, Batch Acc: 0.462285, Tokens per Sec:    20256, Lr: 0.000300
2025-05-29 23:57:55,034 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:57:55,034 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:58:03,232 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.86, ppl:   6.45, acc:   0.48, generation: 8.1918[sec], evaluation: 0.0000[sec]
2025-05-29 23:58:03,760 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/41500.ckpt
2025-05-29 23:58:03,788 - INFO - joeynmt.training - Example #0
2025-05-29 23:58:03,788 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:58:03,788 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:58:03,788 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'l@@', '<unk>', 'et@@', 'i@@', 'ch@@', 'etta', 'ar@@', 'ric@@', 'ci@@', 'ale', 'che', 'l@@', '<unk>', 'et@@', 'i@@', 'ch@@', 'etta', 'che', 'gli', 'el@@', 'ef@@', 'anti', 'di', 'questi', 'due', 's@@', 'ett@@', 'ori', 'che', 'hanno', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'm@@', 'ezz@@', 'o@@', '.', '</s>']
2025-05-29 23:58:03,789 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:58:03,789 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:58:03,789 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive che l<unk> etichetta arricciale che l<unk> etichetta che gli elefanti di questi due settori che hanno fatto per tre milioni di anni di mezzo.
2025-05-29 23:58:03,789 - INFO - joeynmt.training - Example #1
2025-05-29 23:58:03,789 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:58:03,789 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:58:03,789 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'la', 'cosa', 'che', 'non', 'è', 'abbastanza', 'la', 'cosa', 'che', 'non', 'lo', 'fa', 'il', 'problema', 'di', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-29 23:58:03,790 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:58:03,790 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:58:03,790 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte, la cosa che non è abbastanza la cosa che non lo fa il problema di ghiaccio.
2025-05-29 23:58:03,790 - INFO - joeynmt.training - Example #2
2025-05-29 23:58:03,790 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:58:03,790 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:58:03,790 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'ar@@', 't@@', 'ic@@', 'o@@', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'c@@', 'atti@@', 'vo', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:58:03,790 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:58:03,791 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:58:03,791 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cuore artico, il cuore del nostro sistema cattivo globale.
2025-05-29 23:58:03,791 - INFO - joeynmt.training - Example #3
2025-05-29 23:58:03,791 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:58:03,791 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:58:03,791 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'prima', 'di', 'tut@@', 'to@@', ',', 'nel', 'vent@@', 'o@@', ',', 'e', 'il', 't@@', 'asso', 'di', 'mor@@', 't@@', 'alità', 'nel', 's@@', 'ettore', 'del', 'sangu@@', 'e@@', '.', '</s>']
2025-05-29 23:58:03,791 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:58:03,791 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:58:03,792 - INFO - joeynmt.training - 	Hypothesis: E prima di tutto, nel vento, e il tasso di mortalità nel settore del sangue.
2025-05-29 23:58:03,792 - INFO - joeynmt.training - Example #4
2025-05-29 23:58:03,792 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:58:03,792 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:58:03,792 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:58:03,792 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:58:03,792 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:58:03,792 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è un disegno che è successo negli ultimi 25 anni.
2025-05-29 23:58:07,227 - INFO - joeynmt.training - Epoch   5, Step:    44100, Batch Loss:     1.907612, Batch Acc: 0.453059, Tokens per Sec:    17256, Lr: 0.000300
2025-05-29 23:58:10,686 - INFO - joeynmt.training - Epoch   5, Step:    44200, Batch Loss:     1.782958, Batch Acc: 0.459436, Tokens per Sec:    19892, Lr: 0.000300
2025-05-29 23:58:14,136 - INFO - joeynmt.training - Epoch   5, Step:    44300, Batch Loss:     1.866053, Batch Acc: 0.450417, Tokens per Sec:    19723, Lr: 0.000300
2025-05-29 23:58:17,585 - INFO - joeynmt.training - Epoch   5, Step:    44400, Batch Loss:     1.995556, Batch Acc: 0.464140, Tokens per Sec:    19705, Lr: 0.000300
2025-05-29 23:58:21,045 - INFO - joeynmt.training - Epoch   5, Step:    44500, Batch Loss:     1.994946, Batch Acc: 0.459937, Tokens per Sec:    20029, Lr: 0.000300
2025-05-29 23:58:21,045 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:58:21,045 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:58:28,394 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.86, ppl:   6.43, acc:   0.48, generation: 7.3393[sec], evaluation: 0.0000[sec]
2025-05-29 23:58:28,395 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:58:29,031 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/42000.ckpt
2025-05-29 23:58:29,060 - INFO - joeynmt.training - Example #0
2025-05-29 23:58:29,061 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:58:29,061 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:58:29,061 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'ho', 'mostr@@', 'ato', 'che', 'i', 'ghi@@', 'acci@@', 'ai', 'che', 'i', 'ghi@@', 'acci@@', 'ai', 'che', 'i', 'ghi@@', 'acci@@', 'ai', 'di', 'ghi@@', 'accio', 'ar@@', 'ch@@', 'e@@', ',', 'e', 'che', 'ha', 'fatto', 'il', '4@@', '8', 'stati', 'per', 'cento', 'di', 'questi', 'gruppi', 'di', 'n@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'e@@', 'ano', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cento', 'del', '4@@', '8', 'stati', 'per', 'cento', 'per', 'il', '40', 'per', 'cento', 'del', 'li@@', 'mi@@', 'te', 'di', 'circa', '40', 'per', 'cento', 'del', 'li@@', 'mi@@', 'te', 'di', 'circa', '40', 'per', 'cento', 'per', 'cento', 'del', '2@@', '4@@', '8', 'per', 'cento', 'di', 'questi', 'due', 'di@@', 'apos@@', 'iti@@', '.', '</s>']
2025-05-29 23:58:29,062 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:58:29,062 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:58:29,062 - INFO - joeynmt.training - 	Hypothesis: Ho mostrato queste due diapositive che ho mostrato che i ghiacciai che i ghiacciai che i ghiacciai di ghiaccio arche, e che ha fatto il 48 stati per cento di questi gruppi di nell<unk> Oceano 48 stati per cento per cento del 48 stati per cento per il 40 per cento del limite di circa 40 per cento del limite di circa 40 per cento per cento del 248 per cento di questi due diapositi.
2025-05-29 23:58:29,062 - INFO - joeynmt.training - Example #1
2025-05-29 23:58:29,062 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:58:29,062 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:58:29,062 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'la', 'più', 'grande', 'della', 'più', 'grande', 'cos@@', 'a@@', ',', 'perché', 'non', 'lo', 'fa', 'in', 'realtà', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es', 'che', 'si', 'trova', 'in', 'un', 'certo', 'sen@@', 'so@@', '.', '</s>']
2025-05-29 23:58:29,063 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:58:29,063 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:58:29,063 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, la più grande della più grande cosa, perché non lo fa in realtà non è il dibattito dell<unk> Eises che non è il dibattito dell<unk> Eises che si trova in un certo senso.
2025-05-29 23:58:29,063 - INFO - joeynmt.training - Example #2
2025-05-29 23:58:29,063 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:58:29,063 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:58:29,063 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'ar@@', 'ch@@', 'e@@', ',', 'il', 'cuore', 'c@@', 'att@@', 'ico', 'del', 'nostro', 'sistema', 'di', 'c@@', 'atti@@', 'vo@@', '.', '</s>']
2025-05-29 23:58:29,064 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:58:29,064 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:58:29,064 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cuore arche, il cuore cattico del nostro sistema di cattivo.
2025-05-29 23:58:29,064 - INFO - joeynmt.training - Example #3
2025-05-29 23:58:29,064 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:58:29,064 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:58:29,064 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'cominci@@', 'a', 'a', 'a', 's@@', 'v@@', 'egli@@', 'are', 'e', 'dol@@', 'c@@', 'e@@', '.', '</s>']
2025-05-29 23:58:29,064 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:58:29,064 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:58:29,065 - INFO - joeynmt.training - 	Hypothesis: Si comincia a a svegliare e dolce.
2025-05-29 23:58:29,065 - INFO - joeynmt.training - Example #4
2025-05-29 23:58:29,065 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:58:29,065 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:58:29,065 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'una', 'di@@', 'stri@@', 'bu@@', 'zione', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:58:29,065 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:58:29,065 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:58:29,065 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è una distribuzione che è successo negli ultimi 25 anni.
2025-05-29 23:58:32,513 - INFO - joeynmt.training - Epoch   5, Step:    44600, Batch Loss:     1.990247, Batch Acc: 0.461256, Tokens per Sec:    16600, Lr: 0.000300
2025-05-29 23:58:35,998 - INFO - joeynmt.training - Epoch   5, Step:    44700, Batch Loss:     1.867197, Batch Acc: 0.464361, Tokens per Sec:    19904, Lr: 0.000300
2025-05-29 23:58:39,462 - INFO - joeynmt.training - Epoch   5, Step:    44800, Batch Loss:     1.856437, Batch Acc: 0.462588, Tokens per Sec:    20358, Lr: 0.000300
2025-05-29 23:58:42,907 - INFO - joeynmt.training - Epoch   5, Step:    44900, Batch Loss:     1.898946, Batch Acc: 0.457925, Tokens per Sec:    19605, Lr: 0.000300
2025-05-29 23:58:46,355 - INFO - joeynmt.training - Epoch   5, Step:    45000, Batch Loss:     2.079020, Batch Acc: 0.458893, Tokens per Sec:    19767, Lr: 0.000300
2025-05-29 23:58:46,355 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:58:46,355 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:58:54,273 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.86, ppl:   6.43, acc:   0.48, generation: 7.9074[sec], evaluation: 0.0000[sec]
2025-05-29 23:58:54,718 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/43000.ckpt
2025-05-29 23:58:54,746 - INFO - joeynmt.training - Example #0
2025-05-29 23:58:54,746 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:58:54,746 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:58:54,746 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'l@@', '<unk>', 'et@@', 'i@@', 'ch@@', 'etta', 'di', 'ghi@@', 'accio', 'ar@@', 'c@@', 'ic@@', 'l@@', 'o@@', ',', 'che', 'l@@', '<unk>', 'hanno', 'ri@@', 'ma@@', 'sto', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'ri@@', 'durre', 'il', '4@@', '8', 'stati', 'per', 'cent@@', 'o@@', '.', '</s>']
2025-05-29 23:58:54,747 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:58:54,747 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:58:54,747 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive che l<unk> etichetta di ghiaccio arciclo, che l<unk> hanno rimasto per tre milioni di anni per ridurre il 48 stati per cento.
2025-05-29 23:58:54,747 - INFO - joeynmt.training - Example #1
2025-05-29 23:58:54,748 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:58:54,748 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:58:54,748 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'così', 'che', 'la', 'cosa', 'più', 'importante', 'è', 'la', 'cosa', 'più', 'importante', 'è', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es', 'l@@', '<unk>', 'et@@', 'ic@@', 'a@@', '.', '</s>']
2025-05-29 23:58:54,748 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:58:54,748 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:58:54,748 - INFO - joeynmt.training - 	Hypothesis: Ma non è così che la cosa più importante è la cosa più importante è che non è il dibattito dell<unk> Eises l<unk> etica.
2025-05-29 23:58:54,748 - INFO - joeynmt.training - Example #2
2025-05-29 23:58:54,748 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:58:54,749 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:58:54,749 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'cosa', 'più', 'importante', 'del', 'ghi@@', 'accio', 'è', 'il', 'nostro', 'cuore', 'di', 'c@@', 'att@@', 'ic@@', 'lo', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:58:54,749 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:58:54,749 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:58:54,749 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cosa più importante del ghiaccio è il nostro cuore di catticlo globale.
2025-05-29 23:58:54,749 - INFO - joeynmt.training - Example #3
2025-05-29 23:58:54,749 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:58:54,749 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:58:54,749 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'si', 'tratta', 'di', 'in@@', 'ver@@', 'no@@', ',', 'e', 'si', 's@@', 'ente', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-29 23:58:54,750 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:58:54,750 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:58:54,750 - INFO - joeynmt.training - 	Hypothesis: E si tratta di inverno, e si sente in estate.
2025-05-29 23:58:54,750 - INFO - joeynmt.training - Example #4
2025-05-29 23:58:54,750 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:58:54,750 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:58:54,750 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'seg@@', 'no', 'di', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'succ@@', 'ess@@', 'o@@', ',', 'in', 'realtà', 'è', 'succ@@', 'ess@@', 'o@@', '.', '</s>']
2025-05-29 23:58:54,751 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:58:54,751 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:58:54,751 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è un segno di disegno di quello che è successo, in realtà è successo.
2025-05-29 23:58:58,217 - INFO - joeynmt.training - Epoch   5, Step:    45100, Batch Loss:     1.900636, Batch Acc: 0.459439, Tokens per Sec:    18061, Lr: 0.000300
2025-05-29 23:59:01,654 - INFO - joeynmt.training - Epoch   5, Step:    45200, Batch Loss:     2.292686, Batch Acc: 0.461043, Tokens per Sec:    20246, Lr: 0.000300
2025-05-29 23:59:05,088 - INFO - joeynmt.training - Epoch   5, Step:    45300, Batch Loss:     1.842405, Batch Acc: 0.465231, Tokens per Sec:    19874, Lr: 0.000300
2025-05-29 23:59:08,526 - INFO - joeynmt.training - Epoch   5, Step:    45400, Batch Loss:     1.996980, Batch Acc: 0.461968, Tokens per Sec:    20240, Lr: 0.000300
2025-05-29 23:59:11,970 - INFO - joeynmt.training - Epoch   5, Step:    45500, Batch Loss:     1.813550, Batch Acc: 0.460834, Tokens per Sec:    20018, Lr: 0.000300
2025-05-29 23:59:11,970 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:59:11,970 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:59:19,323 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.86, ppl:   6.41, acc:   0.48, generation: 7.3463[sec], evaluation: 0.0000[sec]
2025-05-29 23:59:19,323 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-29 23:59:19,867 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/42500.ckpt
2025-05-29 23:59:19,893 - INFO - joeynmt.training - Example #0
2025-05-29 23:59:19,893 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:59:19,893 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:59:19,893 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'di@@', 'apos@@', 'iti@@', 've', 'di', 'ghi@@', 'acci@@', 'o@@', ',', 'che', 'è', 'il', 'ghi@@', 'acci@@', 'o@@', ',', 'che', 'è', 'stato', 'il', 'ghi@@', 'acci@@', 'o@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'gr@@', 'av@@', 'it@@', 'à@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'gr@@', 'av@@', 'it@@', 'à@@', '.', '</s>']
2025-05-29 23:59:19,893 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:59:19,893 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:59:19,893 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due didiapositive di ghiaccio, che è il ghiaccio, che è stato il ghiaccio, per tre milioni di anni di gravità, per tre milioni di anni di gravità.
2025-05-29 23:59:19,894 - INFO - joeynmt.training - Example #1
2025-05-29 23:59:19,894 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:59:19,894 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:59:19,894 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'la', 'maggior', 'parte', 'delle', 'persone', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'di', 'questo', 'problema', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es', 'in', 'modo', 'che', 'il', 'di@@', 'batt@@', 'ito', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es', 'e', 'la', 'c@@', 'lasse', 'di', 'un', 'sistema', 'di', 'ri@@', 'fer@@', 'imento', 'di', 'un', 'sistema', 'di', 'ri@@', 'fer@@', 'imento', 'di', 'un', 'sistema', 'di', 'ri@@', 'fer@@', 'imento', 'di', 'un', 'sistema', 'di', 'ri@@', 'fer@@', 'imento', 'di', 'un', 'v@@', 'et@@', 'r@@', 'ett@@', 'or@@', 'e@@', ',', 'e', 'non', 'è', 'un', 'po@@', '<unk>', 'di', 'fronte', 'a', 'un', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-29 23:59:19,894 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:59:19,894 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:59:19,894 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, la maggior parte delle persone che non è il dibattito di questo problema perché non è il dibattito dell<unk> Eises in modo che il dibattito dell<unk> Eises e la classe di un sistema di riferimento di un sistema di riferimento di un sistema di riferimento di un sistema di riferimento di un vetrettore, e non è un po<unk> di fronte a un ghiaccio.
2025-05-29 23:59:19,894 - INFO - joeynmt.training - Example #2
2025-05-29 23:59:19,895 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:59:19,895 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:59:19,895 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'lasse', 'ar@@', 't@@', 'ic@@', 'a@@', ',', 'il', 'cuore', 'c@@', 'ru@@', 'ci@@', 'ale', 'del', 'nostro', 'sistema', 'sistema', 'sistema', 'sistema', 'di', 'c@@', 'lim@@', 'as@@', 'o@@', '.', '</s>']
2025-05-29 23:59:19,895 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:59:19,895 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:59:19,895 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la classe artica, il cuore cruciale del nostro sistema sistema sistema sistema di climaso.
2025-05-29 23:59:19,895 - INFO - joeynmt.training - Example #3
2025-05-29 23:59:19,895 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:59:19,895 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:59:19,896 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Com@@', 'inci@@', 'amo', 'a', 'fare', 'il', 'vento', 'e', 's@@', 'otti@@', 'le', 'di', 's@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-29 23:59:19,896 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:59:19,896 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:59:19,896 - INFO - joeynmt.training - 	Hypothesis: Cominciamo a fare il vento e sottile di sommer.
2025-05-29 23:59:19,896 - INFO - joeynmt.training - Example #4
2025-05-29 23:59:19,896 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:59:19,896 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:59:19,896 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'che', 'vi', 'mostr@@', 'o', 'è', 'una', 'ri@@', 'vista', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:59:19,897 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:59:19,897 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:59:19,897 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza che vi mostro è una rivista che è successo negli ultimi 25 anni.
2025-05-29 23:59:23,198 - INFO - joeynmt.training - Epoch   5, Step:    45600, Batch Loss:     1.940952, Batch Acc: 0.463915, Tokens per Sec:    17346, Lr: 0.000300
2025-05-29 23:59:26,479 - INFO - joeynmt.training - Epoch   5, Step:    45700, Batch Loss:     2.119201, Batch Acc: 0.456492, Tokens per Sec:    21385, Lr: 0.000300
2025-05-29 23:59:29,802 - INFO - joeynmt.training - Epoch   5, Step:    45800, Batch Loss:     1.680733, Batch Acc: 0.462988, Tokens per Sec:    21067, Lr: 0.000300
2025-05-29 23:59:33,142 - INFO - joeynmt.training - Epoch   5, Step:    45900, Batch Loss:     1.635327, Batch Acc: 0.450832, Tokens per Sec:    20364, Lr: 0.000300
2025-05-29 23:59:36,488 - INFO - joeynmt.training - Epoch   5, Step:    46000, Batch Loss:     2.093956, Batch Acc: 0.461058, Tokens per Sec:    20339, Lr: 0.000300
2025-05-29 23:59:36,488 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-29 23:59:36,488 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-29 23:59:43,674 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.86, ppl:   6.41, acc:   0.48, generation: 7.1789[sec], evaluation: 0.0000[sec]
2025-05-29 23:59:44,059 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/44000.ckpt
2025-05-29 23:59:44,080 - INFO - joeynmt.training - Example #0
2025-05-29 23:59:44,081 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-29 23:59:44,081 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-29 23:59:44,081 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'il', 'ghi@@', 'acci@@', 'o@@', ',', 'che', 'il', 'ghi@@', 'acci@@', 'o@@', ',', 'che', 'il', 'ghi@@', 'acci@@', 'o@@', ',', 'che', 'per', 'tre', 'milioni', 'di', 'anni', 'il', 'numero', 'di', 'anni', 'di', 'c@@', 'las@@', 'si@@', 'fic@@', 'azione', 'di', '4@@', '8', 'stati', 'per', 'cento', 'di', 'l@@', 'or@@', 'o@@', '.', '</s>']
2025-05-29 23:59:44,081 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-29 23:59:44,082 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-29 23:59:44,082 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive che il ghiaccio, che il ghiaccio, che il ghiaccio, che per tre milioni di anni il numero di anni di classificazione di 48 stati per cento di loro.
2025-05-29 23:59:44,082 - INFO - joeynmt.training - Example #1
2025-05-29 23:59:44,082 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-29 23:59:44,082 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-29 23:59:44,082 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'più', 'forte', 'la', 'nostra', 'er@@', 'st@@', 'ia', 'per', 'la', 'prima', 'volta', 'che', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es', 'che', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es', 'in', 'modo', 'che', 'non', 'si', 'può', 'fare', 'un', 'po@@', '<unk>', 'di', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-29 23:59:44,082 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-29 23:59:44,082 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-29 23:59:44,082 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto più forte la nostra erstia per la prima volta che non è il dell<unk> Eises che non è il dell<unk> Eises in modo che non si può fare un po<unk> di ghiaccio.
2025-05-29 23:59:44,083 - INFO - joeynmt.training - Example #2
2025-05-29 23:59:44,083 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-29 23:59:44,083 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-29 23:59:44,083 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'cosa', 'più', 'importante', 'è', 'che', 'il', 'cuore', 'del', 'nostro', 'sistema', 'c@@', 'atti@@', 'vo', 'di', 'cal@@', 'am@@', 'pio', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-29 23:59:44,083 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-29 23:59:44,083 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-29 23:59:44,083 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cosa più importante è che il cuore del nostro sistema cattivo di calampio globale.
2025-05-29 23:59:44,083 - INFO - joeynmt.training - Example #3
2025-05-29 23:59:44,084 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-29 23:59:44,084 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-29 23:59:44,084 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'in@@', 'ver@@', 'no@@', ',', 'e', 's@@', 'ì@@', ',', 'si', 'è', 'ver@@', 'i@@', 'fic@@', 'ata', 'in', 's@@', 'om@@', 'bra@@', '.', '</s>']
2025-05-29 23:59:44,084 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-29 23:59:44,084 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-29 23:59:44,084 - INFO - joeynmt.training - 	Hypothesis: L<unk> inverno, e sì, si è verificata in sombra.
2025-05-29 23:59:44,084 - INFO - joeynmt.training - Example #4
2025-05-29 23:59:44,084 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-29 23:59:44,084 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-29 23:59:44,084 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'di@@', 'seg@@', 'no', 'di', 'un', 'periodo', 'di', 'di@@', 'seg@@', 'no', 'di', 'questo', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-29 23:59:44,085 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-29 23:59:44,085 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-29 23:59:44,085 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza che vi mostro è un disegno di disegno di un periodo di disegno di questo è successo negli ultimi 25 anni.
2025-05-29 23:59:47,424 - INFO - joeynmt.training - Epoch   5, Step:    46100, Batch Loss:     2.101523, Batch Acc: 0.459041, Tokens per Sec:    17963, Lr: 0.000300
2025-05-29 23:59:50,755 - INFO - joeynmt.training - Epoch   5, Step:    46200, Batch Loss:     1.991019, Batch Acc: 0.454450, Tokens per Sec:    20489, Lr: 0.000300
2025-05-29 23:59:54,075 - INFO - joeynmt.training - Epoch   5, Step:    46300, Batch Loss:     2.018817, Batch Acc: 0.457444, Tokens per Sec:    20370, Lr: 0.000300
2025-05-29 23:59:57,503 - INFO - joeynmt.training - Epoch   5, Step:    46400, Batch Loss:     2.132173, Batch Acc: 0.460221, Tokens per Sec:    19667, Lr: 0.000300
2025-05-30 00:00:00,945 - INFO - joeynmt.training - Epoch   5, Step:    46500, Batch Loss:     1.804642, Batch Acc: 0.457631, Tokens per Sec:    19755, Lr: 0.000300
2025-05-30 00:00:00,945 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:00:00,945 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:00:08,161 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.36, acc:   0.49, generation: 7.2060[sec], evaluation: 0.0000[sec]
2025-05-30 00:00:08,162 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:00:08,800 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/45000.ckpt
2025-05-30 00:00:08,828 - INFO - joeynmt.training - Example #0
2025-05-30 00:00:08,828 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:00:08,828 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:00:08,828 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'ha', 'mostr@@', 'ato', 'che', 'l@@', '<unk>', 'et@@', 'i@@', 'ch@@', 'etta', 'ar@@', 'ric@@', 'ch@@', 'ico', 'che', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', 'del', '4@@', '8', 'stati', 'dei', 'con@@', 'fron@@', 'ti', 'delle', 'più', 'grandi', 'stati', 'di', '4@@', '8', 'stati', 'per', 'cento', 'dei', 'con@@', 'fron@@', 'ti', 'delle', 'più', 'grandi', 'stati', 'dei', '4@@', '8', 'stati', 'dei', '2@@', '.', '</s>']
2025-05-30 00:00:08,829 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:00:08,829 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:00:08,829 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive che ha mostrato che l<unk> etichetta arricchico che per tre milioni di anni per la più grande del 48 stati dei confronti delle più grandi stati di 48 stati per cento dei confronti delle più grandi stati dei 48 stati dei 2.
2025-05-30 00:00:08,829 - INFO - joeynmt.training - Example #1
2025-05-30 00:00:08,829 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:00:08,829 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:00:08,830 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'che', 'è', 'molto', 'più', 'forte', 'di', 'questo', 'problema', 'speci@@', 'ale', 'di', 'questo', 'problema', 'è', 'che', 'non', 'è', 'il', 'di@@', 'sag@@', 'io', 'di', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:00:08,830 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:00:08,830 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:00:08,830 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, che è molto più forte di questo problema speciale di questo problema è che non è il disagio di ghiaccio.
2025-05-30 00:00:08,830 - INFO - joeynmt.training - Example #2
2025-05-30 00:00:08,830 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:00:08,830 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:00:08,830 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'att@@', 'ica', 'del', 'nostro', 'c@@', 'att@@', 'ico', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:00:08,831 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:00:08,831 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:00:08,831 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cattica del nostro cattico globale.
2025-05-30 00:00:08,831 - INFO - joeynmt.training - Example #3
2025-05-30 00:00:08,831 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:00:08,831 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:00:08,831 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'av@@', 'ven@@', 'endo', 'in', 'in@@', 'ver@@', 'no', 'e', 's@@', 'om@@', 'b@@', 'a@@', '.', '</s>']
2025-05-30 00:00:08,832 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:00:08,832 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:00:08,832 - INFO - joeynmt.training - 	Hypothesis: Si sta avvenendo in inverno e somba.
2025-05-30 00:00:08,832 - INFO - joeynmt.training - Example #4
2025-05-30 00:00:08,832 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:00:08,832 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:00:08,832 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'segn@@', 'al@@', 'o', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:00:08,833 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:00:08,833 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:00:08,833 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza che vi mostro è un segnalo che è successo negli ultimi 25 anni.
2025-05-30 00:00:12,287 - INFO - joeynmt.training - Epoch   5, Step:    46600, Batch Loss:     1.962068, Batch Acc: 0.456247, Tokens per Sec:    16124, Lr: 0.000300
2025-05-30 00:00:15,722 - INFO - joeynmt.training - Epoch   5, Step:    46700, Batch Loss:     1.927025, Batch Acc: 0.461929, Tokens per Sec:    19891, Lr: 0.000300
2025-05-30 00:00:19,159 - INFO - joeynmt.training - Epoch   5, Step:    46800, Batch Loss:     1.940117, Batch Acc: 0.460015, Tokens per Sec:    19652, Lr: 0.000300
2025-05-30 00:00:22,597 - INFO - joeynmt.training - Epoch   5, Step:    46900, Batch Loss:     1.926620, Batch Acc: 0.453246, Tokens per Sec:    19812, Lr: 0.000300
2025-05-30 00:00:26,025 - INFO - joeynmt.training - Epoch   5, Step:    47000, Batch Loss:     1.945302, Batch Acc: 0.462453, Tokens per Sec:    19684, Lr: 0.000300
2025-05-30 00:00:26,026 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:00:26,026 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:00:32,925 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.35, acc:   0.49, generation: 6.8923[sec], evaluation: 0.0000[sec]
2025-05-30 00:00:32,925 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:00:33,460 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/43500.ckpt
2025-05-30 00:00:33,484 - INFO - joeynmt.training - Example #0
2025-05-30 00:00:33,485 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:00:33,485 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:00:33,485 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've@@', ',', 'che', 'la', 'c@@', 'ass@@', 'etta', 'ar@@', 't@@', 'ica', 'che', 'la', 'c@@', 'ass@@', 'etta', 'ar@@', 't@@', 'ica', 'che', 'ha', 'avuto', 'il', '4@@', '8@@', '0@@', '%', 'di', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', 'n@@', 'ell@@', '<unk>', '4@@', '8@@', '0@@', '0@@', '.', '</s>']
2025-05-30 00:00:33,485 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:00:33,485 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:00:33,485 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive, che la cassetta artica che la cassetta artica che ha avuto il 480% di tre milioni di anni per la più grande nell<unk> 4800.
2025-05-30 00:00:33,485 - INFO - joeynmt.training - Example #1
2025-05-30 00:00:33,486 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:00:33,486 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:00:33,486 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'così', 'che', 'la', 'cosa', 'più', 'importante', 'per', 'la', 'prima', 'cosa', 'che', 'non', 'è', 'così', 'che', 'non', 'lo', 'fa', 'in', 'modo', 'che', 'non', 'lo', 'fa', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:00:33,486 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:00:33,486 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:00:33,486 - INFO - joeynmt.training - 	Hypothesis: Ma non è così che la cosa più importante per la prima cosa che non è così che non lo fa in modo che non lo fa il dell<unk> Eises.
2025-05-30 00:00:33,486 - INFO - joeynmt.training - Example #2
2025-05-30 00:00:33,487 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:00:33,487 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:00:33,487 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'lasse', 'ar@@', 't@@', 'ica', 'di', 'ghi@@', 'accio', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:00:33,487 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:00:33,487 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:00:33,487 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la classe artica di ghiaccio globale.
2025-05-30 00:00:33,487 - INFO - joeynmt.training - Example #3
2025-05-30 00:00:33,488 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:00:33,488 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:00:33,488 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'prima', 'di', 'tutto', 'il', 'vento', 'e', 's@@', 'otti@@', 'le', 'e', 's@@', 'bar@@', 'azz@@', 'ate', 'in', 'sal@@', 'at@@', 'o@@', '.', '</s>']
2025-05-30 00:00:33,488 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:00:33,488 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:00:33,488 - INFO - joeynmt.training - 	Hypothesis: E prima di tutto il vento e sottile e sbarazzate in salato.
2025-05-30 00:00:33,488 - INFO - joeynmt.training - Example #4
2025-05-30 00:00:33,488 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:00:33,489 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:00:33,489 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'cosa', 'che', 'vi', 'ho', 'mostr@@', 'ato', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:00:33,489 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:00:33,489 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:00:33,489 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza che vi mostrerò è una cosa che vi ho mostrato è successo negli ultimi 25 anni.
2025-05-30 00:00:36,869 - INFO - joeynmt.training - Epoch   5, Step:    47100, Batch Loss:     1.961334, Batch Acc: 0.464432, Tokens per Sec:    17817, Lr: 0.000300
2025-05-30 00:00:38,018 - INFO - joeynmt.training - Epoch   5: total training loss 18263.16
2025-05-30 00:00:38,018 - INFO - joeynmt.training - EPOCH 6
2025-05-30 00:00:40,339 - INFO - joeynmt.training - Epoch   6, Step:    47200, Batch Loss:     1.750309, Batch Acc: 0.470319, Tokens per Sec:    19465, Lr: 0.000300
2025-05-30 00:00:43,807 - INFO - joeynmt.training - Epoch   6, Step:    47300, Batch Loss:     2.021616, Batch Acc: 0.471916, Tokens per Sec:    19832, Lr: 0.000300
2025-05-30 00:00:47,256 - INFO - joeynmt.training - Epoch   6, Step:    47400, Batch Loss:     1.743839, Batch Acc: 0.481517, Tokens per Sec:    19545, Lr: 0.000300
2025-05-30 00:00:50,634 - INFO - joeynmt.training - Epoch   6, Step:    47500, Batch Loss:     1.901675, Batch Acc: 0.477332, Tokens per Sec:    19773, Lr: 0.000300
2025-05-30 00:00:50,634 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:00:50,634 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:00:57,624 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.38, acc:   0.49, generation: 6.9784[sec], evaluation: 0.0000[sec]
2025-05-30 00:00:58,066 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/44500.ckpt
2025-05-30 00:00:58,092 - INFO - joeynmt.training - Example #0
2025-05-30 00:00:58,093 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:00:58,093 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:00:58,093 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've@@', '.', '</s>']
2025-05-30 00:00:58,094 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:00:58,094 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:00:58,094 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive.
2025-05-30 00:00:58,094 - INFO - joeynmt.training - Example #1
2025-05-30 00:00:58,094 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:00:58,094 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:00:58,094 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'molto', 'importante', 'la', 'nostra', 'capacità', 'di', 'es@@', 'prim@@', 'ere', 'questo', 'problema', 'speci@@', 'ale', 'di', 'questo', 'problema', 'speci@@', 'ale', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'a@@', '.', '</s>']
2025-05-30 00:00:58,095 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:00:58,095 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:00:58,095 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto molto importante la nostra capacità di esprimere questo problema speciale di questo problema speciale che non è il dibattito dell<unk> Eisla.
2025-05-30 00:00:58,095 - INFO - joeynmt.training - Example #2
2025-05-30 00:00:58,095 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:00:58,095 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:00:58,095 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'cosa', 'più', 'b@@', 'ella', 'di', 'un', 'cuore', 'di', 'ghi@@', 'accio', 'e', 'il', 'nostro', 'sistema', 'di', 'K@@', 'lim@@', ',', 'il', 'nostro', 'sistema', 'di', 'K@@', 'lim@@', 'as@@', 's', 'è', 'il', 'nostro', 'sistema', 'di', 'c@@', 'au@@', 'se', 'di', 'un', 'sistema', 'di', 'ri@@', 'guard@@', 'o@@', ',', 'e', 'non', 'è', 'un', 'po@@', '<unk>', 'di', 'tempo', 'che', 'si', 'tratta', 'di', 'un', 'po@@', '<unk>', 'di', 'tempo', 'di', 'ri@@', 'fer@@', 'imento', 'di', 'ghi@@', 'accio', 'e', 'di', 'un', 'po@@', '<unk>', 'di', 's@@', 'é@@', '.', '</s>']
2025-05-30 00:00:58,096 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:00:58,096 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:00:58,096 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cosa più bella di un cuore di ghiaccio e il nostro sistema di Klim, il nostro sistema di Klimass è il nostro sistema di cause di un sistema di riguardo, e non è un po<unk> di tempo che si tratta di un po<unk> di tempo di riferimento di ghiaccio e di un po<unk> di sé.
2025-05-30 00:00:58,096 - INFO - joeynmt.training - Example #3
2025-05-30 00:00:58,096 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:00:58,096 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:00:58,096 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'cominci@@', 'a', 'a', 'a', 'gi@@', 'r@@', 'ar@@', 'amente', 'a', 'mal@@', 'ap@@', 'ena', 'e', 'si', 'è', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:00:58,096 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:00:58,097 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:00:58,097 - INFO - joeynmt.training - 	Hypothesis: Si comincia a a giraramente a malapena e si è in estate.
2025-05-30 00:00:58,097 - INFO - joeynmt.training - Example #4
2025-05-30 00:00:58,097 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:00:58,097 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:00:58,097 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'seg@@', 'gi@@', 'ol@@', 'in@@', 'e@@', ',', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:00:58,097 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:00:58,097 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:00:58,097 - INFO - joeynmt.training - 	Hypothesis: La prossima diapositiva che vi mostro è un seggioline, che è successo negli ultimi 25 anni.
2025-05-30 00:01:01,494 - INFO - joeynmt.training - Epoch   6, Step:    47600, Batch Loss:     1.945085, Batch Acc: 0.470266, Tokens per Sec:    17675, Lr: 0.000300
2025-05-30 00:01:04,899 - INFO - joeynmt.training - Epoch   6, Step:    47700, Batch Loss:     1.970867, Batch Acc: 0.479452, Tokens per Sec:    20380, Lr: 0.000300
2025-05-30 00:01:08,286 - INFO - joeynmt.training - Epoch   6, Step:    47800, Batch Loss:     1.985313, Batch Acc: 0.471052, Tokens per Sec:    20021, Lr: 0.000300
2025-05-30 00:01:11,679 - INFO - joeynmt.training - Epoch   6, Step:    47900, Batch Loss:     1.901911, Batch Acc: 0.478237, Tokens per Sec:    20517, Lr: 0.000300
2025-05-30 00:01:15,071 - INFO - joeynmt.training - Epoch   6, Step:    48000, Batch Loss:     1.850259, Batch Acc: 0.473191, Tokens per Sec:    20327, Lr: 0.000300
2025-05-30 00:01:15,071 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:01:15,072 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:01:22,408 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.36, acc:   0.49, generation: 7.3262[sec], evaluation: 0.0000[sec]
2025-05-30 00:01:22,841 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/46000.ckpt
2025-05-30 00:01:22,864 - INFO - joeynmt.training - Example #0
2025-05-30 00:01:22,865 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:01:22,865 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:01:22,865 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'per@@', 'dere', 'che', 'l@@', '<unk>', 'E@@', '<unk>', 'stato', 'un', 'l@@', '<unk>', 'E@@', 'is@@', 'co@@', ',', 'che', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'v@@', 'enti', 'che', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'v@@', 'ari', 'di', 'an@@', 'ni@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'v@@', 'ari', 'di', 'più', 'o', 'meno', 'al', '4@@', '8@@', '.', '</s>']
2025-05-30 00:01:22,867 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:01:22,867 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:01:22,867 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per perdere che l<unk> E<unk> stato un l<unk> Eisco, che per tre milioni di anni di venti che per tre milioni di anni di vari di anni, per tre milioni di anni di vari di più o meno al 48.
2025-05-30 00:01:22,867 - INFO - joeynmt.training - Example #1
2025-05-30 00:01:22,867 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:01:22,867 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:01:22,868 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'forte', 'la', 'nostra', 'storia', 'di', 'questo', 'speci@@', 'ale', 'di', 'questo', 'problema', 'che', 'non', 'è', 'la', 'di@@', 'chiar@@', 'azione', 'di', 'questo', 'speci@@', 'ale', 'perché', 'non', 'è', 'il', 'di@@', 'str@@', 'etto', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'a@@', '.', '</s>']
2025-05-30 00:01:22,868 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:01:22,868 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:01:22,868 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte la nostra storia di questo speciale di questo problema che non è la dichiarazione di questo speciale perché non è il distretto dell<unk> Eisla.
2025-05-30 00:01:22,868 - INFO - joeynmt.training - Example #2
2025-05-30 00:01:22,868 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:01:22,868 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:01:22,868 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'att@@', 'ica', 'di', 'ghi@@', 'accio', 'è', 'il', 'cuore', 'c@@', 'att@@', 'ica', 'del', 'nostro', 'sistema', 'K@@', 'lim@@', 'as@@', 'sist@@', 'enza', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:01:22,869 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:01:22,869 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:01:22,869 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cattica di ghiaccio è il cuore cattica del nostro sistema Klimassistenza globale.
2025-05-30 00:01:22,869 - INFO - joeynmt.training - Example #3
2025-05-30 00:01:22,869 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:01:22,869 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:01:22,869 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'nel', 'in@@', 'ver@@', 'no', 'e', 'dol@@', 'or@@', 'e@@', '.', '</s>']
2025-05-30 00:01:22,869 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:01:22,870 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:01:22,870 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo nel inverno e dolore.
2025-05-30 00:01:22,870 - INFO - joeynmt.training - Example #4
2025-05-30 00:01:22,870 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:01:22,870 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:01:22,870 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'foto', 'di', 'un', 'seg@@', 'gi@@', 'ol@@', 'ino', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:01:22,870 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:01:22,870 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:01:22,870 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una foto di un seggiolino di quello che è successo negli ultimi 25 anni.
2025-05-30 00:01:26,286 - INFO - joeynmt.training - Epoch   6, Step:    48100, Batch Loss:     1.835078, Batch Acc: 0.468438, Tokens per Sec:    17622, Lr: 0.000300
2025-05-30 00:01:29,682 - INFO - joeynmt.training - Epoch   6, Step:    48200, Batch Loss:     1.913168, Batch Acc: 0.471647, Tokens per Sec:    20199, Lr: 0.000300
2025-05-30 00:01:33,060 - INFO - joeynmt.training - Epoch   6, Step:    48300, Batch Loss:     1.772762, Batch Acc: 0.473575, Tokens per Sec:    20037, Lr: 0.000300
2025-05-30 00:01:36,448 - INFO - joeynmt.training - Epoch   6, Step:    48400, Batch Loss:     1.560514, Batch Acc: 0.472744, Tokens per Sec:    20411, Lr: 0.000300
2025-05-30 00:01:39,906 - INFO - joeynmt.training - Epoch   6, Step:    48500, Batch Loss:     1.954781, Batch Acc: 0.473231, Tokens per Sec:    19830, Lr: 0.000300
2025-05-30 00:01:39,907 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:01:39,907 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:01:47,212 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.34, acc:   0.49, generation: 7.2955[sec], evaluation: 0.0000[sec]
2025-05-30 00:01:47,213 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:01:47,874 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/45500.ckpt
2025-05-30 00:01:47,903 - INFO - joeynmt.training - Example #0
2025-05-30 00:01:47,903 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:01:47,903 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:01:47,903 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'l@@', '<unk>', 'ar@@', 'ric@@', 'ch@@', 'etta', 'ar@@', 'ric@@', 'ch@@', 'e@@', ',', 'l@@', '<unk>', 'ar@@', 'ric@@', 'e@@', 'ver@@', 'o@@', ',', 'che', 'è', 'stato', 'per', 'tre', 'milioni', 'di', 'anni', 'il', 'br@@', 'an@@', 'co', 'd@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'e@@', '<unk>', '.', '</s>']
2025-05-30 00:01:47,904 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:01:47,904 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:01:47,904 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive che l<unk> arricchetta arricche, l<unk> arricevero, che è stato per tre milioni di anni il branco dell<unk> Oce<unk> .
2025-05-30 00:01:47,904 - INFO - joeynmt.training - Example #1
2025-05-30 00:01:47,904 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:01:47,904 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:01:47,904 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'forte', 'la', 'prima', 'cosa', 'che', 'la', 'ter@@', 'za', 'è', 'la', 'sua', 'vita', 'di', 'questo', 'problema', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'a@@', '.', '</s>']
2025-05-30 00:01:47,904 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:01:47,905 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:01:47,905 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza forte la prima cosa che la terza è la sua vita di questo problema che non è il dibattito dell<unk> Eisla.
2025-05-30 00:01:47,905 - INFO - joeynmt.training - Example #2
2025-05-30 00:01:47,905 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:01:47,905 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:01:47,905 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'aff@@', 'è', 'la', 'c@@', 'aff@@', 'è', 'la', 'c@@', 'aff@@', 'è', 'la', 'c@@', 'aff@@', 'ett@@', 'ura', 'di', 'c@@', 'au@@', 'se', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:01:47,905 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:01:47,905 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:01:47,905 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la caffè la caffè la caffè la caffettura di cause globale.
2025-05-30 00:01:47,905 - INFO - joeynmt.training - Example #3
2025-05-30 00:01:47,906 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:01:47,906 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:01:47,906 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'av@@', 'ven@@', 'endo', 'a', 'in@@', 'ver@@', 'ti@@', 're', 'e', 's@@', 'ì@@', '.', '</s>']
2025-05-30 00:01:47,906 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:01:47,906 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:01:47,906 - INFO - joeynmt.training - 	Hypothesis: Si sta avvenendo a invertire e sì.
2025-05-30 00:01:47,906 - INFO - joeynmt.training - Example #4
2025-05-30 00:01:47,906 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:01:47,907 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:01:47,907 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'seg@@', 'gi@@', 'ol@@', 'ino', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:01:47,907 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:01:47,907 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:01:47,907 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è un seggiolino che è successo negli ultimi 25 anni.
2025-05-30 00:01:51,392 - INFO - joeynmt.training - Epoch   6, Step:    48600, Batch Loss:     2.085358, Batch Acc: 0.471228, Tokens per Sec:    16760, Lr: 0.000300
2025-05-30 00:01:54,854 - INFO - joeynmt.training - Epoch   6, Step:    48700, Batch Loss:     2.086127, Batch Acc: 0.467829, Tokens per Sec:    20092, Lr: 0.000300
2025-05-30 00:01:58,309 - INFO - joeynmt.training - Epoch   6, Step:    48800, Batch Loss:     1.781415, Batch Acc: 0.472973, Tokens per Sec:    19938, Lr: 0.000300
2025-05-30 00:02:01,770 - INFO - joeynmt.training - Epoch   6, Step:    48900, Batch Loss:     2.058387, Batch Acc: 0.476199, Tokens per Sec:    20198, Lr: 0.000300
2025-05-30 00:02:05,211 - INFO - joeynmt.training - Epoch   6, Step:    49000, Batch Loss:     1.976012, Batch Acc: 0.473207, Tokens per Sec:    19800, Lr: 0.000300
2025-05-30 00:02:05,211 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:02:05,211 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:02:11,937 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.86, ppl:   6.40, acc:   0.49, generation: 6.7161[sec], evaluation: 0.0000[sec]
2025-05-30 00:02:11,949 - INFO - joeynmt.training - Example #0
2025-05-30 00:02:11,949 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:02:11,949 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:02:11,949 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'è', 'l@@', '<unk>', 'et@@', 'i@@', 'ch@@', 'ett@@', 'o@@', ',', 'che', 'il', '4@@', '8@@', ',', 'che', 'è', 'stato', 'chiamato', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'dimen@@', 'sioni', 'di', 'anni', 'di', 'dimen@@', 'sioni', 'd@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'ci@@', 'dent@@', 'e@@', '.', '</s>']
2025-05-30 00:02:11,949 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:02:11,949 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:02:11,949 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive che è l<unk> etichetto, che il 48, che è stato chiamato per tre milioni di anni di dimensioni di anni di dimensioni dell<unk> Occidente.
2025-05-30 00:02:11,950 - INFO - joeynmt.training - Example #1
2025-05-30 00:02:11,950 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:02:11,950 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:02:11,950 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'per', 'quanto', 'riguar@@', 'da', 'il', 'problema', 'che', 'non', 'è', 'il', 'problema', 'che', 'non', 'è', 'il', 'di@@', 'str@@', 'etto', 'di', 'ghi@@', 'accio', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'ami@@', '.', '</s>']
2025-05-30 00:02:11,950 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:02:11,950 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:02:11,950 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, per quanto riguarda il problema che non è il problema che non è il distretto di ghiaccio dell<unk> Eislami.
2025-05-30 00:02:11,950 - INFO - joeynmt.training - Example #2
2025-05-30 00:02:11,950 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:02:11,951 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:02:11,951 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'qualche', 'modo', 'c@@', '<unk>', 'è', 'il', 'c@@', 'av@@', 'o@@', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'c@@', 'atti@@', 'vo', 'del', 'nostro', 'sistema', 'c@@', 'atti@@', 'vo', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:02:11,951 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:02:11,951 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:02:11,951 - INFO - joeynmt.training - 	Hypothesis: In qualche modo c<unk> è il cavo, il cuore del nostro sistema cattivo del nostro sistema cattivo globale.
2025-05-30 00:02:11,951 - INFO - joeynmt.training - Example #3
2025-05-30 00:02:11,951 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:02:11,951 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:02:11,951 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'si', 'ri@@', 'du@@', 'ce', 'la', 'p@@', 'om@@', 'p@@', 'a', 'e', 's@@', 'otti@@', 'le', 'nel', 's@@', 'om@@', 'e@@', '.', '</s>']
2025-05-30 00:02:11,952 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:02:11,952 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:02:11,952 - INFO - joeynmt.training - 	Hypothesis: E si riduce la pompa e sottile nel some.
2025-05-30 00:02:11,952 - INFO - joeynmt.training - Example #4
2025-05-30 00:02:11,952 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:02:11,952 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:02:11,952 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'di@@', 'seg@@', 'no', 'di', 'un', 'di@@', 'seg@@', 'no', 'di', 'di@@', 'segn@@', 'are', 'il', 'mondo', 'in', 'cui', 'è', 'succ@@', 'ess@@', 'o@@', '.', '</s>']
2025-05-30 00:02:11,953 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:02:11,953 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:02:11,953 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza che vi mostro è un disegno di disegno di un disegno di disegnare il mondo in cui è successo.
2025-05-30 00:02:15,411 - INFO - joeynmt.training - Epoch   6, Step:    49100, Batch Loss:     1.906584, Batch Acc: 0.475026, Tokens per Sec:    19608, Lr: 0.000300
2025-05-30 00:02:18,873 - INFO - joeynmt.training - Epoch   6, Step:    49200, Batch Loss:     1.822137, Batch Acc: 0.469974, Tokens per Sec:    19917, Lr: 0.000300
2025-05-30 00:02:22,333 - INFO - joeynmt.training - Epoch   6, Step:    49300, Batch Loss:     1.847109, Batch Acc: 0.473544, Tokens per Sec:    20181, Lr: 0.000300
2025-05-30 00:02:25,778 - INFO - joeynmt.training - Epoch   6, Step:    49400, Batch Loss:     2.011432, Batch Acc: 0.475110, Tokens per Sec:    19779, Lr: 0.000300
2025-05-30 00:02:29,225 - INFO - joeynmt.training - Epoch   6, Step:    49500, Batch Loss:     1.799339, Batch Acc: 0.472161, Tokens per Sec:    19394, Lr: 0.000300
2025-05-30 00:02:29,225 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:02:29,226 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:02:36,344 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.36, acc:   0.49, generation: 7.1094[sec], evaluation: 0.0000[sec]
2025-05-30 00:02:36,729 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/47500.ckpt
2025-05-30 00:02:36,757 - INFO - joeynmt.training - Example #0
2025-05-30 00:02:36,758 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:02:36,758 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:02:36,758 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've@@', ',', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'acci@@', 'aio', 'di', 'ghi@@', 'accio', 'che', 'ha', 'avuto', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'dimen@@', 'sioni', 'che', 'si', 'trov@@', 'ano', 'in', 'un', 'certo', 'sen@@', 'so@@', '.', '</s>']
2025-05-30 00:02:36,758 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:02:36,758 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:02:36,758 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive, per osservare che il ghiacciaio di ghiaccio che ha avuto per tre milioni di anni di dimensioni che si trovano in un certo senso.
2025-05-30 00:02:36,759 - INFO - joeynmt.training - Example #1
2025-05-30 00:02:36,759 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:02:36,759 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:02:36,759 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'la', 'cosa', 'più', 'importante', 'per', 'la', 'ter@@', 'ra@@', ',', 'perché', 'non', 'è', 'la', 'di@@', 'f@@', 'esa', 'd@@', 'ell@@', '<unk>', 'e@@', 'is@@', 'l@@', 'a@@', '.', '</s>']
2025-05-30 00:02:36,759 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:02:36,759 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:02:36,759 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, la cosa più importante per la terra, perché non è la difesa dell<unk> eisla.
2025-05-30 00:02:36,760 - INFO - joeynmt.training - Example #2
2025-05-30 00:02:36,760 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:02:36,760 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:02:36,760 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'l@@', '<unk>', 'e@@', '<unk>', ',', 'l@@', '<unk>', 'e@@', 'x', 'ar@@', 'ch@@', 'e@@', ',', 'il', 'cuore', 'c@@', 'atti@@', 'vo', 'del', 'nostro', 'sistema', 'di', 'c@@', 'au@@', 'se', 'di', 'c@@', 'au@@', 'se', 'di', 'un', 'sistema', 'di', 'ri@@', 'guard@@', 'o@@', '.', '</s>']
2025-05-30 00:02:36,760 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:02:36,760 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:02:36,760 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, l<unk> e<unk> , l<unk> ex arche, il cuore cattivo del nostro sistema di cause di cause di un sistema di riguardo.
2025-05-30 00:02:36,760 - INFO - joeynmt.training - Example #3
2025-05-30 00:02:36,761 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:02:36,761 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:02:36,761 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'poi', 'si', 'è', 'ri@@', 'ma@@', 'sta', 'cresc@@', 'endo', 'in', 's@@', 'om@@', 'i@@', 'a@@', '.', '</s>']
2025-05-30 00:02:36,761 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:02:36,761 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:02:36,761 - INFO - joeynmt.training - 	Hypothesis: E poi si è rimasta crescendo in somia.
2025-05-30 00:02:36,761 - INFO - joeynmt.training - Example #4
2025-05-30 00:02:36,761 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:02:36,761 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:02:36,761 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:02:36,762 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:02:36,762 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:02:36,762 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza che vi mostro è un disegno che è successo negli ultimi 25 anni.
2025-05-30 00:02:40,197 - INFO - joeynmt.training - Epoch   6, Step:    49600, Batch Loss:     1.750638, Batch Acc: 0.467477, Tokens per Sec:    17491, Lr: 0.000300
2025-05-30 00:02:43,638 - INFO - joeynmt.training - Epoch   6, Step:    49700, Batch Loss:     1.959052, Batch Acc: 0.474948, Tokens per Sec:    20526, Lr: 0.000300
2025-05-30 00:02:47,063 - INFO - joeynmt.training - Epoch   6, Step:    49800, Batch Loss:     1.969979, Batch Acc: 0.476395, Tokens per Sec:    19831, Lr: 0.000300
2025-05-30 00:02:50,483 - INFO - joeynmt.training - Epoch   6, Step:    49900, Batch Loss:     1.914531, Batch Acc: 0.469152, Tokens per Sec:    19689, Lr: 0.000300
2025-05-30 00:02:53,898 - INFO - joeynmt.training - Epoch   6, Step:    50000, Batch Loss:     1.788234, Batch Acc: 0.477196, Tokens per Sec:    19337, Lr: 0.000300
2025-05-30 00:02:53,898 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:02:53,898 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:03:01,564 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.38, acc:   0.49, generation: 7.6552[sec], evaluation: 0.0000[sec]
2025-05-30 00:03:01,573 - INFO - joeynmt.training - Example #0
2025-05-30 00:03:01,574 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:03:01,574 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:03:01,574 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'consegu@@', 'enze', 'per', 'osserv@@', 'are', 'che', 'l@@', '<unk>', 'et@@', 'i@@', 'ch@@', 'etta', 'ar@@', 'ch@@', 'e@@', 'a@@', ',', 'che', 'l@@', '<unk>', 'et@@', 'i@@', 'ch@@', 'etta', 'ar@@', 'ch@@', 'e@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'dimen@@', 'sioni', 'd@@', 'ell@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'il', '40', 'per', 'cento', 'del', 'livello', 'di', 'cui', 'il', '4@@', '8', 'stati', 'per', 'cent@@', 'o@@', '.', '</s>']
2025-05-30 00:03:01,575 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:03:01,575 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:03:01,575 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due conseguenze per osservare che l<unk> etichetta archea, che l<unk> etichetta arche, per tre milioni di anni di dimensioni dell<unk> anno scorso, il 40 per cento del livello di cui il 48 stati per cento.
2025-05-30 00:03:01,575 - INFO - joeynmt.training - Example #1
2025-05-30 00:03:01,575 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:03:01,575 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:03:01,575 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'la', 'cosa', 'più', 'importante', 'è', 'che', 'la', 'cosa', 'più', 'importante', 'è', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:03:01,576 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:03:01,576 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:03:01,576 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è molto forte, la cosa più importante è che la cosa più importante è che non è il dibattito del ghiaccio.
2025-05-30 00:03:01,576 - INFO - joeynmt.training - Example #2
2025-05-30 00:03:01,576 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:03:01,576 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:03:01,576 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'cosa', 'più', 'str@@', 'ana', 'è', 'il', 'cuore', 'c@@', 'att@@', 'ico', 'del', 'nostro', 'sistema', 'c@@', 'ef@@', 'al@@', 'ico', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:03:01,577 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:03:01,577 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:03:01,577 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cosa più strana è il cuore cattico del nostro sistema cefalico globale.
2025-05-30 00:03:01,577 - INFO - joeynmt.training - Example #3
2025-05-30 00:03:01,577 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:03:01,577 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:03:01,577 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'prima', 'di', 'tutto', 'il', 'rest@@', 'o@@', ',', 'e', 's@@', 'ì@@', '.', '</s>']
2025-05-30 00:03:01,578 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:03:01,578 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:03:01,578 - INFO - joeynmt.training - 	Hypothesis: E prima di tutto il resto, e sì.
2025-05-30 00:03:01,578 - INFO - joeynmt.training - Example #4
2025-05-30 00:03:01,578 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:03:01,578 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:03:01,578 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'cosa', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:03:01,579 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:03:01,579 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:03:01,579 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza che vi mostrerò è una cosa che è successo negli ultimi 25 anni.
2025-05-30 00:03:05,035 - INFO - joeynmt.training - Epoch   6, Step:    50100, Batch Loss:     1.771029, Batch Acc: 0.471117, Tokens per Sec:    19575, Lr: 0.000300
2025-05-30 00:03:08,451 - INFO - joeynmt.training - Epoch   6, Step:    50200, Batch Loss:     1.983528, Batch Acc: 0.469213, Tokens per Sec:    19586, Lr: 0.000300
2025-05-30 00:03:11,888 - INFO - joeynmt.training - Epoch   6, Step:    50300, Batch Loss:     1.681696, Batch Acc: 0.470250, Tokens per Sec:    20140, Lr: 0.000300
2025-05-30 00:03:15,319 - INFO - joeynmt.training - Epoch   6, Step:    50400, Batch Loss:     1.968666, Batch Acc: 0.469491, Tokens per Sec:    20011, Lr: 0.000300
2025-05-30 00:03:18,753 - INFO - joeynmt.training - Epoch   6, Step:    50500, Batch Loss:     2.126162, Batch Acc: 0.473905, Tokens per Sec:    20307, Lr: 0.000300
2025-05-30 00:03:18,753 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:03:18,753 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:03:26,443 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.36, acc:   0.49, generation: 7.6786[sec], evaluation: 0.0000[sec]
2025-05-30 00:03:26,863 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/48000.ckpt
2025-05-30 00:03:26,885 - INFO - joeynmt.training - Example #0
2025-05-30 00:03:26,886 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:03:26,886 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:03:26,886 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'consegu@@', 'enze', 'per', 'osserv@@', 'are', 'che', 'l@@', '<unk>', 'et@@', 'i@@', 'ch@@', 'etta', 'l@@', '<unk>', 'et@@', 'i@@', 'ch@@', 'ett@@', 'a@@', ',', 'che', 'l@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'e', 'l@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'e', 'il', '4@@', '8@@', '0@@', '%', 'per', 'la', 'dimen@@', 'sione', 'del', '4@@', '8@@', '.', '</s>']
2025-05-30 00:03:26,887 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:03:26,887 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:03:26,887 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due conseguenze per osservare che l<unk> etichetta l<unk> etichetta, che l<unk> anno scorso, e l<unk> anno scorso, e il 480% per la dimensione del 48.
2025-05-30 00:03:26,887 - INFO - joeynmt.training - Example #1
2025-05-30 00:03:26,887 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:03:26,887 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:03:26,887 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'la', 'cosa', 'più', 'importante', 'è', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:03:26,887 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:03:26,888 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:03:26,888 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza forte, la cosa più importante è che non è il dibattito del ghiaccio.
2025-05-30 00:03:26,888 - INFO - joeynmt.training - Example #2
2025-05-30 00:03:26,888 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:03:26,888 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:03:26,888 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'aff@@', 'è', 'l@@', '<unk>', 'ar@@', 't@@', 'ic@@', 'a@@', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'c@@', 'lim@@', 'ass@@', 'e@@', '.', '</s>']
2025-05-30 00:03:26,888 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:03:26,888 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:03:26,889 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la caffè l<unk> artica, il cuore del nostro sistema climasse.
2025-05-30 00:03:26,889 - INFO - joeynmt.training - Example #3
2025-05-30 00:03:26,889 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:03:26,889 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:03:26,889 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'poi', 'si', 'è', 'ri@@', 'ma@@', 'sta', 'in', 'in@@', 'ver@@', 'no@@', '.', '</s>']
2025-05-30 00:03:26,889 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:03:26,889 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:03:26,889 - INFO - joeynmt.training - 	Hypothesis: E poi si è rimasta in inverno.
2025-05-30 00:03:26,889 - INFO - joeynmt.training - Example #4
2025-05-30 00:03:26,890 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:03:26,890 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:03:26,890 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'acca@@', 'duto', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:03:26,890 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:03:26,890 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:03:26,890 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza che vi mostro è un disegno di quello che è accaduto negli ultimi 25 anni.
2025-05-30 00:03:30,364 - INFO - joeynmt.training - Epoch   6, Step:    50600, Batch Loss:     1.852483, Batch Acc: 0.467769, Tokens per Sec:    17792, Lr: 0.000300
2025-05-30 00:03:33,800 - INFO - joeynmt.training - Epoch   6, Step:    50700, Batch Loss:     1.862470, Batch Acc: 0.464146, Tokens per Sec:    19534, Lr: 0.000300
2025-05-30 00:03:37,249 - INFO - joeynmt.training - Epoch   6, Step:    50800, Batch Loss:     1.801502, Batch Acc: 0.469156, Tokens per Sec:    19730, Lr: 0.000300
2025-05-30 00:03:40,708 - INFO - joeynmt.training - Epoch   6, Step:    50900, Batch Loss:     1.814667, Batch Acc: 0.469473, Tokens per Sec:    20392, Lr: 0.000300
2025-05-30 00:03:44,131 - INFO - joeynmt.training - Epoch   6, Step:    51000, Batch Loss:     2.039227, Batch Acc: 0.473750, Tokens per Sec:    20331, Lr: 0.000300
2025-05-30 00:03:44,132 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:03:44,132 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:03:50,791 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.84, ppl:   6.30, acc:   0.49, generation: 6.6527[sec], evaluation: 0.0000[sec]
2025-05-30 00:03:50,792 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:03:51,320 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/49500.ckpt
2025-05-30 00:03:51,345 - INFO - joeynmt.training - Example #0
2025-05-30 00:03:51,345 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:03:51,346 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:03:51,346 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'consegu@@', 'enze', 'per', 'la', 'prima', 'volta', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'ar@@', 'ch@@', 'e@@', ',', 'il', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'ar@@', 'di@@', 'o@@', ',', 'che', 'l@@', '<unk>', 'ar@@', 'c@@', 'lin@@', 'ic@@', 'a@@', ',', 'il', '4@@', '8@@', '0@@', '%', 'del', 'con@@', 'fine', 'a', 'il', '4@@', '8@@', '0@@', '%', 'è', 'successo', 'in', 'questo', 'mod@@', 'o@@', '.', '</s>']
2025-05-30 00:03:51,346 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:03:51,346 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:03:51,346 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due conseguenze per la prima volta che il ghiaccio arktico arche, il ghiaccio arktico ardio, che l<unk> arclinica, il 480% del confine a il 480% è successo in questo modo.
2025-05-30 00:03:51,346 - INFO - joeynmt.training - Example #1
2025-05-30 00:03:51,347 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:03:51,347 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:03:51,347 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'la', 'cosa', 'più', 'importante', 'è', 'che', 'la', 'cosa', 'più', 'importante', 'che', 'non', 'è', 'la', 'di@@', 'chiar@@', 'azione', 'di', 'questo', 'problema', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'a@@', '.', '</s>']
2025-05-30 00:03:51,347 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:03:51,347 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:03:51,347 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza la cosa più importante è che la cosa più importante che non è la dichiarazione di questo problema che non è il dibattito dell<unk> Eisla.
2025-05-30 00:03:51,347 - INFO - joeynmt.training - Example #2
2025-05-30 00:03:51,348 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:03:51,348 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:03:51,348 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'lasse', 'ar@@', 't@@', 'ic@@', 'a@@', ',', 'il', 'cuore', 'della', 'nostra', 'c@@', 'lasse', 'di', 'c@@', 'au@@', 'se', 'del', 'nostro', 'sistema', 'c@@', 'lim@@', 'as@@', 'si', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:03:51,348 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:03:51,348 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:03:51,348 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la classe artica, il cuore della nostra classe di cause del nostro sistema climassi globale.
2025-05-30 00:03:51,348 - INFO - joeynmt.training - Example #3
2025-05-30 00:03:51,348 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:03:51,348 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:03:51,348 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'si', 'ri@@', 'du@@', 'ce', 'e', 's@@', 'os@@', 'pes@@', 'o', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:03:51,349 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:03:51,349 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:03:51,349 - INFO - joeynmt.training - 	Hypothesis: E si riduce e sospeso in estate.
2025-05-30 00:03:51,349 - INFO - joeynmt.training - Example #4
2025-05-30 00:03:51,349 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:03:51,349 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:03:51,349 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:03:51,350 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:03:51,350 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:03:51,350 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza che vi mostro è un disegno di quello che è successo negli ultimi 25 anni.
2025-05-30 00:03:54,739 - INFO - joeynmt.training - Epoch   6, Step:    51100, Batch Loss:     1.800922, Batch Acc: 0.471349, Tokens per Sec:    17228, Lr: 0.000300
2025-05-30 00:03:58,192 - INFO - joeynmt.training - Epoch   6, Step:    51200, Batch Loss:     2.075972, Batch Acc: 0.463338, Tokens per Sec:    19036, Lr: 0.000300
2025-05-30 00:04:01,675 - INFO - joeynmt.training - Epoch   6, Step:    51300, Batch Loss:     1.971491, Batch Acc: 0.476411, Tokens per Sec:    20230, Lr: 0.000300
2025-05-30 00:04:05,143 - INFO - joeynmt.training - Epoch   6, Step:    51400, Batch Loss:     2.021514, Batch Acc: 0.472814, Tokens per Sec:    19465, Lr: 0.000300
2025-05-30 00:04:08,641 - INFO - joeynmt.training - Epoch   6, Step:    51500, Batch Loss:     1.929913, Batch Acc: 0.473371, Tokens per Sec:    20374, Lr: 0.000300
2025-05-30 00:04:08,641 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:04:08,641 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:04:15,740 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.84, ppl:   6.32, acc:   0.49, generation: 7.0887[sec], evaluation: 0.0000[sec]
2025-05-30 00:04:16,174 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/46500.ckpt
2025-05-30 00:04:16,203 - INFO - joeynmt.training - Example #0
2025-05-30 00:04:16,204 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:04:16,204 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:04:16,204 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'consegu@@', 'enze', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'acci@@', 'aio', 'di', 'anni', 'fa@@', ',', 'che', 'il', 'ghi@@', 'acci@@', 'aio', 'che', 'ha', 'avuto', 'un', 't@@', 'ale', 'di', 'nome', 'di', 'un', 'gr@@', 'ave', 'di', 'qu@@', 'ar@@', 'to', 'di', 'tre', 'milioni', 'di', 'anni', 'di', 'gr@@', 'av@@', 'it@@', 'à@@', '.', '</s>']
2025-05-30 00:04:16,204 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:04:16,205 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:04:16,205 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due conseguenze per osservare che il ghiacciaio di anni fa, che il ghiacciaio che ha avuto un tale di nome di un grave di quarto di tre milioni di anni di gravità.
2025-05-30 00:04:16,205 - INFO - joeynmt.training - Example #1
2025-05-30 00:04:16,205 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:04:16,205 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:04:16,205 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'non', 'è', 'abbastanza', 'la', 'ter@@', 'ren@@', 'o', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:04:16,205 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:04:16,206 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:04:16,206 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte, non è abbastanza la terreno che non è il dibattito dell<unk> Eises.
2025-05-30 00:04:16,206 - INFO - joeynmt.training - Example #2
2025-05-30 00:04:16,206 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:04:16,206 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:04:16,206 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'l@@', '<unk>', 'ar@@', 'ric@@', 'ch@@', 'etto', 'è', 'il', 'cuore', 'del', 'nostro', 'sistema', 'c@@', 'atti@@', 'vo', 'di', 'c@@', 'las@@', 'si@@', 'fic@@', 'o@@', '.', '</s>']
2025-05-30 00:04:16,206 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:04:16,207 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:04:16,207 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, l<unk> arricchetto è il cuore del nostro sistema cattivo di classifico.
2025-05-30 00:04:16,207 - INFO - joeynmt.training - Example #3
2025-05-30 00:04:16,207 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:04:16,207 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:04:16,207 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'cominci@@', 'a', 'a', 'a', 's@@', 'é@@', '.', '</s>']
2025-05-30 00:04:16,207 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:04:16,207 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:04:16,207 - INFO - joeynmt.training - 	Hypothesis: E comincia a a sé.
2025-05-30 00:04:16,207 - INFO - joeynmt.training - Example #4
2025-05-30 00:04:16,208 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:04:16,208 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:04:16,208 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'tr@@', 'atto', 'di', 'di@@', 'seg@@', 'no', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:04:16,208 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:04:16,208 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:04:16,208 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è un tratto di disegno di ciò che è successo negli ultimi 25 anni.
2025-05-30 00:04:19,703 - INFO - joeynmt.training - Epoch   6, Step:    51600, Batch Loss:     2.039491, Batch Acc: 0.470017, Tokens per Sec:    17628, Lr: 0.000300
2025-05-30 00:04:23,184 - INFO - joeynmt.training - Epoch   6, Step:    51700, Batch Loss:     1.786105, Batch Acc: 0.472000, Tokens per Sec:    19745, Lr: 0.000300
2025-05-30 00:04:26,654 - INFO - joeynmt.training - Epoch   6, Step:    51800, Batch Loss:     2.082111, Batch Acc: 0.467807, Tokens per Sec:    20108, Lr: 0.000300
2025-05-30 00:04:30,110 - INFO - joeynmt.training - Epoch   6, Step:    51900, Batch Loss:     1.751450, Batch Acc: 0.465913, Tokens per Sec:    19858, Lr: 0.000300
2025-05-30 00:04:33,569 - INFO - joeynmt.training - Epoch   6, Step:    52000, Batch Loss:     1.831541, Batch Acc: 0.466809, Tokens per Sec:    19439, Lr: 0.000300
2025-05-30 00:04:33,569 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:04:33,569 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:04:40,761 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.84, ppl:   6.31, acc:   0.49, generation: 7.1820[sec], evaluation: 0.0000[sec]
2025-05-30 00:04:41,216 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/50500.ckpt
2025-05-30 00:04:41,247 - INFO - joeynmt.training - Example #0
2025-05-30 00:04:41,247 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:04:41,248 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:04:41,248 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've@@', ',', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'ch@@', 'ico', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'ch@@', 'ico', 'che', 'gli', 'chiam@@', 'ano', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'dimen@@', 'sioni', 'è', 'stato', 'il', '4@@', '8', 'stati', 'per', 'cento', 'di', 'questi', 'gruppi', 'di', 'persone', 'che', 'chiam@@', 'ano', 'il', '4@@', '8@@', '0@@', '%', 'è', 'stato', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'm@@', 'ezz@@', 'o@@', '.', '</s>']
2025-05-30 00:04:41,248 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:04:41,248 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:04:41,248 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive, che il ghiaccio archico che il ghiaccio archico che gli chiamano per tre milioni di anni di dimensioni è stato il 48 stati per cento di questi gruppi di persone che chiamano il 480% è stato fatto per tre milioni di anni di mezzo.
2025-05-30 00:04:41,248 - INFO - joeynmt.training - Example #1
2025-05-30 00:04:41,249 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:04:41,249 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:04:41,249 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'la', 'ter@@', 'ri@@', 'bile', 'da', 'questo', 'problema', 'speci@@', 'ale', 'questo', 'problema', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'd@@', 'ell@@', '<unk>', 'em@@', 'a@@', '.', '</s>']
2025-05-30 00:04:41,249 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:04:41,249 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:04:41,249 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte, la terribile da questo problema speciale questo problema che non è il dibattito dell<unk> ema.
2025-05-30 00:04:41,249 - INFO - joeynmt.training - Example #2
2025-05-30 00:04:41,249 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:04:41,250 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:04:41,250 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'att@@', 'ica', 'di', 'ghi@@', 'accio', 'è', 'il', 'cuore', 'del', 'nostro', 'sistema', 'c@@', 'lin@@', 'ic@@', 'o@@', '.', '</s>']
2025-05-30 00:04:41,250 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:04:41,250 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:04:41,250 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cattica di ghiaccio è il cuore del nostro sistema clinico.
2025-05-30 00:04:41,250 - INFO - joeynmt.training - Example #3
2025-05-30 00:04:41,250 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:04:41,250 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:04:41,251 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'è', 'ri@@', 'ma@@', 'sto', 'e', 's@@', 'and@@', 'ando', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:04:41,251 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:04:41,251 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:04:41,251 - INFO - joeynmt.training - 	Hypothesis: Si è rimasto e sandando in estate.
2025-05-30 00:04:41,251 - INFO - joeynmt.training - Example #4
2025-05-30 00:04:41,251 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:04:41,251 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:04:41,251 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'che', 'vi', 'mostr@@', 'o', 'è', 'una', 'cosa', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:04:41,252 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:04:41,252 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:04:41,252 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza che vi mostro è una cosa che è successo negli ultimi 25 anni.
2025-05-30 00:04:44,731 - INFO - joeynmt.training - Epoch   6, Step:    52100, Batch Loss:     1.892094, Batch Acc: 0.468850, Tokens per Sec:    17877, Lr: 0.000300
2025-05-30 00:04:48,172 - INFO - joeynmt.training - Epoch   6, Step:    52200, Batch Loss:     1.759523, Batch Acc: 0.471620, Tokens per Sec:    19761, Lr: 0.000300
2025-05-30 00:04:51,620 - INFO - joeynmt.training - Epoch   6, Step:    52300, Batch Loss:     2.003126, Batch Acc: 0.472625, Tokens per Sec:    20607, Lr: 0.000300
2025-05-30 00:04:55,064 - INFO - joeynmt.training - Epoch   6, Step:    52400, Batch Loss:     1.696702, Batch Acc: 0.472345, Tokens per Sec:    19332, Lr: 0.000300
2025-05-30 00:04:58,521 - INFO - joeynmt.training - Epoch   6, Step:    52500, Batch Loss:     1.899601, Batch Acc: 0.460854, Tokens per Sec:    20195, Lr: 0.000300
2025-05-30 00:04:58,521 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:04:58,521 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:05:06,682 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.84, ppl:   6.30, acc:   0.49, generation: 8.1511[sec], evaluation: 0.0000[sec]
2025-05-30 00:05:06,682 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:05:07,386 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/47000.ckpt
2025-05-30 00:05:07,414 - INFO - joeynmt.training - Example #0
2025-05-30 00:05:07,415 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:05:07,415 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:05:07,415 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've@@', ',', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'c@@', 'ic@@', 'lo', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'la', 'dimen@@', 'sione', 'di', 'questo', 'numero', 'di', 'anni', 'di', 'dimen@@', 'sioni', 'in', 'cui', 'il', '4@@', '8', 'stati', 'per', 'cento', 'per', 'la', 'dimen@@', 'sione', 'è', 'stato', 'un', '4@@', '8', 'per', 'cento', 'di', 'questi', 'gruppi', 'di', 'persone', 'che', 'non', 'hanno', 'bisogno', 'di', 'un', 'po@@', '<unk>', 'di', 'tempo', 'di', 'ri@@', 'durre', 'il', '40', 'per', 'cento', 'di', 'queste', 'dimen@@', 'sioni', 'è', 'che', 'non', 'sono', 'mai', 'stato', 'un', 'po@@', '<unk>', 'di', 'tempo', 'per', 'con@@', 'durre', 'il', 'nostro', 'sistema']
2025-05-30 00:05:07,415 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:05:07,415 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:05:07,416 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive, per osservare che il ghiaccio arciclo che ha fatto per tre milioni di anni la dimensione di questo numero di anni di dimensioni in cui il 48 stati per cento per la dimensione è stato un 48 per cento di questi gruppi di persone che non hanno bisogno di un po<unk> di tempo di ridurre il 40 per cento di queste dimensioni è che non sono mai stato un po<unk> di tempo per condurre il nostro sistema
2025-05-30 00:05:07,416 - INFO - joeynmt.training - Example #1
2025-05-30 00:05:07,416 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:05:07,416 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:05:07,416 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'la', 'cosa', 'che', 'è', 'la', 'prima', 'cosa', 'che', 'non', 'è', 'molto', 'più', 'for@@', 'te@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'accio', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'ca', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:05:07,416 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:05:07,416 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:05:07,416 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, la cosa che è la prima cosa che non è molto più forte, perché non è il dibattito del ghiaccio dell<unk> Eisca dell<unk> Eises.
2025-05-30 00:05:07,417 - INFO - joeynmt.training - Example #2
2025-05-30 00:05:07,417 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:05:07,417 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:05:07,417 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'è', 'la', 'c@@', 'att@@', 'ica', 'ar@@', 'ric@@', 'ca', 'di', 'ghi@@', 'accio', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:05:07,417 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:05:07,417 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:05:07,417 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, è la cattica arricca di ghiaccio globale.
2025-05-30 00:05:07,417 - INFO - joeynmt.training - Example #3
2025-05-30 00:05:07,418 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:05:07,418 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:05:07,418 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'inizi@@', 'a', 'a', 'a', 'c@@', 'att@@', 'ur@@', 'are', 'e', 'dol@@', 'c@@', 'e@@', '.', '</s>']
2025-05-30 00:05:07,418 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:05:07,418 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:05:07,418 - INFO - joeynmt.training - 	Hypothesis: E inizia a a catturare e dolce.
2025-05-30 00:05:07,418 - INFO - joeynmt.training - Example #4
2025-05-30 00:05:07,419 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:05:07,419 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:05:07,419 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'cosa', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:05:07,419 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:05:07,419 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:05:07,419 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una cosa che è successo negli ultimi 25 anni.
2025-05-30 00:05:10,908 - INFO - joeynmt.training - Epoch   6, Step:    52600, Batch Loss:     1.889248, Batch Acc: 0.471559, Tokens per Sec:    16360, Lr: 0.000300
2025-05-30 00:05:14,385 - INFO - joeynmt.training - Epoch   6, Step:    52700, Batch Loss:     1.892903, Batch Acc: 0.469752, Tokens per Sec:    20289, Lr: 0.000300
2025-05-30 00:05:17,850 - INFO - joeynmt.training - Epoch   6, Step:    52800, Batch Loss:     1.852983, Batch Acc: 0.468497, Tokens per Sec:    20110, Lr: 0.000300
2025-05-30 00:05:21,316 - INFO - joeynmt.training - Epoch   6, Step:    52900, Batch Loss:     2.000928, Batch Acc: 0.464841, Tokens per Sec:    19838, Lr: 0.000300
2025-05-30 00:05:24,768 - INFO - joeynmt.training - Epoch   6, Step:    53000, Batch Loss:     1.856547, Batch Acc: 0.468132, Tokens per Sec:    19565, Lr: 0.000300
2025-05-30 00:05:24,768 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:05:24,768 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:05:32,364 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.84, ppl:   6.27, acc:   0.49, generation: 7.5860[sec], evaluation: 0.0000[sec]
2025-05-30 00:05:32,364 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:05:32,954 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/48500.ckpt
2025-05-30 00:05:32,984 - INFO - joeynmt.training - Example #0
2025-05-30 00:05:32,984 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:05:32,984 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:05:32,984 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've@@', ',', 'per', 'per@@', 'dere', 'che', 'la', 'maggior', 'parte', 'dei', 'ghi@@', 'ac@@', 'ci', 'che', 'l@@', '<unk>', 'ar@@', 'c@@', 'ci@@', 'one', 'di', 'tre', 'milioni', 'di', 'anni', 'la', 'quantità', 'di', 'li@@', 'sta', 'di', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'e@@', '<unk>', '.', '</s>']
2025-05-30 00:05:32,985 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:05:32,985 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:05:32,985 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive, per perdere che la maggior parte dei ghiacci che l<unk> arccione di tre milioni di anni la quantità di lista di tre milioni di anni per la dimensione dell<unk> Oce<unk> .
2025-05-30 00:05:32,985 - INFO - joeynmt.training - Example #1
2025-05-30 00:05:32,985 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:05:32,985 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:05:32,985 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'la', 'sost@@', 'eni@@', 'bilità', 'di', 'questo', 'problema', 'speci@@', 'ale', 'che', 'non', 'è', 'la', 'di@@', 'stri@@', 'bu@@', 'zione', 'del', 'ghi@@', 'accio', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'amento', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:05:32,986 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:05:32,986 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:05:32,986 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, la sostenibilità di questo problema speciale che non è la distribuzione del ghiaccio dell<unk> Eislamento dell<unk> Eises.
2025-05-30 00:05:32,986 - INFO - joeynmt.training - Example #2
2025-05-30 00:05:32,986 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:05:32,986 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:05:32,986 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'cosa', 'più', 'importante', 'è', 'che', 'il', 'cuore', 'del', 'nostro', 'cuore', 'del', 'nostro', 'c@@', 'atti@@', 'mo', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:05:32,987 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:05:32,987 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:05:32,987 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cosa più importante è che il cuore del nostro cuore del nostro cattimo globale.
2025-05-30 00:05:32,987 - INFO - joeynmt.training - Example #3
2025-05-30 00:05:32,987 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:05:32,987 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:05:32,987 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'cosa', 'che', 'sta', 'succe@@', 'dendo', 'nel', 'vent@@', 'o@@', ',', 'e', 's@@', 'com@@', 'par@@', 'sa', 'nel', 's@@', 'ab@@', 'i@@', 'li@@', ',', 'e', 'il', 'suo', 's@@', 'guar@@', 'do', 'in', 'S@@', 'om@@', 'in@@', 'at@@', 'o@@', '.', '</s>']
2025-05-30 00:05:32,987 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:05:32,988 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:05:32,988 - INFO - joeynmt.training - 	Hypothesis: La prima cosa che sta succedendo nel vento, e scomparsa nel sabili, e il suo sguardo in Sominato.
2025-05-30 00:05:32,988 - INFO - joeynmt.training - Example #4
2025-05-30 00:05:32,988 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:05:32,988 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:05:32,988 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:05:32,988 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:05:32,988 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:05:32,988 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è un disegno di quello che è successo negli ultimi 25 anni.
2025-05-30 00:05:36,480 - INFO - joeynmt.training - Epoch   6, Step:    53100, Batch Loss:     1.846949, Batch Acc: 0.466530, Tokens per Sec:    16772, Lr: 0.000300
2025-05-30 00:05:39,964 - INFO - joeynmt.training - Epoch   6, Step:    53200, Batch Loss:     1.832728, Batch Acc: 0.472375, Tokens per Sec:    19731, Lr: 0.000300
2025-05-30 00:05:43,451 - INFO - joeynmt.training - Epoch   6, Step:    53300, Batch Loss:     1.919587, Batch Acc: 0.464813, Tokens per Sec:    19426, Lr: 0.000300
2025-05-30 00:05:46,935 - INFO - joeynmt.training - Epoch   6, Step:    53400, Batch Loss:     1.911847, Batch Acc: 0.470454, Tokens per Sec:    20058, Lr: 0.000300
2025-05-30 00:05:50,402 - INFO - joeynmt.training - Epoch   6, Step:    53500, Batch Loss:     1.952778, Batch Acc: 0.465513, Tokens per Sec:    19104, Lr: 0.000300
2025-05-30 00:05:50,403 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:05:50,403 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:05:59,367 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.83, ppl:   6.26, acc:   0.49, generation: 8.9537[sec], evaluation: 0.0000[sec]
2025-05-30 00:05:59,368 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:06:00,006 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/51500.ckpt
2025-05-30 00:06:00,036 - INFO - joeynmt.training - Example #0
2025-05-30 00:06:00,037 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:06:00,037 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:06:00,037 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'sono', 'il', 'ghi@@', 'acci@@', 'ato', 'ar@@', 'c@@', 'ci@@', 'one', 'ar@@', 'ch@@', 'e@@', ',', 'che', 'è', 'stato', 'chiam@@', 'ato', '<unk>', 'E@@', 'is@@', 'k@@', 'et@@', ',', 'che', 'ha', 'avuto', 'il', '4@@', '8', 'stati', 'per', 'cento', 'di', 'questi', 'gruppi', 'di', 'persone', 'che', 'hanno', 'avuto', 'il', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cent@@', 'o@@', '.', '</s>']
2025-05-30 00:06:00,037 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:06:00,037 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:06:00,038 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive che sono il ghiacciato arccione arche, che è stato chiamato <unk> Eisket, che ha avuto il 48 stati per cento di questi gruppi di persone che hanno avuto il 48 stati per cento per cento.
2025-05-30 00:06:00,038 - INFO - joeynmt.training - Example #1
2025-05-30 00:06:00,038 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:06:00,038 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:06:00,038 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'ti', 'la', 'sost@@', 'eni@@', 'bilità', 'di', 'questo', 'problema', 'speci@@', 'al@@', 'e@@', ',', 'perché', 'non', 'lo', 'fa', 'il', 't@@', 'asso', 'di', 'di@@', 'sag@@', 'i@@', 'o@@', '.', '</s>']
2025-05-30 00:06:00,038 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:06:00,038 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:06:00,039 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forti la sostenibilità di questo problema speciale, perché non lo fa il tasso di disagio.
2025-05-30 00:06:00,039 - INFO - joeynmt.training - Example #2
2025-05-30 00:06:00,039 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:06:00,039 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:06:00,039 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'senso', 'è', 'il', 'cal@@', 'am@@', 'aro', 'di', 'ghi@@', 'accio', 'ar@@', 'ch@@', 'e@@', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'c@@', 'lim@@', 'ass@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:06:00,039 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:06:00,039 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:06:00,039 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il senso è il calamaro di ghiaccio arche, il cuore del nostro sistema climassale.
2025-05-30 00:06:00,040 - INFO - joeynmt.training - Example #3
2025-05-30 00:06:00,040 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:06:00,040 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:06:00,040 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'cominci@@', 'a', 'a', 'a', 'di@@', 'et@@', 'ro@@', ',', 'e', 's@@', 'ì@@', '.', '</s>']
2025-05-30 00:06:00,040 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:06:00,040 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:06:00,040 - INFO - joeynmt.training - 	Hypothesis: Si comincia a a dietro, e sì.
2025-05-30 00:06:00,040 - INFO - joeynmt.training - Example #4
2025-05-30 00:06:00,041 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:06:00,041 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:06:00,041 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'un', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:06:00,041 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:06:00,041 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:06:00,041 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è un disegno di un disegno di quello che è successo negli ultimi 25 anni.
2025-05-30 00:06:03,551 - INFO - joeynmt.training - Epoch   6, Step:    53600, Batch Loss:     1.859836, Batch Acc: 0.468936, Tokens per Sec:    16915, Lr: 0.000300
2025-05-30 00:06:07,047 - INFO - joeynmt.training - Epoch   6, Step:    53700, Batch Loss:     1.960669, Batch Acc: 0.468774, Tokens per Sec:    20414, Lr: 0.000300
2025-05-30 00:06:10,519 - INFO - joeynmt.training - Epoch   6, Step:    53800, Batch Loss:     1.807363, Batch Acc: 0.469264, Tokens per Sec:    20173, Lr: 0.000300
2025-05-30 00:06:13,988 - INFO - joeynmt.training - Epoch   6, Step:    53900, Batch Loss:     1.748084, Batch Acc: 0.474372, Tokens per Sec:    19744, Lr: 0.000300
2025-05-30 00:06:17,462 - INFO - joeynmt.training - Epoch   6, Step:    54000, Batch Loss:     2.006175, Batch Acc: 0.468226, Tokens per Sec:    19633, Lr: 0.000300
2025-05-30 00:06:17,463 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:06:17,463 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:06:25,006 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.83, ppl:   6.25, acc:   0.49, generation: 7.5339[sec], evaluation: 0.0000[sec]
2025-05-30 00:06:25,007 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:06:25,588 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/52000.ckpt
2025-05-30 00:06:25,617 - INFO - joeynmt.training - Example #0
2025-05-30 00:06:25,617 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:06:25,618 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:06:25,618 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'vedere', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'c@@', 'ob@@', 'al@@', 'e@@', ',', 'che', 'il', 'c@@', 'aff@@', 'è@@', ',', 'che', 'chiam@@', 'ano', 'il', 'li@@', 'mi@@', 'te', 'di', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'maggior', 'parte', 'delle', 'dimen@@', 'sioni', 'di', 'circa', '4@@', '8', 'stati', 'per', 'cento', 'per', 'la', 'dimen@@', 'sione', 'è', 'stata', 'la', 'più', 'grande', 'del', '4@@', '8', 'stati', 'per', 'cento', 'di', 'questi', 'due', 'di@@', 'apos@@', 'iti@@', 'v@@', 'a@@', '.', '</s>']
2025-05-30 00:06:25,618 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:06:25,618 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:06:25,618 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per vedere che il ghiaccio arcobale, che il caffè, che chiamano il limite di tre milioni di anni per la maggior parte delle dimensioni di circa 48 stati per cento per la dimensione è stata la più grande del 48 stati per cento di questi due diapositiva.
2025-05-30 00:06:25,618 - INFO - joeynmt.training - Example #1
2025-05-30 00:06:25,619 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:06:25,619 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:06:25,619 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'forte', 'la', 'prima', 'cosa', 'che', 'la', 'Terra', 'non', 'è', 'abbastanza', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:06:25,619 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:06:25,619 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:06:25,619 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte la prima cosa che la Terra non è abbastanza che non è il dibattito dell<unk> Eises.
2025-05-30 00:06:25,619 - INFO - joeynmt.training - Example #2
2025-05-30 00:06:25,620 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:06:25,620 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:06:25,620 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'ar@@', 'c@@', 'ci@@', 'one', 'ar@@', 't@@', 'ico', 'di', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:06:25,620 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:06:25,620 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:06:25,620 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cuore arccione artico di ghiaccio.
2025-05-30 00:06:25,620 - INFO - joeynmt.training - Example #3
2025-05-30 00:06:25,620 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:06:25,620 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:06:25,621 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'cominci@@', 'a', 'a', 'a', 'a', 'fu@@', 'ori@@', ',', 'e', 'sc@@', 'r@@', 'oc@@', 'ci@@', 'ata', 'nel', 's@@', 'om@@', 'e@@', '.', '</s>']
2025-05-30 00:06:25,621 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:06:25,621 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:06:25,621 - INFO - joeynmt.training - 	Hypothesis: Si comincia a a a fuori, e scrocciata nel some.
2025-05-30 00:06:25,621 - INFO - joeynmt.training - Example #4
2025-05-30 00:06:25,621 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:06:25,621 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:06:25,621 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 'consegu@@', 'enza', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:06:25,622 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:06:25,622 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:06:25,622 - INFO - joeynmt.training - 	Hypothesis: La prossima conseguenza che vi mostro è un disegno che è successo negli ultimi 25 anni.
2025-05-30 00:06:29,050 - INFO - joeynmt.training - Epoch   6, Step:    54100, Batch Loss:     1.669568, Batch Acc: 0.471130, Tokens per Sec:    17401, Lr: 0.000300
2025-05-30 00:06:32,485 - INFO - joeynmt.training - Epoch   6, Step:    54200, Batch Loss:     1.944494, Batch Acc: 0.467696, Tokens per Sec:    19969, Lr: 0.000300
2025-05-30 00:06:35,952 - INFO - joeynmt.training - Epoch   6, Step:    54300, Batch Loss:     1.736720, Batch Acc: 0.471850, Tokens per Sec:    19506, Lr: 0.000300
2025-05-30 00:06:39,436 - INFO - joeynmt.training - Epoch   6, Step:    54400, Batch Loss:     1.988231, Batch Acc: 0.470160, Tokens per Sec:    19754, Lr: 0.000300
2025-05-30 00:06:42,909 - INFO - joeynmt.training - Epoch   6, Step:    54500, Batch Loss:     1.901903, Batch Acc: 0.467739, Tokens per Sec:    20491, Lr: 0.000300
2025-05-30 00:06:42,910 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:06:42,910 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:06:50,306 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.83, ppl:   6.24, acc:   0.49, generation: 7.3852[sec], evaluation: 0.0000[sec]
2025-05-30 00:06:50,306 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:06:50,979 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/51000.ckpt
2025-05-30 00:06:51,007 - INFO - joeynmt.training - Example #0
2025-05-30 00:06:51,008 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:06:51,008 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:06:51,008 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'consegu@@', 'enze', 'per', 'osserv@@', 'are', 'che', 'la', 'maggior', 'parte', 'delle', 'persone', 'che', 'chiam@@', 'ano', 'l@@', '<unk>', 'ar@@', 'c@@', 'ic@@', 'l@@', 'a@@', ',', 'che', 'chiam@@', 'ano', 'il', '4@@', '8', 'stati', 'per', 'cento', 'di', 'anni', 'per', 'la', 'dimen@@', 'sione', 'di', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cento', 'di', 'l@@', '<unk>', 'ann@@', 'o@@', '.', '</s>']
2025-05-30 00:06:51,009 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:06:51,009 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:06:51,009 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due conseguenze per osservare che la maggior parte delle persone che chiamano l<unk> arcicla, che chiamano il 48 stati per cento di anni per la dimensione di 48 stati per cento per cento di l<unk> anno.
2025-05-30 00:06:51,009 - INFO - joeynmt.training - Example #1
2025-05-30 00:06:51,009 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:06:51,009 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:06:51,009 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'pot@@', 'ente', 'da', 'questa', 'cosa', 'non', 'è', 'abbastanza', 'la', 'prima', 'da', 'questa', 'particolare', 'problema', 'che', 'non', 'è', 'la', 'di@@', 'chiar@@', 'azione', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:06:51,010 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:06:51,010 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:06:51,010 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza potente da questa cosa non è abbastanza la prima da questa particolare problema che non è la dichiarazione del ghiaccio.
2025-05-30 00:06:51,010 - INFO - joeynmt.training - Example #2
2025-05-30 00:06:51,010 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:06:51,010 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:06:51,010 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'cosa', 'più', 'str@@', 'ana', 'è', 'la', 'cosa', 'più', 'c@@', 'atti@@', 'va', 'del', 'nostro', 'sistema', 'c@@', 'atti@@', 'vo', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:06:51,010 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:06:51,011 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:06:51,011 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cosa più strana è la cosa più cattiva del nostro sistema cattivo globale.
2025-05-30 00:06:51,011 - INFO - joeynmt.training - Example #3
2025-05-30 00:06:51,011 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:06:51,011 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:06:51,011 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'la', 'prima', 'volta', 'in', 'in@@', 'ver@@', 'no@@', ',', 'e', 'la', 'p@@', 'ura', 'di', 'est@@', 'i@@', '.', '</s>']
2025-05-30 00:06:51,011 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:06:51,011 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:06:51,011 - INFO - joeynmt.training - 	Hypothesis: E la prima volta in inverno, e la pura di esti.
2025-05-30 00:06:51,012 - INFO - joeynmt.training - Example #4
2025-05-30 00:06:51,012 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:06:51,012 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:06:51,012 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'cosa', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:06:51,012 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:06:51,012 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:06:51,012 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una cosa che è successo negli ultimi 25 anni.
2025-05-30 00:06:54,514 - INFO - joeynmt.training - Epoch   6, Step:    54600, Batch Loss:     1.888463, Batch Acc: 0.466659, Tokens per Sec:    16430, Lr: 0.000300
2025-05-30 00:06:57,946 - INFO - joeynmt.training - Epoch   6, Step:    54700, Batch Loss:     1.955418, Batch Acc: 0.469553, Tokens per Sec:    20204, Lr: 0.000300
2025-05-30 00:07:01,421 - INFO - joeynmt.training - Epoch   6, Step:    54800, Batch Loss:     1.942190, Batch Acc: 0.467874, Tokens per Sec:    19545, Lr: 0.000300
2025-05-30 00:07:04,890 - INFO - joeynmt.training - Epoch   6, Step:    54900, Batch Loss:     1.927397, Batch Acc: 0.467844, Tokens per Sec:    19948, Lr: 0.000300
2025-05-30 00:07:08,356 - INFO - joeynmt.training - Epoch   6, Step:    55000, Batch Loss:     1.901844, Batch Acc: 0.469835, Tokens per Sec:    19631, Lr: 0.000300
2025-05-30 00:07:08,357 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:07:08,357 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:07:15,293 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.83, ppl:   6.22, acc:   0.49, generation: 6.9294[sec], evaluation: 0.0000[sec]
2025-05-30 00:07:15,293 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:07:15,860 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/52500.ckpt
2025-05-30 00:07:15,888 - INFO - joeynmt.training - Example #0
2025-05-30 00:07:15,888 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:07:15,889 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:07:15,889 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've@@', ',', 'per', 'la', 'con@@', 'serv@@', 'azione', 'di', 'ghi@@', 'acci@@', 'o@@', ',', 'che', 'la', 'maggior', 'parte', 'delle', 'dimen@@', 'sioni', 'di', 'tre', 'milioni', 'di', 'an@@', 'ni@@', ',', 'per', 'tre', 'milioni', 'di', 'an@@', 'ni@@', ',', 'per', 'tre', 'milioni', 'di', 'an@@', 'ni@@', ',', 'per', 'la', 'maggior', 'parte', 'delle', 'dimen@@', 'sioni', 'è', 'successo', 'al', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cent@@', 'o@@', '.', '</s>']
2025-05-30 00:07:15,889 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:07:15,889 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:07:15,889 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive, per la conservazione di ghiaccio, che la maggior parte delle dimensioni di tre milioni di anni, per tre milioni di anni, per tre milioni di anni, per la maggior parte delle dimensioni è successo al 48 stati per cento per cento.
2025-05-30 00:07:15,889 - INFO - joeynmt.training - Example #1
2025-05-30 00:07:15,890 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:07:15,890 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:07:15,890 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'più', 'forte', 'la', 'sua', 'storia', 'di', 'questo', 'problema', 'speci@@', 'ale', 'di', 'questo', 'problema', 'speci@@', 'ale', 'perché', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:07:15,890 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:07:15,890 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:07:15,890 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto più forte la sua storia di questo problema speciale di questo problema speciale perché non è il dell<unk> Eises.
2025-05-30 00:07:15,890 - INFO - joeynmt.training - Example #2
2025-05-30 00:07:15,891 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:07:15,891 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:07:15,891 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'att@@', 'ica', 'di', 'ghi@@', 'accio', 'è', 'il', 'cuore', 'più', 'intelli@@', 'g@@', 'ent@@', 'e@@', '.', '</s>']
2025-05-30 00:07:15,891 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:07:15,891 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:07:15,891 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cattica di ghiaccio è il cuore più intelligente.
2025-05-30 00:07:15,891 - INFO - joeynmt.training - Example #3
2025-05-30 00:07:15,891 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:07:15,892 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:07:15,892 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'prima', 'di', 'tut@@', 'to@@', ',', 'e', 's@@', 'ì@@', ',', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:07:15,892 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:07:15,892 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:07:15,892 - INFO - joeynmt.training - 	Hypothesis: E prima di tutto, e sì, in estate.
2025-05-30 00:07:15,892 - INFO - joeynmt.training - Example #4
2025-05-30 00:07:15,892 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:07:15,892 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:07:15,892 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'cosa', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:07:15,893 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:07:15,893 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:07:15,893 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una cosa che è successo negli ultimi 25 anni.
2025-05-30 00:07:19,341 - INFO - joeynmt.training - Epoch   6, Step:    55100, Batch Loss:     2.019684, Batch Acc: 0.466749, Tokens per Sec:    17188, Lr: 0.000300
2025-05-30 00:07:22,817 - INFO - joeynmt.training - Epoch   6, Step:    55200, Batch Loss:     1.814914, Batch Acc: 0.466822, Tokens per Sec:    20025, Lr: 0.000300
2025-05-30 00:07:26,311 - INFO - joeynmt.training - Epoch   6, Step:    55300, Batch Loss:     1.919776, Batch Acc: 0.478173, Tokens per Sec:    20327, Lr: 0.000300
2025-05-30 00:07:29,772 - INFO - joeynmt.training - Epoch   6, Step:    55400, Batch Loss:     1.824666, Batch Acc: 0.472954, Tokens per Sec:    19630, Lr: 0.000300
2025-05-30 00:07:33,243 - INFO - joeynmt.training - Epoch   6, Step:    55500, Batch Loss:     1.804939, Batch Acc: 0.473984, Tokens per Sec:    19800, Lr: 0.000300
2025-05-30 00:07:33,244 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:07:33,244 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:07:40,681 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.82, ppl:   6.18, acc:   0.49, generation: 7.4267[sec], evaluation: 0.0000[sec]
2025-05-30 00:07:40,681 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:07:41,509 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/53000.ckpt
2025-05-30 00:07:41,538 - INFO - joeynmt.training - Example #0
2025-05-30 00:07:41,538 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:07:41,539 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:07:41,539 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'con@@', 'net@@', 'tere', 'che', 'gli', 'E@@', 'b@@', 'ben@@', 'e@@', ',', 'che', 'i', 'c@@', 'avi', 'di', 'ghi@@', 'accio', 'che', 'ha', 'ann@@', 'u@@', 'almente', 'tre', 'milioni', 'di', 'anni', 'di', 'an@@', 'ni@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'circa', 'il', '4@@', '8@@', '0@@', '%', 'per', 'la', 'dimen@@', 'sione', 'è', 'successo', 'in', 'questo', 'mod@@', 'o@@', '.', '</s>']
2025-05-30 00:07:41,539 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:07:41,539 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:07:41,539 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per connettere che gli Ebbene, che i cavi di ghiaccio che ha annualmente tre milioni di anni di anni, per tre milioni di anni di circa il 480% per la dimensione è successo in questo modo.
2025-05-30 00:07:41,539 - INFO - joeynmt.training - Example #1
2025-05-30 00:07:41,540 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:07:41,540 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:07:41,540 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'che', 'è', 'la', 'più', 'grande', 'di', 'questo', 'probl@@', 'em@@', 'a@@', ',', 'perché', 'non', 'è', 'il', 't@@', 'asso', 'di', 'ghi@@', 'accio', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:07:41,540 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:07:41,540 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:07:41,540 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte, che è la più grande di questo problema, perché non è il tasso di ghiaccio dell<unk> Eises.
2025-05-30 00:07:41,540 - INFO - joeynmt.training - Example #2
2025-05-30 00:07:41,540 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:07:41,541 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:07:41,541 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'att@@', 'ica', 'di', 'ghi@@', 'accio', 'ar@@', 't@@', 'ico', 'è', 'il', 'cuore', 'c@@', 'atti@@', 'vo', 'del', 'nostro', 'sistema', 'c@@', 'lim@@', 'ass@@', 'a@@', '.', '</s>']
2025-05-30 00:07:41,541 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:07:41,541 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:07:41,541 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cattica di ghiaccio artico è il cuore cattivo del nostro sistema climassa.
2025-05-30 00:07:41,541 - INFO - joeynmt.training - Example #3
2025-05-30 00:07:41,541 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:07:41,541 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:07:41,541 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'in', 'in@@', 'ver@@', 'no', 'e', 'sci@@', 'am@@', 'ent@@', 'e@@', '.', '</s>']
2025-05-30 00:07:41,542 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:07:41,542 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:07:41,542 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo in inverno e sciamente.
2025-05-30 00:07:41,542 - INFO - joeynmt.training - Example #4
2025-05-30 00:07:41,542 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:07:41,542 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:07:41,542 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:07:41,543 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:07:41,543 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:07:41,543 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è un disegno che è successo negli ultimi 25 anni.
2025-05-30 00:07:45,016 - INFO - joeynmt.training - Epoch   6, Step:    55600, Batch Loss:     2.211995, Batch Acc: 0.467598, Tokens per Sec:    15789, Lr: 0.000300
2025-05-30 00:07:48,470 - INFO - joeynmt.training - Epoch   6, Step:    55700, Batch Loss:     1.814620, Batch Acc: 0.473549, Tokens per Sec:    19213, Lr: 0.000300
2025-05-30 00:07:51,909 - INFO - joeynmt.training - Epoch   6, Step:    55800, Batch Loss:     1.882063, Batch Acc: 0.470837, Tokens per Sec:    19507, Lr: 0.000300
2025-05-30 00:07:55,370 - INFO - joeynmt.training - Epoch   6, Step:    55900, Batch Loss:     1.776152, Batch Acc: 0.467686, Tokens per Sec:    19726, Lr: 0.000300
2025-05-30 00:07:58,803 - INFO - joeynmt.training - Epoch   6, Step:    56000, Batch Loss:     1.870846, Batch Acc: 0.465695, Tokens per Sec:    20429, Lr: 0.000300
2025-05-30 00:07:58,803 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:07:58,803 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:08:06,031 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.82, ppl:   6.16, acc:   0.50, generation: 7.2176[sec], evaluation: 0.0000[sec]
2025-05-30 00:08:06,031 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:08:06,690 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/53500.ckpt
2025-05-30 00:08:06,717 - INFO - joeynmt.training - Example #0
2025-05-30 00:08:06,717 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:08:06,718 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:08:06,718 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'che', 'le', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'ch@@', 'e@@', ',', 'il', 'ghi@@', 'accio', 'ar@@', 'ch@@', 'ico', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'in@@', 'fer@@', 'mi@@', 'era', 'che', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:08:06,718 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:08:06,718 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:08:06,718 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato che le due diapositive che il ghiaccio arche, il ghiaccio archico che ha fatto per tre milioni di anni la dimensione dell<unk> infermiera che per tre milioni di anni di anni.
2025-05-30 00:08:06,718 - INFO - joeynmt.training - Example #1
2025-05-30 00:08:06,718 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:08:06,718 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:08:06,718 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'la', 'maggior', 'parte', 'di', 'questo', 'problema', 'speci@@', 'al@@', 'e@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:08:06,719 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:08:06,719 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:08:06,719 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte, la maggior parte di questo problema speciale, perché non è il dibattito dell<unk> Eises.
2025-05-30 00:08:06,719 - INFO - joeynmt.training - Example #2
2025-05-30 00:08:06,719 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:08:06,719 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:08:06,719 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'aff@@', 'è', 'la', 'c@@', 'app@@', 'ola', 'ar@@', 'ch@@', 'e@@', ',', 'il', 'cuore', 'c@@', 'ru@@', 'ci@@', 'ale', 'del', 'nostro', 'sistema', 'c@@', 'atti@@', 'vo@@', '.', '</s>']
2025-05-30 00:08:06,720 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:08:06,720 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:08:06,720 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la caffè la cappola arche, il cuore cruciale del nostro sistema cattivo.
2025-05-30 00:08:06,720 - INFO - joeynmt.training - Example #3
2025-05-30 00:08:06,720 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:08:06,720 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:08:06,720 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'cominci@@', 'a', 'a', 'a', 's@@', 'v@@', 'egli@@', 'are', 'in', 'in@@', 'ver@@', 'no@@', '.', '</s>']
2025-05-30 00:08:06,720 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:08:06,720 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:08:06,720 - INFO - joeynmt.training - 	Hypothesis: Si comincia a a svegliare in inverno.
2025-05-30 00:08:06,720 - INFO - joeynmt.training - Example #4
2025-05-30 00:08:06,721 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:08:06,721 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:08:06,721 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'cosa', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:08:06,721 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:08:06,721 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:08:06,721 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una cosa che è successo negli ultimi 25 anni.
2025-05-30 00:08:10,179 - INFO - joeynmt.training - Epoch   6, Step:    56100, Batch Loss:     1.826226, Batch Acc: 0.475990, Tokens per Sec:    16758, Lr: 0.000300
2025-05-30 00:08:13,619 - INFO - joeynmt.training - Epoch   6, Step:    56200, Batch Loss:     1.769016, Batch Acc: 0.471741, Tokens per Sec:    20229, Lr: 0.000300
2025-05-30 00:08:17,048 - INFO - joeynmt.training - Epoch   6, Step:    56300, Batch Loss:     2.038924, Batch Acc: 0.467674, Tokens per Sec:    20345, Lr: 0.000300
2025-05-30 00:08:20,497 - INFO - joeynmt.training - Epoch   6, Step:    56400, Batch Loss:     1.890183, Batch Acc: 0.470885, Tokens per Sec:    20331, Lr: 0.000300
2025-05-30 00:08:23,972 - INFO - joeynmt.training - Epoch   6, Step:    56500, Batch Loss:     1.907706, Batch Acc: 0.469929, Tokens per Sec:    19665, Lr: 0.000300
2025-05-30 00:08:23,972 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:08:23,972 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:08:32,010 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.82, ppl:   6.18, acc:   0.49, generation: 8.0283[sec], evaluation: 0.0000[sec]
2025-05-30 00:08:32,424 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/54000.ckpt
2025-05-30 00:08:32,447 - INFO - joeynmt.training - Example #0
2025-05-30 00:08:32,448 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:08:32,448 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:08:32,448 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'i', 'ghi@@', 'acci@@', 'anti', 'ar@@', 'kt@@', 'ico', 'che', 'i', 'ghi@@', 'acci@@', 'anti', 'ar@@', 'kt@@', 'ico', 'che', 'ha', 'ann@@', 'u@@', 'ato', 'tre', 'milioni', 'di', 'anni', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'in@@', 'fer@@', 'i@@', 'ore', 'di', '3@@', '3@@', '0@@', '<unk>', '.', '</s>']
2025-05-30 00:08:32,448 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:08:32,448 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:08:32,448 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive che i ghiaccianti arktico che i ghiaccianti arktico che ha annuato tre milioni di anni la dimensione dell<unk> inferiore di 330<unk> .
2025-05-30 00:08:32,449 - INFO - joeynmt.training - Example #1
2025-05-30 00:08:32,449 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:08:32,449 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:08:32,449 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'for@@', 'ti', 'da', 'una', 'parte', 'di', 'queste', 'cose', 'che', 'non', 'si', 'è', 'est@@', 'eso', 'il', 'problema', 'speci@@', 'ale', 'di', 'ghi@@', 'accio', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:08:32,449 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:08:32,449 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:08:32,449 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forti da una parte di queste cose che non si è esteso il problema speciale di ghiaccio dell<unk> Eises.
2025-05-30 00:08:32,450 - INFO - joeynmt.training - Example #2
2025-05-30 00:08:32,450 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:08:32,450 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:08:32,450 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'att@@', 'ura', 'ar@@', 'kt@@', 'ica', 'di', 'ghi@@', 'accio', 'e', 'il', 'cuore', 'c@@', 'ru@@', 'ci@@', 'ale', 'del', 'nostro', 'c@@', 'ag@@', 'n@@', 'ante', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:08:32,450 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:08:32,450 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:08:32,450 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cattura arktica di ghiaccio e il cuore cruciale del nostro cagnante globale.
2025-05-30 00:08:32,450 - INFO - joeynmt.training - Example #3
2025-05-30 00:08:32,451 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:08:32,451 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:08:32,451 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'nel', 'vent@@', 'o@@', ',', 'e', 'dol@@', 'c@@', 'e@@', '.', '</s>']
2025-05-30 00:08:32,451 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:08:32,451 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:08:32,451 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo nel vento, e dolce.
2025-05-30 00:08:32,451 - INFO - joeynmt.training - Example #4
2025-05-30 00:08:32,451 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:08:32,452 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:08:32,452 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'una', 'di@@', 'stri@@', 'bu@@', 'zione', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:08:32,452 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:08:32,452 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:08:32,452 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è una distribuzione che è successo negli ultimi 25 anni.
2025-05-30 00:08:33,819 - INFO - joeynmt.training - Epoch   6: total training loss 17821.65
2025-05-30 00:08:33,819 - INFO - joeynmt.training - EPOCH 7
2025-05-30 00:08:35,953 - INFO - joeynmt.training - Epoch   7, Step:    56600, Batch Loss:     1.880762, Batch Acc: 0.475761, Tokens per Sec:    19698, Lr: 0.000300
2025-05-30 00:08:39,424 - INFO - joeynmt.training - Epoch   7, Step:    56700, Batch Loss:     1.734607, Batch Acc: 0.485813, Tokens per Sec:    19673, Lr: 0.000300
2025-05-30 00:08:42,910 - INFO - joeynmt.training - Epoch   7, Step:    56800, Batch Loss:     1.787012, Batch Acc: 0.485153, Tokens per Sec:    20260, Lr: 0.000300
2025-05-30 00:08:46,388 - INFO - joeynmt.training - Epoch   7, Step:    56900, Batch Loss:     1.625844, Batch Acc: 0.490749, Tokens per Sec:    19837, Lr: 0.000300
2025-05-30 00:08:49,871 - INFO - joeynmt.training - Epoch   7, Step:    57000, Batch Loss:     1.783553, Batch Acc: 0.483648, Tokens per Sec:    20070, Lr: 0.000300
2025-05-30 00:08:49,871 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:08:49,871 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:08:56,685 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.81, ppl:   6.13, acc:   0.49, generation: 6.8023[sec], evaluation: 0.0000[sec]
2025-05-30 00:08:56,686 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:08:57,328 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/54500.ckpt
2025-05-30 00:08:57,357 - INFO - joeynmt.training - Example #0
2025-05-30 00:08:57,357 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:08:57,357 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:08:57,357 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'il', 'ghi@@', 'acci@@', 'aio', 'ar@@', 'ch@@', 'e@@', ',', 'che', 'il', 'ghi@@', 'acci@@', 'aio', 'ar@@', 'ch@@', 'e@@', ',', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'an@@', 'ni@@', ',', 'per', 'tre', 'milioni', 'di', 'an@@', 'ni@@', ',', 'per', 'tre', 'milioni', 'di', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:08:57,358 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:08:57,358 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:08:57,358 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive che il ghiacciaio arche, che il ghiacciaio arche, che ha fatto per tre milioni di anni, per tre milioni di anni, per tre milioni di anni.
2025-05-30 00:08:57,358 - INFO - joeynmt.training - Example #1
2025-05-30 00:08:57,358 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:08:57,358 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:08:57,358 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'ma', 'non', 'è', 'abbastanza', 'forte', 'da', 'questa', 'cosa', 'che', 'non', 'è', 'il', 'di@@', 'v@@', 'ario', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:08:57,359 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:08:57,359 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:08:57,359 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte, ma non è abbastanza forte da questa cosa che non è il divario del ghiaccio.
2025-05-30 00:08:57,359 - INFO - joeynmt.training - Example #2
2025-05-30 00:08:57,359 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:08:57,359 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:08:57,359 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'ris@@', 'i', 'ar@@', 'c@@', 'atti@@', 'vi', 'la', 'c@@', 'atti@@', 'va', 'del', 'nostro', 'sistema', 'K@@', 'lim@@', 'as@@', 'sist@@', 'o@@', '.', '</s>']
2025-05-30 00:08:57,360 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:08:57,360 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:08:57,360 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la crisi arcattivi la cattiva del nostro sistema Klimassisto.
2025-05-30 00:08:57,360 - INFO - joeynmt.training - Example #3
2025-05-30 00:08:57,360 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:08:57,360 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:08:57,360 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'nel', 'vent@@', 'o@@', ',', 'e', 'dol@@', 'c@@', 'emente', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:08:57,360 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:08:57,361 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:08:57,361 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo nel vento, e dolcemente in estate.
2025-05-30 00:08:57,361 - INFO - joeynmt.training - Example #4
2025-05-30 00:08:57,361 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:08:57,361 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:08:57,361 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'una', 'foto', 'di', 'un', 'di@@', 'seg@@', 'no', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:08:57,361 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:08:57,361 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:08:57,361 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è una foto di un disegno che è successo negli ultimi 25 anni.
2025-05-30 00:09:00,861 - INFO - joeynmt.training - Epoch   7, Step:    57100, Batch Loss:     1.823012, Batch Acc: 0.485307, Tokens per Sec:    16715, Lr: 0.000300
2025-05-30 00:09:04,329 - INFO - joeynmt.training - Epoch   7, Step:    57200, Batch Loss:     1.919286, Batch Acc: 0.479050, Tokens per Sec:    19890, Lr: 0.000300
2025-05-30 00:09:07,830 - INFO - joeynmt.training - Epoch   7, Step:    57300, Batch Loss:     1.902779, Batch Acc: 0.483694, Tokens per Sec:    19905, Lr: 0.000300
2025-05-30 00:09:11,294 - INFO - joeynmt.training - Epoch   7, Step:    57400, Batch Loss:     1.913182, Batch Acc: 0.484926, Tokens per Sec:    19238, Lr: 0.000300
2025-05-30 00:09:14,770 - INFO - joeynmt.training - Epoch   7, Step:    57500, Batch Loss:     1.739674, Batch Acc: 0.478157, Tokens per Sec:    18874, Lr: 0.000300
2025-05-30 00:09:14,771 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:09:14,771 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:09:21,479 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.82, ppl:   6.19, acc:   0.49, generation: 6.6988[sec], evaluation: 0.0000[sec]
2025-05-30 00:09:22,094 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/55000.ckpt
2025-05-30 00:09:22,124 - INFO - joeynmt.training - Example #0
2025-05-30 00:09:22,124 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:09:22,124 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:09:22,124 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'con@@', 'fr@@', 'ont@@', 'are', 'che', 'il', 'ghi@@', 'acci@@', 'aio', 'ar@@', 'ch@@', 'e@@', ',', 'che', 'gli', 'str@@', 'ati', 'per', 'tre', 'milioni', 'di', 'an@@', 'ni@@', ',', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:09:22,125 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:09:22,125 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:09:22,125 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per confrontare che il ghiacciaio arche, che gli strati per tre milioni di anni, che ha fatto per tre milioni di anni.
2025-05-30 00:09:22,125 - INFO - joeynmt.training - Example #1
2025-05-30 00:09:22,125 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:09:22,125 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:09:22,125 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'la', 'più', 'grande', 'grande', 'quantità', 'di', 'problemi', 'speci@@', 'ali@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:09:22,126 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:09:22,126 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:09:22,126 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, la più grande grande quantità di problemi speciali, perché non è il dibattito del ghiaccio.
2025-05-30 00:09:22,126 - INFO - joeynmt.training - Example #2
2025-05-30 00:09:22,126 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:09:22,126 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:09:22,126 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'di', 'ghi@@', 'accio', 'è', 'il', 'cuore', 'di', 'ghi@@', 'accio', 'che', 'si', 'tratta', 'di', 'un', 'cuore', 'di', 'c@@', 'atti@@', 'vo', 'del', 'nostro', 'c@@', 'atti@@', 'vo', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:09:22,127 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:09:22,127 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:09:22,127 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cuore di ghiaccio è il cuore di ghiaccio che si tratta di un cuore di cattivo del nostro cattivo globale.
2025-05-30 00:09:22,127 - INFO - joeynmt.training - Example #3
2025-05-30 00:09:22,127 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:09:22,127 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:09:22,127 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'nel', 'in@@', 'ver@@', 'no@@', ',', 'e', 'dol@@', 'ce', 'in', 'est@@', 'i@@', 'a@@', '.', '</s>']
2025-05-30 00:09:22,128 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:09:22,128 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:09:22,128 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo nel inverno, e dolce in estia.
2025-05-30 00:09:22,128 - INFO - joeynmt.training - Example #4
2025-05-30 00:09:22,128 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:09:22,128 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:09:22,128 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'cosa', 'di', 'tem@@', 'po@@', ',', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:09:22,128 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:09:22,128 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:09:22,129 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una cosa di tempo, che è successo negli ultimi 25 anni.
2025-05-30 00:09:25,623 - INFO - joeynmt.training - Epoch   7, Step:    57600, Batch Loss:     1.905904, Batch Acc: 0.477236, Tokens per Sec:    16483, Lr: 0.000300
2025-05-30 00:09:29,094 - INFO - joeynmt.training - Epoch   7, Step:    57700, Batch Loss:     1.896044, Batch Acc: 0.479795, Tokens per Sec:    19449, Lr: 0.000300
2025-05-30 00:09:32,579 - INFO - joeynmt.training - Epoch   7, Step:    57800, Batch Loss:     1.904636, Batch Acc: 0.479657, Tokens per Sec:    20081, Lr: 0.000300
2025-05-30 00:09:36,050 - INFO - joeynmt.training - Epoch   7, Step:    57900, Batch Loss:     1.776436, Batch Acc: 0.484505, Tokens per Sec:    19850, Lr: 0.000300
2025-05-30 00:09:39,546 - INFO - joeynmt.training - Epoch   7, Step:    58000, Batch Loss:     1.941468, Batch Acc: 0.483925, Tokens per Sec:    20001, Lr: 0.000300
2025-05-30 00:09:39,546 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:09:39,546 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:09:46,607 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.82, ppl:   6.17, acc:   0.49, generation: 7.0507[sec], evaluation: 0.0000[sec]
2025-05-30 00:09:47,073 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/57500.ckpt
2025-05-30 00:09:47,096 - INFO - joeynmt.training - Example #0
2025-05-30 00:09:47,096 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:09:47,096 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:09:47,097 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'con@@', 'ver@@', 't@@', 'ito', 'che', 'il', 'ghi@@', 'acci@@', 'aio', 'ar@@', 'c@@', 'ic@@', 'lo', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', 'quantità', 'di', 'ghi@@', 'accio', 'che', 'ha', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cent@@', '<unk>', '.', '</s>']
2025-05-30 00:09:47,097 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:09:47,097 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:09:47,097 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per convertito che il ghiacciaio arciclo che ha fatto per tre milioni di anni per la più grande quantità di ghiaccio che ha 48 stati per cento per cent<unk> .
2025-05-30 00:09:47,097 - INFO - joeynmt.training - Example #1
2025-05-30 00:09:47,097 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:09:47,098 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:09:47,098 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'non', 'è', 'abbastanza', 'la', 'ter@@', 'ri@@', 'bile', 'di', 'questo', 'problema', 'speci@@', 'ale', 'perché', 'non', 'lo', 'fa', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'amento', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:09:47,098 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:09:47,098 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:09:47,098 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, non è abbastanza la terribile di questo problema speciale perché non lo fa il dell<unk> Eislamento del ghiaccio.
2025-05-30 00:09:47,098 - INFO - joeynmt.training - Example #2
2025-05-30 00:09:47,098 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:09:47,098 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:09:47,098 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'att@@', 'ica', 'di', 'ghi@@', 'accio', 'è', 'il', 'cuore', 'del', 'nostro', 'sistema', 'c@@', 'lim@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:09:47,099 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:09:47,099 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:09:47,099 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cattica di ghiaccio è il cuore del nostro sistema climale.
2025-05-30 00:09:47,099 - INFO - joeynmt.training - Example #3
2025-05-30 00:09:47,099 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:09:47,099 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:09:47,099 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'nel', 'in@@', 'ver@@', 'no@@', ',', 'e', 's@@', 'com@@', 'par@@', 'e@@', ',', 'e', 'si', 's@@', 'vol@@', 'ge', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:09:47,100 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:09:47,100 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:09:47,100 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo nel inverno, e scompare, e si svolge in estate.
2025-05-30 00:09:47,100 - INFO - joeynmt.training - Example #4
2025-05-30 00:09:47,100 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:09:47,100 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:09:47,100 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'cosa', 'che', 'è', 'succ@@', 'ess@@', 'o@@', ',', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:09:47,100 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:09:47,100 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:09:47,100 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una cosa che è successo, che è successo negli ultimi 25 anni.
2025-05-30 00:09:50,616 - INFO - joeynmt.training - Epoch   7, Step:    58100, Batch Loss:     1.785622, Batch Acc: 0.485280, Tokens per Sec:    17484, Lr: 0.000300
2025-05-30 00:09:54,114 - INFO - joeynmt.training - Epoch   7, Step:    58200, Batch Loss:     1.840192, Batch Acc: 0.480338, Tokens per Sec:    19336, Lr: 0.000300
2025-05-30 00:09:57,596 - INFO - joeynmt.training - Epoch   7, Step:    58300, Batch Loss:     1.725941, Batch Acc: 0.480581, Tokens per Sec:    19735, Lr: 0.000300
2025-05-30 00:10:01,066 - INFO - joeynmt.training - Epoch   7, Step:    58400, Batch Loss:     1.754747, Batch Acc: 0.479170, Tokens per Sec:    19870, Lr: 0.000300
2025-05-30 00:10:04,520 - INFO - joeynmt.training - Epoch   7, Step:    58500, Batch Loss:     1.978123, Batch Acc: 0.478864, Tokens per Sec:    20501, Lr: 0.000300
2025-05-30 00:10:04,520 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:10:04,520 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:10:11,533 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.82, ppl:   6.15, acc:   0.50, generation: 7.0031[sec], evaluation: 0.0000[sec]
2025-05-30 00:10:11,977 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/56500.ckpt
2025-05-30 00:10:12,006 - INFO - joeynmt.training - Example #0
2025-05-30 00:10:12,007 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:10:12,007 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:10:12,007 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'di@@', 'vi@@', 'dere', 'che', 'il', '40', 'per', 'cento', 'dei', 'li@@', 'velli', 'di', 'ghi@@', 'accio', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'an@@', 'ni@@', ',', 'per', 'tre', 'milioni', 'di', 'an@@', 'ni@@', ',', 'il', '40', 'per', 'cento', 'della', 'dimen@@', 'sione', 'dei', 'v@@', 'ari', 'paesi', 'in', 'via', 'di', 'svilupp@@', 'o@@', '.', '</s>']
2025-05-30 00:10:12,007 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:10:12,007 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:10:12,007 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per dividere che il 40 per cento dei livelli di ghiaccio che ha fatto per tre milioni di anni, per tre milioni di anni, il 40 per cento della dimensione dei vari paesi in via di sviluppo.
2025-05-30 00:10:12,007 - INFO - joeynmt.training - Example #1
2025-05-30 00:10:12,008 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:10:12,008 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:10:12,008 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'la', 'più', 'grande', 'quantità', 'di', 'problemi', 'speci@@', 'ali@@', ',', 'perché', 'non', 'è', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'amento', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:10:12,008 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:10:12,008 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:10:12,008 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, la più grande quantità di problemi speciali, perché non è la dimensione dell<unk> Eislamento del ghiaccio.
2025-05-30 00:10:12,008 - INFO - joeynmt.training - Example #2
2025-05-30 00:10:12,009 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:10:12,009 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:10:12,009 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'atti@@', 'va', 'di', 'ghi@@', 'accio', 'ar@@', 't@@', 'ico', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:10:12,009 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:10:12,009 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:10:12,009 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cattiva di ghiaccio artico globale.
2025-05-30 00:10:12,009 - INFO - joeynmt.training - Example #3
2025-05-30 00:10:12,009 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:10:12,010 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:10:12,010 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'in', 'in@@', 'ver@@', 'no', 'e', 's@@', 'ì@@', '.', '</s>']
2025-05-30 00:10:12,010 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:10:12,010 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:10:12,010 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo in inverno e sì.
2025-05-30 00:10:12,010 - INFO - joeynmt.training - Example #4
2025-05-30 00:10:12,010 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:10:12,010 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:10:12,010 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'una', 'cosa', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:10:12,011 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:10:12,011 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:10:12,011 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è una cosa che è successo negli ultimi 25 anni.
2025-05-30 00:10:15,514 - INFO - joeynmt.training - Epoch   7, Step:    58600, Batch Loss:     1.735849, Batch Acc: 0.478967, Tokens per Sec:    17637, Lr: 0.000300
2025-05-30 00:10:18,976 - INFO - joeynmt.training - Epoch   7, Step:    58700, Batch Loss:     1.849874, Batch Acc: 0.485480, Tokens per Sec:    19981, Lr: 0.000300
2025-05-30 00:10:22,442 - INFO - joeynmt.training - Epoch   7, Step:    58800, Batch Loss:     1.954998, Batch Acc: 0.480232, Tokens per Sec:    19717, Lr: 0.000300
2025-05-30 00:10:25,906 - INFO - joeynmt.training - Epoch   7, Step:    58900, Batch Loss:     1.923677, Batch Acc: 0.477899, Tokens per Sec:    19759, Lr: 0.000300
2025-05-30 00:10:29,358 - INFO - joeynmt.training - Epoch   7, Step:    59000, Batch Loss:     1.970567, Batch Acc: 0.473376, Tokens per Sec:    19326, Lr: 0.000300
2025-05-30 00:10:29,359 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:10:29,359 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:10:36,743 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.82, ppl:   6.15, acc:   0.49, generation: 7.3779[sec], evaluation: 0.0000[sec]
2025-05-30 00:10:37,106 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/55500.ckpt
2025-05-30 00:10:37,133 - INFO - joeynmt.training - Example #0
2025-05-30 00:10:37,133 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:10:37,133 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:10:37,134 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'di@@', 'vi@@', 'dere', 'che', 'il', 'ghi@@', 'acci@@', 'aio', 'ar@@', 'ch@@', 'e@@', ',', 'che', 'l@@', '<unk>', 'ep@@', 'oc@@', 'a@@', ',', 'che', 'si', 'chiam@@', 'ano', 'il', '4@@', '8@@', '0@@', '%', 'dei', 'dimen@@', 'sioni', 'più', 'grandi', 'dimen@@', 'sioni', 'del', '4@@', '8@@', '0@@', '%', 'del', 'dimen@@', 'sioni', 'è', 'stato', 'il', '4@@', '8@@', '0@@', '%', 'del', 'dimen@@', 'sioni', 'è', 'stato', 'il', '4@@', '8@@', '0@@', '%', 'del', 'dimen@@', 'si@@', 'onale', 'del', '4@@', '8@@', '0@@', '%', 'dei', 'dimen@@', 'si@@', 'oni@@', '.', '</s>']
2025-05-30 00:10:37,134 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:10:37,134 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:10:37,134 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per dividere che il ghiacciaio arche, che l<unk> epoca, che si chiamano il 480% dei dimensioni più grandi dimensioni del 480% del dimensioni è stato il 480% del dimensioni è stato il 480% del dimensionale del 480% dei dimensioni.
2025-05-30 00:10:37,134 - INFO - joeynmt.training - Example #1
2025-05-30 00:10:37,135 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:10:37,135 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:10:37,135 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'la', 'più', 'grande', 'è', 'la', 'più', 'grande', 'di', 'questo', 'problema', 'speci@@', 'fic@@', 'o@@', ',', 'perché', 'non', 'lo', 'di@@', 'mostra', 'il', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:10:37,135 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:10:37,135 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:10:37,135 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, la più grande è la più grande di questo problema specifico, perché non lo dimostra il ghiaccio.
2025-05-30 00:10:37,135 - INFO - joeynmt.training - Example #2
2025-05-30 00:10:37,136 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:10:37,136 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:10:37,136 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'di', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ic@@', 'o@@', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'c@@', 'atti@@', 'vo', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:10:37,136 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:10:37,136 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:10:37,136 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cuore di ghiaccio arktico, il cuore del nostro sistema cattivo globale.
2025-05-30 00:10:37,136 - INFO - joeynmt.training - Example #3
2025-05-30 00:10:37,136 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:10:37,136 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:10:37,136 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'cominci@@', 'a', 'a', 'a', 'in@@', 'ver@@', 'no', 'e', 's@@', 'con@@', 'vol@@', 'to', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:10:37,137 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:10:37,137 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:10:37,137 - INFO - joeynmt.training - 	Hypothesis: Si comincia a a inverno e sconvolto in estate.
2025-05-30 00:10:37,137 - INFO - joeynmt.training - Example #4
2025-05-30 00:10:37,137 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:10:37,137 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:10:37,137 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:10:37,138 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:10:37,138 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:10:37,138 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è un disegno di quello che è successo negli ultimi 25 anni.
2025-05-30 00:10:40,620 - INFO - joeynmt.training - Epoch   7, Step:    59100, Batch Loss:     1.785087, Batch Acc: 0.481115, Tokens per Sec:    17827, Lr: 0.000300
2025-05-30 00:10:44,085 - INFO - joeynmt.training - Epoch   7, Step:    59200, Batch Loss:     1.882345, Batch Acc: 0.478961, Tokens per Sec:    19580, Lr: 0.000300
2025-05-30 00:10:47,543 - INFO - joeynmt.training - Epoch   7, Step:    59300, Batch Loss:     1.672449, Batch Acc: 0.477038, Tokens per Sec:    19796, Lr: 0.000300
2025-05-30 00:10:51,024 - INFO - joeynmt.training - Epoch   7, Step:    59400, Batch Loss:     1.879487, Batch Acc: 0.475248, Tokens per Sec:    20541, Lr: 0.000300
2025-05-30 00:10:54,496 - INFO - joeynmt.training - Epoch   7, Step:    59500, Batch Loss:     1.667492, Batch Acc: 0.482216, Tokens per Sec:    20218, Lr: 0.000300
2025-05-30 00:10:54,497 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:10:54,497 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:11:02,804 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.82, ppl:   6.15, acc:   0.50, generation: 8.2966[sec], evaluation: 0.0000[sec]
2025-05-30 00:11:03,261 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/58000.ckpt
2025-05-30 00:11:03,289 - INFO - joeynmt.training - Example #0
2025-05-30 00:11:03,290 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:11:03,290 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:11:03,290 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'con@@', 'net@@', 'ter@@', 'vi', 'che', 'la', 'c@@', 'ris@@', 'i', 'ar@@', 'c@@', 'is@@', 'c@@', 'ano', 'che', 'ha', 'chiamato', '<unk>', 'E@@', 'is@@', 'k@@', ',', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'il', '4@@', '8@@', ',', 'il', '4@@', '8@@', ',', 'il', '4@@', '8@@', '0@@', '%', 'di', 't@@', 'ale', 'è', 'il', '4@@', '8@@', '0@@', '%', 'di', 't@@', 'ale', 'in', 'questo', 'mod@@', 'o@@', '.', '</s>']
2025-05-30 00:11:03,290 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:11:03,290 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:11:03,290 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per connettervi che la crisi arciscano che ha chiamato <unk> Eisk, che ha fatto per tre milioni di anni per il 48, il 48, il 480% di tale è il 480% di tale in questo modo.
2025-05-30 00:11:03,291 - INFO - joeynmt.training - Example #1
2025-05-30 00:11:03,291 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:11:03,291 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:11:03,291 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'la', 'più', 'grande', 'di', 'questo', 'problema', 'speci@@', 'ale', 'che', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'amento', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:11:03,291 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:11:03,291 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:11:03,291 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, la più grande di questo problema speciale che non è il dell<unk> Eislamento del ghiaccio.
2025-05-30 00:11:03,292 - INFO - joeynmt.training - Example #2
2025-05-30 00:11:03,292 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:11:03,292 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:11:03,292 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'nostro', 'cuore', 'è', 'il', 'c@@', 'att@@', 'ico', 'ar@@', 'kt@@', 'ic@@', 'o@@', '.', '</s>']
2025-05-30 00:11:03,292 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:11:03,292 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:11:03,292 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il nostro cuore è il cattico arktico.
2025-05-30 00:11:03,293 - INFO - joeynmt.training - Example #3
2025-05-30 00:11:03,293 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:11:03,293 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:11:03,293 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 's@@', 'v@@', 'egli@@', 'ano', 'in', 'in@@', 'ver@@', 'no', 'e', 's@@', 'con@@', 'cer@@', 't@@', 'amente', 's@@', 'con@@', 'cer@@', 't@@', 'amente', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:11:03,293 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:11:03,293 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:11:03,293 - INFO - joeynmt.training - 	Hypothesis: Si svegliano in inverno e sconcertamente sconcertamente in estate.
2025-05-30 00:11:03,293 - INFO - joeynmt.training - Example #4
2025-05-30 00:11:03,293 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:11:03,294 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:11:03,294 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'periodo', 'di', 'di@@', 'seg@@', 'no', 'di', 'ciò', 'che', 'è', 'acca@@', 'duto', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:11:03,294 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:11:03,294 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:11:03,294 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è un periodo di disegno di ciò che è accaduto negli ultimi 25 anni.
2025-05-30 00:11:06,788 - INFO - joeynmt.training - Epoch   7, Step:    59600, Batch Loss:     1.942379, Batch Acc: 0.476303, Tokens per Sec:    17459, Lr: 0.000300
2025-05-30 00:11:10,256 - INFO - joeynmt.training - Epoch   7, Step:    59700, Batch Loss:     1.747102, Batch Acc: 0.481094, Tokens per Sec:    19907, Lr: 0.000300
2025-05-30 00:11:13,726 - INFO - joeynmt.training - Epoch   7, Step:    59800, Batch Loss:     2.104604, Batch Acc: 0.482328, Tokens per Sec:    20206, Lr: 0.000300
2025-05-30 00:11:17,157 - INFO - joeynmt.training - Epoch   7, Step:    59900, Batch Loss:     1.765682, Batch Acc: 0.476642, Tokens per Sec:    19620, Lr: 0.000300
2025-05-30 00:11:20,581 - INFO - joeynmt.training - Epoch   7, Step:    60000, Batch Loss:     2.108275, Batch Acc: 0.473392, Tokens per Sec:    19179, Lr: 0.000300
2025-05-30 00:11:20,581 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:11:20,581 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:11:28,288 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.82, ppl:   6.17, acc:   0.49, generation: 7.7003[sec], evaluation: 0.0000[sec]
2025-05-30 00:11:28,301 - INFO - joeynmt.training - Example #0
2025-05-30 00:11:28,301 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:11:28,301 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:11:28,301 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've@@', ',', 'per', 'vedere', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'c@@', 'lin@@', 'ic@@', 'o@@', ',', 'che', 'il', 'ghi@@', 'acci@@', 'aio', 'di', 'anni', 'di', 'dimen@@', 'sioni', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'dimen@@', 'sione', 'più', 'grande', '4@@', '8@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'il', '40', 'per', 'cento', 'per', 'la', 'dimen@@', 'sione', 'è', 'ri@@', 'ma@@', 'sta', 'per', 'ri@@', 'vel@@', 'are', 'il', '40', 'per', 'cento', 'è', 'ri@@', 'ma@@', 'sta', 'per', 'la', 'dimen@@', 'sione', 'del', '4@@', '8@@', '0@@', '%', 'è', 'stata', 'sc@@', 'att@@', 'ata', 'da', 'un', 'po@@', '<unk>', 'di', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've@@']
2025-05-30 00:11:28,301 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:11:28,301 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:11:28,301 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive, per vedere che il ghiaccio arclinico, che il ghiacciaio di anni di dimensioni che ha fatto per tre milioni di anni per la dimensione più grande 48, per tre milioni di anni per il 40 per cento per la dimensione è rimasta per rivelare il 40 per cento è rimasta per la dimensione del 480% è stata scattata da un po<unk> di queste due diapositive
2025-05-30 00:11:28,301 - INFO - joeynmt.training - Example #1
2025-05-30 00:11:28,302 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:11:28,302 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:11:28,302 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'la', 'ter@@', 'ri@@', 'bile', 'di', 'questo', 'problema', 'speci@@', 'ale', 'che', 'non', 'lo', 'fa', 'il', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:11:28,302 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:11:28,302 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:11:28,302 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte, la terribile di questo problema speciale che non lo fa il ghiaccio.
2025-05-30 00:11:28,302 - INFO - joeynmt.training - Example #2
2025-05-30 00:11:28,302 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:11:28,302 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:11:28,302 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'ghi@@', 'accio', 'ar@@', 'co', 'di', 'ghi@@', 'accio', 'ar@@', 't@@', 'ic@@', 'ale', 'del', 'nostro', 'sistema', 'c@@', 'lim@@', 'ati@@', 'vo@@', '.', '</s>']
2025-05-30 00:11:28,302 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:11:28,302 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:11:28,302 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il ghiaccio arco di ghiaccio articale del nostro sistema climativo.
2025-05-30 00:11:28,302 - INFO - joeynmt.training - Example #3
2025-05-30 00:11:28,302 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:11:28,303 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:11:28,303 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'si', 'ri@@', 'du@@', 'ce', 'al', 'in@@', 'ver@@', 'no@@', ',', 'e', 's@@', 'con@@', 'vol@@', 'to', 'in', 'S@@', 'om@@', '.', '</s>']
2025-05-30 00:11:28,303 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:11:28,303 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:11:28,303 - INFO - joeynmt.training - 	Hypothesis: E si riduce al inverno, e sconvolto in Som.
2025-05-30 00:11:28,303 - INFO - joeynmt.training - Example #4
2025-05-30 00:11:28,303 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:11:28,303 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:11:28,303 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'video', 'di', 'un', 'periodo', 'di', 'tem@@', 'po@@', '.', '</s>']
2025-05-30 00:11:28,303 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:11:28,303 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:11:28,303 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è un video di un periodo di tempo.
2025-05-30 00:11:31,758 - INFO - joeynmt.training - Epoch   7, Step:    60100, Batch Loss:     2.004888, Batch Acc: 0.477338, Tokens per Sec:    20146, Lr: 0.000300
2025-05-30 00:11:35,230 - INFO - joeynmt.training - Epoch   7, Step:    60200, Batch Loss:     1.736745, Batch Acc: 0.484583, Tokens per Sec:    19486, Lr: 0.000300
2025-05-30 00:11:38,711 - INFO - joeynmt.training - Epoch   7, Step:    60300, Batch Loss:     1.633287, Batch Acc: 0.482720, Tokens per Sec:    19372, Lr: 0.000300
2025-05-30 00:11:42,188 - INFO - joeynmt.training - Epoch   7, Step:    60400, Batch Loss:     1.877164, Batch Acc: 0.475251, Tokens per Sec:    19465, Lr: 0.000300
2025-05-30 00:11:45,684 - INFO - joeynmt.training - Epoch   7, Step:    60500, Batch Loss:     1.951056, Batch Acc: 0.477733, Tokens per Sec:    20019, Lr: 0.000300
2025-05-30 00:11:45,684 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:11:45,684 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:11:53,342 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.82, ppl:   6.16, acc:   0.49, generation: 7.6483[sec], evaluation: 0.0000[sec]
2025-05-30 00:11:53,347 - INFO - joeynmt.training - Example #0
2025-05-30 00:11:53,347 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:11:53,347 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:11:53,347 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'la', 'c@@', 'app@@', 'a', 'a', 'per@@', 'dere', 'l@@', '<unk>', 'ar@@', 'c@@', 'ic@@', 'lo', 'di', 'tre', 'milioni', 'di', 'anni', 'di', 'dimen@@', 'sioni', 'che', 'chiam@@', 'ano', 'il', '4@@', '8@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'dimen@@', 'sioni', 'in', 'cui', 'è', 'il', '40', 'per', 'cento', 'di', 't@@', 'on@@', 'n@@', 'ell@@', 'ate', 'di', '4@@', '8', 'stati', 'per', 'cento', 'di', 'queste', 'dimen@@', 'sioni', 'è', 'il', '4@@', '8@@', '.', '</s>']
2025-05-30 00:11:53,347 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:11:53,347 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:11:53,347 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per osservare che la cappa a perdere l<unk> arciclo di tre milioni di anni di dimensioni che chiamano il 48, per tre milioni di anni di dimensioni in cui è il 40 per cento di tonnellate di 48 stati per cento di queste dimensioni è il 48.
2025-05-30 00:11:53,347 - INFO - joeynmt.training - Example #1
2025-05-30 00:11:53,348 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:11:53,348 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:11:53,348 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'più', 'forte', 'la', 'sost@@', 'eni@@', 'bilità', 'di', 'questo', 'problema', 'speci@@', 'ale', 'di', 'questo', 'problema', 'speci@@', 'ale', 'perché', 'non', 'è', 'il', 'mu@@', 'c@@', 'chio', 'di', 'ghi@@', 'accio', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'at@@', 'o@@', '.', '</s>']
2025-05-30 00:11:53,348 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:11:53,348 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:11:53,348 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto più forte la sostenibilità di questo problema speciale di questo problema speciale perché non è il mucchio di ghiaccio dell<unk> Eislato.
2025-05-30 00:11:53,348 - INFO - joeynmt.training - Example #2
2025-05-30 00:11:53,348 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:11:53,348 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:11:53,348 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'att@@', 'ura', 'di', 'ghi@@', 'accio', 'è', 'il', 'nostro', 'sistema', 'c@@', 'atti@@', 'vo', 'globale', 'di', 'c@@', 'opp@@', 'ie', 'globale', '</s>']
2025-05-30 00:11:53,348 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:11:53,348 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:11:53,348 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cattura di ghiaccio è il nostro sistema cattivo globale di coppie globale
2025-05-30 00:11:53,348 - INFO - joeynmt.training - Example #3
2025-05-30 00:11:53,348 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:11:53,349 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:11:53,349 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'si', 'sta', 'cresc@@', 'endo', 'in', 'in@@', 'ver@@', 'no', 'e', 's@@', 'com@@', 'par@@', 'sa', 'in', 'est@@', 'at@@', 'a@@', '.', '</s>']
2025-05-30 00:11:53,349 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:11:53,349 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:11:53,349 - INFO - joeynmt.training - 	Hypothesis: E si sta crescendo in inverno e scomparsa in estata.
2025-05-30 00:11:53,349 - INFO - joeynmt.training - Example #4
2025-05-30 00:11:53,349 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:11:53,349 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:11:53,349 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'cosa', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:11:53,349 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:11:53,349 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:11:53,349 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una cosa che è successo negli ultimi 25 anni.
2025-05-30 00:11:56,838 - INFO - joeynmt.training - Epoch   7, Step:    60600, Batch Loss:     1.688932, Batch Acc: 0.472300, Tokens per Sec:    19558, Lr: 0.000300
2025-05-30 00:12:00,311 - INFO - joeynmt.training - Epoch   7, Step:    60700, Batch Loss:     2.025716, Batch Acc: 0.472785, Tokens per Sec:    19666, Lr: 0.000300
2025-05-30 00:12:03,789 - INFO - joeynmt.training - Epoch   7, Step:    60800, Batch Loss:     2.067139, Batch Acc: 0.472928, Tokens per Sec:    19679, Lr: 0.000300
2025-05-30 00:12:07,284 - INFO - joeynmt.training - Epoch   7, Step:    60900, Batch Loss:     1.700870, Batch Acc: 0.481655, Tokens per Sec:    20275, Lr: 0.000300
2025-05-30 00:12:10,756 - INFO - joeynmt.training - Epoch   7, Step:    61000, Batch Loss:     1.674709, Batch Acc: 0.480855, Tokens per Sec:    19845, Lr: 0.000300
2025-05-30 00:12:10,756 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:12:10,757 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:12:17,138 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.81, ppl:   6.14, acc:   0.50, generation: 6.3713[sec], evaluation: 0.0000[sec]
2025-05-30 00:12:17,769 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/56000.ckpt
2025-05-30 00:12:17,798 - INFO - joeynmt.training - Example #0
2025-05-30 00:12:17,799 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:12:17,799 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:12:17,799 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'per@@', 'dere', 'che', 'l@@', '<unk>', 'E@@', 'is@@', 'k@@', 'ap@@', ',', 'che', 'l@@', '<unk>', 'E@@', 'is@@', 'k@@', 'ap@@', ',', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'ci@@', 'dent@@', 'e@@', '.', '</s>']
2025-05-30 00:12:17,799 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:12:17,799 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:12:17,799 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per perdere che l<unk> Eiskap, che l<unk> Eiskap, che ha fatto per tre milioni di anni la dimensione dell<unk> Occidente.
2025-05-30 00:12:17,799 - INFO - joeynmt.training - Example #1
2025-05-30 00:12:17,800 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:12:17,800 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:12:17,800 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'la', 'nostra', 'st@@', 'azione', 'di', 'questo', 'problema', 'speci@@', 'ale', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'accio', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:12:17,800 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:12:17,800 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:12:17,800 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte, la nostra stazione di questo problema speciale che non è il dibattito del ghiaccio dell<unk> Eises.
2025-05-30 00:12:17,800 - INFO - joeynmt.training - Example #2
2025-05-30 00:12:17,800 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:12:17,801 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:12:17,801 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'sistema', 'ghi@@', 'accio', 'che', 'è', 'il', 'sistema', 'di', 'ghi@@', 'accio', 'che', 'è', 'il', 'nostro', 'sistema', 'c@@', 'atti@@', 'vo', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:12:17,801 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:12:17,801 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:12:17,801 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il sistema ghiaccio che è il sistema di ghiaccio che è il nostro sistema cattivo globale.
2025-05-30 00:12:17,801 - INFO - joeynmt.training - Example #3
2025-05-30 00:12:17,801 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:12:17,801 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:12:17,801 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Com@@', 'inci@@', 'a', 'a', 'a', 'in@@', 'ver@@', 'no', 'e', 'dol@@', 'c@@', 'e@@', ',', 'e', 'dol@@', 'c@@', 'e@@', '.', '</s>']
2025-05-30 00:12:17,802 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:12:17,802 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:12:17,802 - INFO - joeynmt.training - 	Hypothesis: Comincia a a inverno e dolce, e dolce.
2025-05-30 00:12:17,802 - INFO - joeynmt.training - Example #4
2025-05-30 00:12:17,802 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:12:17,802 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:12:17,802 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'una', 'cosa', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:12:17,803 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:12:17,803 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:12:17,803 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è una cosa che è successo negli ultimi 25 anni.
2025-05-30 00:12:21,266 - INFO - joeynmt.training - Epoch   7, Step:    61100, Batch Loss:     2.090694, Batch Acc: 0.472876, Tokens per Sec:    16465, Lr: 0.000300
2025-05-30 00:12:24,732 - INFO - joeynmt.training - Epoch   7, Step:    61200, Batch Loss:     1.822774, Batch Acc: 0.483646, Tokens per Sec:    20122, Lr: 0.000300
2025-05-30 00:12:28,215 - INFO - joeynmt.training - Epoch   7, Step:    61300, Batch Loss:     1.958249, Batch Acc: 0.474984, Tokens per Sec:    20041, Lr: 0.000300
2025-05-30 00:12:31,669 - INFO - joeynmt.training - Epoch   7, Step:    61400, Batch Loss:     1.874857, Batch Acc: 0.475302, Tokens per Sec:    19487, Lr: 0.000300
2025-05-30 00:12:35,141 - INFO - joeynmt.training - Epoch   7, Step:    61500, Batch Loss:     1.721022, Batch Acc: 0.476158, Tokens per Sec:    19810, Lr: 0.000300
2025-05-30 00:12:35,141 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:12:35,142 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:12:42,410 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.81, ppl:   6.11, acc:   0.50, generation: 7.2585[sec], evaluation: 0.0000[sec]
2025-05-30 00:12:42,410 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:12:43,070 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/59000.ckpt
2025-05-30 00:12:43,098 - INFO - joeynmt.training - Example #0
2025-05-30 00:12:43,099 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:12:43,099 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:12:43,099 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'per@@', 'dere', 'che', 'gli', 'anni', 'di', 'ghi@@', 'accio', 'che', 'l@@', '<unk>', 'ar@@', 'ch@@', 'e@@', ',', 'che', 'ha', 'chiam@@', 'ato', 'il', 'ghi@@', 'acci@@', 'aio', 'di', 'ghi@@', 'accio', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', '4@@', '8', 'stati', 'per', 'cento', 'per', 'la', 'maggior', 'parte', 'dei', '4@@', '8', 'stati', 'per', 'cento', 'di', 'questi', 'due', 's@@', 'ett@@', 'e@@', '.', '</s>']
2025-05-30 00:12:43,100 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:12:43,100 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:12:43,100 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per perdere che gli anni di ghiaccio che l<unk> arche, che ha chiamato il ghiacciaio di ghiaccio che ha fatto per tre milioni di anni per la più grande 48 stati per cento per la maggior parte dei 48 stati per cento di questi due sette.
2025-05-30 00:12:43,100 - INFO - joeynmt.training - Example #1
2025-05-30 00:12:43,100 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:12:43,100 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:12:43,100 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'la', 'sost@@', 'eni@@', 'bilità', 'di', 'questo', 'problema', 'speci@@', 'ale', 'che', 'non', 'è', 'il', 't@@', 'asso', 'di', 'ghi@@', 'accio', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'at@@', 'o@@', '.', '</s>']
2025-05-30 00:12:43,101 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:12:43,101 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:12:43,101 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte, la sostenibilità di questo problema speciale che non è il tasso di ghiaccio dell<unk> Eislato.
2025-05-30 00:12:43,101 - INFO - joeynmt.training - Example #2
2025-05-30 00:12:43,101 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:12:43,101 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:12:43,101 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 't@@', 'asso', 'di', 'ghi@@', 'accio', 'è', 'il', 'cuore', 'più', 'pover@@', 'o@@', ',', 'il', 'cuore', 'più', 'br@@', 'ut@@', 'ta', 'del', 'nostro', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:12:43,101 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:12:43,101 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:12:43,101 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il tasso di ghiaccio è il cuore più povero, il cuore più brutta del nostro sistema climatico globale.
2025-05-30 00:12:43,101 - INFO - joeynmt.training - Example #3
2025-05-30 00:12:43,102 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:12:43,102 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:12:43,102 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 's@@', 'ente', 'nel', 'in@@', 'ver@@', 'no', 'e', 'dol@@', 'c@@', 'e@@', ',', 'e', 'poi', 'si', 's@@', 'vol@@', 'ge', 'nel', 's@@', 'ettore', 'd@@', 'ell@@', '<unk>', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:12:43,102 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:12:43,102 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:12:43,102 - INFO - joeynmt.training - 	Hypothesis: Si sente nel inverno e dolce, e poi si svolge nel settore dell<unk> estate.
2025-05-30 00:12:43,102 - INFO - joeynmt.training - Example #4
2025-05-30 00:12:43,102 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:12:43,103 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:12:43,103 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'una', 'foto', 'di', 'un', 'periodo', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:12:43,103 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:12:43,103 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:12:43,103 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è una foto di un periodo di quello che è successo negli ultimi 25 anni.
2025-05-30 00:12:46,585 - INFO - joeynmt.training - Epoch   7, Step:    61600, Batch Loss:     1.798559, Batch Acc: 0.485904, Tokens per Sec:    15787, Lr: 0.000300
2025-05-30 00:12:50,054 - INFO - joeynmt.training - Epoch   7, Step:    61700, Batch Loss:     1.896105, Batch Acc: 0.475638, Tokens per Sec:    19451, Lr: 0.000300
2025-05-30 00:12:53,512 - INFO - joeynmt.training - Epoch   7, Step:    61800, Batch Loss:     1.673251, Batch Acc: 0.473077, Tokens per Sec:    19497, Lr: 0.000300
2025-05-30 00:12:56,973 - INFO - joeynmt.training - Epoch   7, Step:    61900, Batch Loss:     1.639378, Batch Acc: 0.477479, Tokens per Sec:    19771, Lr: 0.000300
2025-05-30 00:13:00,431 - INFO - joeynmt.training - Epoch   7, Step:    62000, Batch Loss:     1.849464, Batch Acc: 0.472974, Tokens per Sec:    19750, Lr: 0.000300
2025-05-30 00:13:00,431 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:13:00,431 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:13:07,877 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.81, ppl:   6.10, acc:   0.50, generation: 7.4359[sec], evaluation: 0.0000[sec]
2025-05-30 00:13:07,878 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:13:08,527 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/58500.ckpt
2025-05-30 00:13:08,556 - INFO - joeynmt.training - Example #0
2025-05-30 00:13:08,556 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:13:08,556 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:13:08,556 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'a@@', 'in@@', ',', 'che', 'l@@', '<unk>', 'ar@@', 'c@@', 'op@@', 'pia', 'di', 'ghi@@', 'accio', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'la', 'dimen@@', 'sione', 'della', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', '4@@', '8', 'stati', 'per', 'cent@@', 'o@@', '.', '</s>']
2025-05-30 00:13:08,557 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:13:08,557 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:13:08,557 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per osservare che il ghiaccio arktain, che l<unk> arcoppia di ghiaccio che ha fatto per tre milioni di anni la dimensione della dimensione dell<unk> 48 stati per cento.
2025-05-30 00:13:08,557 - INFO - joeynmt.training - Example #1
2025-05-30 00:13:08,557 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:13:08,557 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:13:08,557 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'la', 'cosa', 'più', 'importante', 'è', 'che', 'la', 'ter@@', 'ri@@', 'bile', 'è', 'che', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'a@@', '.', '</s>']
2025-05-30 00:13:08,558 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:13:08,558 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:13:08,558 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, la cosa più importante è che la terribile è che non è il dell<unk> Eisla.
2025-05-30 00:13:08,558 - INFO - joeynmt.training - Example #2
2025-05-30 00:13:08,558 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:13:08,558 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:13:08,558 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'c@@', 'atti@@', 'vo', 'd@@', '<unk>', 'et@@', 'i@@', 'ch@@', 'etta', 'il', 'nostro', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:13:08,558 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:13:08,559 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:13:08,559 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cattivo d<unk> etichetta il nostro sistema climatico globale.
2025-05-30 00:13:08,559 - INFO - joeynmt.training - Example #3
2025-05-30 00:13:08,559 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:13:08,559 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:13:08,559 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'nel', 'vent@@', 'o@@', ',', 'e', 'si', 'sci@@', 'vol@@', 'ano', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:13:08,559 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:13:08,559 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:13:08,559 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo nel vento, e si scivolano in estate.
2025-05-30 00:13:08,560 - INFO - joeynmt.training - Example #4
2025-05-30 00:13:08,560 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:13:08,560 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:13:08,560 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'una', 'foto', 'di', 'un', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:13:08,560 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:13:08,560 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:13:08,560 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è una foto di un disegno di quello che è successo negli ultimi 25 anni.
2025-05-30 00:13:12,039 - INFO - joeynmt.training - Epoch   7, Step:    62100, Batch Loss:     2.144821, Batch Acc: 0.475476, Tokens per Sec:    15960, Lr: 0.000300
2025-05-30 00:13:15,485 - INFO - joeynmt.training - Epoch   7, Step:    62200, Batch Loss:     1.824678, Batch Acc: 0.477086, Tokens per Sec:    19206, Lr: 0.000300
2025-05-30 00:13:18,945 - INFO - joeynmt.training - Epoch   7, Step:    62300, Batch Loss:     1.795576, Batch Acc: 0.485338, Tokens per Sec:    19707, Lr: 0.000300
2025-05-30 00:13:22,394 - INFO - joeynmt.training - Epoch   7, Step:    62400, Batch Loss:     1.809719, Batch Acc: 0.479989, Tokens per Sec:    19207, Lr: 0.000300
2025-05-30 00:13:25,801 - INFO - joeynmt.training - Epoch   7, Step:    62500, Batch Loss:     2.187407, Batch Acc: 0.471623, Tokens per Sec:    19387, Lr: 0.000300
2025-05-30 00:13:25,801 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:13:25,802 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:13:32,855 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.81, ppl:   6.08, acc:   0.50, generation: 7.0437[sec], evaluation: 0.0000[sec]
2025-05-30 00:13:32,856 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:13:33,502 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/59500.ckpt
2025-05-30 00:13:33,530 - INFO - joeynmt.training - Example #0
2025-05-30 00:13:33,531 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:13:33,531 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:13:33,531 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'per@@', 'dere', 'che', 'la', 'maggior', 'parte', 'delle', 'persone', 'che', 'chiam@@', 'ano', '<unk>', 'ghi@@', 'accio', 'per', 'tre', 'milioni', 'di', 'anni', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'e@@', '<unk>', ',', 'il', '4@@', '8@@', '0@@', '%', 'del', '4@@', '8@@', '0@@', '%', 'del', 'livello', 'di', 's@@', 'ess@@', 'o@@', '.', '</s>']
2025-05-30 00:13:33,531 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:13:33,532 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:13:33,532 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per perdere che la maggior parte delle persone che chiamano <unk> ghiaccio per tre milioni di anni la dimensione dell<unk> Oce<unk> , il 480% del 480% del livello di sesso.
2025-05-30 00:13:33,532 - INFO - joeynmt.training - Example #1
2025-05-30 00:13:33,532 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:13:33,532 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:13:33,532 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'non', 'è', 'abbastanza', 'la', 'più', 'grande', 'che', 'non', 'è', 'il', 'di@@', 'seg@@', 'no@@', '.', '</s>']
2025-05-30 00:13:33,532 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:13:33,532 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:13:33,532 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, non è abbastanza la più grande che non è il disegno.
2025-05-30 00:13:33,533 - INFO - joeynmt.training - Example #2
2025-05-30 00:13:33,533 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:13:33,533 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:13:33,533 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'vero', 'vero', 'vero', 'vero', 'il', 'cuore', 'c@@', 'att@@', 'ico', 'del', 'nostro', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:13:33,533 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:13:33,533 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:13:33,533 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il vero vero vero vero il cuore cattico del nostro sistema climatico globale.
2025-05-30 00:13:33,533 - INFO - joeynmt.training - Example #3
2025-05-30 00:13:33,534 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:13:33,534 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:13:33,534 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'si', 'sta', 'av@@', 'ven@@', 'endo', 'il', 'suo', 'in@@', 'ver@@', 'no@@', ',', 'e', 'dol@@', 'ore', 'al', 's@@', 'om@@', 'om@@', 'o@@', '.', '</s>']
2025-05-30 00:13:33,534 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:13:33,534 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:13:33,534 - INFO - joeynmt.training - 	Hypothesis: E si sta avvenendo il suo inverno, e dolore al somomo.
2025-05-30 00:13:33,534 - INFO - joeynmt.training - Example #4
2025-05-30 00:13:33,535 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:13:33,535 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:13:33,535 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'una', 'foto', 'di', 'un', 'di@@', 'seg@@', 'no', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:13:33,535 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:13:33,535 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:13:33,535 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è una foto di un disegno che è successo negli ultimi 25 anni.
2025-05-30 00:13:37,048 - INFO - joeynmt.training - Epoch   7, Step:    62600, Batch Loss:     1.757269, Batch Acc: 0.478254, Tokens per Sec:    16366, Lr: 0.000300
2025-05-30 00:13:40,525 - INFO - joeynmt.training - Epoch   7, Step:    62700, Batch Loss:     1.917313, Batch Acc: 0.480788, Tokens per Sec:    19642, Lr: 0.000300
2025-05-30 00:13:43,996 - INFO - joeynmt.training - Epoch   7, Step:    62800, Batch Loss:     1.743700, Batch Acc: 0.477750, Tokens per Sec:    19336, Lr: 0.000300
2025-05-30 00:13:47,491 - INFO - joeynmt.training - Epoch   7, Step:    62900, Batch Loss:     1.823178, Batch Acc: 0.479815, Tokens per Sec:    20286, Lr: 0.000300
2025-05-30 00:13:50,965 - INFO - joeynmt.training - Epoch   7, Step:    63000, Batch Loss:     1.848179, Batch Acc: 0.474492, Tokens per Sec:    19470, Lr: 0.000300
2025-05-30 00:13:50,965 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:13:50,965 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:13:58,426 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.81, ppl:   6.08, acc:   0.50, generation: 7.4505[sec], evaluation: 0.0000[sec]
2025-05-30 00:13:58,889 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/61000.ckpt
2025-05-30 00:13:58,917 - INFO - joeynmt.training - Example #0
2025-05-30 00:13:58,918 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:13:58,918 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:13:58,918 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'per@@', 'dere', 'che', 'l@@', '<unk>', 'ar@@', 't@@', 'ic@@', 'a@@', ',', 'che', 'l@@', '<unk>', 'ar@@', 't@@', 'ic@@', 'o@@', ',', 'che', 'ha', 'ri@@', 'vel@@', 'ato', 'per', 'tre', 'milioni', 'di', 'anni', 'che', 'ha', 'ri@@', 'vel@@', 'ato', 'il', '4@@', '8', 'per', 'cento', 'dei', 'li@@', 'velli', 'di', '4@@', '8', 'stati', 'per', 'cento', 'per', 'il', '4@@', '8', 'per', 'cento', 'dei', 'li@@', 'velli', 'di', '4@@', '8', 'per', 'cento', 'di', 'questi', 'due', 'di@@', 'apos@@', 'iti@@', 've@@', '.', '</s>']
2025-05-30 00:13:58,918 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:13:58,918 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:13:58,919 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per perdere che l<unk> artica, che l<unk> artico, che ha rivelato per tre milioni di anni che ha rivelato il 48 per cento dei livelli di 48 stati per cento per il 48 per cento dei livelli di 48 per cento di questi due diapositive.
2025-05-30 00:13:58,919 - INFO - joeynmt.training - Example #1
2025-05-30 00:13:58,919 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:13:58,919 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:13:58,919 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'la', 'più', 'grande', 'quantità', 'di', 'questo', 'problema', 'speci@@', 'ale', 'di', 'questo', 'problema', 'speci@@', 'fic@@', 'o@@', ',', 'perché', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:13:58,919 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:13:58,920 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:13:58,920 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto la più grande quantità di questo problema speciale di questo problema specifico, perché non è il dell<unk> Eises.
2025-05-30 00:13:58,920 - INFO - joeynmt.training - Example #2
2025-05-30 00:13:58,920 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:13:58,920 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:13:58,920 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'c@@', 'att@@', 'ico', 'ar@@', 't@@', 'ico', 'del', 'nostro', 'sistema', 'K@@', 'lim@@', 'as@@', 'k@@', 'app@@', 'e@@', '.', '</s>']
2025-05-30 00:13:58,920 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:13:58,920 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:13:58,921 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cattico artico del nostro sistema Klimaskappe.
2025-05-30 00:13:58,921 - INFO - joeynmt.training - Example #3
2025-05-30 00:13:58,921 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:13:58,921 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:13:58,921 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'inizi@@', 'a', 'a', 'a', 'fu@@', 'i@@', 're@@', ',', 'e', 'l@@', '<unk>', 'in@@', 'ver@@', 'no@@', '.', '</s>']
2025-05-30 00:13:58,921 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:13:58,921 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:13:58,921 - INFO - joeynmt.training - 	Hypothesis: E inizia a a fuire, e l<unk> inverno.
2025-05-30 00:13:58,921 - INFO - joeynmt.training - Example #4
2025-05-30 00:13:58,922 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:13:58,922 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:13:58,922 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'una', 'foto', 'di', 'un', 'di@@', 'seg@@', 'no', 'di', 'di@@', 'seg@@', 'no', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:13:58,922 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:13:58,922 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:13:58,922 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è una foto di un disegno di disegno che è successo negli ultimi 25 anni.
2025-05-30 00:14:02,425 - INFO - joeynmt.training - Epoch   7, Step:    63100, Batch Loss:     1.723788, Batch Acc: 0.470122, Tokens per Sec:    17180, Lr: 0.000300
2025-05-30 00:14:05,917 - INFO - joeynmt.training - Epoch   7, Step:    63200, Batch Loss:     2.016268, Batch Acc: 0.479242, Tokens per Sec:    19529, Lr: 0.000300
2025-05-30 00:14:09,405 - INFO - joeynmt.training - Epoch   7, Step:    63300, Batch Loss:     2.009012, Batch Acc: 0.475529, Tokens per Sec:    19984, Lr: 0.000300
2025-05-30 00:14:12,889 - INFO - joeynmt.training - Epoch   7, Step:    63400, Batch Loss:     1.984375, Batch Acc: 0.479570, Tokens per Sec:    20138, Lr: 0.000300
2025-05-30 00:14:16,368 - INFO - joeynmt.training - Epoch   7, Step:    63500, Batch Loss:     1.988008, Batch Acc: 0.471011, Tokens per Sec:    19490, Lr: 0.000300
2025-05-30 00:14:16,368 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:14:16,368 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:14:23,104 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.82, ppl:   6.15, acc:   0.49, generation: 6.7253[sec], evaluation: 0.0000[sec]
2025-05-30 00:14:23,108 - INFO - joeynmt.training - Example #0
2025-05-30 00:14:23,108 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:14:23,109 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:14:23,109 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'per@@', 'dere', 'che', 'l@@', '<unk>', 'ar@@', 'ric@@', 'ch@@', 'etta', 'ar@@', 'ric@@', 'ch@@', 'e@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'in@@', 'fer@@', 'i@@', 'ore', 'di', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'maggior', 'parte', 'dei', 'con@@', 'fin@@', 'i', 'delle', 'più', 'grandi', 'stati', 'di', '4@@', '8', 'anni', 'per', 'la', 'maggior', 'parte', 'dei', 'con@@', 'fin@@', 'i', 'di', 'questi', 'due', 'di@@', 'apos@@', 'iti@@', '.', '</s>']
2025-05-30 00:14:23,109 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:14:23,109 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:14:23,109 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per perdere che l<unk> arricchetta arricche, per tre milioni di anni la dimensione dell<unk> inferiore di tre milioni di anni per la maggior parte dei confini delle più grandi stati di 48 anni per la maggior parte dei confini di questi due diapositi.
2025-05-30 00:14:23,109 - INFO - joeynmt.training - Example #1
2025-05-30 00:14:23,109 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:14:23,109 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:14:23,109 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'più', 'for@@', 'ti', 'di', 'quanto', 'la', 'più', 'grande', 'è', 'la', 'più', 'grande', 'cosa', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'accio', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:14:23,109 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:14:23,109 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:14:23,109 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto più forti di quanto la più grande è la più grande cosa che non è il dibattito del ghiaccio dell<unk> Eises.
2025-05-30 00:14:23,109 - INFO - joeynmt.training - Example #2
2025-05-30 00:14:23,109 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:14:23,110 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:14:23,110 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', 'è', 'l@@', '<unk>', 'er@@', 'ba', 'ar@@', 't@@', 'ic@@', 'o@@', ',', 'il', 'cuore', 'di', 'un', 'sistema', 'di', 'c@@', 'au@@', 'se', 'del', 'nostro', 'sistema', 'cli@@', 'ma', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:14:23,110 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:14:23,110 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:14:23,110 - INFO - joeynmt.training - 	Hypothesis: In un certo senso è l<unk> erba artico, il cuore di un sistema di cause del nostro sistema clima globale.
2025-05-30 00:14:23,110 - INFO - joeynmt.training - Example #3
2025-05-30 00:14:23,110 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:14:23,110 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:14:23,110 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'cominci@@', 'a', 'a', 'a', 'parlare', 'di', 'in@@', 'ver@@', 'no@@', ',', 'e', 'poi', 'si', 's@@', 'om@@', 'b@@', 'ano', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:14:23,110 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:14:23,110 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:14:23,110 - INFO - joeynmt.training - 	Hypothesis: E comincia a a parlare di inverno, e poi si sombano in estate.
2025-05-30 00:14:23,110 - INFO - joeynmt.training - Example #4
2025-05-30 00:14:23,110 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:14:23,110 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:14:23,110 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:14:23,111 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:14:23,111 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:14:23,111 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è un disegno che è successo negli ultimi 25 anni.
2025-05-30 00:14:26,570 - INFO - joeynmt.training - Epoch   7, Step:    63600, Batch Loss:     1.839371, Batch Acc: 0.477990, Tokens per Sec:    19432, Lr: 0.000300
2025-05-30 00:14:29,989 - INFO - joeynmt.training - Epoch   7, Step:    63700, Batch Loss:     1.953843, Batch Acc: 0.476039, Tokens per Sec:    20535, Lr: 0.000300
2025-05-30 00:14:33,464 - INFO - joeynmt.training - Epoch   7, Step:    63800, Batch Loss:     1.527006, Batch Acc: 0.480677, Tokens per Sec:    19926, Lr: 0.000300
2025-05-30 00:14:36,936 - INFO - joeynmt.training - Epoch   7, Step:    63900, Batch Loss:     1.750812, Batch Acc: 0.479556, Tokens per Sec:    19584, Lr: 0.000300
2025-05-30 00:14:40,394 - INFO - joeynmt.training - Epoch   7, Step:    64000, Batch Loss:     1.769811, Batch Acc: 0.478783, Tokens per Sec:    19678, Lr: 0.000300
2025-05-30 00:14:40,394 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:14:40,394 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:14:46,478 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.80, ppl:   6.03, acc:   0.50, generation: 6.0773[sec], evaluation: 0.0000[sec]
2025-05-30 00:14:46,478 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:14:47,034 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/57000.ckpt
2025-05-30 00:14:47,056 - INFO - joeynmt.training - Example #0
2025-05-30 00:14:47,056 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:14:47,057 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:14:47,057 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'che', 'l@@', '<unk>', 'ar@@', 'co', 'ar@@', 'ric@@', 'e', 'che', 'l@@', '<unk>', 'ar@@', 'co', 'ar@@', 'ric@@', 'e', 'che', 'ha', 'ann@@', 'un@@', 'ci@@', 'ato', 'per', 'tre', 'milioni', 'di', 'anni', 'il', 'livello', 'di', 't@@', 'ant@@', 'o@@', '.', '</s>']
2025-05-30 00:14:47,057 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:14:47,057 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:14:47,057 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive che l<unk> arco arrice che l<unk> arco arrice che ha annunciato per tre milioni di anni il livello di tanto.
2025-05-30 00:14:47,057 - INFO - joeynmt.training - Example #1
2025-05-30 00:14:47,057 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:14:47,058 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:14:47,058 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'ti@@', ',', 'non', 'è', 'molto', 'più', 'for@@', 'ti@@', ',', 'ma', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:14:47,058 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:14:47,058 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:14:47,058 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forti, non è molto più forti, ma non è il dell<unk> Eises.
2025-05-30 00:14:47,058 - INFO - joeynmt.training - Example #2
2025-05-30 00:14:47,058 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:14:47,058 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:14:47,058 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'c@@', 'aff@@', 'è', 'di', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ic@@', 'o@@', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'c@@', 'lim@@', 'ati@@', 'do@@', '.', '</s>']
2025-05-30 00:14:47,059 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:14:47,059 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:14:47,059 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il caffè di ghiaccio arktico, il cuore del nostro sistema climatido.
2025-05-30 00:14:47,059 - INFO - joeynmt.training - Example #3
2025-05-30 00:14:47,059 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:14:47,059 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:14:47,059 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'in@@', 'ver@@', 'no@@', ',', 'e', 'il', 'p@@', 'om@@', 'p@@', 'a', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:14:47,060 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:14:47,060 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:14:47,060 - INFO - joeynmt.training - 	Hypothesis: L<unk> inverno, e il pompa in estate.
2025-05-30 00:14:47,060 - INFO - joeynmt.training - Example #4
2025-05-30 00:14:47,060 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:14:47,060 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:14:47,060 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'una', 'cosa', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:14:47,061 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:14:47,061 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:14:47,061 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è una cosa che è successo negli ultimi 25 anni.
2025-05-30 00:14:50,507 - INFO - joeynmt.training - Epoch   7, Step:    64100, Batch Loss:     1.879792, Batch Acc: 0.478871, Tokens per Sec:    16495, Lr: 0.000300
2025-05-30 00:14:53,987 - INFO - joeynmt.training - Epoch   7, Step:    64200, Batch Loss:     1.868691, Batch Acc: 0.475872, Tokens per Sec:    20007, Lr: 0.000300
2025-05-30 00:14:57,448 - INFO - joeynmt.training - Epoch   7, Step:    64300, Batch Loss:     1.990589, Batch Acc: 0.472914, Tokens per Sec:    19706, Lr: 0.000300
2025-05-30 00:15:00,911 - INFO - joeynmt.training - Epoch   7, Step:    64400, Batch Loss:     1.895805, Batch Acc: 0.477502, Tokens per Sec:    19858, Lr: 0.000300
2025-05-30 00:15:04,402 - INFO - joeynmt.training - Epoch   7, Step:    64500, Batch Loss:     1.815830, Batch Acc: 0.480918, Tokens per Sec:    19616, Lr: 0.000300
2025-05-30 00:15:04,402 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:15:04,402 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:15:11,000 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.81, ppl:   6.09, acc:   0.50, generation: 6.5880[sec], evaluation: 0.0000[sec]
2025-05-30 00:15:11,430 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/61500.ckpt
2025-05-30 00:15:11,459 - INFO - joeynmt.training - Example #0
2025-05-30 00:15:11,459 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:15:11,459 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:15:11,459 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've@@', ',', 'e', 'ho', 'fatto', 'un', 'po@@', '<unk>', 'di', 'ghi@@', 'accio', 'che', 'l@@', '<unk>', 'ar@@', 'cat@@', 'o@@', 'dio', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'k@@', 'app@@', ',', 'che', 'chiam@@', 'ano', 'per', 'tre', 'milioni', 'di', 'anni', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', '4@@', '8', 'per', 'cento', 'di', 'questi', 'gruppi', 'di', 'hanno', 'acc@@', 'esso', 'ai', '4@@', '8', 'per', 'cento', 'di', 'queste', 'persone', 'che', 'si', 'è', 'ri@@', 'vel@@', 'ata', 'in', 'questo', 'mod@@', 'o@@', '.', '</s>']
2025-05-30 00:15:11,460 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:15:11,460 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:15:11,460 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive, e ho fatto un po<unk> di ghiaccio che l<unk> arcatodio dell<unk> Eiskapp, che chiamano per tre milioni di anni la dimensione dell<unk> 48 per cento di questi gruppi di hanno accesso ai 48 per cento di queste persone che si è rivelata in questo modo.
2025-05-30 00:15:11,460 - INFO - joeynmt.training - Example #1
2025-05-30 00:15:11,460 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:15:11,460 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:15:11,460 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'perché', 'non', 'è', 'molto', 'più', 'forte', 'di', 'questo', 'problema', 'speci@@', 'al@@', 'e@@', ',', 'perché', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:15:11,461 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:15:11,461 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:15:11,461 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, perché non è molto più forte di questo problema speciale, perché non è il dell<unk> Eises.
2025-05-30 00:15:11,461 - INFO - joeynmt.training - Example #2
2025-05-30 00:15:11,461 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:15:11,461 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:15:11,461 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'd@@', 'ell@@', '<unk>', 'ar@@', 'ist@@', 'a@@', ',', 'il', 'cuore', 'c@@', 'ru@@', 'ci@@', 'ale', 'del', 'nostro', 'sistema', 'K@@', 'lim@@', 'as@@', 'e@@', '.', '</s>']
2025-05-30 00:15:11,462 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:15:11,462 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:15:11,462 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cuore dell<unk> arista, il cuore cruciale del nostro sistema Klimase.
2025-05-30 00:15:11,462 - INFO - joeynmt.training - Example #3
2025-05-30 00:15:11,462 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:15:11,462 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:15:11,462 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'in@@', 'ver@@', 'no@@', ',', 'e', 'il', 'suo', 's@@', 'ab@@', 'ato', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:15:11,463 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:15:11,463 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:15:11,463 - INFO - joeynmt.training - 	Hypothesis: L<unk> inverno, e il suo sabato in estate.
2025-05-30 00:15:11,463 - INFO - joeynmt.training - Example #4
2025-05-30 00:15:11,463 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:15:11,463 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:15:11,463 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'momento', 'di', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:15:11,463 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:15:11,463 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:15:11,463 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è un momento di disegno di quello che è successo negli ultimi 25 anni.
2025-05-30 00:15:14,947 - INFO - joeynmt.training - Epoch   7, Step:    64600, Batch Loss:     1.832777, Batch Acc: 0.477363, Tokens per Sec:    16585, Lr: 0.000300
2025-05-30 00:15:18,437 - INFO - joeynmt.training - Epoch   7, Step:    64700, Batch Loss:     1.771119, Batch Acc: 0.472347, Tokens per Sec:    20365, Lr: 0.000300
2025-05-30 00:15:21,897 - INFO - joeynmt.training - Epoch   7, Step:    64800, Batch Loss:     1.845056, Batch Acc: 0.475954, Tokens per Sec:    19886, Lr: 0.000300
2025-05-30 00:15:25,361 - INFO - joeynmt.training - Epoch   7, Step:    64900, Batch Loss:     1.732049, Batch Acc: 0.480622, Tokens per Sec:    20124, Lr: 0.000300
2025-05-30 00:15:28,834 - INFO - joeynmt.training - Epoch   7, Step:    65000, Batch Loss:     1.770414, Batch Acc: 0.476546, Tokens per Sec:    19796, Lr: 0.000300
2025-05-30 00:15:28,834 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:15:28,835 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:15:35,812 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.80, ppl:   6.07, acc:   0.50, generation: 6.9677[sec], evaluation: 0.0000[sec]
2025-05-30 00:15:36,191 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/62000.ckpt
2025-05-30 00:15:36,219 - INFO - joeynmt.training - Example #0
2025-05-30 00:15:36,219 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:15:36,219 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:15:36,220 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've@@', ',', 'per', 'per@@', 'dere', 'che', 'i', 'li@@', 'velli', 'ar@@', 'c@@', 'lin@@', 'ici', 'che', 'chiam@@', 'ano', 'il', 'li@@', 'mi@@', 'te', 'di', 'tre', 'milioni', 'di', 'anni', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'in@@', 'fer@@', 'ma@@', ',', 'il', '4@@', '8', 'pa@@', 'es@@', 'i@@', ',', 'il', '4@@', '8@@', '0@@', '%', 'per', 'la', 'maggior', 'parte', 'dei', 'con@@', 'fin@@', 'i', 'in', 'modo', 'che', 'la', 'maggior', 'parte', 'delle', 'persone', 'che', 'si', 'trov@@', 'ano', 'in', 'modo', 'che', 'la', 'maggior', 'parte', 'delle', 'persone', 'che', 'non', 'sono', 'mai', 'più', 'import@@', 'anti@@', '.', '</s>']
2025-05-30 00:15:36,220 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:15:36,220 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:15:36,220 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive, per perdere che i livelli arclinici che chiamano il limite di tre milioni di anni la dimensione dell<unk> inferma, il 48 paesi, il 480% per la maggior parte dei confini in modo che la maggior parte delle persone che si trovano in modo che la maggior parte delle persone che non sono mai più importanti.
2025-05-30 00:15:36,220 - INFO - joeynmt.training - Example #1
2025-05-30 00:15:36,220 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:15:36,220 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:15:36,220 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'ti@@', ',', 'per', 'la', 'prima', 'volta', 'che', 'la', 'maggior', 'parte', 'di', 'questo', 'problema', 'speci@@', 'fic@@', 'o@@', ',', 'perché', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:15:36,221 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:15:36,221 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:15:36,221 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forti, per la prima volta che la maggior parte di questo problema specifico, perché non è il dell<unk> Eises.
2025-05-30 00:15:36,221 - INFO - joeynmt.training - Example #2
2025-05-30 00:15:36,221 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:15:36,221 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:15:36,221 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'cosa', 'più', 'importante', 'è', 'che', 'la', 'cosa', 'più', 'importante', 'del', 'nostro', 'sistema', 'c@@', 'atti@@', 'vo', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:15:36,222 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:15:36,222 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:15:36,222 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cosa più importante è che la cosa più importante del nostro sistema cattivo globale.
2025-05-30 00:15:36,222 - INFO - joeynmt.training - Example #3
2025-05-30 00:15:36,222 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:15:36,222 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:15:36,222 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ei', 'è', 'stato', 'un', 'po@@', '<unk>', 'di', 's@@', 'om@@', 'mer@@', '.', '</s>']
2025-05-30 00:15:36,222 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:15:36,222 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:15:36,223 - INFO - joeynmt.training - 	Hypothesis: Lei è stato un po<unk> di sommer.
2025-05-30 00:15:36,223 - INFO - joeynmt.training - Example #4
2025-05-30 00:15:36,223 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:15:36,223 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:15:36,223 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'cosa', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:15:36,223 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:15:36,223 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:15:36,223 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una cosa che è successo negli ultimi 25 anni.
2025-05-30 00:15:39,714 - INFO - joeynmt.training - Epoch   7, Step:    65100, Batch Loss:     1.910287, Batch Acc: 0.478607, Tokens per Sec:    17890, Lr: 0.000300
2025-05-30 00:15:43,189 - INFO - joeynmt.training - Epoch   7, Step:    65200, Batch Loss:     1.776895, Batch Acc: 0.476089, Tokens per Sec:    19336, Lr: 0.000300
2025-05-30 00:15:46,661 - INFO - joeynmt.training - Epoch   7, Step:    65300, Batch Loss:     1.954355, Batch Acc: 0.474156, Tokens per Sec:    19598, Lr: 0.000300
2025-05-30 00:15:50,121 - INFO - joeynmt.training - Epoch   7, Step:    65400, Batch Loss:     1.748467, Batch Acc: 0.480862, Tokens per Sec:    19697, Lr: 0.000300
2025-05-30 00:15:53,588 - INFO - joeynmt.training - Epoch   7, Step:    65500, Batch Loss:     1.710740, Batch Acc: 0.479712, Tokens per Sec:    19609, Lr: 0.000300
2025-05-30 00:15:53,589 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:15:53,589 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:16:00,877 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.80, ppl:   6.07, acc:   0.50, generation: 7.2788[sec], evaluation: 0.0000[sec]
2025-05-30 00:16:01,304 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/64500.ckpt
2025-05-30 00:16:01,332 - INFO - joeynmt.training - Example #0
2025-05-30 00:16:01,332 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:16:01,332 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:16:01,332 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 's@@', 'li@@', 'de', 'per', 'osserv@@', 'are', 'che', 'la', 'gente', 'ha', 'fatto', 'che', 'la', 'chiam@@', 'ava', 'l@@', '<unk>', 'E@@', 'is@@', 'co@@', ',', 'che', 'ha', 'avuto', 'il', 'li@@', 'mi@@', 'te', 'di', 'tre', 'milioni', 'di', 'anni', 'di', 'dimen@@', 'sioni', 'd@@', 'ell@@', '<unk>', 'ann@@', 'o@@', ',', 'il', '4@@', '8@@', ',', 'il', '4@@', '8@@', ',', 'il', '4@@', '8@@', '.', '</s>']
2025-05-30 00:16:01,333 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:16:01,333 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:16:01,333 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due slide per osservare che la gente ha fatto che la chiamava l<unk> Eisco, che ha avuto il limite di tre milioni di anni di dimensioni dell<unk> anno, il 48, il 48, il 48.
2025-05-30 00:16:01,333 - INFO - joeynmt.training - Example #1
2025-05-30 00:16:01,333 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:16:01,333 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:16:01,333 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'che', 'non', 'è', 'abbastanza', 'la', 'prima', 'da', 'questo', 'problema', 'che', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', ',', 'perché', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:16:01,334 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:16:01,334 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:16:01,334 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, che non è abbastanza la prima da questo problema che non è il dell<unk> Eises, perché non è il dell<unk> Eises.
2025-05-30 00:16:01,334 - INFO - joeynmt.training - Example #2
2025-05-30 00:16:01,334 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:16:01,334 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:16:01,334 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'cosa', 'più', 'importante', 'è', 'che', 'il', 'cuore', 'c@@', 'ru@@', 'ci@@', 'ale', 'della', 'nostra', 'c@@', 'ris@@', 'i@@', '.', '</s>']
2025-05-30 00:16:01,335 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:16:01,335 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:16:01,335 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cosa più importante è che il cuore cruciale della nostra crisi.
2025-05-30 00:16:01,335 - INFO - joeynmt.training - Example #3
2025-05-30 00:16:01,335 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:16:01,335 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:16:01,335 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'cominci@@', 'a', 'a', 'a', 'cresc@@', 'ere', 'e', 'sc@@', 'ar@@', 'e@@', '.', '</s>']
2025-05-30 00:16:01,336 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:16:01,336 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:16:01,336 - INFO - joeynmt.training - 	Hypothesis: Si comincia a a crescere e scare.
2025-05-30 00:16:01,336 - INFO - joeynmt.training - Example #4
2025-05-30 00:16:01,336 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:16:01,336 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:16:01,336 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'di@@', 'seg@@', 'no', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:16:01,336 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:16:01,336 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:16:01,337 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è un disegno di disegno che è successo negli ultimi 25 anni.
2025-05-30 00:16:04,817 - INFO - joeynmt.training - Epoch   7, Step:    65600, Batch Loss:     1.894863, Batch Acc: 0.475382, Tokens per Sec:    17146, Lr: 0.000300
2025-05-30 00:16:08,290 - INFO - joeynmt.training - Epoch   7, Step:    65700, Batch Loss:     2.043989, Batch Acc: 0.477897, Tokens per Sec:    19567, Lr: 0.000300
2025-05-30 00:16:11,749 - INFO - joeynmt.training - Epoch   7, Step:    65800, Batch Loss:     1.889046, Batch Acc: 0.479402, Tokens per Sec:    19547, Lr: 0.000300
2025-05-30 00:16:15,216 - INFO - joeynmt.training - Epoch   7, Step:    65900, Batch Loss:     1.980188, Batch Acc: 0.479343, Tokens per Sec:    19502, Lr: 0.000300
2025-05-30 00:16:18,132 - INFO - joeynmt.training - Epoch   7: total training loss 17586.37
2025-05-30 00:16:18,132 - INFO - joeynmt.training - EPOCH 8
2025-05-30 00:16:18,692 - INFO - joeynmt.training - Epoch   8, Step:    66000, Batch Loss:     1.733940, Batch Acc: 0.496147, Tokens per Sec:    20228, Lr: 0.000300
2025-05-30 00:16:18,692 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:16:18,692 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:16:25,458 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.80, ppl:   6.04, acc:   0.50, generation: 6.7541[sec], evaluation: 0.0000[sec]
2025-05-30 00:16:26,092 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/63000.ckpt
2025-05-30 00:16:26,121 - INFO - joeynmt.training - Example #0
2025-05-30 00:16:26,121 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:16:26,121 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:16:26,121 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've@@', ',', 'per', 'osserv@@', 'are', 'che', 'i', 'ghi@@', 'acci@@', 'ai', 'ar@@', 'c@@', 'ic@@', 'li', 'di', 'ar@@', 'c@@', 'op@@', 'ie', 'che', 'per', 'tre', 'milioni', 'di', 'anni', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'in@@', 'fer@@', 'mi@@', 'er@@', 'e@@', ',', 'il', 'numero', 'di', 'tre', 'milioni', 'di', 'anni', 'di', 'grandi', 'stati', 'in', 'grado', 'di', 'ri@@', 'pro@@', 'durre', 'il', '40', 'per', 'cento', 'dei', 'paesi', 'in', 'via', 'di', 'svilupp@@', 'o@@', '.', '</s>']
2025-05-30 00:16:26,122 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:16:26,122 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:16:26,122 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive, per osservare che i ghiacciai arcicli di arcopie che per tre milioni di anni la dimensione dell<unk> infermiere, il numero di tre milioni di anni di grandi stati in grado di riprodurre il 40 per cento dei paesi in via di sviluppo.
2025-05-30 00:16:26,122 - INFO - joeynmt.training - Example #1
2025-05-30 00:16:26,122 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:16:26,122 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:16:26,122 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'che', 'è', 'la', 'più', 'grande', 'che', 'questo', 'è', 'un', 'problema', 'speci@@', 'fic@@', 'o@@', ',', 'perché', 'non', 'è', 'il', 'D@@', 'ic@@', 'ke', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:16:26,123 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:16:26,123 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:16:26,123 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, che è la più grande che questo è un problema specifico, perché non è il Dicke dell<unk> Eises.
2025-05-30 00:16:26,123 - INFO - joeynmt.training - Example #2
2025-05-30 00:16:26,123 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:16:26,123 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:16:26,123 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'è', 'l@@', '<unk>', 'er@@', 'a@@', ',', 'l@@', '<unk>', 'Her@@', 'z@@', ',', 'il', 'cuore', 'c@@', 'ru@@', 'ci@@', 'ale', 'della', 'nostra', 'c@@', 'lasse', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:16:26,124 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:16:26,124 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:16:26,124 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, è l<unk> era, l<unk> Herz, il cuore cruciale della nostra classe globale.
2025-05-30 00:16:26,124 - INFO - joeynmt.training - Example #3
2025-05-30 00:16:26,124 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:16:26,124 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:16:26,124 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'prima', 'di', 'tut@@', 'to@@', ',', 'nel', 's@@', 'om@@', 'mer@@', 'e@@', '.', '</s>']
2025-05-30 00:16:26,125 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:16:26,125 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:16:26,125 - INFO - joeynmt.training - 	Hypothesis: E prima di tutto, nel sommere.
2025-05-30 00:16:26,125 - INFO - joeynmt.training - Example #4
2025-05-30 00:16:26,125 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:16:26,125 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:16:26,125 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:16:26,125 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:16:26,125 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:16:26,125 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è un disegno di quello che è successo negli ultimi 25 anni.
2025-05-30 00:16:29,629 - INFO - joeynmt.training - Epoch   8, Step:    66100, Batch Loss:     1.809967, Batch Acc: 0.492134, Tokens per Sec:    16508, Lr: 0.000300
2025-05-30 00:16:33,100 - INFO - joeynmt.training - Epoch   8, Step:    66200, Batch Loss:     1.901368, Batch Acc: 0.496130, Tokens per Sec:    19879, Lr: 0.000300
2025-05-30 00:16:36,538 - INFO - joeynmt.training - Epoch   8, Step:    66300, Batch Loss:     1.736218, Batch Acc: 0.492723, Tokens per Sec:    19949, Lr: 0.000300
2025-05-30 00:16:40,006 - INFO - joeynmt.training - Epoch   8, Step:    66400, Batch Loss:     1.905001, Batch Acc: 0.501772, Tokens per Sec:    19779, Lr: 0.000300
2025-05-30 00:16:43,469 - INFO - joeynmt.training - Epoch   8, Step:    66500, Batch Loss:     1.776161, Batch Acc: 0.486502, Tokens per Sec:    19896, Lr: 0.000300
2025-05-30 00:16:43,470 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:16:43,470 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:16:50,271 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.80, ppl:   6.04, acc:   0.50, generation: 6.7911[sec], evaluation: 0.0000[sec]
2025-05-30 00:16:50,693 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/62500.ckpt
2025-05-30 00:16:50,723 - INFO - joeynmt.training - Example #0
2025-05-30 00:16:50,724 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:16:50,724 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:16:50,724 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'dimostr@@', 'ato', 'che', 'l@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'che', 'l@@', '<unk>', 'ar@@', 'ric@@', 'ch@@', 'etto', 'ar@@', 'ric@@', 'ci@@', 'ano', 'che', 'gli', 'ar@@', 'chi', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'gruppi', 'di', 'più', 'grande', '4@@', '8', 'nazi@@', 'onali', 'per', 'cento', 'dei', 'gruppi', 'di', 'più', 'grande', '4@@', '8', 'nazi@@', 'onali', 'per', 'cento', 'per', 'cento', 'della', 'popolazione', 'mon@@', 'di@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:16:50,724 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:16:50,725 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:16:50,725 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho dimostrato che l<unk> anno scorso, che l<unk> arricchetto arricciano che gli archi per tre milioni di anni di gruppi di più grande 48 nazionali per cento dei gruppi di più grande 48 nazionali per cento per cento della popolazione mondiale.
2025-05-30 00:16:50,725 - INFO - joeynmt.training - Example #1
2025-05-30 00:16:50,725 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:16:50,725 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:16:50,725 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'per', 'la', 'prima', 'cosa', 'che', 'la', 'maggior', 'parte', 'di', 'questo', 'problema', 'speci@@', 'fic@@', 'o@@', ',', 'perché', 'non', 'è', 'il', 'c@@', 'ic@@', 'co', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:16:50,725 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:16:50,725 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:16:50,726 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, per la prima cosa che la maggior parte di questo problema specifico, perché non è il cicco dell<unk> Eises.
2025-05-30 00:16:50,726 - INFO - joeynmt.training - Example #2
2025-05-30 00:16:50,726 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:16:50,726 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:16:50,726 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'l@@', '<unk>', 'ar@@', 'c@@', 'ic@@', 'lo', 'di', 'ghi@@', 'accio', 'c@@', 'atti@@', 'vo', 'il', 'nostro', 'c@@', 'av@@', 'o@@', ',', 'il', 'nostro', 'sistema', 'globale', 'di', 'c@@', 'au@@', 'se', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:16:50,726 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:16:50,726 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:16:50,726 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, l<unk> arciclo di ghiaccio cattivo il nostro cavo, il nostro sistema globale di cause globale.
2025-05-30 00:16:50,727 - INFO - joeynmt.training - Example #3
2025-05-30 00:16:50,727 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:16:50,727 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:16:50,727 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'cominci@@', 'a', 'a', 'a', 'cresc@@', 'ere', 'in', 'est@@', 'at@@', 'e@@', ',', 'e', 'si', 's@@', 'om@@', 'in@@', 'are', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:16:50,727 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:16:50,727 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:16:50,727 - INFO - joeynmt.training - 	Hypothesis: E comincia a a crescere in estate, e si sominare in estate.
2025-05-30 00:16:50,727 - INFO - joeynmt.training - Example #4
2025-05-30 00:16:50,728 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:16:50,728 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:16:50,728 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:16:50,728 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:16:50,728 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:16:50,728 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è un disegno di quello che è successo negli ultimi 25 anni.
2025-05-30 00:16:54,206 - INFO - joeynmt.training - Epoch   8, Step:    66600, Batch Loss:     1.908925, Batch Acc: 0.498376, Tokens per Sec:    17369, Lr: 0.000300
2025-05-30 00:16:57,672 - INFO - joeynmt.training - Epoch   8, Step:    66700, Batch Loss:     1.957482, Batch Acc: 0.485954, Tokens per Sec:    19637, Lr: 0.000300
2025-05-30 00:17:01,139 - INFO - joeynmt.training - Epoch   8, Step:    66800, Batch Loss:     1.742000, Batch Acc: 0.492459, Tokens per Sec:    19495, Lr: 0.000300
2025-05-30 00:17:04,597 - INFO - joeynmt.training - Epoch   8, Step:    66900, Batch Loss:     2.207962, Batch Acc: 0.493950, Tokens per Sec:    19838, Lr: 0.000300
2025-05-30 00:17:08,047 - INFO - joeynmt.training - Epoch   8, Step:    67000, Batch Loss:     1.767710, Batch Acc: 0.489322, Tokens per Sec:    19050, Lr: 0.000300
2025-05-30 00:17:08,047 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:17:08,047 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:17:15,315 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.81, ppl:   6.09, acc:   0.50, generation: 7.2583[sec], evaluation: 0.0000[sec]
2025-05-30 00:17:15,325 - INFO - joeynmt.training - Example #0
2025-05-30 00:17:15,326 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:17:15,326 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:17:15,326 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'per@@', 'dere', 'la', 'maggior', 'parte', 'delle', 'em@@', 'issi@@', 'oni', 'di', 'ghi@@', 'acci@@', 'o@@', ',', 'che', 'ha', 'avuto', 'il', '4@@', '8@@', ',', '4@@', '8@@', ',', '4@@', '8@@', ',', '4@@', '8@@', ',', '40', 'per', 'cento', 'di', 'questi', 'gruppi', 'di', 'persone', 'che', 'aveva', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cento', 'per', 'cento', 'per', 'cento', 'del', '4@@', '8@@', '0@@', '<unk>', '.', '</s>']
2025-05-30 00:17:15,326 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:17:15,327 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:17:15,327 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per perdere la maggior parte delle emissioni di ghiaccio, che ha avuto il 48, 48, 48, 48, 40 per cento di questi gruppi di persone che aveva 48 stati per cento per cento per cento per cento del 480<unk> .
2025-05-30 00:17:15,327 - INFO - joeynmt.training - Example #1
2025-05-30 00:17:15,327 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:17:15,327 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:17:15,327 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'non', 'è', 'abbastanza', 'la', 'più', 'grande', 'di', 'questo', 'problema', 'speci@@', 'fic@@', 'o@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:17:15,328 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:17:15,328 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:17:15,328 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, non è abbastanza la più grande di questo problema specifico, perché non è il dibattito del ghiaccio.
2025-05-30 00:17:15,328 - INFO - joeynmt.training - Example #2
2025-05-30 00:17:15,328 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:17:15,328 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:17:15,328 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'è', 'l@@', '<unk>', 'ar@@', 'c@@', 'lin@@', 'ic@@', 'o@@', ',', 'il', 'cuore', 'c@@', 'ru@@', 'ci@@', 'ale', 'del', 'nostro', 'sistema', 'c@@', 'lim@@', 'ass@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:17:15,329 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:17:15,329 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:17:15,329 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, è l<unk> arclinico, il cuore cruciale del nostro sistema climassale.
2025-05-30 00:17:15,329 - INFO - joeynmt.training - Example #3
2025-05-30 00:17:15,329 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:17:15,329 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:17:15,329 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'inizi@@', 'a', 'a', 'a', 'cresc@@', 'ere', 'e', 'sc@@', 'sc@@', 'sc@@', 'sc@@', 'ambi@@', 'o', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:17:15,330 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:17:15,330 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:17:15,330 - INFO - joeynmt.training - 	Hypothesis: E inizia a a crescere e scscscscambio in estate.
2025-05-30 00:17:15,330 - INFO - joeynmt.training - Example #4
2025-05-30 00:17:15,330 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:17:15,330 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:17:15,330 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'foto', 'di', 'un', 'di@@', 'seg@@', 'no', 'di', 'un', 'atti@@', 'mo', 'di', 'tem@@', 'po@@', '.', '</s>']
2025-05-30 00:17:15,330 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:17:15,331 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:17:15,331 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una foto di un disegno di un attimo di tempo.
2025-05-30 00:17:18,824 - INFO - joeynmt.training - Epoch   8, Step:    67100, Batch Loss:     1.767828, Batch Acc: 0.489962, Tokens per Sec:    19580, Lr: 0.000300
2025-05-30 00:17:22,298 - INFO - joeynmt.training - Epoch   8, Step:    67200, Batch Loss:     1.684030, Batch Acc: 0.493091, Tokens per Sec:    20068, Lr: 0.000300
2025-05-30 00:17:25,774 - INFO - joeynmt.training - Epoch   8, Step:    67300, Batch Loss:     1.844270, Batch Acc: 0.492666, Tokens per Sec:    20108, Lr: 0.000300
2025-05-30 00:17:29,238 - INFO - joeynmt.training - Epoch   8, Step:    67400, Batch Loss:     1.778113, Batch Acc: 0.487462, Tokens per Sec:    19474, Lr: 0.000300
2025-05-30 00:17:32,723 - INFO - joeynmt.training - Epoch   8, Step:    67500, Batch Loss:     1.852404, Batch Acc: 0.489251, Tokens per Sec:    20123, Lr: 0.000300
2025-05-30 00:17:32,723 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:17:32,723 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:17:39,313 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.80, ppl:   6.06, acc:   0.50, generation: 6.5797[sec], evaluation: 0.0000[sec]
2025-05-30 00:17:39,761 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/65500.ckpt
2025-05-30 00:17:39,786 - INFO - joeynmt.training - Example #0
2025-05-30 00:17:39,786 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:17:39,786 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:17:39,786 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'consegu@@', 'enze', 'per', 'la', 'di@@', 'stri@@', 'bu@@', 'zione', 'di', 'ghi@@', 'accio', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'ar@@', 'ch@@', 'e@@', ',', 'che', 'la', 'chiam@@', 'ano', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'dimen@@', 'sioni', 'che', 'chiam@@', 'ano', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'dimen@@', 'sioni', 'di', 'dimen@@', 'sioni', 'sott@@', 'ov@@', 'al@@', 'ut@@', 'o@@', '.', '</s>']
2025-05-30 00:17:39,787 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:17:39,787 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:17:39,787 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due conseguenze per la distribuzione di ghiaccio che il ghiaccio arktico arche, che la chiamano per tre milioni di anni di dimensioni che chiamano per tre milioni di anni di dimensioni di dimensioni sottovaluto.
2025-05-30 00:17:39,787 - INFO - joeynmt.training - Example #1
2025-05-30 00:17:39,787 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:17:39,787 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:17:39,787 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'la', 'più', 'grande', 'della', 'prima', 'st@@', 'azione', 'di', 'questo', 'particolare', 'problema', 'che', 'non', 'è', 'la', 'cosa', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'accio', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:17:39,788 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:17:39,788 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:17:39,788 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto la più grande della prima stazione di questo particolare problema che non è la cosa che non è il dibattito del ghiaccio dell<unk> Eises.
2025-05-30 00:17:39,788 - INFO - joeynmt.training - Example #2
2025-05-30 00:17:39,788 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:17:39,788 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:17:39,788 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'c@@', 'ant@@', 'ante', 'ar@@', 'ric@@', 'e', 'di', 'ghi@@', 'accio', 'è', 'il', 'cuore', 'c@@', 'ru@@', 'ci@@', 'ale', 'del', 'nostro', 'c@@', 'aff@@', 'è', 'di', 'ghi@@', 'accio', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:17:39,789 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:17:39,789 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:17:39,789 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cantante arrice di ghiaccio è il cuore cruciale del nostro caffè di ghiaccio globale.
2025-05-30 00:17:39,789 - INFO - joeynmt.training - Example #3
2025-05-30 00:17:39,789 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:17:39,789 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:17:39,789 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'in@@', 'ver@@', 'no@@', ',', 'nel', 'm@@', 'om@@', 'ent@@', 'o@@', ',', 'e', 's@@', 'ì@@', '.', '</s>']
2025-05-30 00:17:39,790 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:17:39,790 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:17:39,790 - INFO - joeynmt.training - 	Hypothesis: L<unk> inverno, nel momento, e sì.
2025-05-30 00:17:39,790 - INFO - joeynmt.training - Example #4
2025-05-30 00:17:39,790 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:17:39,790 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:17:39,790 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'cosa', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:17:39,790 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:17:39,790 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:17:39,791 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una cosa che è successo negli ultimi 25 anni.
2025-05-30 00:17:43,294 - INFO - joeynmt.training - Epoch   8, Step:    67600, Batch Loss:     1.752058, Batch Acc: 0.486533, Tokens per Sec:    17629, Lr: 0.000300
2025-05-30 00:17:46,758 - INFO - joeynmt.training - Epoch   8, Step:    67700, Batch Loss:     1.804666, Batch Acc: 0.485470, Tokens per Sec:    19417, Lr: 0.000300
2025-05-30 00:17:50,210 - INFO - joeynmt.training - Epoch   8, Step:    67800, Batch Loss:     1.536513, Batch Acc: 0.485316, Tokens per Sec:    19380, Lr: 0.000300
2025-05-30 00:17:53,662 - INFO - joeynmt.training - Epoch   8, Step:    67900, Batch Loss:     1.873510, Batch Acc: 0.480336, Tokens per Sec:    19586, Lr: 0.000300
2025-05-30 00:17:57,121 - INFO - joeynmt.training - Epoch   8, Step:    68000, Batch Loss:     1.720267, Batch Acc: 0.483942, Tokens per Sec:    20102, Lr: 0.000300
2025-05-30 00:17:57,122 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:17:57,122 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:18:03,733 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.80, ppl:   6.07, acc:   0.50, generation: 6.6014[sec], evaluation: 0.0000[sec]
2025-05-30 00:18:04,340 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/65000.ckpt
2025-05-30 00:18:04,369 - INFO - joeynmt.training - Example #0
2025-05-30 00:18:04,370 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:18:04,370 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:18:04,370 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'consegu@@', 'enze', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'acci@@', 'ato', 'ar@@', 'ric@@', 'o@@', 'stru@@', 'ito', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'ch@@', 'etto', 'ar@@', 'ric@@', 'ch@@', 'etto', 'di', 'tre', 'milioni', 'di', 'anni', 'di', 'con@@', 'fin@@', 'i', 'dei', 'con@@', 'fin@@', 'i', 'dei', 'con@@', 'fin@@', 'i', 'delle', 'dimen@@', 'sioni', 'd@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'e@@', 'ano', 'per', 'il', '40', 'per', 'cento', 'del', '2@@', '0@@', '%', 'del', 'numero', 'di', 'persone', 'che', 'si', 'è', 'ri@@', 'vel@@', 'ato', 'al', '40', 'per', 'cento', 'del', '2@@', '4@@', '8', 'nazi@@', 'on@@', 'e@@', '.', '</s>']
2025-05-30 00:18:04,370 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:18:04,370 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:18:04,370 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due conseguenze per osservare che il ghiacciato arricostruito che il ghiaccio archetto arricchetto di tre milioni di anni di confini dei confini dei confini delle dimensioni dell<unk> Oceano per il 40 per cento del 20% del numero di persone che si è rivelato al 40 per cento del 248 nazione.
2025-05-30 00:18:04,371 - INFO - joeynmt.training - Example #1
2025-05-30 00:18:04,371 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:18:04,371 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:18:04,371 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'ma', 'non', 'è', 'molto', 'la', 'più', 'grande', 'di', 'questo', 'problema', 'che', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:18:04,371 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:18:04,371 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:18:04,371 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, ma non è molto la più grande di questo problema che non è il dell<unk> Eises.
2025-05-30 00:18:04,372 - INFO - joeynmt.training - Example #2
2025-05-30 00:18:04,372 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:18:04,372 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:18:04,372 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'senso', 'è', 'il', 'c@@', 'ru@@', 'del@@', 'e', 'il', 'cuore', 'c@@', 'ru@@', 'del@@', 'e', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:18:04,372 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:18:04,372 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:18:04,372 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il senso è il crudele il cuore crudele globale.
2025-05-30 00:18:04,372 - INFO - joeynmt.training - Example #3
2025-05-30 00:18:04,373 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:18:04,373 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:18:04,373 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'nel', 'vent@@', 'o@@', ',', 'e', 'si', 's@@', 'for@@', 'z@@', 'a@@', '.', '</s>']
2025-05-30 00:18:04,373 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:18:04,373 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:18:04,373 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo nel vento, e si sforza.
2025-05-30 00:18:04,373 - INFO - joeynmt.training - Example #4
2025-05-30 00:18:04,373 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:18:04,373 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:18:04,374 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'atti@@', 'ro', 'di', 'tem@@', 'po@@', ',', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:18:04,374 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:18:04,374 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:18:04,374 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è un attiro di tempo, che è successo negli ultimi 25 anni.
2025-05-30 00:18:07,868 - INFO - joeynmt.training - Epoch   8, Step:    68100, Batch Loss:     1.805271, Batch Acc: 0.485044, Tokens per Sec:    16634, Lr: 0.000300
2025-05-30 00:18:11,356 - INFO - joeynmt.training - Epoch   8, Step:    68200, Batch Loss:     2.105047, Batch Acc: 0.487997, Tokens per Sec:    19855, Lr: 0.000300
2025-05-30 00:18:14,849 - INFO - joeynmt.training - Epoch   8, Step:    68300, Batch Loss:     1.692153, Batch Acc: 0.488217, Tokens per Sec:    20751, Lr: 0.000300
2025-05-30 00:18:18,339 - INFO - joeynmt.training - Epoch   8, Step:    68400, Batch Loss:     1.985525, Batch Acc: 0.487499, Tokens per Sec:    20062, Lr: 0.000300
2025-05-30 00:18:21,821 - INFO - joeynmt.training - Epoch   8, Step:    68500, Batch Loss:     1.726153, Batch Acc: 0.487053, Tokens per Sec:    19970, Lr: 0.000300
2025-05-30 00:18:21,821 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:18:21,821 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:18:28,407 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.80, ppl:   6.04, acc:   0.50, generation: 6.5791[sec], evaluation: 0.0000[sec]
2025-05-30 00:18:28,802 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/68000.ckpt
2025-05-30 00:18:28,830 - INFO - joeynmt.training - Example #0
2025-05-30 00:18:28,830 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:18:28,830 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:18:28,831 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've@@', ',', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'ric@@', 'o@@', ',', 'che', 'è', 'stato', 'ri@@', 'vel@@', 'ato', 'per', 'tre', 'milioni', 'di', 'anni', 'i', 'dimen@@', 'sioni', 'di', 'dimen@@', 'sioni', 'che', 'si', 'è', 'ri@@', 'vel@@', 'ato', 'per', 'tre', 'milioni', 'di', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:18:28,831 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:18:28,831 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:18:28,831 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive, per osservare che il ghiaccio arrico, che è stato rivelato per tre milioni di anni i dimensioni di dimensioni che si è rivelato per tre milioni di anni.
2025-05-30 00:18:28,831 - INFO - joeynmt.training - Example #1
2025-05-30 00:18:28,831 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:18:28,831 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:18:28,832 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'non', 'è', 'abbastanza', 'la', 'più', 'grande', 'questo', 'problema', 'speci@@', 'al@@', 'e@@', ',', 'perché', 'non', 'è', 'il', 'vi@@', 'll@@', 'aggio', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'l@@', 'and@@', 'o@@', '.', '</s>']
2025-05-30 00:18:28,832 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:18:28,832 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:18:28,832 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, non è abbastanza la più grande questo problema speciale, perché non è il villaggio dell<unk> Eislando.
2025-05-30 00:18:28,832 - INFO - joeynmt.training - Example #2
2025-05-30 00:18:28,832 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:18:28,832 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:18:28,832 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'senso', 'è', 'il', 'cuore', 'ar@@', 'ric@@', 'co', 'di', 'ghi@@', 'accio', 'del', 'nostro', 'sistema', 'c@@', 'lim@@', 'as@@', 'sist@@', 'ente', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:18:28,833 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:18:28,833 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:18:28,833 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il senso è il cuore arricco di ghiaccio del nostro sistema climassistente globale.
2025-05-30 00:18:28,833 - INFO - joeynmt.training - Example #3
2025-05-30 00:18:28,833 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:18:28,833 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:18:28,833 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ei', 'è', 'cres@@', 'ce', 'nel', 'in@@', 'ver@@', 'no@@', '.', '</s>']
2025-05-30 00:18:28,834 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:18:28,834 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:18:28,834 - INFO - joeynmt.training - 	Hypothesis: Lei è cresce nel inverno.
2025-05-30 00:18:28,834 - INFO - joeynmt.training - Example #4
2025-05-30 00:18:28,834 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:18:28,834 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:18:28,834 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:18:28,835 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:18:28,835 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:18:28,835 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è un disegno di disegno di quello che è successo negli ultimi 25 anni.
2025-05-30 00:18:32,320 - INFO - joeynmt.training - Epoch   8, Step:    68600, Batch Loss:     1.821974, Batch Acc: 0.486639, Tokens per Sec:    17431, Lr: 0.000210
2025-05-30 00:18:35,773 - INFO - joeynmt.training - Epoch   8, Step:    68700, Batch Loss:     1.815166, Batch Acc: 0.486441, Tokens per Sec:    19816, Lr: 0.000210
2025-05-30 00:18:39,230 - INFO - joeynmt.training - Epoch   8, Step:    68800, Batch Loss:     1.705790, Batch Acc: 0.486704, Tokens per Sec:    19987, Lr: 0.000210
2025-05-30 00:18:42,680 - INFO - joeynmt.training - Epoch   8, Step:    68900, Batch Loss:     1.918170, Batch Acc: 0.494318, Tokens per Sec:    20593, Lr: 0.000210
2025-05-30 00:18:46,132 - INFO - joeynmt.training - Epoch   8, Step:    69000, Batch Loss:     1.847451, Batch Acc: 0.497152, Tokens per Sec:    19890, Lr: 0.000210
2025-05-30 00:18:46,132 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:18:46,132 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:18:52,621 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.78, ppl:   5.90, acc:   0.51, generation: 6.4794[sec], evaluation: 0.0000[sec]
2025-05-30 00:18:52,621 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:18:53,266 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/67500.ckpt
2025-05-30 00:18:53,297 - INFO - joeynmt.training - Example #0
2025-05-30 00:18:53,298 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:18:53,298 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:18:53,298 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've@@', ',', 'che', 'l@@', '<unk>', 'ar@@', 'c@@', 'ic@@', 'lo', 'di', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'che', 'ha', 'avuto', 'tre', 'milioni', 'di', 'anni', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'e@@', '<unk>', ',', 'il', '4@@', '8@@', '0@@', '%', 'per', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'ci@@', 'dent@@', 'e@@', '.', '</s>']
2025-05-30 00:18:53,298 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:18:53,299 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:18:53,299 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive, che l<unk> arciclo di ghiaccio arktico che ha avuto tre milioni di anni la dimensione dell<unk> Oce<unk> , il 480% per la dimensione dell<unk> Occidente.
2025-05-30 00:18:53,299 - INFO - joeynmt.training - Example #1
2025-05-30 00:18:53,299 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:18:53,299 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:18:53,299 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'non', 'è', 'abbastanza', 'la', 'prima', 'cosa', 'che', 'non', 'è', 'molto', 'più', 'importante', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:18:53,300 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:18:53,300 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:18:53,300 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, non è abbastanza la prima cosa che non è molto più importante che non è il dibattito dell<unk> Eises.
2025-05-30 00:18:53,300 - INFO - joeynmt.training - Example #2
2025-05-30 00:18:53,300 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:18:53,300 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:18:53,300 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'c@@', 'aff@@', 'è@@', ',', 'il', 'cuore', 'ar@@', 'c@@', 'ru@@', 'ci@@', 'ale', 'del', 'nostro', 'c@@', 'ag@@', 'no', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:18:53,301 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:18:53,301 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:18:53,301 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il caffè, il cuore arcruciale del nostro cagno globale.
2025-05-30 00:18:53,301 - INFO - joeynmt.training - Example #3
2025-05-30 00:18:53,301 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:18:53,301 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:18:53,301 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'nel', 'vent@@', 'o@@', ',', 'e', 'si', 's@@', 'mon@@', 't@@', 'ato', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:18:53,301 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:18:53,301 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:18:53,302 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo nel vento, e si smontato in estate.
2025-05-30 00:18:53,302 - INFO - joeynmt.training - Example #4
2025-05-30 00:18:53,302 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:18:53,302 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:18:53,302 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'cosa', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:18:53,302 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:18:53,302 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:18:53,302 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una cosa che è successo negli ultimi 25 anni.
2025-05-30 00:18:56,788 - INFO - joeynmt.training - Epoch   8, Step:    69100, Batch Loss:     1.611095, Batch Acc: 0.499254, Tokens per Sec:    16578, Lr: 0.000210
2025-05-30 00:19:00,254 - INFO - joeynmt.training - Epoch   8, Step:    69200, Batch Loss:     2.108199, Batch Acc: 0.494136, Tokens per Sec:    19809, Lr: 0.000210
2025-05-30 00:19:03,700 - INFO - joeynmt.training - Epoch   8, Step:    69300, Batch Loss:     1.903072, Batch Acc: 0.493278, Tokens per Sec:    19433, Lr: 0.000210
2025-05-30 00:19:07,153 - INFO - joeynmt.training - Epoch   8, Step:    69400, Batch Loss:     1.661076, Batch Acc: 0.496474, Tokens per Sec:    20175, Lr: 0.000210
2025-05-30 00:19:10,608 - INFO - joeynmt.training - Epoch   8, Step:    69500, Batch Loss:     1.792283, Batch Acc: 0.490622, Tokens per Sec:    19973, Lr: 0.000210
2025-05-30 00:19:10,608 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:19:10,608 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:19:17,570 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.77, ppl:   5.86, acc:   0.51, generation: 6.9520[sec], evaluation: 0.0000[sec]
2025-05-30 00:19:17,571 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:19:18,253 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/66000.ckpt
2025-05-30 00:19:18,284 - INFO - joeynmt.training - Example #0
2025-05-30 00:19:18,284 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:19:18,284 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:19:18,284 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'con@@', 'v@@', 'inc@@', 'ere', 'che', 'il', 'ghi@@', 'acci@@', 'aio', 'ar@@', 'kt@@', 'ico', 'ar@@', 'c@@', 'ci@@', 'one', 'che', 'per', 'tre', 'milioni', 'di', 'anni', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'e@@', 'ano', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'raggi@@', 'ungere', 'il', '40', 'per', 'cento', 'per', 'cento', 'è', 'successo', 'il', '40', 'per', 'cento', 'è', 'successo', 'nel', '40', 'per', 'cento', 'del', '2@@', '0@@', '%', 'è', 'successo', 'in', 'questo', 'mod@@', 'o@@', '.', '</s>']
2025-05-30 00:19:18,285 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:19:18,285 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:19:18,285 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per convincere che il ghiacciaio arktico arccione che per tre milioni di anni la dimensione dell<unk> Oceano che ha fatto per tre milioni di anni per raggiungere il 40 per cento per cento è successo il 40 per cento è successo nel 40 per cento del 20% è successo in questo modo.
2025-05-30 00:19:18,285 - INFO - joeynmt.training - Example #1
2025-05-30 00:19:18,285 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:19:18,285 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:19:18,285 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'perché', 'non', 'è', 'il', 'problema', 'speci@@', 'fic@@', 'o@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:19:18,286 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:19:18,286 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:19:18,286 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, non è molto forte, perché non è il problema specifico, perché non è il dibattito del ghiaccio.
2025-05-30 00:19:18,286 - INFO - joeynmt.training - Example #2
2025-05-30 00:19:18,286 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:19:18,286 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:19:18,286 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'c@@', 'aff@@', 'è@@', ',', 'il', 'cuore', 'ar@@', 'kt@@', 'ico', 'del', 'nostro', 'c@@', 'aff@@', 'è@@', ',', 'il', 'cuore', 'c@@', 'ru@@', 'ci@@', 'ale', 'del', 'nostro', 'c@@', 'av@@', 'allo', 'di', 'c@@', 'ru@@', 'ci@@', 'ale', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:19:18,287 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:19:18,287 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:19:18,287 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il caffè, il cuore arktico del nostro caffè, il cuore cruciale del nostro cavallo di cruciale globale.
2025-05-30 00:19:18,287 - INFO - joeynmt.training - Example #3
2025-05-30 00:19:18,287 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:19:18,287 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:19:18,287 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'nel', 'in@@', 'ver@@', 'no@@', ',', 'e', 'sc@@', 'r@@', 'um@@', 'o@@', '.', '</s>']
2025-05-30 00:19:18,288 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:19:18,288 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:19:18,288 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo nel inverno, e scrumo.
2025-05-30 00:19:18,288 - INFO - joeynmt.training - Example #4
2025-05-30 00:19:18,288 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:19:18,288 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:19:18,288 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:19:18,288 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:19:18,288 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:19:18,289 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è un disegno di disegno di quello che è successo negli ultimi 25 anni.
2025-05-30 00:19:21,786 - INFO - joeynmt.training - Epoch   8, Step:    69600, Batch Loss:     1.683900, Batch Acc: 0.491884, Tokens per Sec:    16457, Lr: 0.000210
2025-05-30 00:19:25,266 - INFO - joeynmt.training - Epoch   8, Step:    69700, Batch Loss:     1.792034, Batch Acc: 0.490939, Tokens per Sec:    19653, Lr: 0.000210
2025-05-30 00:19:28,744 - INFO - joeynmt.training - Epoch   8, Step:    69800, Batch Loss:     1.680521, Batch Acc: 0.495011, Tokens per Sec:    20095, Lr: 0.000210
2025-05-30 00:19:32,217 - INFO - joeynmt.training - Epoch   8, Step:    69900, Batch Loss:     1.853979, Batch Acc: 0.498318, Tokens per Sec:    19956, Lr: 0.000210
2025-05-30 00:19:35,696 - INFO - joeynmt.training - Epoch   8, Step:    70000, Batch Loss:     2.046045, Batch Acc: 0.493505, Tokens per Sec:    20031, Lr: 0.000210
2025-05-30 00:19:35,696 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:19:35,696 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:19:42,504 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.77, ppl:   5.86, acc:   0.51, generation: 6.7985[sec], evaluation: 0.0000[sec]
2025-05-30 00:19:42,934 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/66500.ckpt
2025-05-30 00:19:42,957 - INFO - joeynmt.training - Example #0
2025-05-30 00:19:42,958 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:19:42,958 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:19:42,958 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'con@@', 'ver@@', 'ti@@', 're', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'c@@', 'ci@@', 'one', 'ar@@', 'ric@@', 'o@@', 'stru@@', 'ito', 'che', 'per', 'tre', 'milioni', 'di', 'anni', 'è', 'stata', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'e@@', '<unk>', '.', '</s>']
2025-05-30 00:19:42,958 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:19:42,958 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:19:42,958 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per convertire che il ghiaccio arccione arricostruito che per tre milioni di anni è stata la dimensione dell<unk> Oce<unk> .
2025-05-30 00:19:42,958 - INFO - joeynmt.training - Example #1
2025-05-30 00:19:42,959 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:19:42,959 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:19:42,959 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'forte', 'la', 'più', 'grande', 'è', 'la', 'più', 'grande', 'cos@@', 'ci@@', 'enza', 'di', 'questo', 'particolare', 'problema', 'che', 'non', 'è', 'il', 'di@@', 'seg@@', 'nat@@', 'ore', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:19:42,959 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:19:42,959 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:19:42,959 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte la più grande è la più grande coscienza di questo particolare problema che non è il disegnatore dell<unk> Eises.
2025-05-30 00:19:42,959 - INFO - joeynmt.training - Example #2
2025-05-30 00:19:42,959 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:19:42,959 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:19:42,959 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'ar@@', 't@@', 'ico', 'di', 'ghi@@', 'accio', 'del', 'nostro', 'c@@', 'ru@@', 'ci@@', 'ale', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:19:42,960 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:19:42,960 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:19:42,960 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il ghiaccio arktico artico di ghiaccio del nostro cruciale globale.
2025-05-30 00:19:42,960 - INFO - joeynmt.training - Example #3
2025-05-30 00:19:42,960 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:19:42,960 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:19:42,960 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'nel', 'in@@', 'ver@@', 'no', 'e', 'si', 's@@', 'v@@', 'egli@@', 'a@@', '.', '</s>']
2025-05-30 00:19:42,961 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:19:42,961 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:19:42,961 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo nel inverno e si sveglia.
2025-05-30 00:19:42,961 - INFO - joeynmt.training - Example #4
2025-05-30 00:19:42,961 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:19:42,961 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:19:42,961 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 'video', 'di', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:19:42,961 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:19:42,961 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:19:42,962 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è un video di disegno di quello che è successo negli ultimi 25 anni.
2025-05-30 00:19:46,429 - INFO - joeynmt.training - Epoch   8, Step:    70100, Batch Loss:     1.686801, Batch Acc: 0.481494, Tokens per Sec:    17152, Lr: 0.000210
2025-05-30 00:19:49,921 - INFO - joeynmt.training - Epoch   8, Step:    70200, Batch Loss:     1.863498, Batch Acc: 0.493645, Tokens per Sec:    19988, Lr: 0.000210
2025-05-30 00:19:53,390 - INFO - joeynmt.training - Epoch   8, Step:    70300, Batch Loss:     2.040964, Batch Acc: 0.487839, Tokens per Sec:    19537, Lr: 0.000210
2025-05-30 00:19:56,861 - INFO - joeynmt.training - Epoch   8, Step:    70400, Batch Loss:     1.888629, Batch Acc: 0.490427, Tokens per Sec:    19631, Lr: 0.000210
2025-05-30 00:20:00,340 - INFO - joeynmt.training - Epoch   8, Step:    70500, Batch Loss:     1.729386, Batch Acc: 0.492098, Tokens per Sec:    19316, Lr: 0.000210
2025-05-30 00:20:00,341 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:20:00,341 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:20:06,802 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.76, ppl:   5.84, acc:   0.51, generation: 6.4513[sec], evaluation: 0.0000[sec]
2025-05-30 00:20:06,802 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:20:07,390 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/68500.ckpt
2025-05-30 00:20:07,418 - INFO - joeynmt.training - Example #0
2025-05-30 00:20:07,418 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:20:07,418 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:20:07,418 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 's@@', 'li@@', 'de', 'per', 'con@@', 'ver@@', 't@@', 'ito', 'che', 'la', 'chiam@@', 'ano', 'E@@', 'is@@', 'k@@', 'app@@', ',', 'che', 'la', 'chiam@@', 'ano', '<unk>', 'E@@', 'is@@', 'co@@', ',', 'che', 'ha', 'avuto', 'la', 'più', 'grande', 'quantità', 'di', 'ter@@', 'ra@@', ',', 'il', '4@@', '8@@', '0@@', '%', 'per', 'la', 'più', 'grande', 'del', '4@@', '8@@', '0@@', '%', 'per', 'cento', 'è', 'stata', 'ri@@', 'ma@@', 'sto', 'in@@', 'gen@@', 'do', 'il', '4@@', '8@@', '0@@', '%', 'di', 'questi', 'due', 's@@', 'iti', 'è', 'stata', 'ri@@', 'ma@@', 'sto', 'per', 'la', 'prima', 'cosa', 'che', 'si', 'può', 'fare', 'con', 'la', 'stessa', 'cos@@', 'a@@', '.', '</s>']
2025-05-30 00:20:07,419 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:20:07,419 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:20:07,419 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due slide per convertito che la chiamano Eiskapp, che la chiamano <unk> Eisco, che ha avuto la più grande quantità di terra, il 480% per la più grande del 480% per cento è stata rimasto ingendo il 480% di questi due siti è stata rimasto per la prima cosa che si può fare con la stessa cosa.
2025-05-30 00:20:07,419 - INFO - joeynmt.training - Example #1
2025-05-30 00:20:07,419 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:20:07,419 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:20:07,419 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'molto', 'più', 'for@@', 'te@@', ',', 'la', 'più', 'grande', 'è', 'il', 'problema', 'speci@@', 'al@@', 'e@@', ',', 'perché', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:20:07,420 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:20:07,420 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:20:07,420 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto molto più forte, la più grande è il problema speciale, perché non è il dell<unk> Eises.
2025-05-30 00:20:07,420 - INFO - joeynmt.training - Example #2
2025-05-30 00:20:07,420 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:20:07,420 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:20:07,420 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'ris@@', 'i', 'd@@', 'ell@@', '<unk>', 'ar@@', 'c@@', 'ru@@', 'ci@@', 'ale', 'del', 'nostro', 'c@@', 'ru@@', 'ci@@', 'ale', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:20:07,421 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:20:07,421 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:20:07,421 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la crisi dell<unk> arcruciale del nostro cruciale globale.
2025-05-30 00:20:07,421 - INFO - joeynmt.training - Example #3
2025-05-30 00:20:07,421 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:20:07,421 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:20:07,421 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'sta', 'cresc@@', 'endo', 'in', 'in@@', 'ver@@', 'no', 'e', 'sc@@', 'att@@', 'ata', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:20:07,421 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:20:07,421 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:20:07,421 - INFO - joeynmt.training - 	Hypothesis: E sta crescendo in inverno e scattata in estate.
2025-05-30 00:20:07,422 - INFO - joeynmt.training - Example #4
2025-05-30 00:20:07,422 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:20:07,422 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:20:07,422 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'foto', 'di', 'un', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:20:07,422 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:20:07,422 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:20:07,422 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una foto di un disegno di quello che è successo negli ultimi 25 anni.
2025-05-30 00:20:10,863 - INFO - joeynmt.training - Epoch   8, Step:    70600, Batch Loss:     1.662734, Batch Acc: 0.498321, Tokens per Sec:    16937, Lr: 0.000210
2025-05-30 00:20:14,285 - INFO - joeynmt.training - Epoch   8, Step:    70700, Batch Loss:     1.803018, Batch Acc: 0.494228, Tokens per Sec:    20159, Lr: 0.000210
2025-05-30 00:20:17,691 - INFO - joeynmt.training - Epoch   8, Step:    70800, Batch Loss:     2.008560, Batch Acc: 0.492217, Tokens per Sec:    19807, Lr: 0.000210
2025-05-30 00:20:21,110 - INFO - joeynmt.training - Epoch   8, Step:    70900, Batch Loss:     1.733367, Batch Acc: 0.492395, Tokens per Sec:    20029, Lr: 0.000210
2025-05-30 00:20:24,505 - INFO - joeynmt.training - Epoch   8, Step:    71000, Batch Loss:     1.918340, Batch Acc: 0.494022, Tokens per Sec:    19666, Lr: 0.000210
2025-05-30 00:20:24,505 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:20:24,505 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:20:31,311 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.76, ppl:   5.81, acc:   0.51, generation: 6.7962[sec], evaluation: 0.0000[sec]
2025-05-30 00:20:31,311 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:20:31,925 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/64000.ckpt
2025-05-30 00:20:31,946 - INFO - joeynmt.training - Example #0
2025-05-30 00:20:31,946 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:20:31,946 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:20:31,947 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'la', '<unk>', 'g@@', 'all@@', 'eggi@@', 'amento', 'ar@@', 'c@@', 'ic@@', 'ano', 'che', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cento', 'è', 'cres@@', 'ciuto', 'per', 'cento', 'è', 'cres@@', 'ci@@', 'ut@@', 'a@@', '.', '</s>']
2025-05-30 00:20:31,947 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:20:31,947 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:20:31,947 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per osservare che la <unk> galleggiamento arcicano che per tre milioni di anni per la dimensione dell<unk> 48 stati per cento per cento è cresciuto per cento è cresciuta.
2025-05-30 00:20:31,947 - INFO - joeynmt.training - Example #1
2025-05-30 00:20:31,947 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:20:31,947 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:20:31,948 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'perché', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:20:31,948 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:20:31,948 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:20:31,948 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, non è abbastanza forte, perché non è il dell<unk> Eises.
2025-05-30 00:20:31,948 - INFO - joeynmt.training - Example #2
2025-05-30 00:20:31,948 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:20:31,948 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:20:31,948 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'di', 'ghi@@', 'accio', 'è', 'il', 'cuore', 'del', 'nostro', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:20:31,949 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:20:31,949 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:20:31,949 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il ghiaccio arktico di ghiaccio è il cuore del nostro sistema climatico globale.
2025-05-30 00:20:31,949 - INFO - joeynmt.training - Example #3
2025-05-30 00:20:31,949 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:20:31,949 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:20:31,949 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'in', 'in@@', 'ver@@', 'no', 'e', 'si', 'sci@@', 'oc@@', 'ca', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:20:31,949 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:20:31,949 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:20:31,949 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo in inverno e si sciocca in estate.
2025-05-30 00:20:31,950 - INFO - joeynmt.training - Example #4
2025-05-30 00:20:31,950 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:20:31,950 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:20:31,950 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:20:31,950 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:20:31,950 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:20:31,950 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è un disegno di quello che è successo negli ultimi 25 anni.
2025-05-30 00:20:35,437 - INFO - joeynmt.training - Epoch   8, Step:    71100, Batch Loss:     2.123816, Batch Acc: 0.491914, Tokens per Sec:    16474, Lr: 0.000210
2025-05-30 00:20:38,933 - INFO - joeynmt.training - Epoch   8, Step:    71200, Batch Loss:     1.978900, Batch Acc: 0.494659, Tokens per Sec:    19768, Lr: 0.000210
2025-05-30 00:20:42,426 - INFO - joeynmt.training - Epoch   8, Step:    71300, Batch Loss:     1.797388, Batch Acc: 0.492111, Tokens per Sec:    19636, Lr: 0.000210
2025-05-30 00:20:45,909 - INFO - joeynmt.training - Epoch   8, Step:    71400, Batch Loss:     1.751452, Batch Acc: 0.490914, Tokens per Sec:    19964, Lr: 0.000210
2025-05-30 00:20:49,366 - INFO - joeynmt.training - Epoch   8, Step:    71500, Batch Loss:     1.775652, Batch Acc: 0.495670, Tokens per Sec:    19974, Lr: 0.000210
2025-05-30 00:20:49,367 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:20:49,367 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:20:56,416 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.76, ppl:   5.82, acc:   0.51, generation: 7.0396[sec], evaluation: 0.0000[sec]
2025-05-30 00:20:56,846 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/69000.ckpt
2025-05-30 00:20:56,877 - INFO - joeynmt.training - Example #0
2025-05-30 00:20:56,878 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:20:56,878 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:20:56,878 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'con@@', 'serv@@', 'are', 'che', 'la', 'c@@', 'av@@', 'al@@', 'it@@', 'à@@', ',', 'che', 'la', 'chiam@@', 'ano', '<unk>', 'E@@', 'is@@', 'co@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', 'del', '4@@', '8@@', '0@@', '%', 'dei', 'limit@@', 'i', 'd@@', 'ell@@', '<unk>', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cent@@', 'o@@', '.', '</s>']
2025-05-30 00:20:56,878 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:20:56,878 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:20:56,878 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per conservare che la cavalità, che la chiamano <unk> Eisco, per tre milioni di anni per la più grande del 480% dei limiti dell<unk> 48 stati per cento per cento.
2025-05-30 00:20:56,878 - INFO - joeynmt.training - Example #1
2025-05-30 00:20:56,879 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:20:56,879 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:20:56,879 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'non', 'è', 'molto', 'più', 'for@@', 'te@@', ',', 'che', 'non', 'è', 'il', 'problema', 'in', 'cui', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'accio', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:20:56,879 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:20:56,879 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:20:56,879 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, non è molto più forte, che non è il problema in cui non è il dibattito del ghiaccio dell<unk> Eises.
2025-05-30 00:20:56,879 - INFO - joeynmt.training - Example #2
2025-05-30 00:20:56,880 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:20:56,880 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:20:56,880 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'l@@', '<unk>', 'ar@@', 'c@@', 'ob@@', 'al@@', 'g@@', 'ore', 'di', 'ghi@@', 'accio', 'e', 'il', 'nostro', 'cuore', 'c@@', 'atti@@', 'vo', 'del', 'nostro', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:20:56,880 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:20:56,880 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:20:56,880 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, l<unk> arcobalgore di ghiaccio e il nostro cuore cattivo del nostro sistema globale.
2025-05-30 00:20:56,880 - INFO - joeynmt.training - Example #3
2025-05-30 00:20:56,881 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:20:56,881 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:20:56,881 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'in', 'in@@', 'ver@@', 'no', 'e', 'si', 's@@', 'v@@', 'egli@@', 'a@@', '.', '</s>']
2025-05-30 00:20:56,881 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:20:56,881 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:20:56,881 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo in inverno e si sveglia.
2025-05-30 00:20:56,881 - INFO - joeynmt.training - Example #4
2025-05-30 00:20:56,881 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:20:56,881 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:20:56,882 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'foto', 'di', 'un', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:20:56,882 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:20:56,882 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:20:56,882 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una foto di un disegno di quello che è successo negli ultimi 25 anni.
2025-05-30 00:21:00,365 - INFO - joeynmt.training - Epoch   8, Step:    71600, Batch Loss:     2.095819, Batch Acc: 0.494092, Tokens per Sec:    16869, Lr: 0.000210
2025-05-30 00:21:03,858 - INFO - joeynmt.training - Epoch   8, Step:    71700, Batch Loss:     1.745234, Batch Acc: 0.491985, Tokens per Sec:    20166, Lr: 0.000210
2025-05-30 00:21:07,328 - INFO - joeynmt.training - Epoch   8, Step:    71800, Batch Loss:     1.763998, Batch Acc: 0.491644, Tokens per Sec:    19892, Lr: 0.000210
2025-05-30 00:21:10,793 - INFO - joeynmt.training - Epoch   8, Step:    71900, Batch Loss:     1.746051, Batch Acc: 0.499105, Tokens per Sec:    19505, Lr: 0.000210
2025-05-30 00:21:14,252 - INFO - joeynmt.training - Epoch   8, Step:    72000, Batch Loss:     1.841584, Batch Acc: 0.493370, Tokens per Sec:    19517, Lr: 0.000210
2025-05-30 00:21:14,252 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:21:14,252 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:21:20,991 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.76, ppl:   5.79, acc:   0.51, generation: 6.7288[sec], evaluation: 0.0000[sec]
2025-05-30 00:21:20,992 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:21:21,631 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/70000.ckpt
2025-05-30 00:21:21,663 - INFO - joeynmt.training - Example #0
2025-05-30 00:21:21,664 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:21:21,664 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:21:21,664 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'con@@', 'serv@@', 'are', 'che', 'la', 'p@@', 'elle', 'ar@@', 'c@@', 'op@@', 'pi@@', 'ch@@', 'e@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'dimen@@', 'sioni', 'è', 'successo', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'dimen@@', 'sioni', 'è', 'successo', 'in', 'cui', 'il', '4@@', '8@@', '0@@', '%', 'di', 'questi', 'due', 's@@', 'ett@@', 'e@@', '.', '</s>']
2025-05-30 00:21:21,665 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:21:21,665 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:21:21,665 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per conservare che la pelle arcoppiche, per tre milioni di anni di dimensioni è successo per tre milioni di anni di dimensioni è successo in cui il 480% di questi due sette.
2025-05-30 00:21:21,665 - INFO - joeynmt.training - Example #1
2025-05-30 00:21:21,665 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:21:21,665 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:21:21,665 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'che', 'è', 'molto', 'più', 'forte', 'la', 'prima', 'volta', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:21:21,666 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:21:21,666 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:21:21,666 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, che è molto più forte la prima volta che non è il dibattito del ghiaccio.
2025-05-30 00:21:21,666 - INFO - joeynmt.training - Example #2
2025-05-30 00:21:21,666 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:21:21,666 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:21:21,666 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'l@@', '<unk>', 'ar@@', 'c@@', 'ob@@', 'al@@', 'g@@', 'orit@@', 'mo', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:21:21,667 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:21:21,667 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:21:21,667 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, l<unk> arcobalgoritmo globale.
2025-05-30 00:21:21,667 - INFO - joeynmt.training - Example #3
2025-05-30 00:21:21,667 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:21:21,667 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:21:21,667 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'nel', 'in@@', 'ver@@', 'no', 'e', 'sp@@', 'ing@@', 'ere', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:21:21,667 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:21:21,668 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:21:21,668 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo nel inverno e spingere in estate.
2025-05-30 00:21:21,668 - INFO - joeynmt.training - Example #4
2025-05-30 00:21:21,668 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:21:21,668 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:21:21,668 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 't@@', 'ale', 'di', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:21:21,668 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:21:21,668 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:21:21,668 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è un tale di disegno di quello che è successo negli ultimi 25 anni.
2025-05-30 00:21:25,156 - INFO - joeynmt.training - Epoch   8, Step:    72100, Batch Loss:     1.801401, Batch Acc: 0.496381, Tokens per Sec:    16622, Lr: 0.000210
2025-05-30 00:21:28,614 - INFO - joeynmt.training - Epoch   8, Step:    72200, Batch Loss:     2.019182, Batch Acc: 0.493141, Tokens per Sec:    20050, Lr: 0.000210
2025-05-30 00:21:32,065 - INFO - joeynmt.training - Epoch   8, Step:    72300, Batch Loss:     1.832175, Batch Acc: 0.494747, Tokens per Sec:    19839, Lr: 0.000210
2025-05-30 00:21:35,518 - INFO - joeynmt.training - Epoch   8, Step:    72400, Batch Loss:     1.641624, Batch Acc: 0.492979, Tokens per Sec:    19741, Lr: 0.000210
2025-05-30 00:21:38,979 - INFO - joeynmt.training - Epoch   8, Step:    72500, Batch Loss:     1.835407, Batch Acc: 0.499857, Tokens per Sec:    20142, Lr: 0.000210
2025-05-30 00:21:38,980 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:21:38,980 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:21:45,967 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.75, ppl:   5.77, acc:   0.51, generation: 6.9766[sec], evaluation: 0.0000[sec]
2025-05-30 00:21:45,967 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:21:46,631 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/69500.ckpt
2025-05-30 00:21:46,661 - INFO - joeynmt.training - Example #0
2025-05-30 00:21:46,661 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:21:46,662 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:21:46,662 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'con@@', 'durre', 'che', 'il', 'ghi@@', 'acci@@', 'aio', 'di', 'ghi@@', 'accio', 'che', 'la', 'p@@', 'om@@', 'p@@', 'a', 'di', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', 'del', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cento', 'di', 'anni', 'è', 'stata', 'ri@@', 'vel@@', 'ata', 'il', '40', 'per', 'cento', 'del', '40', 'per', 'cento', 'è', 'stata', 'ri@@', 'vel@@', 'ata', 'il', '40', 'per', 'cento', 'è', 'stata', 'ri@@', 'vel@@', 'ata', 'il', '40', 'per', 'cento', 'è', 'stata', 'ri@@', 'vel@@', 'ata', 'il', 'livello', 'di', 'ghi@@', 'acci@@', 'aio', 'che', 'ha', 'fatto', 'il', '4@@', '8', 'per', 'cento', 'è', 'stata', 'ri@@', 'vel@@', 'ata', 'da', 'un', 'po@@', '<unk>']
2025-05-30 00:21:46,662 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:21:46,662 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:21:46,662 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per condurre che il ghiacciaio di ghiaccio che la pompa di tre milioni di anni per la più grande del 48 stati per cento per cento di anni è stata rivelata il 40 per cento del 40 per cento è stata rivelata il 40 per cento è stata rivelata il 40 per cento è stata rivelata il livello di ghiacciaio che ha fatto il 48 per cento è stata rivelata da un po<unk>
2025-05-30 00:21:46,662 - INFO - joeynmt.training - Example #1
2025-05-30 00:21:46,663 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:21:46,663 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:21:46,663 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'la', 'più', 'grande', 'di', 'questo', 'problema', 'speci@@', 'fic@@', 'o@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:21:46,663 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:21:46,663 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:21:46,663 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, la più grande di questo problema specifico, perché non è il dibattito dell<unk> Eises.
2025-05-30 00:21:46,663 - INFO - joeynmt.training - Example #2
2025-05-30 00:21:46,663 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:21:46,663 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:21:46,664 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'att@@', 'ica', 'del', 'ghi@@', 'accio', 'che', 'il', 'cuore', 'c@@', 'att@@', 'ico', 'del', 'nostro', 'sistema', 'globale', 'del', 'c@@', 'lim@@', 'as@@', 'sist@@', 'ente', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:21:46,664 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:21:46,664 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:21:46,664 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cattica del ghiaccio che il cuore cattico del nostro sistema globale del climassistente globale.
2025-05-30 00:21:46,664 - INFO - joeynmt.training - Example #3
2025-05-30 00:21:46,664 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:21:46,664 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:21:46,664 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'cominci@@', 'a', 'a', 'cresc@@', 'ere', 'nel', 'in@@', 'ver@@', 'no', 'e', 'si', 's@@', 'mon@@', 't@@', 'ato', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:21:46,665 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:21:46,665 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:21:46,665 - INFO - joeynmt.training - 	Hypothesis: E comincia a crescere nel inverno e si smontato in estate.
2025-05-30 00:21:46,665 - INFO - joeynmt.training - Example #4
2025-05-30 00:21:46,665 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:21:46,665 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:21:46,665 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'foto', 'di', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:21:46,666 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:21:46,666 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:21:46,666 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una foto di disegno di quello che è successo negli ultimi 25 anni.
2025-05-30 00:21:50,152 - INFO - joeynmt.training - Epoch   8, Step:    72600, Batch Loss:     1.742156, Batch Acc: 0.490010, Tokens per Sec:    16721, Lr: 0.000210
2025-05-30 00:21:53,598 - INFO - joeynmt.training - Epoch   8, Step:    72700, Batch Loss:     1.834831, Batch Acc: 0.490080, Tokens per Sec:    19461, Lr: 0.000210
2025-05-30 00:21:57,066 - INFO - joeynmt.training - Epoch   8, Step:    72800, Batch Loss:     1.698264, Batch Acc: 0.486364, Tokens per Sec:    19978, Lr: 0.000210
2025-05-30 00:22:00,537 - INFO - joeynmt.training - Epoch   8, Step:    72900, Batch Loss:     2.001994, Batch Acc: 0.488299, Tokens per Sec:    19775, Lr: 0.000210
2025-05-30 00:22:03,999 - INFO - joeynmt.training - Epoch   8, Step:    73000, Batch Loss:     1.803320, Batch Acc: 0.496701, Tokens per Sec:    19264, Lr: 0.000210
2025-05-30 00:22:04,000 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:22:04,000 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:22:11,143 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.75, ppl:   5.77, acc:   0.51, generation: 7.1365[sec], evaluation: 0.0000[sec]
2025-05-30 00:22:11,143 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:22:11,661 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/70500.ckpt
2025-05-30 00:22:11,681 - INFO - joeynmt.training - Example #0
2025-05-30 00:22:11,682 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:22:11,682 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:22:11,682 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'acci@@', 'aio', 'ar@@', 'ch@@', 'e@@', ',', 'che', 'è', 'stato', 'chiamato', 'il', 'ghi@@', 'acci@@', 'o@@', ',', 'che', 'si', 'è', 'ri@@', 'd@@', 'otto', 'stati', 'in', 'cui', 'il', '4@@', '8', 'stati', 'per', 'cento', 'è', 'stata', 'ri@@', 'd@@', 'otta', 'il', '4@@', '8', 'stati', 'per', 'cento', 'è', 'cres@@', 'ci@@', 'uta', 'il', '4@@', '8', 'stati', 'per', 'cento', 'è', 'stata', 'sc@@', 'att@@', 'ata', 'il', '4@@', '8', 'stati', 'di', '1@@', '5@@', '0@@', '.', '</s>']
2025-05-30 00:22:11,682 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:22:11,682 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:22:11,682 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per osservare che il ghiacciaio arche, che è stato chiamato il ghiaccio, che si è ridotto stati in cui il 48 stati per cento è stata ridotta il 48 stati per cento è cresciuta il 48 stati per cento è stata scattata il 48 stati di 150.
2025-05-30 00:22:11,682 - INFO - joeynmt.training - Example #1
2025-05-30 00:22:11,682 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:22:11,682 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:22:11,683 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'più', 'for@@', 'ti', 'della', 'prima', 'cosa', 'che', 'si', 'trova', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:22:11,683 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:22:11,683 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:22:11,683 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto più forti della prima cosa che si trova in un certo senso, perché non è il dibattito dell<unk> Eises.
2025-05-30 00:22:11,683 - INFO - joeynmt.training - Example #2
2025-05-30 00:22:11,683 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:22:11,683 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:22:11,683 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'ris@@', 'i', 'ar@@', 't@@', 'ici', 'è', 'la', 'c@@', 'ad@@', 'uta', 'del', 'nostro', 'cuore', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:22:11,684 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:22:11,684 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:22:11,684 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la crisi artici è la caduta del nostro cuore globale.
2025-05-30 00:22:11,684 - INFO - joeynmt.training - Example #3
2025-05-30 00:22:11,684 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:22:11,684 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:22:11,684 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'nel', 'in@@', 'ver@@', 'no@@', ',', 'e', 'si', 's@@', 'mon@@', 't@@', 'ato', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:22:11,684 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:22:11,684 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:22:11,684 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo nel inverno, e si smontato in estate.
2025-05-30 00:22:11,684 - INFO - joeynmt.training - Example #4
2025-05-30 00:22:11,685 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:22:11,685 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:22:11,685 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'tem@@', 'po@@', ',', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:22:11,685 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:22:11,685 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:22:11,685 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è un disegno di tempo, che è successo negli ultimi 25 anni.
2025-05-30 00:22:15,163 - INFO - joeynmt.training - Epoch   8, Step:    73100, Batch Loss:     1.706225, Batch Acc: 0.498435, Tokens per Sec:    16848, Lr: 0.000210
2025-05-30 00:22:18,630 - INFO - joeynmt.training - Epoch   8, Step:    73200, Batch Loss:     1.707107, Batch Acc: 0.494235, Tokens per Sec:    19593, Lr: 0.000210
2025-05-30 00:22:22,105 - INFO - joeynmt.training - Epoch   8, Step:    73300, Batch Loss:     1.753484, Batch Acc: 0.489995, Tokens per Sec:    19709, Lr: 0.000210
2025-05-30 00:22:25,572 - INFO - joeynmt.training - Epoch   8, Step:    73400, Batch Loss:     1.796130, Batch Acc: 0.497607, Tokens per Sec:    19771, Lr: 0.000210
2025-05-30 00:22:29,055 - INFO - joeynmt.training - Epoch   8, Step:    73500, Batch Loss:     1.948007, Batch Acc: 0.493777, Tokens per Sec:    19867, Lr: 0.000210
2025-05-30 00:22:29,055 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:22:29,055 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:22:36,427 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.75, ppl:   5.74, acc:   0.51, generation: 7.3617[sec], evaluation: 0.0000[sec]
2025-05-30 00:22:36,427 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:22:37,333 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/71500.ckpt
2025-05-30 00:22:37,424 - INFO - joeynmt.training - Example #0
2025-05-30 00:22:37,424 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:22:37,424 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:22:37,424 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'con@@', 'fr@@', 'ont@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'ch@@', 'ico', 'ar@@', 'ric@@', 'co', 'di', 'ghi@@', 'accio', 'che', 'ha', 'avuto', 'per', 'tre', 'milioni', 'di', 'anni', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'U@@', 'c@@', 'ci@@', 'a@@', ',', 'e', 'il', '4@@', '8@@', '0@@', '%', 'è', 'stato', 'ri@@', 'mos@@', 'so', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'cent@@', 'o@@', '.', '</s>']
2025-05-30 00:22:37,425 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:22:37,425 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:22:37,425 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per confrontare che il ghiaccio archico arricco di ghiaccio che ha avuto per tre milioni di anni la dimensione dell<unk> Uccia, e il 480% è stato rimosso per tre milioni di anni per cento.
2025-05-30 00:22:37,425 - INFO - joeynmt.training - Example #1
2025-05-30 00:22:37,425 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:22:37,426 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:22:37,426 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'for@@', 'ti', 'da', 'quella', 'che', 'è', 'la', 'più', 'grande', 'cos@@', 'ci@@', 'enza', 'di', 'questo', 'problema', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:22:37,426 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:22:37,426 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:22:37,426 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forti da quella che è la più grande coscienza di questo problema che non è il dibattito dell<unk> Eises.
2025-05-30 00:22:37,426 - INFO - joeynmt.training - Example #2
2025-05-30 00:22:37,426 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:22:37,426 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:22:37,427 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'c@@', 'aff@@', 'è', 'il', 'cuore', 'c@@', 'att@@', 'ic@@', 'lo', 'del', 'nostro', 'sistema', 'globale', 'di', 'c@@', 'au@@', 'se', 'del', 'nostro', 'sistema', 'globale', 'di', 'c@@', 'au@@', 'se', 'di', 'un', 'sistema', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:22:37,427 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:22:37,427 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:22:37,427 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il caffè il cuore catticlo del nostro sistema globale di cause del nostro sistema globale di cause di un sistema globale.
2025-05-30 00:22:37,427 - INFO - joeynmt.training - Example #3
2025-05-30 00:22:37,427 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:22:37,427 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:22:37,427 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'cominci@@', 'a', 'a', 'fare', 'un', 'po@@', '<unk>', 'e', 'si', 's@@', 'mon@@', 't@@', 'are', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:22:37,428 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:22:37,428 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:22:37,428 - INFO - joeynmt.training - 	Hypothesis: E comincia a fare un po<unk> e si smontare in estate.
2025-05-30 00:22:37,428 - INFO - joeynmt.training - Example #4
2025-05-30 00:22:37,428 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:22:37,428 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:22:37,428 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'tem@@', 'po@@', '.', '</s>']
2025-05-30 00:22:37,429 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:22:37,429 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:22:37,429 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è un disegno di tempo.
2025-05-30 00:22:41,421 - INFO - joeynmt.training - Epoch   8, Step:    73600, Batch Loss:     1.794919, Batch Acc: 0.485045, Tokens per Sec:    14142, Lr: 0.000210
2025-05-30 00:22:44,885 - INFO - joeynmt.training - Epoch   8, Step:    73700, Batch Loss:     1.784175, Batch Acc: 0.489009, Tokens per Sec:    19612, Lr: 0.000210
2025-05-30 00:22:48,350 - INFO - joeynmt.training - Epoch   8, Step:    73800, Batch Loss:     1.667009, Batch Acc: 0.493964, Tokens per Sec:    19322, Lr: 0.000210
2025-05-30 00:22:51,806 - INFO - joeynmt.training - Epoch   8, Step:    73900, Batch Loss:     1.789393, Batch Acc: 0.489824, Tokens per Sec:    19927, Lr: 0.000210
2025-05-30 00:22:55,249 - INFO - joeynmt.training - Epoch   8, Step:    74000, Batch Loss:     1.891664, Batch Acc: 0.488136, Tokens per Sec:    20070, Lr: 0.000210
2025-05-30 00:22:55,249 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:22:55,249 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:23:01,568 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.75, ppl:   5.74, acc:   0.51, generation: 6.3094[sec], evaluation: 0.0000[sec]
2025-05-30 00:23:02,028 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/71000.ckpt
2025-05-30 00:23:02,055 - INFO - joeynmt.training - Example #0
2025-05-30 00:23:02,055 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:23:02,056 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:23:02,056 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've@@', ',', 'per', 'la', 'prima', 'volta', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'c@@', 'ci@@', 'one', 'che', 'la', 'chiam@@', 'ano', '<unk>', 'E@@', 'is@@', 'k@@', ',', 'che', 'chiam@@', 'ano', 'il', '4@@', '8@@', '0@@', '%', 'dei', 'v@@', 'ari', 'd@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'e@@', 'ano', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'l@@', '<unk>', '8@@', '0@@', '<unk>', '.', '</s>']
2025-05-30 00:23:02,056 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:23:02,056 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:23:02,056 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive, per la prima volta che il ghiaccio arccione che la chiamano <unk> Eisk, che chiamano il 480% dei vari dell<unk> Oceano per tre milioni di anni per l<unk> 80<unk> .
2025-05-30 00:23:02,056 - INFO - joeynmt.training - Example #1
2025-05-30 00:23:02,057 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:23:02,057 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:23:02,057 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'che', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'ma', 'non', 'è', 'il', 'punto', 'di', 'vista', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:23:02,057 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:23:02,057 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:23:02,057 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte, che non è abbastanza forte, ma non è il punto di vista del ghiaccio.
2025-05-30 00:23:02,057 - INFO - joeynmt.training - Example #2
2025-05-30 00:23:02,058 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:23:02,058 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:23:02,058 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'ar@@', 'kt@@', 'ico', 'ar@@', 't@@', 'ico', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:23:02,058 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:23:02,058 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:23:02,058 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il ghiaccio arktico arktico artico globale.
2025-05-30 00:23:02,058 - INFO - joeynmt.training - Example #3
2025-05-30 00:23:02,058 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:23:02,058 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:23:02,058 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'nel', 'in@@', 'ver@@', 'no', 'e', 'dol@@', 'ce', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:23:02,059 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:23:02,059 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:23:02,059 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo nel inverno e dolce in estate.
2025-05-30 00:23:02,059 - INFO - joeynmt.training - Example #4
2025-05-30 00:23:02,059 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:23:02,059 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:23:02,059 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'un', 'di@@', 'seg@@', 'no', 'di', 'tem@@', 'po@@', '.', '</s>']
2025-05-30 00:23:02,060 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:23:02,060 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:23:02,060 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è un disegno di un disegno di tempo.
2025-05-30 00:23:05,551 - INFO - joeynmt.training - Epoch   8, Step:    74100, Batch Loss:     1.943709, Batch Acc: 0.497128, Tokens per Sec:    17049, Lr: 0.000210
2025-05-30 00:23:10,876 - INFO - joeynmt.training - Epoch   8, Step:    74200, Batch Loss:     1.836502, Batch Acc: 0.492858, Tokens per Sec:    13564, Lr: 0.000210
2025-05-30 00:23:15,870 - INFO - joeynmt.training - Epoch   8, Step:    74300, Batch Loss:     1.736104, Batch Acc: 0.497132, Tokens per Sec:    13792, Lr: 0.000210
2025-05-30 00:23:19,316 - INFO - joeynmt.training - Epoch   8, Step:    74400, Batch Loss:     1.912236, Batch Acc: 0.495052, Tokens per Sec:    19447, Lr: 0.000210
2025-05-30 00:23:22,764 - INFO - joeynmt.training - Epoch   8, Step:    74500, Batch Loss:     1.725743, Batch Acc: 0.489658, Tokens per Sec:    19889, Lr: 0.000210
2025-05-30 00:23:22,764 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:23:22,764 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:23:29,330 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.75, ppl:   5.75, acc:   0.51, generation: 6.5558[sec], evaluation: 0.0000[sec]
2025-05-30 00:23:29,786 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/72000.ckpt
2025-05-30 00:23:29,809 - INFO - joeynmt.training - Example #0
2025-05-30 00:23:29,809 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:23:29,809 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:23:29,809 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've@@', ',', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'c@@', 'ci@@', 'one', 'ar@@', 'ic@@', 'o@@', ',', 'che', 'chiam@@', 'ano', 'il', 'ghi@@', 'acci@@', 'o@@', ',', 'che', 'si', 'chiam@@', 'ano', 'il', '4@@', '8', 'stati', 'per', 'cento', 'di', 'anni', 'per', 'la', 'dimen@@', 'sione', 'è', 'stata', 'ri@@', 'd@@', 'otta', 'il', '40', 'per', 'cento', 'della', 'popolazione', 'è', 'cres@@', 'ci@@', 'uta', 'in', 'cui', 'si', 'è', 'ri@@', 'ma@@', 'sto', 'in@@', 'tor@@', 'no@@', '.', '</s>']
2025-05-30 00:23:29,810 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:23:29,810 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:23:29,810 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive, per osservare che il ghiaccio arccione arico, che chiamano il ghiaccio, che si chiamano il 48 stati per cento di anni per la dimensione è stata ridotta il 40 per cento della popolazione è cresciuta in cui si è rimasto intorno.
2025-05-30 00:23:29,810 - INFO - joeynmt.training - Example #1
2025-05-30 00:23:29,810 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:23:29,810 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:23:29,810 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'forte', 'la', 'prima', 'st@@', 'ag@@', 'ione', 'di', 'questo', 'problema', 'speci@@', 'ale', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:23:29,811 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:23:29,811 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:23:29,811 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte la prima stagione di questo problema speciale che non è il dibattito del ghiaccio.
2025-05-30 00:23:29,811 - INFO - joeynmt.training - Example #2
2025-05-30 00:23:29,811 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:23:29,811 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:23:29,811 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'cosa', 'più', 'importante', 'è', 'il', 'c@@', 'ru@@', 'ci@@', 'ale', 'del', 'nostro', 'sistema', 'c@@', 'lim@@', 'ati@@', 'vo@@', '.', '</s>']
2025-05-30 00:23:29,812 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:23:29,812 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:23:29,812 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cosa più importante è il cruciale del nostro sistema climativo.
2025-05-30 00:23:29,812 - INFO - joeynmt.training - Example #3
2025-05-30 00:23:29,812 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:23:29,812 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:23:29,812 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'cominci@@', 'a', 'a', 'a', 'cresc@@', 'ere', 'e', 'dol@@', 'or@@', 'os@@', 'o@@', '.', '</s>']
2025-05-30 00:23:29,812 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:23:29,812 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:23:29,813 - INFO - joeynmt.training - 	Hypothesis: Si comincia a a crescere e doloroso.
2025-05-30 00:23:29,813 - INFO - joeynmt.training - Example #4
2025-05-30 00:23:29,813 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:23:29,813 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:23:29,813 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'foto', 'di', 'un', 'di@@', 'seg@@', 'no', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:23:29,813 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:23:29,813 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:23:29,813 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una foto di un disegno che è successo negli ultimi 25 anni.
2025-05-30 00:23:33,316 - INFO - joeynmt.training - Epoch   8, Step:    74600, Batch Loss:     1.955539, Batch Acc: 0.498113, Tokens per Sec:    17553, Lr: 0.000210
2025-05-30 00:23:36,791 - INFO - joeynmt.training - Epoch   8, Step:    74700, Batch Loss:     1.588840, Batch Acc: 0.494979, Tokens per Sec:    20470, Lr: 0.000210
2025-05-30 00:23:41,079 - INFO - joeynmt.training - Epoch   8, Step:    74800, Batch Loss:     1.670599, Batch Acc: 0.497824, Tokens per Sec:    15702, Lr: 0.000210
2025-05-30 00:23:45,519 - INFO - joeynmt.training - Epoch   8, Step:    74900, Batch Loss:     1.714316, Batch Acc: 0.493243, Tokens per Sec:    15587, Lr: 0.000210
2025-05-30 00:23:48,964 - INFO - joeynmt.training - Epoch   8, Step:    75000, Batch Loss:     1.870318, Batch Acc: 0.493889, Tokens per Sec:    19409, Lr: 0.000210
2025-05-30 00:23:48,964 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:23:48,965 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:23:56,193 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.75, ppl:   5.76, acc:   0.51, generation: 7.2225[sec], evaluation: 0.0000[sec]
2025-05-30 00:23:56,805 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/72500.ckpt
2025-05-30 00:23:56,826 - INFO - joeynmt.training - Example #0
2025-05-30 00:23:56,827 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:23:56,827 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:23:56,827 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'ch@@', 'ico', 'che', 'la', 'più', 'ar@@', 'chi', 'che', 'chiam@@', 'ano', '<unk>', 'E@@', 'is@@', 'k@@', ',', 'che', 'chiam@@', 'ano', 'il', '4@@', '8', 'stati', 'per', 'cento', 'di', 'anni', 'la', 'dimen@@', 'sione', 'è', 'stata', 'sc@@', 'att@@', 'ata', 'il', '4@@', '8', 'stati', 'per', 'cento', 'di', '1@@', '5@@', '0@@', '.', '</s>']
2025-05-30 00:23:56,828 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:23:56,828 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:23:56,828 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per osservare che il ghiaccio archico che la più archi che chiamano <unk> Eisk, che chiamano il 48 stati per cento di anni la dimensione è stata scattata il 48 stati per cento di 150.
2025-05-30 00:23:56,828 - INFO - joeynmt.training - Example #1
2025-05-30 00:23:56,828 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:23:56,828 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:23:56,828 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'ma', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:23:56,828 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:23:56,829 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:23:56,829 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, ma non è abbastanza forte, perché non è il dibattito del ghiaccio.
2025-05-30 00:23:56,829 - INFO - joeynmt.training - Example #2
2025-05-30 00:23:56,829 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:23:56,829 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:23:56,829 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'ris@@', 'i', 'ar@@', 't@@', 'ici', 'ar@@', 'ch@@', 'e@@', ',', 'il', 'cuore', 'più', 'c@@', 'ru@@', 'ci@@', 'ale', 'del', 'nostro', 'sistema', 'cli@@', 'mat@@', 'ic@@', 'o@@', '.', '</s>']
2025-05-30 00:23:56,829 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:23:56,829 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:23:56,830 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la crisi artici arche, il cuore più cruciale del nostro sistema climatico.
2025-05-30 00:23:56,830 - INFO - joeynmt.training - Example #3
2025-05-30 00:23:56,830 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:23:56,830 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:23:56,830 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'cominci@@', 'a', 'a', 'a', 'fare', 'un', 'po@@', '<unk>', 'e', 'si', 's@@', 'v@@', 'egli@@', 'a@@', '.', '</s>']
2025-05-30 00:23:56,830 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:23:56,830 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:23:56,830 - INFO - joeynmt.training - 	Hypothesis: Si comincia a a fare un po<unk> e si sveglia.
2025-05-30 00:23:56,830 - INFO - joeynmt.training - Example #4
2025-05-30 00:23:56,831 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:23:56,831 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:23:56,831 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'una', 'foto', 'di', 'un', 'di@@', 'seg@@', 'no', 'di', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:23:56,831 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:23:56,831 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:23:56,831 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è una foto di un disegno di disegno di quello che è successo negli ultimi 25 anni.
2025-05-30 00:24:00,272 - INFO - joeynmt.training - Epoch   8, Step:    75100, Batch Loss:     2.025034, Batch Acc: 0.494452, Tokens per Sec:    16552, Lr: 0.000210
2025-05-30 00:24:03,715 - INFO - joeynmt.training - Epoch   8, Step:    75200, Batch Loss:     1.854666, Batch Acc: 0.494049, Tokens per Sec:    19575, Lr: 0.000210
2025-05-30 00:24:07,173 - INFO - joeynmt.training - Epoch   8, Step:    75300, Batch Loss:     1.608548, Batch Acc: 0.490316, Tokens per Sec:    19792, Lr: 0.000210
2025-05-30 00:24:10,643 - INFO - joeynmt.training - Epoch   8, Step:    75400, Batch Loss:     1.671178, Batch Acc: 0.494771, Tokens per Sec:    20255, Lr: 0.000210
2025-05-30 00:24:10,787 - INFO - joeynmt.training - Epoch   8: total training loss 17021.95
2025-05-30 00:24:10,787 - INFO - joeynmt.training - EPOCH 9
2025-05-30 00:24:14,122 - INFO - joeynmt.training - Epoch   9, Step:    75500, Batch Loss:     1.571066, Batch Acc: 0.515229, Tokens per Sec:    19834, Lr: 0.000210
2025-05-30 00:24:14,123 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:24:14,123 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:24:20,798 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.75, ppl:   5.75, acc:   0.51, generation: 6.6643[sec], evaluation: 0.0000[sec]
2025-05-30 00:24:21,279 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/73000.ckpt
2025-05-30 00:24:21,302 - INFO - joeynmt.training - Example #0
2025-05-30 00:24:21,303 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:24:21,303 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:24:21,303 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've@@', ',', 'per', 'osserv@@', 'are', 'che', 'l@@', '<unk>', 'et@@', 'ica', 'ar@@', 'ric@@', 'ca', 'che', 'ar@@', 'ric@@', 'chi@@', 'ata', 'a', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', '4@@', '8', 'stati', 'per', 'cento', 'dei', 'gruppi', 'di', 'persone', 'che', 'hanno', 'avuto', 'il', '4@@', '8', 'stati', 'per', 'cento', 'del', '4@@', '8', 'stati', 'per', 'cento', 'del', '2@@', '0@@', '%', 'è', 'stato', 'di@@', 'chiar@@', 'ato', 'di', 'un', '4@@', '8', 'per', 'cent@@', 'o@@', '.', '</s>']
2025-05-30 00:24:21,304 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:24:21,304 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:24:21,304 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive, per osservare che l<unk> etica arricca che arricchiata a tre milioni di anni per la più grande 48 stati per cento dei gruppi di persone che hanno avuto il 48 stati per cento del 48 stati per cento del 20% è stato dichiarato di un 48 per cento.
2025-05-30 00:24:21,304 - INFO - joeynmt.training - Example #1
2025-05-30 00:24:21,304 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:24:21,304 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:24:21,304 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'ma', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'perché', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:24:21,304 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:24:21,304 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:24:21,305 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, ma non è abbastanza forte, perché non è il dell<unk> Eises.
2025-05-30 00:24:21,305 - INFO - joeynmt.training - Example #2
2025-05-30 00:24:21,305 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:24:21,305 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:24:21,305 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'l@@', '<unk>', 'ar@@', 't@@', 'ica', 'ar@@', 'ric@@', 'ca', 'di', 'ghi@@', 'accio', 'che', 'il', 'cuore', 'c@@', 'atti@@', 'vo', 'del', 'nostro', 'sistema', 'cli@@', 'mat@@', 'ic@@', 'o@@', '.', '</s>']
2025-05-30 00:24:21,305 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:24:21,305 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:24:21,305 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, l<unk> artica arricca di ghiaccio che il cuore cattivo del nostro sistema climatico.
2025-05-30 00:24:21,306 - INFO - joeynmt.training - Example #3
2025-05-30 00:24:21,306 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:24:21,306 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:24:21,306 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'cominci@@', 'a', 'a', 'a', 'gi@@', 'rare', 'e', 'sp@@', 'os@@', 'a', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:24:21,306 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:24:21,306 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:24:21,306 - INFO - joeynmt.training - 	Hypothesis: E comincia a a girare e sposa in estate.
2025-05-30 00:24:21,306 - INFO - joeynmt.training - Example #4
2025-05-30 00:24:21,307 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:24:21,307 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:24:21,307 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'a@@', ',', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:24:21,307 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:24:21,307 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:24:21,307 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostra, è un disegno di quello che è successo negli ultimi 25 anni.
2025-05-30 00:24:24,805 - INFO - joeynmt.training - Epoch   9, Step:    75600, Batch Loss:     1.612489, Batch Acc: 0.509630, Tokens per Sec:    17067, Lr: 0.000210
2025-05-30 00:24:28,274 - INFO - joeynmt.training - Epoch   9, Step:    75700, Batch Loss:     1.759685, Batch Acc: 0.513636, Tokens per Sec:    19931, Lr: 0.000210
2025-05-30 00:24:31,743 - INFO - joeynmt.training - Epoch   9, Step:    75800, Batch Loss:     1.878354, Batch Acc: 0.512511, Tokens per Sec:    20108, Lr: 0.000210
2025-05-30 00:24:35,209 - INFO - joeynmt.training - Epoch   9, Step:    75900, Batch Loss:     1.570388, Batch Acc: 0.505599, Tokens per Sec:    20130, Lr: 0.000210
2025-05-30 00:24:38,643 - INFO - joeynmt.training - Epoch   9, Step:    76000, Batch Loss:     1.731419, Batch Acc: 0.506789, Tokens per Sec:    19285, Lr: 0.000210
2025-05-30 00:24:38,643 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:24:38,644 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:24:45,853 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.76, ppl:   5.79, acc:   0.51, generation: 7.1998[sec], evaluation: 0.0000[sec]
2025-05-30 00:24:45,865 - INFO - joeynmt.training - Example #0
2025-05-30 00:24:45,866 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:24:45,866 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:24:45,866 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'acci@@', 'aio', 'ar@@', 'di@@', 'te', 'di', 'ghi@@', 'accio', 'che', 'si', 'chiam@@', 'ano', '<unk>', 'O@@', 'c@@', 'ci@@', 'dent@@', 'e@@', ',', 'per', 'tre', 'milioni', 'di', 'an@@', 'ni@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'dimen@@', 'sioni', 'è', 'ri@@', 'ma@@', 'sto', 'per', 'tre', 'milioni', 'di', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:24:45,867 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:24:45,867 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:24:45,867 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per osservare che il ghiacciaio ardite di ghiaccio che si chiamano <unk> Occidente, per tre milioni di anni, per tre milioni di anni di dimensioni è rimasto per tre milioni di anni.
2025-05-30 00:24:45,867 - INFO - joeynmt.training - Example #1
2025-05-30 00:24:45,867 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:24:45,867 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:24:45,867 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'ma', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:24:45,868 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:24:45,868 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:24:45,868 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte, non è abbastanza forte, ma non è il dibattito del ghiaccio.
2025-05-30 00:24:45,868 - INFO - joeynmt.training - Example #2
2025-05-30 00:24:45,868 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:24:45,868 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:24:45,868 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'l@@', '<unk>', 'ar@@', 'c@@', 'is@@', 'co@@', ',', 'il', 'cuore', 'c@@', 'atti@@', 'vo', 'del', 'nostro', 'sistema', 'c@@', 'atti@@', 'vo', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:24:45,869 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:24:45,869 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:24:45,869 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, l<unk> arcisco, il cuore cattivo del nostro sistema cattivo globale.
2025-05-30 00:24:45,869 - INFO - joeynmt.training - Example #3
2025-05-30 00:24:45,869 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:24:45,869 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:24:45,869 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'cominci@@', 'a', 'a', 'a', 'cresc@@', 'ere', 'e', 'dol@@', 'c@@', 'e@@', ',', 'e', 'dol@@', 'c@@', 'e@@', '.', '</s>']
2025-05-30 00:24:45,870 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:24:45,870 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:24:45,870 - INFO - joeynmt.training - 	Hypothesis: Si comincia a a crescere e dolce, e dolce.
2025-05-30 00:24:45,870 - INFO - joeynmt.training - Example #4
2025-05-30 00:24:45,870 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:24:45,870 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:24:45,870 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'foto', 'di', 'un', 'di@@', 'seg@@', 'no', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:24:45,871 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:24:45,871 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:24:45,871 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una foto di un disegno che è successo negli ultimi 25 anni.
2025-05-30 00:24:49,353 - INFO - joeynmt.training - Epoch   9, Step:    76100, Batch Loss:     1.476172, Batch Acc: 0.510895, Tokens per Sec:    18797, Lr: 0.000210
2025-05-30 00:24:52,821 - INFO - joeynmt.training - Epoch   9, Step:    76200, Batch Loss:     1.831459, Batch Acc: 0.508398, Tokens per Sec:    19696, Lr: 0.000210
2025-05-30 00:24:56,298 - INFO - joeynmt.training - Epoch   9, Step:    76300, Batch Loss:     1.582284, Batch Acc: 0.507719, Tokens per Sec:    19864, Lr: 0.000210
2025-05-30 00:24:59,766 - INFO - joeynmt.training - Epoch   9, Step:    76400, Batch Loss:     1.682813, Batch Acc: 0.501969, Tokens per Sec:    19992, Lr: 0.000210
2025-05-30 00:25:03,181 - INFO - joeynmt.training - Epoch   9, Step:    76500, Batch Loss:     1.841828, Batch Acc: 0.505624, Tokens per Sec:    19843, Lr: 0.000210
2025-05-30 00:25:03,182 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:25:03,182 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:25:10,698 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.76, ppl:   5.82, acc:   0.51, generation: 7.5064[sec], evaluation: 0.0000[sec]
2025-05-30 00:25:10,709 - INFO - joeynmt.training - Example #0
2025-05-30 00:25:10,710 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:25:10,710 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:25:10,710 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'che', 'ha', 'avuto', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'dimen@@', 'sioni', 'che', 'chiam@@', 'ano', 'il', '4@@', '8', 'stati', 'per', 'cento', 'dei', 'v@@', 'ari', 'di', 'questi', 'due', 'gruppi', 'di', 'persone', 'che', 'hanno', 'avuto', 'il', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cent@@', 'o@@', '.', '</s>']
2025-05-30 00:25:10,710 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:25:10,711 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:25:10,711 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per osservare che il ghiaccio arktico che ha avuto per tre milioni di anni di dimensioni che chiamano il 48 stati per cento dei vari di questi due gruppi di persone che hanno avuto il 48 stati per cento per cento.
2025-05-30 00:25:10,711 - INFO - joeynmt.training - Example #1
2025-05-30 00:25:10,711 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:25:10,711 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:25:10,711 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'forte', 'la', 'prima', 'cosa', 'che', 'la', 'ser@@', 'ia', 'è', 'che', 'la', 'cosa', 'più', 'importante', 'è', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'accio', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:25:10,712 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:25:10,712 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:25:10,712 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte la prima cosa che la seria è che la cosa più importante è che non è il dibattito del ghiaccio dell<unk> Eises.
2025-05-30 00:25:10,712 - INFO - joeynmt.training - Example #2
2025-05-30 00:25:10,712 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:25:10,712 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:25:10,712 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'cosa', 'più', 'importante', 'è', 'che', 'il', 'cuore', 'del', 'nostro', 'cuore', 'c@@', 'ru@@', 'ci@@', 'ale', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:25:10,712 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:25:10,713 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:25:10,713 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cosa più importante è che il cuore del nostro cuore cruciale globale.
2025-05-30 00:25:10,713 - INFO - joeynmt.training - Example #3
2025-05-30 00:25:10,713 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:25:10,713 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:25:10,713 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'nel', 'in@@', 'ver@@', 'no@@', ',', 'e', 'si', 's@@', 'v@@', 'egli@@', 'a@@', '.', '</s>']
2025-05-30 00:25:10,713 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:25:10,713 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:25:10,714 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo nel inverno, e si sveglia.
2025-05-30 00:25:10,714 - INFO - joeynmt.training - Example #4
2025-05-30 00:25:10,714 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:25:10,714 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:25:10,714 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'di@@', 'seg@@', 'no', 'di', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:25:10,714 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:25:10,714 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:25:10,714 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è un disegno di disegno di ciò che è successo negli ultimi 25 anni.
2025-05-30 00:25:14,190 - INFO - joeynmt.training - Epoch   9, Step:    76600, Batch Loss:     1.973359, Batch Acc: 0.502964, Tokens per Sec:    19766, Lr: 0.000210
2025-05-30 00:25:17,650 - INFO - joeynmt.training - Epoch   9, Step:    76700, Batch Loss:     1.709569, Batch Acc: 0.506666, Tokens per Sec:    19649, Lr: 0.000210
2025-05-30 00:25:21,111 - INFO - joeynmt.training - Epoch   9, Step:    76800, Batch Loss:     1.898923, Batch Acc: 0.514096, Tokens per Sec:    20299, Lr: 0.000210
2025-05-30 00:25:24,570 - INFO - joeynmt.training - Epoch   9, Step:    76900, Batch Loss:     1.786930, Batch Acc: 0.504282, Tokens per Sec:    19989, Lr: 0.000210
2025-05-30 00:25:28,022 - INFO - joeynmt.training - Epoch   9, Step:    77000, Batch Loss:     1.731499, Batch Acc: 0.505761, Tokens per Sec:    20022, Lr: 0.000210
2025-05-30 00:25:28,022 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:25:28,022 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:25:35,043 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.77, ppl:   5.85, acc:   0.51, generation: 7.0106[sec], evaluation: 0.0000[sec]
2025-05-30 00:25:35,052 - INFO - joeynmt.training - Example #0
2025-05-30 00:25:35,053 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:25:35,053 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:25:35,053 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'con@@', 'v@@', 'inc@@', 'ere', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'c@@', 'ci@@', 'one', 'ar@@', 'ic@@', 'o@@', ',', 'che', 'ha', 'chiam@@', 'ato', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', '4@@', '8', 'stati', 'per', 'cento', 'delle', 'N@@', 'azioni', 'Uni@@', 'te', 'per', 'tre', 'milioni', 'di', 'anni', 'è', 'ri@@', 'ma@@', 'sto', 'per', 'ri@@', 'durre', 'il', '40', 'per', 'cento', 'del', '4@@', '8', 'stati', 'per', 'cento', 'del', '4@@', '8', 'per', 'cento', 'è', 'cres@@', 'ci@@', 'uta', 'al', '4@@', '8', 'per', 'cento', 'del', '4@@', '8', 'per', 'cento', 'del', '4@@', '8', 'per', 'cento', 'è', 'cres@@', 'ci@@', 'uta', 'in', 'un', 'certo', 'sen@@', 'so@@']
2025-05-30 00:25:35,054 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:25:35,054 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:25:35,054 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per convincere che il ghiaccio arccione arico, che ha chiamato per tre milioni di anni per la più grande 48 stati per cento delle Nazioni Unite per tre milioni di anni è rimasto per ridurre il 40 per cento del 48 stati per cento del 48 per cento è cresciuta al 48 per cento del 48 per cento del 48 per cento è cresciuta in un certo senso
2025-05-30 00:25:35,054 - INFO - joeynmt.training - Example #1
2025-05-30 00:25:35,054 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:25:35,054 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:25:35,054 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'ti', 'la', 'più', 'grande', 'è', 'la', 'prima', 'cosa', 'che', 'la', 'cosa', 'più', 'importante', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'accio', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:25:35,055 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:25:35,055 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:25:35,055 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forti la più grande è la prima cosa che la cosa più importante che non è il dibattito del ghiaccio dell<unk> Eises.
2025-05-30 00:25:35,055 - INFO - joeynmt.training - Example #2
2025-05-30 00:25:35,055 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:25:35,055 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:25:35,055 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'c@@', 'atti@@', 'vo', 'è', 'il', 'cuore', 'ar@@', 'c@@', 'atti@@', 'vo', 'del', 'nostro', 'sistema', 'c@@', 'ru@@', 'ci@@', 'ale', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:25:35,056 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:25:35,056 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:25:35,056 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cattivo è il cuore arcattivo del nostro sistema cruciale globale.
2025-05-30 00:25:35,056 - INFO - joeynmt.training - Example #3
2025-05-30 00:25:35,056 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:25:35,056 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:25:35,056 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'nel', 'in@@', 'ver@@', 'no@@', ',', 'e', 'si', 's@@', 'for@@', 'z@@', 'ano', 'in', 'est@@', 'i@@', '.', '</s>']
2025-05-30 00:25:35,057 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:25:35,057 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:25:35,057 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo nel inverno, e si sforzano in esti.
2025-05-30 00:25:35,057 - INFO - joeynmt.training - Example #4
2025-05-30 00:25:35,057 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:25:35,057 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:25:35,057 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'foto', 'di', 'un', 'di@@', 'seg@@', 'no', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:25:35,058 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:25:35,058 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:25:35,058 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una foto di un disegno che è successo negli ultimi 25 anni.
2025-05-30 00:25:38,533 - INFO - joeynmt.training - Epoch   9, Step:    77100, Batch Loss:     1.469674, Batch Acc: 0.504666, Tokens per Sec:    19715, Lr: 0.000210
2025-05-30 00:25:42,016 - INFO - joeynmt.training - Epoch   9, Step:    77200, Batch Loss:     1.688564, Batch Acc: 0.497153, Tokens per Sec:    19571, Lr: 0.000210
2025-05-30 00:25:45,473 - INFO - joeynmt.training - Epoch   9, Step:    77300, Batch Loss:     1.627329, Batch Acc: 0.500301, Tokens per Sec:    19738, Lr: 0.000210
2025-05-30 00:25:48,941 - INFO - joeynmt.training - Epoch   9, Step:    77400, Batch Loss:     1.829431, Batch Acc: 0.503393, Tokens per Sec:    19549, Lr: 0.000210
2025-05-30 00:25:52,410 - INFO - joeynmt.training - Epoch   9, Step:    77500, Batch Loss:     1.578277, Batch Acc: 0.499437, Tokens per Sec:    19209, Lr: 0.000210
2025-05-30 00:25:52,411 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:25:52,411 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:25:59,773 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.76, ppl:   5.80, acc:   0.51, generation: 7.3500[sec], evaluation: 0.0000[sec]
2025-05-30 00:25:59,785 - INFO - joeynmt.training - Example #0
2025-05-30 00:25:59,786 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:25:59,786 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:25:59,786 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'c@@', 'ic@@', 'lo', 'di', 'ghi@@', 'accio', 'ar@@', 'c@@', 'ci@@', 'one', 'di', 'ghi@@', 'accio', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'dimen@@', 'sioni', 'che', 'si', 'è', 'ri@@', 'vel@@', 'ato', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'dimen@@', 'sioni', 'è', 'ri@@', 'ma@@', 'sta', 'per', 'ri@@', 'durre', 'il', '40', 'per', 'cent@@', 'o@@', '.', '</s>']
2025-05-30 00:25:59,787 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:25:59,787 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:25:59,787 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per osservare che il ghiaccio arciclo di ghiaccio arccione di ghiaccio per tre milioni di anni di dimensioni che si è rivelato per tre milioni di anni di dimensioni è rimasta per ridurre il 40 per cento.
2025-05-30 00:25:59,787 - INFO - joeynmt.training - Example #1
2025-05-30 00:25:59,787 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:25:59,787 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:25:59,787 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'ma', 'non', 'è', 'il', 'problema', 'speci@@', 'al@@', 'e@@', ',', 'perché', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'ic@@', 'ona', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:25:59,788 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:25:59,788 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:25:59,788 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, non è abbastanza forte, ma non è il problema speciale, perché non è il dell<unk> icona del ghiaccio.
2025-05-30 00:25:59,788 - INFO - joeynmt.training - Example #2
2025-05-30 00:25:59,788 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:25:59,788 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:25:59,788 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'l@@', '<unk>', 'ar@@', 'c@@', 'is@@', 'co@@', ',', 'il', 'cuore', 'c@@', 'ru@@', 'ci@@', 'ale', 'del', 'nostro', 'sistema', 'c@@', 'lim@@', 'asi@@', '.', '</s>']
2025-05-30 00:25:59,789 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:25:59,789 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:25:59,789 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, l<unk> arcisco, il cuore cruciale del nostro sistema climasi.
2025-05-30 00:25:59,789 - INFO - joeynmt.training - Example #3
2025-05-30 00:25:59,789 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:25:59,789 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:25:59,789 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'in', 'in@@', 'ver@@', 'no@@', ',', 'e', 'si', 's@@', 'v@@', 'eg@@', 'lia', 'in', 'est@@', 'ate', 'est@@', 'ate', 'e', 'si', 'è', 'ri@@', 'vel@@', 'ato', 'che', 'il', 'nostro', 'modo', 'di', 'pensare', 'a', 'come', 'si', 'possa', 'fare', 'in', 'modo', 'da', 'far@@', 'e@@', '.', '</s>']
2025-05-30 00:25:59,790 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:25:59,790 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:25:59,790 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo in inverno, e si sveglia in estate estate e si è rivelato che il nostro modo di pensare a come si possa fare in modo da fare.
2025-05-30 00:25:59,790 - INFO - joeynmt.training - Example #4
2025-05-30 00:25:59,790 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:25:59,790 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:25:59,790 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'ri@@', 'fer@@', 'imento', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:25:59,791 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:25:59,791 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:25:59,791 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è un disegno di riferimento che è successo negli ultimi 25 anni.
2025-05-30 00:26:03,255 - INFO - joeynmt.training - Epoch   9, Step:    77600, Batch Loss:     1.734117, Batch Acc: 0.509341, Tokens per Sec:    19080, Lr: 0.000210
2025-05-30 00:26:06,699 - INFO - joeynmt.training - Epoch   9, Step:    77700, Batch Loss:     1.913158, Batch Acc: 0.501557, Tokens per Sec:    20059, Lr: 0.000210
2025-05-30 00:26:10,087 - INFO - joeynmt.training - Epoch   9, Step:    77800, Batch Loss:     1.523095, Batch Acc: 0.505837, Tokens per Sec:    19702, Lr: 0.000210
2025-05-30 00:26:13,526 - INFO - joeynmt.training - Epoch   9, Step:    77900, Batch Loss:     1.684267, Batch Acc: 0.506848, Tokens per Sec:    20319, Lr: 0.000210
2025-05-30 00:26:16,955 - INFO - joeynmt.training - Epoch   9, Step:    78000, Batch Loss:     1.618125, Batch Acc: 0.503974, Tokens per Sec:    20407, Lr: 0.000210
2025-05-30 00:26:16,955 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:26:16,955 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:26:23,413 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.75, ppl:   5.78, acc:   0.51, generation: 6.4482[sec], evaluation: 0.0000[sec]
2025-05-30 00:26:23,425 - INFO - joeynmt.training - Example #0
2025-05-30 00:26:23,425 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:26:23,426 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:26:23,426 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'c@@', 'ic@@', 'lo', 'di', 'ghi@@', 'accio', 'ar@@', 'cat@@', 'en@@', 'e@@', ',', 'che', 'chiam@@', 'ano', '<unk>', 'il', 'più', 'grande', '4@@', '8', 'stati', 'per', 'cento', 'di', 'popolazione', 'è', 'sc@@', 'ad@@', 'uta', 'al', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cento', 'del', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cento', 'del', '2@@', '4@@', '8', 'stati', 'per', 'cento', 'del', '2@@', '0@@', '%', 'di', 'questi', 'sono', 'stati', 'in', 'grado', 'di', 'ri@@', 'durre', 'il', '40', 'per', 'cento', 'del', '2@@', '5@@', '%', 'del', '2@@', '0@@', '%', 'di', 'questi', 'due', 's@@', 'essi@@', '.', '</s>']
2025-05-30 00:26:23,426 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:26:23,426 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:26:23,426 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per osservare che il ghiaccio arciclo di ghiaccio arcatene, che chiamano <unk> il più grande 48 stati per cento di popolazione è scaduta al 48 stati per cento per cento del 48 stati per cento per cento del 248 stati per cento del 20% di questi sono stati in grado di ridurre il 40 per cento del 25% del 20% di questi due sessi.
2025-05-30 00:26:23,426 - INFO - joeynmt.training - Example #1
2025-05-30 00:26:23,427 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:26:23,427 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:26:23,427 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'forte', 'la', 'cosa', 'più', 'importante', 'che', 'la', 'cosa', 'più', 'importante', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:26:23,427 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:26:23,427 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:26:23,427 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte la cosa più importante che la cosa più importante che non è il dibattito del ghiaccio.
2025-05-30 00:26:23,427 - INFO - joeynmt.training - Example #2
2025-05-30 00:26:23,428 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:26:23,428 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:26:23,428 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'c@@', 'ant@@', 'ino', 'ar@@', 't@@', 'ico', 'ar@@', 't@@', 'ico', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:26:23,428 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:26:23,428 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:26:23,428 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cantino artico artico globale.
2025-05-30 00:26:23,428 - INFO - joeynmt.training - Example #3
2025-05-30 00:26:23,429 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:26:23,429 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:26:23,429 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'nel', 'in@@', 'ver@@', 'no', 'e', 'sp@@', 'ing@@', 'eva', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:26:23,429 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:26:23,429 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:26:23,429 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo nel inverno e spingeva in estate.
2025-05-30 00:26:23,429 - INFO - joeynmt.training - Example #4
2025-05-30 00:26:23,429 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:26:23,430 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:26:23,430 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 't@@', 'asso', 'di', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:26:23,430 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:26:23,430 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:26:23,430 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è un tasso di disegno di quello che è successo negli ultimi 25 anni.
2025-05-30 00:26:26,823 - INFO - joeynmt.training - Epoch   9, Step:    78100, Batch Loss:     1.690578, Batch Acc: 0.505653, Tokens per Sec:    20025, Lr: 0.000147
2025-05-30 00:26:30,057 - INFO - joeynmt.training - Epoch   9, Step:    78200, Batch Loss:     1.788614, Batch Acc: 0.496316, Tokens per Sec:    20697, Lr: 0.000147
2025-05-30 00:26:33,455 - INFO - joeynmt.training - Epoch   9, Step:    78300, Batch Loss:     1.774383, Batch Acc: 0.508765, Tokens per Sec:    20941, Lr: 0.000147
2025-05-30 00:26:36,880 - INFO - joeynmt.training - Epoch   9, Step:    78400, Batch Loss:     1.880723, Batch Acc: 0.511381, Tokens per Sec:    19707, Lr: 0.000147
2025-05-30 00:26:40,316 - INFO - joeynmt.training - Epoch   9, Step:    78500, Batch Loss:     1.795098, Batch Acc: 0.504892, Tokens per Sec:    19453, Lr: 0.000147
2025-05-30 00:26:40,317 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:26:40,317 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:26:46,691 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.69, acc:   0.51, generation: 6.3652[sec], evaluation: 0.0000[sec]
2025-05-30 00:26:46,692 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:26:47,323 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/75000.ckpt
2025-05-30 00:26:47,346 - INFO - joeynmt.training - Example #0
2025-05-30 00:26:47,347 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:26:47,347 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:26:47,347 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'l@@', '<unk>', 'E@@', 'is@@', 'co@@', ',', 'che', 'l@@', '<unk>', 'ar@@', 'c@@', 'ic@@', 'l@@', 'o@@', ',', 'che', 'chiam@@', 'ano', '<unk>', 'O@@', 'c@@', 'e@@', 'a@@', ',', 'che', 'si', 'chiama', 'il', '4@@', '8', 'stati', 'per', 'cento', 'di', 'popolazione', 'è', 'sc@@', 'att@@', 'ata', 'al', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cento', 'di', 'l@@', '<unk>', 'altr@@', 'o@@', '.', '</s>']
2025-05-30 00:26:47,347 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:26:47,347 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:26:47,348 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per osservare che l<unk> Eisco, che l<unk> arciclo, che chiamano <unk> Ocea, che si chiama il 48 stati per cento di popolazione è scattata al 48 stati per cento per cento di l<unk> altro.
2025-05-30 00:26:47,348 - INFO - joeynmt.training - Example #1
2025-05-30 00:26:47,348 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:26:47,348 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:26:47,348 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'la', 'cosa', 'più', 'importante', 'di', 'questo', 'problema', 'speci@@', 'al@@', 'e@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:26:47,348 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:26:47,348 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:26:47,348 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, la cosa più importante di questo problema speciale, perché non è il dibattito del ghiaccio.
2025-05-30 00:26:47,348 - INFO - joeynmt.training - Example #2
2025-05-30 00:26:47,349 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:26:47,349 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:26:47,349 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'att@@', 'ica', 'ar@@', 't@@', 'ica', 'ar@@', 't@@', 'ica', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:26:47,349 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:26:47,349 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:26:47,349 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cattica artica artica globale.
2025-05-30 00:26:47,349 - INFO - joeynmt.training - Example #3
2025-05-30 00:26:47,349 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:26:47,349 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:26:47,350 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'in', 'in@@', 'ver@@', 'no', 'e', 'dol@@', 'c@@', 'e@@', '.', '</s>']
2025-05-30 00:26:47,350 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:26:47,350 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:26:47,350 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo in inverno e dolce.
2025-05-30 00:26:47,350 - INFO - joeynmt.training - Example #4
2025-05-30 00:26:47,350 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:26:47,350 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:26:47,350 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'che', 'è', 'stato', 'un', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:26:47,351 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:26:47,351 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:26:47,351 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è un disegno che è stato un disegno di quello che è successo negli ultimi 25 anni.
2025-05-30 00:26:50,850 - INFO - joeynmt.training - Epoch   9, Step:    78600, Batch Loss:     1.831224, Batch Acc: 0.510713, Tokens per Sec:    16769, Lr: 0.000147
2025-05-30 00:26:54,318 - INFO - joeynmt.training - Epoch   9, Step:    78700, Batch Loss:     1.711395, Batch Acc: 0.506619, Tokens per Sec:    19718, Lr: 0.000147
2025-05-30 00:26:57,783 - INFO - joeynmt.training - Epoch   9, Step:    78800, Batch Loss:     1.818249, Batch Acc: 0.511553, Tokens per Sec:    19615, Lr: 0.000147
2025-05-30 00:27:01,243 - INFO - joeynmt.training - Epoch   9, Step:    78900, Batch Loss:     1.901009, Batch Acc: 0.507569, Tokens per Sec:    19808, Lr: 0.000147
2025-05-30 00:27:04,707 - INFO - joeynmt.training - Epoch   9, Step:    79000, Batch Loss:     1.664536, Batch Acc: 0.510318, Tokens per Sec:    19826, Lr: 0.000147
2025-05-30 00:27:04,707 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:27:04,707 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:27:11,411 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.67, acc:   0.52, generation: 6.6939[sec], evaluation: 0.0000[sec]
2025-05-30 00:27:11,411 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:27:12,027 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/74500.ckpt
2025-05-30 00:27:12,046 - INFO - joeynmt.training - Example #0
2025-05-30 00:27:12,046 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:27:12,046 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:27:12,046 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'acci@@', 'aio', 'ar@@', 'ch@@', 'ic@@', 'o@@', ',', 'che', 'chiam@@', 'ano', '<unk>', 'E@@', 'is@@', 'co@@', ',', 'che', 'chiam@@', 'ano', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', 'del', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cento', 'per', 'cento', 'del', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cento', 'per', 'cent@@', 'o@@', '.', '</s>']
2025-05-30 00:27:12,047 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:27:12,047 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:27:12,047 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per osservare che il ghiacciaio archico, che chiamano <unk> Eisco, che chiamano per tre milioni di anni per la più grande del 48 stati per cento per cento per cento del 48 stati per cento per cento per cento.
2025-05-30 00:27:12,047 - INFO - joeynmt.training - Example #1
2025-05-30 00:27:12,047 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:27:12,047 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:27:12,047 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'forte', 'la', 'più', 'grande', 'è', 'la', 'prima', 'cosa', 'che', 'la', 'cosa', 'più', 'importante', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'd@@', 'ell@@', '<unk>', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:27:12,047 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:27:12,047 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:27:12,047 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte la più grande è la prima cosa che la cosa più importante che non è il dibattito dell<unk> ghiaccio.
2025-05-30 00:27:12,047 - INFO - joeynmt.training - Example #2
2025-05-30 00:27:12,048 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:27:12,048 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:27:12,048 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'ar@@', 'kt@@', 'ico', 'globale', 'che', 'ha', 'il', 'cuore', 'del', 'nostro', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:27:12,048 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:27:12,048 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:27:12,048 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il ghiaccio arktico arktico globale che ha il cuore del nostro sistema climatico globale.
2025-05-30 00:27:12,048 - INFO - joeynmt.training - Example #3
2025-05-30 00:27:12,048 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:27:12,048 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:27:12,048 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'in', 'in@@', 'ver@@', 'no@@', ',', 'e', 'si', 's@@', 'v@@', 'eg@@', 'lia', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:27:12,049 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:27:12,049 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:27:12,049 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo in inverno, e si sveglia in estate.
2025-05-30 00:27:12,049 - INFO - joeynmt.training - Example #4
2025-05-30 00:27:12,049 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:27:12,049 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:27:12,049 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 'di@@', 'seg@@', 'no', 'che', 'è', 'un', 'di@@', 'seg@@', 'no', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:27:12,049 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:27:12,049 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:27:12,050 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è un disegno che è un disegno che è successo negli ultimi 25 anni.
2025-05-30 00:27:15,501 - INFO - joeynmt.training - Epoch   9, Step:    79100, Batch Loss:     1.926886, Batch Acc: 0.510081, Tokens per Sec:    17099, Lr: 0.000147
2025-05-30 00:27:18,941 - INFO - joeynmt.training - Epoch   9, Step:    79200, Batch Loss:     1.714439, Batch Acc: 0.512324, Tokens per Sec:    19882, Lr: 0.000147
2025-05-30 00:27:22,412 - INFO - joeynmt.training - Epoch   9, Step:    79300, Batch Loss:     1.814662, Batch Acc: 0.502282, Tokens per Sec:    19701, Lr: 0.000147
2025-05-30 00:27:25,872 - INFO - joeynmt.training - Epoch   9, Step:    79400, Batch Loss:     1.790316, Batch Acc: 0.505666, Tokens per Sec:    19845, Lr: 0.000147
2025-05-30 00:27:29,330 - INFO - joeynmt.training - Epoch   9, Step:    79500, Batch Loss:     1.587062, Batch Acc: 0.507386, Tokens per Sec:    19782, Lr: 0.000147
2025-05-30 00:27:29,330 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:27:29,330 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:27:36,285 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.68, acc:   0.51, generation: 6.9486[sec], evaluation: 0.0000[sec]
2025-05-30 00:27:36,633 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/75500.ckpt
2025-05-30 00:27:36,651 - INFO - joeynmt.training - Example #0
2025-05-30 00:27:36,652 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:27:36,652 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:27:36,652 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'cat@@', 'ch@@', 'e@@', ',', 'che', 'la', 'chiam@@', 'ano', '<unk>', 'E@@', 'is@@', 'co@@', ',', 'che', 'chiam@@', 'ano', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', 'nazi@@', 'on@@', 'e@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'e', 'il', '4@@', '8', 'stati', 'per', 'cent@@', 'o@@', '.', '</s>']
2025-05-30 00:27:36,652 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:27:36,653 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:27:36,653 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per osservare che il ghiaccio arcatche, che la chiamano <unk> Eisco, che chiamano per tre milioni di anni per la più grande nazione, per tre milioni di anni e il 48 stati per cento.
2025-05-30 00:27:36,653 - INFO - joeynmt.training - Example #1
2025-05-30 00:27:36,653 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:27:36,653 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:27:36,653 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'forte', 'la', 'più', 'grande', 'è', 'la', 'più', 'grande', 'di', 'questo', 'problema', 'speci@@', 'al@@', 'e@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:27:36,653 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:27:36,654 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:27:36,654 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte la più grande è la più grande di questo problema speciale, perché non è il dibattito dell<unk> Eises.
2025-05-30 00:27:36,654 - INFO - joeynmt.training - Example #2
2025-05-30 00:27:36,654 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:27:36,654 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:27:36,654 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'c@@', 'aff@@', 'è', 'il', 'cuore', 'ar@@', 'ric@@', 'co', 'del', 'nostro', 'sistema', 'c@@', 'ru@@', 'ci@@', 'ale', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:27:36,654 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:27:36,655 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:27:36,655 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il caffè il cuore arricco del nostro sistema cruciale globale.
2025-05-30 00:27:36,655 - INFO - joeynmt.training - Example #3
2025-05-30 00:27:36,655 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:27:36,655 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:27:36,655 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'in', 'in@@', 'ver@@', 'no', 'e', 'si', 'sci@@', 'ogli@@', 'e@@', '.', '</s>']
2025-05-30 00:27:36,655 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:27:36,655 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:27:36,655 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo in inverno e si scioglie.
2025-05-30 00:27:36,656 - INFO - joeynmt.training - Example #4
2025-05-30 00:27:36,656 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:27:36,656 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:27:36,656 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'foto', 'di', 'un', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:27:36,656 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:27:36,656 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:27:36,656 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una foto di un disegno di quello che è successo negli ultimi 25 anni.
2025-05-30 00:27:40,097 - INFO - joeynmt.training - Epoch   9, Step:    79600, Batch Loss:     1.833375, Batch Acc: 0.511645, Tokens per Sec:    18576, Lr: 0.000147
2025-05-30 00:27:43,574 - INFO - joeynmt.training - Epoch   9, Step:    79700, Batch Loss:     1.890379, Batch Acc: 0.513185, Tokens per Sec:    19835, Lr: 0.000147
2025-05-30 00:27:47,040 - INFO - joeynmt.training - Epoch   9, Step:    79800, Batch Loss:     1.751892, Batch Acc: 0.507070, Tokens per Sec:    19390, Lr: 0.000147
2025-05-30 00:27:50,512 - INFO - joeynmt.training - Epoch   9, Step:    79900, Batch Loss:     1.927827, Batch Acc: 0.505779, Tokens per Sec:    19417, Lr: 0.000147
2025-05-30 00:27:53,958 - INFO - joeynmt.training - Epoch   9, Step:    80000, Batch Loss:     1.865629, Batch Acc: 0.509655, Tokens per Sec:    18940, Lr: 0.000147
2025-05-30 00:27:53,958 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:27:53,959 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:28:00,215 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.69, acc:   0.51, generation: 6.2471[sec], evaluation: 0.0000[sec]
2025-05-30 00:28:00,634 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/74000.ckpt
2025-05-30 00:28:00,653 - INFO - joeynmt.training - Example #0
2025-05-30 00:28:00,653 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:28:00,653 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:28:00,653 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've@@', ',', 'per', 'osserv@@', 'are', 'che', 'la', 'gente', 'ar@@', 'ric@@', 'ch@@', 'ezza', 'ar@@', 'ch@@', 'ic@@', 'a@@', ',', 'che', 'chiam@@', 'ano', 'il', '4@@', '8@@', '0@@', '%', 'dei', 'con@@', 'fin@@', 'i', 'd@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'ci@@', 'dent@@', 'e@@', ',', 'il', '4@@', '8@@', '0@@', '%', 'del', 'con@@', 'fine', 'a', 'il', '4@@', '8@@', '0@@', '%', 'del', 'li@@', 'mi@@', 'te', 'è', 'stato', 'p@@', 'om@@', 'p@@', 'e@@', 'fac@@', 'ent@@', 'e@@', '.', '</s>']
2025-05-30 00:28:00,654 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:28:00,654 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:28:00,654 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive, per osservare che la gente arricchezza archica, che chiamano il 480% dei confini dell<unk> Occidente, il 480% del confine a il 480% del limite è stato pompefacente.
2025-05-30 00:28:00,654 - INFO - joeynmt.training - Example #1
2025-05-30 00:28:00,654 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:28:00,654 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:28:00,654 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'forte', 'la', 'prima', 'volta', 'che', 'la', 'prima', 'volta', 'la', 'mia', 'st@@', 'abil@@', 'it@@', 'à@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:28:00,655 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:28:00,655 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:28:00,655 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte la prima volta che la prima volta la mia stabilità, perché non è il dibattito del ghiaccio.
2025-05-30 00:28:00,655 - INFO - joeynmt.training - Example #2
2025-05-30 00:28:00,655 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:28:00,655 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:28:00,655 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'c@@', 'aff@@', 'è', 'il', 'cuore', 'ar@@', 'ch@@', 'ico', 'ar@@', 'ric@@', 'co', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:28:00,656 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:28:00,656 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:28:00,656 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il caffè il cuore archico arricco globale.
2025-05-30 00:28:00,656 - INFO - joeynmt.training - Example #3
2025-05-30 00:28:00,656 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:28:00,656 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:28:00,656 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'nel', 'in@@', 'ver@@', 'no@@', ',', 'e', 'si', 'è', 's@@', 'om@@', 'mer@@', 'e@@', '.', '</s>']
2025-05-30 00:28:00,657 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:28:00,657 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:28:00,657 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo nel inverno, e si è sommere.
2025-05-30 00:28:00,657 - INFO - joeynmt.training - Example #4
2025-05-30 00:28:00,657 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:28:00,657 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:28:00,657 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'foto', 'di', 'un', 'di@@', 'seg@@', 'no', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:28:00,657 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:28:00,657 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:28:00,657 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una foto di un disegno che è successo negli ultimi 25 anni.
2025-05-30 00:28:04,152 - INFO - joeynmt.training - Epoch   9, Step:    80100, Batch Loss:     1.889060, Batch Acc: 0.513785, Tokens per Sec:    16973, Lr: 0.000147
2025-05-30 00:28:07,609 - INFO - joeynmt.training - Epoch   9, Step:    80200, Batch Loss:     1.549953, Batch Acc: 0.508508, Tokens per Sec:    19863, Lr: 0.000147
2025-05-30 00:28:11,088 - INFO - joeynmt.training - Epoch   9, Step:    80300, Batch Loss:     1.804791, Batch Acc: 0.506888, Tokens per Sec:    19637, Lr: 0.000147
2025-05-30 00:28:14,543 - INFO - joeynmt.training - Epoch   9, Step:    80400, Batch Loss:     1.561498, Batch Acc: 0.509225, Tokens per Sec:    19822, Lr: 0.000147
2025-05-30 00:28:17,960 - INFO - joeynmt.training - Epoch   9, Step:    80500, Batch Loss:     1.610776, Batch Acc: 0.508247, Tokens per Sec:    19770, Lr: 0.000147
2025-05-30 00:28:17,960 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:28:17,960 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:28:25,085 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.73, ppl:   5.65, acc:   0.52, generation: 7.1156[sec], evaluation: 0.0000[sec]
2025-05-30 00:28:25,086 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:28:25,670 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/73500.ckpt
2025-05-30 00:28:25,686 - INFO - joeynmt.training - Example #0
2025-05-30 00:28:25,686 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:28:25,686 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:28:25,686 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'per@@', 'dere', 'che', 'la', 'p@@', 'elle', 'ar@@', 'ric@@', 'ch@@', 'ic@@', 'ana', 'ar@@', 'ric@@', 'ca', 'che', 'aveva', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', 'nazione', 'della', 'N@@', 'azione', 'della', 'N@@', 'azione', 'è', 'stata', 'sc@@', 'att@@', 'ata', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', 'nazi@@', 'on@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:28:25,687 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:28:25,687 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:28:25,687 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per perdere che la pelle arricchicana arricca che aveva tre milioni di anni per la più grande nazione della Nazione della Nazione è stata scattata per tre milioni di anni per la più grande nazionale.
2025-05-30 00:28:25,687 - INFO - joeynmt.training - Example #1
2025-05-30 00:28:25,687 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:28:25,687 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:28:25,687 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'forte', 'la', 'prima', 'cosa', 'che', 'la', 'più', 'grande', 'di', 'questo', 'problema', 'speci@@', 'ale', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:28:25,688 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:28:25,688 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:28:25,688 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte la prima cosa che la più grande di questo problema speciale perché non è il dibattito del ghiaccio.
2025-05-30 00:28:25,688 - INFO - joeynmt.training - Example #2
2025-05-30 00:28:25,688 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:28:25,688 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:28:25,688 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'ris@@', 'i', 'ar@@', 'kt@@', 'ico', 'ar@@', 't@@', 'ico', 'globale', 'del', 'nostro', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:28:25,688 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:28:25,689 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:28:25,689 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la crisi arktico artico globale del nostro sistema climatico globale.
2025-05-30 00:28:25,689 - INFO - joeynmt.training - Example #3
2025-05-30 00:28:25,689 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:28:25,689 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:28:25,689 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'nel', 'in@@', 'ver@@', 'no@@', ',', 'e', 'si', 'sci@@', 'oc@@', 'ca', 'nel', 'est@@', 'at@@', 'o@@', '.', '</s>']
2025-05-30 00:28:25,689 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:28:25,689 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:28:25,689 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo nel inverno, e si sciocca nel estato.
2025-05-30 00:28:25,689 - INFO - joeynmt.training - Example #4
2025-05-30 00:28:25,690 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:28:25,690 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:28:25,690 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'una', 'foto', 'di', 'un', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:28:25,690 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:28:25,690 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:28:25,690 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è una foto di un disegno di quello che è successo negli ultimi 25 anni.
2025-05-30 00:28:29,119 - INFO - joeynmt.training - Epoch   9, Step:    80600, Batch Loss:     1.754215, Batch Acc: 0.509933, Tokens per Sec:    16974, Lr: 0.000147
2025-05-30 00:28:32,557 - INFO - joeynmt.training - Epoch   9, Step:    80700, Batch Loss:     1.900528, Batch Acc: 0.511851, Tokens per Sec:    20171, Lr: 0.000147
2025-05-30 00:28:36,008 - INFO - joeynmt.training - Epoch   9, Step:    80800, Batch Loss:     1.537715, Batch Acc: 0.509576, Tokens per Sec:    19901, Lr: 0.000147
2025-05-30 00:28:39,467 - INFO - joeynmt.training - Epoch   9, Step:    80900, Batch Loss:     1.898255, Batch Acc: 0.501863, Tokens per Sec:    19866, Lr: 0.000147
2025-05-30 00:28:42,916 - INFO - joeynmt.training - Epoch   9, Step:    81000, Batch Loss:     1.689484, Batch Acc: 0.510591, Tokens per Sec:    19768, Lr: 0.000147
2025-05-30 00:28:42,916 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:28:42,917 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:28:49,489 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.73, ppl:   5.63, acc:   0.52, generation: 6.5630[sec], evaluation: 0.0000[sec]
2025-05-30 00:28:49,490 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:28:50,126 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/78500.ckpt
2025-05-30 00:28:50,155 - INFO - joeynmt.training - Example #0
2025-05-30 00:28:50,155 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:28:50,155 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:28:50,156 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'ch@@', 'ic@@', 'o@@', ',', 'che', 'si', 'chiama', '<unk>', 'ghi@@', 'acci@@', 'o@@', ',', 'che', 'chiam@@', 'ano', 'per', 'tre', 'milioni', 'di', 'anni', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'e@@', 'ano', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', 'quantità', 'di', 'stati', 'in@@', 'vi@@', 'ati', 'per', 'tre', 'milioni', 'di', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:28:50,156 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:28:50,156 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:28:50,156 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per osservare che il ghiaccio archico, che si chiama <unk> ghiaccio, che chiamano per tre milioni di anni la dimensione dell<unk> Oceano per tre milioni di anni per la più grande quantità di stati inviati per tre milioni di anni.
2025-05-30 00:28:50,156 - INFO - joeynmt.training - Example #1
2025-05-30 00:28:50,156 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:28:50,157 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:28:50,157 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'forte', 'la', 'prima', 'cosa', 'che', 'la', 'più', 'grande', 'cos@@', 'a@@', ',', 'perché', 'non', 'è', 'il', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:28:50,157 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:28:50,157 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:28:50,157 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte la prima cosa che la più grande cosa, perché non è il dell<unk> Eises.
2025-05-30 00:28:50,157 - INFO - joeynmt.training - Example #2
2025-05-30 00:28:50,157 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:28:50,157 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:28:50,157 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'c@@', 'atti@@', 'vo', 'è', 'il', 'cuore', 'c@@', 'ru@@', 'ci@@', 'ale', 'del', 'nostro', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:28:50,158 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:28:50,158 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:28:50,158 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cattivo è il cuore cruciale del nostro sistema climatico globale.
2025-05-30 00:28:50,158 - INFO - joeynmt.training - Example #3
2025-05-30 00:28:50,158 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:28:50,158 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:28:50,158 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'nel', 'vent@@', 'o@@', ',', 'e', 'si', 'è', 's@@', 'om@@', 'in@@', 'ato', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:28:50,159 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:28:50,159 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:28:50,159 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo nel vento, e si è sominato in estate.
2025-05-30 00:28:50,159 - INFO - joeynmt.training - Example #4
2025-05-30 00:28:50,159 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:28:50,159 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:28:50,159 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'foto', 'di', 'un', 'tr@@', 'atto', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:28:50,160 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:28:50,160 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:28:50,160 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una foto di un tratto di quello che è successo negli ultimi 25 anni.
2025-05-30 00:28:53,644 - INFO - joeynmt.training - Epoch   9, Step:    81100, Batch Loss:     1.825932, Batch Acc: 0.505083, Tokens per Sec:    16079, Lr: 0.000147
2025-05-30 00:28:57,134 - INFO - joeynmt.training - Epoch   9, Step:    81200, Batch Loss:     1.826941, Batch Acc: 0.514655, Tokens per Sec:    20245, Lr: 0.000147
2025-05-30 00:29:00,599 - INFO - joeynmt.training - Epoch   9, Step:    81300, Batch Loss:     1.792271, Batch Acc: 0.505245, Tokens per Sec:    19369, Lr: 0.000147
2025-05-30 00:29:04,065 - INFO - joeynmt.training - Epoch   9, Step:    81400, Batch Loss:     1.895846, Batch Acc: 0.505411, Tokens per Sec:    19682, Lr: 0.000147
2025-05-30 00:29:07,540 - INFO - joeynmt.training - Epoch   9, Step:    81500, Batch Loss:     1.710327, Batch Acc: 0.510207, Tokens per Sec:    19995, Lr: 0.000147
2025-05-30 00:29:07,540 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:29:07,540 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:29:13,648 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.73, ppl:   5.65, acc:   0.52, generation: 6.0984[sec], evaluation: 0.0000[sec]
2025-05-30 00:29:14,083 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/80000.ckpt
2025-05-30 00:29:14,112 - INFO - joeynmt.training - Example #0
2025-05-30 00:29:14,112 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:29:14,112 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:29:14,112 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've@@', ',', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'c@@', 'ru@@', 'ci@@', 'ale', 'che', 'si', 'chiama', '<unk>', 'ghi@@', 'acci@@', 'aio', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'grandi', 'stati', 'in', 'cui', 'si', 'è', 'ri@@', 'vel@@', 'ato', 'il', '4@@', '8@@', '0@@', '%', 'per', 'cent@@', 'o@@', '.', '</s>']
2025-05-30 00:29:14,113 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:29:14,113 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:29:14,113 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive, per osservare che il ghiaccio arcruciale che si chiama <unk> ghiacciaio per tre milioni di anni di grandi stati in cui si è rivelato il 480% per cento.
2025-05-30 00:29:14,113 - INFO - joeynmt.training - Example #1
2025-05-30 00:29:14,113 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:29:14,113 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:29:14,113 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'la', 'prima', 'cosa', 'che', 'la', 'cosa', 'più', 'importante', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:29:14,114 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:29:14,114 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:29:14,114 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, la prima cosa che la cosa più importante che non è il dibattito del ghiaccio.
2025-05-30 00:29:14,114 - INFO - joeynmt.training - Example #2
2025-05-30 00:29:14,114 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:29:14,114 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:29:14,114 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'aff@@', 'è', 'il', 'cuore', 'ar@@', 'ric@@', 'co', 'di', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:29:14,115 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:29:14,115 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:29:14,115 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la caffè il cuore arricco di ghiaccio arktico globale.
2025-05-30 00:29:14,115 - INFO - joeynmt.training - Example #3
2025-05-30 00:29:14,115 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:29:14,115 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:29:14,115 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'nel', 'in@@', 'ver@@', 'no', 'e', 'sp@@', 'um@@', 'ento', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:29:14,116 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:29:14,116 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:29:14,116 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo nel inverno e spumento in estate.
2025-05-30 00:29:14,116 - INFO - joeynmt.training - Example #4
2025-05-30 00:29:14,116 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:29:14,116 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:29:14,116 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'ri@@', 'fi@@', 'uti', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:29:14,116 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:29:14,116 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:29:14,116 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è un disegno di rifiuti che è successo negli ultimi 25 anni.
2025-05-30 00:29:17,585 - INFO - joeynmt.training - Epoch   9, Step:    81600, Batch Loss:     1.771189, Batch Acc: 0.511714, Tokens per Sec:    17275, Lr: 0.000147
2025-05-30 00:29:21,030 - INFO - joeynmt.training - Epoch   9, Step:    81700, Batch Loss:     1.737112, Batch Acc: 0.507450, Tokens per Sec:    20265, Lr: 0.000147
2025-05-30 00:29:24,513 - INFO - joeynmt.training - Epoch   9, Step:    81800, Batch Loss:     1.846091, Batch Acc: 0.511639, Tokens per Sec:    20358, Lr: 0.000147
2025-05-30 00:29:27,977 - INFO - joeynmt.training - Epoch   9, Step:    81900, Batch Loss:     1.853415, Batch Acc: 0.509025, Tokens per Sec:    20144, Lr: 0.000147
2025-05-30 00:29:31,428 - INFO - joeynmt.training - Epoch   9, Step:    82000, Batch Loss:     1.684559, Batch Acc: 0.508802, Tokens per Sec:    19556, Lr: 0.000147
2025-05-30 00:29:31,429 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:29:31,429 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:29:38,051 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.73, ppl:   5.64, acc:   0.52, generation: 6.6124[sec], evaluation: 0.0000[sec]
2025-05-30 00:29:50,025 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/79500.ckpt
2025-05-30 00:29:50,046 - INFO - joeynmt.training - Example #0
2025-05-30 00:29:50,046 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:29:50,046 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:29:50,046 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'ar@@', 'ch@@', 'e@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'grandi', 'stati', 'in', 'cui', 'l@@', '<unk>', 'unico', 'momento', 'in', 'cui', 'il', '4@@', '8@@', '8@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'anni', 'di', 'grandi', 'stati', 'in', 'grado', 'di', 'ri@@', 'fer@@', 'im@@', 'enti', 'per', 'il', '40', 'per', 'cento', 'è', 'stato', 'ri@@', 'ma@@', 'sto', 'per', 'cent@@', 'o@@', '.', '</s>']
2025-05-30 00:29:50,047 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:29:50,047 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:29:50,047 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per osservare che il ghiaccio arktico arche, per tre milioni di anni di grandi stati in cui l<unk> unico momento in cui il 488, per tre milioni di anni di anni di grandi stati in grado di riferimenti per il 40 per cento è stato rimasto per cento.
2025-05-30 00:29:50,047 - INFO - joeynmt.training - Example #1
2025-05-30 00:29:50,047 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:29:50,048 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:29:50,048 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'ma', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:29:50,048 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:29:50,048 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:29:50,048 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, non è abbastanza forte, ma non è il dibattito del ghiaccio.
2025-05-30 00:29:50,048 - INFO - joeynmt.training - Example #2
2025-05-30 00:29:50,048 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:29:50,048 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:29:50,048 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'di', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:29:50,049 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:29:50,049 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:29:50,049 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il ghiaccio arktico di ghiaccio arktico globale.
2025-05-30 00:29:50,049 - INFO - joeynmt.training - Example #3
2025-05-30 00:29:50,049 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:29:50,049 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:29:50,049 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'n@@', 'ell@@', '<unk>', 'in@@', 'ver@@', 'no', 'e', 'si', 'è', 's@@', 'ì@@', '.', '</s>']
2025-05-30 00:29:50,050 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:29:50,050 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:29:50,050 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo nell<unk> inverno e si è sì.
2025-05-30 00:29:50,050 - INFO - joeynmt.training - Example #4
2025-05-30 00:29:50,050 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:29:50,050 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:29:50,050 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'foto', 'di', 'un', 'tr@@', 'atto', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:29:50,050 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:29:50,050 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:29:50,051 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una foto di un tratto di quello che è successo negli ultimi 25 anni.
2025-05-30 00:29:53,548 - INFO - joeynmt.training - Epoch   9, Step:    82100, Batch Loss:     1.847797, Batch Acc: 0.509139, Tokens per Sec:     4459, Lr: 0.000147
2025-05-30 00:29:57,005 - INFO - joeynmt.training - Epoch   9, Step:    82200, Batch Loss:     1.824569, Batch Acc: 0.508762, Tokens per Sec:    18935, Lr: 0.000147
2025-05-30 00:30:00,477 - INFO - joeynmt.training - Epoch   9, Step:    82300, Batch Loss:     2.073526, Batch Acc: 0.506104, Tokens per Sec:    20275, Lr: 0.000147
2025-05-30 00:30:03,922 - INFO - joeynmt.training - Epoch   9, Step:    82400, Batch Loss:     1.608160, Batch Acc: 0.510932, Tokens per Sec:    19863, Lr: 0.000147
2025-05-30 00:30:07,354 - INFO - joeynmt.training - Epoch   9, Step:    82500, Batch Loss:     1.601372, Batch Acc: 0.507083, Tokens per Sec:    20352, Lr: 0.000147
2025-05-30 00:30:07,354 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:30:07,355 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:30:13,283 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.73, ppl:   5.63, acc:   0.52, generation: 5.9189[sec], evaluation: 0.0000[sec]
2025-05-30 00:30:13,283 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:30:13,923 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/79000.ckpt
2025-05-30 00:30:13,946 - INFO - joeynmt.training - Example #0
2025-05-30 00:30:13,946 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:30:13,946 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:30:13,946 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'ar@@', 'ch@@', 'e@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'ci@@', 'dent@@', 'e@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'persone', 'che', 'si', 'è', 'ri@@', 'vel@@', 'ato', 'il', '4@@', '8@@', '0@@', '%', 'per', 'cent@@', 'o@@', '.', '</s>']
2025-05-30 00:30:13,947 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:30:13,947 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:30:13,947 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per osservare che il ghiaccio arktico arche, per tre milioni di anni la dimensione dell<unk> Occidente, per tre milioni di anni di persone che si è rivelato il 480% per cento.
2025-05-30 00:30:13,947 - INFO - joeynmt.training - Example #1
2025-05-30 00:30:13,947 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:30:13,947 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:30:13,947 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'ma', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'accio', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:30:13,947 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:30:13,947 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:30:13,947 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, ma non è abbastanza forte, perché non è il dibattito del ghiaccio del ghiaccio.
2025-05-30 00:30:13,947 - INFO - joeynmt.training - Example #2
2025-05-30 00:30:13,948 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:30:13,948 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:30:13,948 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'c@@', 'aff@@', 'è', 'la', 'c@@', 'app@@', 'ello', 'del', 'nostro', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:30:13,948 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:30:13,948 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:30:13,948 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il caffè la cappello del nostro sistema climatico globale.
2025-05-30 00:30:13,948 - INFO - joeynmt.training - Example #3
2025-05-30 00:30:13,948 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:30:13,948 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:30:13,948 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'in', 'in@@', 'ver@@', 'no', 'e', 'si', 'è', 's@@', 'fortun@@', 'at@@', 'o@@', '.', '</s>']
2025-05-30 00:30:13,949 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:30:13,949 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:30:13,949 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo in inverno e si è sfortunato.
2025-05-30 00:30:13,949 - INFO - joeynmt.training - Example #4
2025-05-30 00:30:13,949 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:30:13,949 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:30:13,949 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:30:13,950 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:30:13,950 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:30:13,950 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è un disegno che è successo negli ultimi 25 anni.
2025-05-30 00:30:17,397 - INFO - joeynmt.training - Epoch   9, Step:    82600, Batch Loss:     1.941479, Batch Acc: 0.518712, Tokens per Sec:    16219, Lr: 0.000147
2025-05-30 00:30:20,834 - INFO - joeynmt.training - Epoch   9, Step:    82700, Batch Loss:     1.957258, Batch Acc: 0.504799, Tokens per Sec:    20589, Lr: 0.000147
2025-05-30 00:30:24,224 - INFO - joeynmt.training - Epoch   9, Step:    82800, Batch Loss:     1.928588, Batch Acc: 0.508938, Tokens per Sec:    19941, Lr: 0.000147
2025-05-30 00:30:27,692 - INFO - joeynmt.training - Epoch   9, Step:    82900, Batch Loss:     1.908795, Batch Acc: 0.506191, Tokens per Sec:    19500, Lr: 0.000147
2025-05-30 00:30:31,163 - INFO - joeynmt.training - Epoch   9, Step:    83000, Batch Loss:     1.991675, Batch Acc: 0.505147, Tokens per Sec:    19566, Lr: 0.000147
2025-05-30 00:30:31,163 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:30:31,163 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:30:37,646 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.72, ppl:   5.61, acc:   0.52, generation: 6.4732[sec], evaluation: 0.0000[sec]
2025-05-30 00:30:37,646 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:30:38,365 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/80500.ckpt
2025-05-30 00:30:38,395 - INFO - joeynmt.training - Example #0
2025-05-30 00:30:38,395 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:30:38,396 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:30:38,396 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'l@@', '<unk>', 'ar@@', 'c@@', 'ic@@', 'lo', 'di', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'a@@', 'in@@', ',', 'che', 'per', 'tre', 'milioni', 'di', 'anni', 'la', 'più', 'grande', '4@@', '8@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'li@@', 'velli', 'di', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cento', 'per', 'cento', 'è', 'stato', 'ri@@', 'ma@@', 'sto', 'in@@', 'cor@@', 'por@@', 'ato', 'al', '4@@', '8@@', '.', '</s>']
2025-05-30 00:30:38,396 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:30:38,396 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:30:38,396 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per osservare che l<unk> arciclo di ghiaccio arktain, che per tre milioni di anni la più grande 48, per tre milioni di anni di livelli di 48 stati per cento per cento per cento è stato rimasto incorporato al 48.
2025-05-30 00:30:38,396 - INFO - joeynmt.training - Example #1
2025-05-30 00:30:38,397 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:30:38,397 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:30:38,397 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'ma', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:30:38,397 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:30:38,397 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:30:38,397 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, ma non è abbastanza forte, perché non è il dibattito del ghiaccio.
2025-05-30 00:30:38,397 - INFO - joeynmt.training - Example #2
2025-05-30 00:30:38,398 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:30:38,398 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:30:38,398 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'l@@', '<unk>', 'ar@@', 'kt@@', 'ico', 'di', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:30:38,398 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:30:38,398 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:30:38,398 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, l<unk> arktico di ghiaccio arktico globale.
2025-05-30 00:30:38,398 - INFO - joeynmt.training - Example #3
2025-05-30 00:30:38,398 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:30:38,399 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:30:38,399 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'in', 'in@@', 'ver@@', 'no', 'e', 'si', 's@@', 'om@@', 'br@@', 'ato', 'in', 'est@@', 'i@@', '.', '</s>']
2025-05-30 00:30:38,399 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:30:38,399 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:30:38,399 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo in inverno e si sombrato in esti.
2025-05-30 00:30:38,399 - INFO - joeynmt.training - Example #4
2025-05-30 00:30:38,399 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:30:38,399 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:30:38,399 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 'di@@', 'seg@@', 'no', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:30:38,400 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:30:38,400 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:30:38,400 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è un disegno che è successo negli ultimi 25 anni.
2025-05-30 00:30:41,884 - INFO - joeynmt.training - Epoch   9, Step:    83100, Batch Loss:     1.698773, Batch Acc: 0.505771, Tokens per Sec:    16337, Lr: 0.000147
2025-05-30 00:30:45,327 - INFO - joeynmt.training - Epoch   9, Step:    83200, Batch Loss:     1.703581, Batch Acc: 0.504306, Tokens per Sec:    19939, Lr: 0.000147
2025-05-30 00:30:48,777 - INFO - joeynmt.training - Epoch   9, Step:    83300, Batch Loss:     1.820511, Batch Acc: 0.503125, Tokens per Sec:    19484, Lr: 0.000147
2025-05-30 00:30:52,229 - INFO - joeynmt.training - Epoch   9, Step:    83400, Batch Loss:     1.715227, Batch Acc: 0.505779, Tokens per Sec:    19929, Lr: 0.000147
2025-05-30 00:30:55,688 - INFO - joeynmt.training - Epoch   9, Step:    83500, Batch Loss:     1.642898, Batch Acc: 0.510480, Tokens per Sec:    19922, Lr: 0.000147
2025-05-30 00:30:55,689 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:30:55,689 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:31:02,312 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.72, ppl:   5.60, acc:   0.52, generation: 6.6130[sec], evaluation: 0.0000[sec]
2025-05-30 00:31:02,313 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:31:02,978 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/81500.ckpt
2025-05-30 00:31:03,003 - INFO - joeynmt.training - Example #0
2025-05-30 00:31:03,004 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:31:03,004 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:31:03,004 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'ar@@', 'ch@@', 'e@@', ',', 'che', 'chiam@@', 'ano', '<unk>', 'ghi@@', 'acci@@', 'o@@', ',', 'che', 'aveva', 'tre', 'milioni', 'di', 'anni', 'la', 'più', 'grande', 'del', '4@@', '8@@', ',', 'il', '4@@', '8@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'ricerca', 'e', 'il', '4@@', '8@@', '0@@', '.', '</s>']
2025-05-30 00:31:03,004 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:31:03,004 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:31:03,005 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per osservare che il ghiaccio arktico arche, che chiamano <unk> ghiaccio, che aveva tre milioni di anni la più grande del 48, il 48, per tre milioni di anni di ricerca e il 480.
2025-05-30 00:31:03,005 - INFO - joeynmt.training - Example #1
2025-05-30 00:31:03,005 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:31:03,005 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:31:03,005 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'ma', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'accio', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:31:03,005 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:31:03,005 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:31:03,005 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, ma non è abbastanza forte, perché non è il dibattito del ghiaccio dell<unk> Eises.
2025-05-30 00:31:03,006 - INFO - joeynmt.training - Example #2
2025-05-30 00:31:03,006 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:31:03,006 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:31:03,006 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'c@@', 'ru@@', 'ci@@', 'ale', 'ar@@', 'kt@@', 'ico', 'di', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:31:03,006 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:31:03,006 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:31:03,006 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cruciale arktico di ghiaccio arktico globale.
2025-05-30 00:31:03,006 - INFO - joeynmt.training - Example #3
2025-05-30 00:31:03,007 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:31:03,007 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:31:03,007 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'in', 'in@@', 'ver@@', 'no', 'e', 'sc@@', 'att@@', 'ato', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:31:03,007 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:31:03,007 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:31:03,007 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo in inverno e scattato in estate.
2025-05-30 00:31:03,007 - INFO - joeynmt.training - Example #4
2025-05-30 00:31:03,007 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:31:03,007 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:31:03,007 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 't@@', 'ale', 'di', 'di@@', 'seg@@', 'no', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:31:03,008 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:31:03,008 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:31:03,008 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è un tale di disegno che è successo negli ultimi 25 anni.
2025-05-30 00:31:06,489 - INFO - joeynmt.training - Epoch   9, Step:    83600, Batch Loss:     1.694476, Batch Acc: 0.506206, Tokens per Sec:    16147, Lr: 0.000147
2025-05-30 00:31:09,956 - INFO - joeynmt.training - Epoch   9, Step:    83700, Batch Loss:     1.628115, Batch Acc: 0.508936, Tokens per Sec:    19549, Lr: 0.000147
2025-05-30 00:31:13,421 - INFO - joeynmt.training - Epoch   9, Step:    83800, Batch Loss:     1.968155, Batch Acc: 0.506982, Tokens per Sec:    20057, Lr: 0.000147
2025-05-30 00:31:16,883 - INFO - joeynmt.training - Epoch   9, Step:    83900, Batch Loss:     1.843547, Batch Acc: 0.506423, Tokens per Sec:    20036, Lr: 0.000147
2025-05-30 00:31:20,341 - INFO - joeynmt.training - Epoch   9, Step:    84000, Batch Loss:     1.594067, Batch Acc: 0.503983, Tokens per Sec:    19829, Lr: 0.000147
2025-05-30 00:31:20,341 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:31:20,341 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:31:26,993 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.72, ppl:   5.60, acc:   0.52, generation: 6.6424[sec], evaluation: 0.0000[sec]
2025-05-30 00:31:26,993 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:31:27,627 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/82000.ckpt
2025-05-30 00:31:27,657 - INFO - joeynmt.training - Example #0
2025-05-30 00:31:27,657 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:31:27,658 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:31:27,658 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'ar@@', 'ch@@', 'e@@', ',', 'che', 'chiam@@', 'ano', '<unk>', 'ghi@@', 'acci@@', 'o@@', ',', 'che', 'chiam@@', 'ano', '<unk>', 'ghi@@', 'acci@@', 'o@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'grandi', 'stati', 'in@@', 'vi@@', 'ati', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'v@@', 'ag@@', 'i@@', 're@@', '.', '</s>']
2025-05-30 00:31:27,658 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:31:27,658 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:31:27,658 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per osservare che il ghiaccio arktico arche, che chiamano <unk> ghiaccio, che chiamano <unk> ghiaccio, per tre milioni di anni di grandi stati inviati per tre milioni di anni di vagire.
2025-05-30 00:31:27,658 - INFO - joeynmt.training - Example #1
2025-05-30 00:31:27,659 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:31:27,659 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:31:27,659 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'ma', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'accio', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:31:27,659 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:31:27,659 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:31:27,659 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, ma non è abbastanza forte, perché non è il dibattito del ghiaccio dell<unk> Eises.
2025-05-30 00:31:27,659 - INFO - joeynmt.training - Example #2
2025-05-30 00:31:27,660 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:31:27,660 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:31:27,660 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'c@@', 'aff@@', 'è', 'il', 'cuore', 'ar@@', 'c@@', 'atti@@', 'vo', 'del', 'nostro', 'sistema', 'c@@', 'ru@@', 'ci@@', 'ale', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:31:27,660 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:31:27,660 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:31:27,660 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il caffè il cuore arcattivo del nostro sistema cruciale globale.
2025-05-30 00:31:27,660 - INFO - joeynmt.training - Example #3
2025-05-30 00:31:27,660 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:31:27,660 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:31:27,661 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'in', 'in@@', 'ver@@', 'no@@', ',', 'e', 'si', 'sci@@', 'vol@@', 'ano', 'in', 'est@@', 'i@@', 'an@@', 'o@@', '.', '</s>']
2025-05-30 00:31:27,661 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:31:27,661 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:31:27,661 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo in inverno, e si scivolano in estiano.
2025-05-30 00:31:27,661 - INFO - joeynmt.training - Example #4
2025-05-30 00:31:27,661 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:31:27,661 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:31:27,661 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'foto', 'di', 'un', 'att@@', 'eggi@@', 'amento', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:31:27,662 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:31:27,662 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:31:27,662 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una foto di un atteggiamento che è successo negli ultimi 25 anni.
2025-05-30 00:31:31,130 - INFO - joeynmt.training - Epoch   9, Step:    84100, Batch Loss:     1.954807, Batch Acc: 0.500474, Tokens per Sec:    17070, Lr: 0.000147
2025-05-30 00:31:34,602 - INFO - joeynmt.training - Epoch   9, Step:    84200, Batch Loss:     1.643372, Batch Acc: 0.506259, Tokens per Sec:    19880, Lr: 0.000147
2025-05-30 00:31:38,075 - INFO - joeynmt.training - Epoch   9, Step:    84300, Batch Loss:     2.021917, Batch Acc: 0.514024, Tokens per Sec:    19540, Lr: 0.000147
2025-05-30 00:31:41,578 - INFO - joeynmt.training - Epoch   9, Step:    84400, Batch Loss:     1.784267, Batch Acc: 0.511034, Tokens per Sec:    20408, Lr: 0.000147
2025-05-30 00:31:45,066 - INFO - joeynmt.training - Epoch   9, Step:    84500, Batch Loss:     1.816267, Batch Acc: 0.508012, Tokens per Sec:    19991, Lr: 0.000147
2025-05-30 00:31:45,066 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:31:45,066 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:31:51,943 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.72, ppl:   5.57, acc:   0.52, generation: 6.8670[sec], evaluation: 0.0000[sec]
2025-05-30 00:31:51,943 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:31:52,778 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/81000.ckpt
2025-05-30 00:31:52,802 - INFO - joeynmt.training - Example #0
2025-05-30 00:31:52,803 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:31:52,803 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:31:52,803 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'per@@', 'der@@', 'e@@', ',', 'che', 'l@@', '<unk>', 'ar@@', 'c@@', 'ob@@', 'al@@', 'e@@', ',', 'che', 'ha', 'avuto', 'tre', 'milioni', 'di', 'anni', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'e@@', 'ano', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cento', 'per', 'cento', 'è', 'stato', 'ri@@', 'ma@@', 'sto', 'per', 'il', '40', 'per', 'cento', 'di', 'questi', 'due', 's@@', 'ett@@', 'ori@@', '.', '</s>']
2025-05-30 00:31:52,803 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:31:52,803 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:31:52,803 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per perdere, che l<unk> arcobale, che ha avuto tre milioni di anni la dimensione dell<unk> Oceano per tre milioni di anni per la più grande 48 stati per cento per cento per cento è stato rimasto per il 40 per cento di questi due settori.
2025-05-30 00:31:52,803 - INFO - joeynmt.training - Example #1
2025-05-30 00:31:52,804 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:31:52,804 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:31:52,804 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'ma', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:31:52,804 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:31:52,804 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:31:52,804 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte, ma non è abbastanza forte, perché non è il dibattito del ghiaccio.
2025-05-30 00:31:52,804 - INFO - joeynmt.training - Example #2
2025-05-30 00:31:52,804 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:31:52,804 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:31:52,804 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'c@@', 'atti@@', 'vo', 'ar@@', 'kt@@', 'ic@@', 'ano', 'ar@@', 't@@', 'ic@@', 'o@@', ',', 'il', 'cuore', 'c@@', 'ru@@', 'ci@@', 'ale', 'del', 'nostro', 'sistema', 'cli@@', 'mat@@', 'ic@@', 'o@@', '.', '</s>']
2025-05-30 00:31:52,805 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:31:52,805 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:31:52,805 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cattivo arkticano artico, il cuore cruciale del nostro sistema climatico.
2025-05-30 00:31:52,805 - INFO - joeynmt.training - Example #3
2025-05-30 00:31:52,805 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:31:52,805 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:31:52,805 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'nel', 'in@@', 'ver@@', 'no@@', ',', 'e', 'si', 'sp@@', 'um@@', 'or@@', 'os@@', 'a@@', '.', '</s>']
2025-05-30 00:31:52,806 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:31:52,806 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:31:52,806 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo nel inverno, e si spumorosa.
2025-05-30 00:31:52,806 - INFO - joeynmt.training - Example #4
2025-05-30 00:31:52,806 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:31:52,806 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:31:52,806 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 't@@', 'ale', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:31:52,806 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:31:52,806 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:31:52,807 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è un tale disegno di quello che è successo negli ultimi 25 anni.
2025-05-30 00:31:56,301 - INFO - joeynmt.training - Epoch   9, Step:    84600, Batch Loss:     1.516219, Batch Acc: 0.503264, Tokens per Sec:    15785, Lr: 0.000147
2025-05-30 00:31:59,783 - INFO - joeynmt.training - Epoch   9, Step:    84700, Batch Loss:     1.583365, Batch Acc: 0.505942, Tokens per Sec:    19719, Lr: 0.000147
2025-05-30 00:32:03,271 - INFO - joeynmt.training - Epoch   9, Step:    84800, Batch Loss:     1.750553, Batch Acc: 0.512198, Tokens per Sec:    19646, Lr: 0.000147
2025-05-30 00:32:04,827 - INFO - joeynmt.training - Epoch   9: total training loss 16477.37
2025-05-30 00:32:04,827 - INFO - joeynmt.training - EPOCH 10
2025-05-30 00:32:06,736 - INFO - joeynmt.training - Epoch  10, Step:    84900, Batch Loss:     1.565264, Batch Acc: 0.530208, Tokens per Sec:    19985, Lr: 0.000147
2025-05-30 00:32:10,200 - INFO - joeynmt.training - Epoch  10, Step:    85000, Batch Loss:     1.742423, Batch Acc: 0.528638, Tokens per Sec:    19645, Lr: 0.000147
2025-05-30 00:32:10,200 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:32:10,200 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:32:16,555 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.72, ppl:   5.60, acc:   0.52, generation: 6.3445[sec], evaluation: 0.0000[sec]
2025-05-30 00:32:16,970 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/82500.ckpt
2025-05-30 00:32:16,998 - INFO - joeynmt.training - Example #0
2025-05-30 00:32:16,998 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:32:16,998 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:32:16,998 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'con@@', 'fr@@', 'ont@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ic@@', 'o@@', ',', 'che', 'chiam@@', 'ano', '<unk>', 'ghi@@', 'acci@@', 'ai@@', 'o@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'e@@', 'ano', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', 'quantità', 'di', 'stati', 'in@@', 'fer@@', 'i@@', 'ori', 'di', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cent@@', 'o@@', '.', '</s>']
2025-05-30 00:32:16,999 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:32:16,999 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:32:16,999 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per confrontare che il ghiaccio arktico, che chiamano <unk> ghiacciaio, per tre milioni di anni la dimensione dell<unk> Oceano per tre milioni di anni per la più grande quantità di stati inferiori di 48 stati per cento per cento.
2025-05-30 00:32:16,999 - INFO - joeynmt.training - Example #1
2025-05-30 00:32:16,999 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:32:16,999 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:32:16,999 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'forte', 'la', 'più', 'grande', 'è', 'la', 'più', 'grande', 'di', 'questo', 'problema', 'speci@@', 'al@@', 'e@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:32:17,000 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:32:17,000 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:32:17,000 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte la più grande è la più grande di questo problema speciale, perché non è il dibattito del ghiaccio.
2025-05-30 00:32:17,000 - INFO - joeynmt.training - Example #2
2025-05-30 00:32:17,000 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:32:17,000 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:32:17,000 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'senso', 'è', 'il', 'cuore', 'ar@@', 'c@@', 'atti@@', 'vo', 'del', 'nostro', 'sistema', 'c@@', 'lim@@', 'asi@@', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'c@@', 'lim@@', 'as@@', 'si', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:32:17,001 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:32:17,001 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:32:17,001 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il senso è il cuore arcattivo del nostro sistema climasi, il cuore del nostro sistema climassi globale.
2025-05-30 00:32:17,001 - INFO - joeynmt.training - Example #3
2025-05-30 00:32:17,001 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:32:17,001 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:32:17,001 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'cres@@', 'ce', 'n@@', '<unk>', 'in@@', 'ver@@', 'no', 'e', 'sp@@', 'ing@@', 'ono', 'in', 'est@@', 'i@@', '.', '</s>']
2025-05-30 00:32:17,002 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:32:17,002 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:32:17,002 - INFO - joeynmt.training - 	Hypothesis: Si cresce n<unk> inverno e spingono in esti.
2025-05-30 00:32:17,002 - INFO - joeynmt.training - Example #4
2025-05-30 00:32:17,002 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:32:17,002 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:32:17,002 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 't@@', 'ale', 'di', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:32:17,002 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:32:17,002 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:32:17,002 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è un tale di disegno di quello che è successo negli ultimi 25 anni.
2025-05-30 00:32:20,482 - INFO - joeynmt.training - Epoch  10, Step:    85100, Batch Loss:     1.756698, Batch Acc: 0.521613, Tokens per Sec:    17301, Lr: 0.000147
2025-05-30 00:32:23,954 - INFO - joeynmt.training - Epoch  10, Step:    85200, Batch Loss:     1.592250, Batch Acc: 0.537720, Tokens per Sec:    20010, Lr: 0.000147
2025-05-30 00:32:27,410 - INFO - joeynmt.training - Epoch  10, Step:    85300, Batch Loss:     1.663555, Batch Acc: 0.525433, Tokens per Sec:    19966, Lr: 0.000147
2025-05-30 00:32:30,864 - INFO - joeynmt.training - Epoch  10, Step:    85400, Batch Loss:     1.730182, Batch Acc: 0.521928, Tokens per Sec:    19851, Lr: 0.000147
2025-05-30 00:32:34,289 - INFO - joeynmt.training - Epoch  10, Step:    85500, Batch Loss:     1.560705, Batch Acc: 0.520210, Tokens per Sec:    19765, Lr: 0.000147
2025-05-30 00:32:34,289 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:32:34,289 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:32:41,312 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.73, ppl:   5.65, acc:   0.52, generation: 7.0126[sec], evaluation: 0.0000[sec]
2025-05-30 00:32:41,317 - INFO - joeynmt.training - Example #0
2025-05-30 00:32:41,317 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:32:41,317 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:32:41,317 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'l@@', '<unk>', 'E@@', 'is@@', 'co@@', ',', 'che', 'chiam@@', 'ano', '<unk>', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'a@@', ',', 'che', 'chiam@@', 'ano', 'il', '4@@', '8', 'stati', 'per', 'cento', 'della', 'popolazione', 'sott@@', 'ost@@', 'itu@@', 'din@@', 'e@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', 'quantità', 'di', 'stati', 's@@', 'ov@@', 'rap@@', 'por@@', 'ti', 'in', 'modo', 'che', 'si', 'è', 'ri@@', 'vel@@', 'ato', 'il', '4@@', '8', 'stati', 'di', 'cent@@', 'o@@', '.', '</s>']
2025-05-30 00:32:41,317 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:32:41,317 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:32:41,317 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per osservare che l<unk> Eisco, che chiamano <unk> ghiaccio arkta, che chiamano il 48 stati per cento della popolazione sottostitudine, per tre milioni di anni per la più grande quantità di stati sovrapporti in modo che si è rivelato il 48 stati di cento.
2025-05-30 00:32:41,317 - INFO - joeynmt.training - Example #1
2025-05-30 00:32:41,317 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:32:41,317 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:32:41,318 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'ma', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'accio', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:32:41,318 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:32:41,318 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:32:41,318 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, ma non è abbastanza forte, perché non è il dibattito del ghiaccio dell<unk> Eises.
2025-05-30 00:32:41,318 - INFO - joeynmt.training - Example #2
2025-05-30 00:32:41,318 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:32:41,318 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:32:41,318 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'c@@', 'atti@@', 'vo', 'di', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ic@@', 'o@@', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'cli@@', 'mat@@', 'ic@@', 'o@@', '.', '</s>']
2025-05-30 00:32:41,318 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:32:41,318 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:32:41,318 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cattivo di ghiaccio arktico, il cuore del nostro sistema climatico.
2025-05-30 00:32:41,318 - INFO - joeynmt.training - Example #3
2025-05-30 00:32:41,318 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:32:41,318 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:32:41,318 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'in', 'in@@', 'ver@@', 'no', 'e', 'sp@@', 'um@@', 'enti', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:32:41,318 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:32:41,318 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:32:41,318 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo in inverno e spumenti in estate.
2025-05-30 00:32:41,319 - INFO - joeynmt.training - Example #4
2025-05-30 00:32:41,319 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:32:41,319 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:32:41,319 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'a@@', ',', 'è', 'una', 'foto', 'di', 'un', 'di@@', 'seg@@', 'no', 'di', 'un', 'tr@@', 'atto', 'di', 'di@@', 'seg@@', 'no', 'di', 'un', 'anno', 'per', 'il', 'nostro', 'lavor@@', 'o@@', '.', '</s>']
2025-05-30 00:32:41,319 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:32:41,319 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:32:41,319 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostra, è una foto di un disegno di un tratto di disegno di un anno per il nostro lavoro.
2025-05-30 00:32:44,788 - INFO - joeynmt.training - Epoch  10, Step:    85600, Batch Loss:     1.684156, Batch Acc: 0.520800, Tokens per Sec:    19804, Lr: 0.000147
2025-05-30 00:32:48,245 - INFO - joeynmt.training - Epoch  10, Step:    85700, Batch Loss:     1.671682, Batch Acc: 0.519877, Tokens per Sec:    19433, Lr: 0.000147
2025-05-30 00:32:51,712 - INFO - joeynmt.training - Epoch  10, Step:    85800, Batch Loss:     1.593294, Batch Acc: 0.520735, Tokens per Sec:    19519, Lr: 0.000147
2025-05-30 00:32:55,168 - INFO - joeynmt.training - Epoch  10, Step:    85900, Batch Loss:     1.772143, Batch Acc: 0.519799, Tokens per Sec:    19882, Lr: 0.000147
2025-05-30 00:32:58,630 - INFO - joeynmt.training - Epoch  10, Step:    86000, Batch Loss:     1.524075, Batch Acc: 0.527311, Tokens per Sec:    20069, Lr: 0.000147
2025-05-30 00:32:58,630 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:32:58,630 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:33:04,745 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.73, ppl:   5.64, acc:   0.52, generation: 6.1053[sec], evaluation: 0.0000[sec]
2025-05-30 00:33:04,762 - INFO - joeynmt.training - Example #0
2025-05-30 00:33:04,762 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:33:04,762 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:33:04,763 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'o@@', ',', 'che', 'si', 'chiam@@', 'ano', 'E@@', 'is@@', 'co@@', ',', 'che', 'chiam@@', 'ano', '<unk>', 'ghi@@', 'acci@@', 'ai@@', 'o@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'e@@', '<unk>', '.', '</s>']
2025-05-30 00:33:04,763 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:33:04,763 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:33:04,763 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per osservare che il ghiaccio arkto, che si chiamano Eisco, che chiamano <unk> ghiacciaio, per tre milioni di anni la dimensione dell<unk> Oce<unk> .
2025-05-30 00:33:04,763 - INFO - joeynmt.training - Example #1
2025-05-30 00:33:04,764 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:33:04,764 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:33:04,764 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'la', 'prima', 'volta', 'che', 'la', 'ser@@', 'ia', 'è', 'un', 'problema', 'speci@@', 'ale', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:33:04,764 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:33:04,764 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:33:04,764 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, la prima volta che la seria è un problema speciale che non è il dibattito del ghiaccio.
2025-05-30 00:33:04,764 - INFO - joeynmt.training - Example #2
2025-05-30 00:33:04,765 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:33:04,765 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:33:04,765 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'è', 'il', 'c@@', 'atti@@', 'vo', 'ar@@', 'ric@@', 'co', 'di', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:33:04,765 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:33:04,765 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:33:04,765 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, è il cattivo arricco di ghiaccio arktico globale.
2025-05-30 00:33:04,765 - INFO - joeynmt.training - Example #3
2025-05-30 00:33:04,766 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:33:04,766 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:33:04,766 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'nel', 'in@@', 'ver@@', 'no', 'e', 'si', 'sci@@', 'oc@@', 'ca', 'in', 'est@@', 'i@@', 'vo@@', '.', '</s>']
2025-05-30 00:33:04,766 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:33:04,766 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:33:04,766 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo nel inverno e si sciocca in estivo.
2025-05-30 00:33:04,766 - INFO - joeynmt.training - Example #4
2025-05-30 00:33:04,766 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:33:04,767 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:33:04,767 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'ri@@', 'fi@@', 'ut@@', 'o@@', ',', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:33:04,767 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:33:04,767 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:33:04,767 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è un disegno di rifiuto, che è successo negli ultimi 25 anni.
2025-05-30 00:33:08,270 - INFO - joeynmt.training - Epoch  10, Step:    86100, Batch Loss:     1.590315, Batch Acc: 0.528198, Tokens per Sec:    19875, Lr: 0.000147
2025-05-30 00:33:11,736 - INFO - joeynmt.training - Epoch  10, Step:    86200, Batch Loss:     1.639256, Batch Acc: 0.524100, Tokens per Sec:    20150, Lr: 0.000147
2025-05-30 00:33:15,196 - INFO - joeynmt.training - Epoch  10, Step:    86300, Batch Loss:     1.817875, Batch Acc: 0.511731, Tokens per Sec:    19565, Lr: 0.000147
2025-05-30 00:33:18,663 - INFO - joeynmt.training - Epoch  10, Step:    86400, Batch Loss:     1.682175, Batch Acc: 0.515883, Tokens per Sec:    19918, Lr: 0.000147
2025-05-30 00:33:22,112 - INFO - joeynmt.training - Epoch  10, Step:    86500, Batch Loss:     1.623528, Batch Acc: 0.516647, Tokens per Sec:    19861, Lr: 0.000147
2025-05-30 00:33:22,113 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:33:22,113 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:33:28,252 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.73, ppl:   5.62, acc:   0.52, generation: 6.1334[sec], evaluation: 0.0000[sec]
2025-05-30 00:33:28,261 - INFO - joeynmt.training - Example #0
2025-05-30 00:33:28,261 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:33:28,261 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:33:28,261 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've@@', ',', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'o@@', ',', 'che', 'chiam@@', 'ano', '<unk>', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'a@@', ',', 'che', 'chiam@@', 'ano', 'il', '4@@', '8', 'stati', 'per', 'cento', 'è', 'stato', 'ri@@', 'ma@@', 'sto', 's@@', 'vol@@', 'gen@@', 'do', 'il', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cento', 'è', 'stato', 'ri@@', 'lasci@@', 'ato', 'il', '4@@', '8', 'stati', 'per', 'cento', 'è', 'stato', 'ri@@', 'd@@', 'ott@@', 'ato', 'per', 'il', '4@@', '8', 'stati', 'ha', 'fatto', 'un', 'po@@', '<unk>', '.', '</s>']
2025-05-30 00:33:28,261 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:33:28,261 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:33:28,261 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive, per osservare che il ghiaccio arkto, che chiamano <unk> ghiaccio arkta, che chiamano il 48 stati per cento è stato rimasto svolgendo il 48 stati per cento per cento è stato rilasciato il 48 stati per cento è stato ridottato per il 48 stati ha fatto un po<unk> .
2025-05-30 00:33:28,261 - INFO - joeynmt.training - Example #1
2025-05-30 00:33:28,261 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:33:28,261 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:33:28,262 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'che', 'è', 'un', 'problema', 'speci@@', 'ale', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'accio', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:33:28,262 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:33:28,262 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:33:28,262 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, che è un problema speciale che non è il dibattito del ghiaccio dell<unk> Eises.
2025-05-30 00:33:28,262 - INFO - joeynmt.training - Example #2
2025-05-30 00:33:28,262 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:33:28,262 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:33:28,262 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'c@@', 'atti@@', 'vo', 'del', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:33:28,262 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:33:28,262 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:33:28,262 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cattivo del ghiaccio arktico globale.
2025-05-30 00:33:28,262 - INFO - joeynmt.training - Example #3
2025-05-30 00:33:28,262 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:33:28,262 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:33:28,262 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'in', 'in@@', 'ver@@', 'no', 'e', 'si', 'sp@@', 'um@@', 'enti', 'in', 'est@@', 'i@@', 'vi@@', '.', '</s>']
2025-05-30 00:33:28,262 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:33:28,262 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:33:28,262 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo in inverno e si spumenti in estivi.
2025-05-30 00:33:28,262 - INFO - joeynmt.training - Example #4
2025-05-30 00:33:28,263 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:33:28,263 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:33:28,263 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'ri@@', 'fi@@', 'ut@@', 'o@@', ',', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:33:28,263 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:33:28,263 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:33:28,263 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è un disegno di rifiuto, che è successo negli ultimi 25 anni.
2025-05-30 00:33:31,720 - INFO - joeynmt.training - Epoch  10, Step:    86600, Batch Loss:     1.563835, Batch Acc: 0.524761, Tokens per Sec:    19606, Lr: 0.000147
2025-05-30 00:33:35,192 - INFO - joeynmt.training - Epoch  10, Step:    86700, Batch Loss:     1.790044, Batch Acc: 0.514302, Tokens per Sec:    20072, Lr: 0.000147
2025-05-30 00:33:38,631 - INFO - joeynmt.training - Epoch  10, Step:    86800, Batch Loss:     1.699129, Batch Acc: 0.520197, Tokens per Sec:    20073, Lr: 0.000147
2025-05-30 00:33:42,086 - INFO - joeynmt.training - Epoch  10, Step:    86900, Batch Loss:     1.673898, Batch Acc: 0.520738, Tokens per Sec:    20162, Lr: 0.000147
2025-05-30 00:33:45,562 - INFO - joeynmt.training - Epoch  10, Step:    87000, Batch Loss:     1.778986, Batch Acc: 0.516484, Tokens per Sec:    20473, Lr: 0.000147
2025-05-30 00:33:45,563 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:33:45,563 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:33:51,880 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.73, ppl:   5.63, acc:   0.52, generation: 6.3081[sec], evaluation: 0.0000[sec]
2025-05-30 00:33:51,894 - INFO - joeynmt.training - Example #0
2025-05-30 00:33:51,894 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:33:51,894 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:33:51,894 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'ar@@', 'ric@@', 'co', 'di', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'o@@', ',', 'che', 'si', 'chiam@@', 'ano', 'per', 'tre', 'milioni', 'di', 'anni', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'ci@@', 'dent@@', 'e@@', '.', '</s>']
2025-05-30 00:33:51,895 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:33:51,895 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:33:51,895 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per osservare che il ghiaccio arktico arricco di ghiaccio arkto, che si chiamano per tre milioni di anni la dimensione dell<unk> Occidente.
2025-05-30 00:33:51,895 - INFO - joeynmt.training - Example #1
2025-05-30 00:33:51,896 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:33:51,896 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:33:51,896 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'la', 'sost@@', 'itu@@', 'zione', 'di', 'questo', 'problema', 'speci@@', 'al@@', 'e@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:33:51,896 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:33:51,896 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:33:51,896 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, la sostituzione di questo problema speciale, perché non è il dibattito del ghiaccio.
2025-05-30 00:33:51,896 - INFO - joeynmt.training - Example #2
2025-05-30 00:33:51,897 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:33:51,897 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:33:51,897 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'c@@', 'atti@@', 'vo', 'ar@@', 'ic@@', 'l@@', 'o@@', ',', 'il', 'cuore', 'c@@', 'atti@@', 'vo', 'del', 'nostro', 'sistema', 'c@@', 'lim@@', 'as@@', 'sist@@', 'ente', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:33:51,897 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:33:51,897 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:33:51,897 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cattivo ariclo, il cuore cattivo del nostro sistema climassistente globale.
2025-05-30 00:33:51,897 - INFO - joeynmt.training - Example #3
2025-05-30 00:33:51,897 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:33:51,898 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:33:51,898 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'nel', 'in@@', 'ver@@', 'no', 'e', 'sp@@', 'ing@@', 'ono', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:33:51,898 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:33:51,898 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:33:51,898 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo nel inverno e spingono in estate.
2025-05-30 00:33:51,898 - INFO - joeynmt.training - Example #4
2025-05-30 00:33:51,898 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:33:51,898 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:33:51,899 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'cosa', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:33:51,899 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:33:51,899 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:33:51,899 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una cosa che è successo negli ultimi 25 anni.
2025-05-30 00:33:55,386 - INFO - joeynmt.training - Epoch  10, Step:    87100, Batch Loss:     1.499028, Batch Acc: 0.518001, Tokens per Sec:    19326, Lr: 0.000147
2025-05-30 00:33:58,853 - INFO - joeynmt.training - Epoch  10, Step:    87200, Batch Loss:     1.558976, Batch Acc: 0.513760, Tokens per Sec:    19713, Lr: 0.000147
2025-05-30 00:34:02,312 - INFO - joeynmt.training - Epoch  10, Step:    87300, Batch Loss:     1.736123, Batch Acc: 0.516168, Tokens per Sec:    19824, Lr: 0.000147
2025-05-30 00:34:05,779 - INFO - joeynmt.training - Epoch  10, Step:    87400, Batch Loss:     1.567522, Batch Acc: 0.514106, Tokens per Sec:    20014, Lr: 0.000147
2025-05-30 00:34:09,257 - INFO - joeynmt.training - Epoch  10, Step:    87500, Batch Loss:     1.668749, Batch Acc: 0.522066, Tokens per Sec:    19830, Lr: 0.000147
2025-05-30 00:34:09,257 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:34:09,257 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:34:16,258 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.73, ppl:   5.64, acc:   0.52, generation: 6.9911[sec], evaluation: 0.0000[sec]
2025-05-30 00:34:16,270 - INFO - joeynmt.training - Example #0
2025-05-30 00:34:16,270 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:34:16,270 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:34:16,270 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'che', 'la', 'chiam@@', 'ano', '<unk>', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'che', 'chiam@@', 'ano', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', 'del', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cento', 'milioni', 'di', 'anni', 'per', 'la', 'dimen@@', 'sione', 'è', 'stato', 'fatto', 'a', 'un', 'certo', 'punto', 'è', 'stato', 'ri@@', 'ma@@', 'sto', 'per', 'tre', 'milioni', 'di', 'anni', 'fa@@', '.', '</s>']
2025-05-30 00:34:16,271 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:34:16,271 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:34:16,271 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per osservare che il ghiaccio arktico che la chiamano <unk> ghiaccio arktico che chiamano per tre milioni di anni per la più grande del 48 stati per cento per cento milioni di anni per la dimensione è stato fatto a un certo punto è stato rimasto per tre milioni di anni fa.
2025-05-30 00:34:16,271 - INFO - joeynmt.training - Example #1
2025-05-30 00:34:16,271 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:34:16,271 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:34:16,271 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'che', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:34:16,272 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:34:16,272 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:34:16,272 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, che non è molto forte, perché non è il dibattito del ghiaccio.
2025-05-30 00:34:16,272 - INFO - joeynmt.training - Example #2
2025-05-30 00:34:16,272 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:34:16,272 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:34:16,273 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'c@@', 'atti@@', 'vo', 'ar@@', 'ch@@', 'ico', 'del', 'nostro', 'sistema', 'c@@', 'atti@@', 'vo', 'del', 'nostro', 'sistema', 'c@@', 'lim@@', 'as@@', 'sist@@', 'o', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:34:16,273 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:34:16,273 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:34:16,273 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cattivo archico del nostro sistema cattivo del nostro sistema climassisto globale.
2025-05-30 00:34:16,273 - INFO - joeynmt.training - Example #3
2025-05-30 00:34:16,273 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:34:16,273 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:34:16,273 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'nel', 'in@@', 'ver@@', 'no', 'e', 'sp@@', 'os@@', 'ato', 'in', 'est@@', 'i@@', '.', '</s>']
2025-05-30 00:34:16,274 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:34:16,274 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:34:16,274 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo nel inverno e sposato in esti.
2025-05-30 00:34:16,274 - INFO - joeynmt.training - Example #4
2025-05-30 00:34:16,274 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:34:16,274 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:34:16,274 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 'di@@', 'seg@@', 'no', 'di', 'ri@@', 'fi@@', 'ut@@', 'o@@', ',', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:34:16,275 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:34:16,275 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:34:16,275 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è un disegno di rifiuto, che è successo negli ultimi 25 anni.
2025-05-30 00:34:19,753 - INFO - joeynmt.training - Epoch  10, Step:    87600, Batch Loss:     1.896885, Batch Acc: 0.519883, Tokens per Sec:    19445, Lr: 0.000147
2025-05-30 00:34:23,224 - INFO - joeynmt.training - Epoch  10, Step:    87700, Batch Loss:     1.716826, Batch Acc: 0.518409, Tokens per Sec:    19469, Lr: 0.000147
2025-05-30 00:34:26,677 - INFO - joeynmt.training - Epoch  10, Step:    87800, Batch Loss:     1.518653, Batch Acc: 0.518962, Tokens per Sec:    20080, Lr: 0.000147
2025-05-30 00:34:30,132 - INFO - joeynmt.training - Epoch  10, Step:    87900, Batch Loss:     1.709820, Batch Acc: 0.518516, Tokens per Sec:    19600, Lr: 0.000147
2025-05-30 00:34:33,605 - INFO - joeynmt.training - Epoch  10, Step:    88000, Batch Loss:     1.548072, Batch Acc: 0.516146, Tokens per Sec:    19966, Lr: 0.000147
2025-05-30 00:34:33,606 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:34:33,606 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:34:40,352 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.73, ppl:   5.64, acc:   0.52, generation: 6.7399[sec], evaluation: 0.0000[sec]
2025-05-30 00:34:40,361 - INFO - joeynmt.training - Example #0
2025-05-30 00:34:40,361 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:34:40,361 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:34:40,361 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'o@@', ',', 'che', 'la', 'chiam@@', 'ano', '<unk>', 'E@@', 'is@@', 'co@@', ',', 'che', 'aveva', 'tre', 'milioni', 'di', 'anni', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'e@@', 'ano', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', 'del', '4@@', '8', 'stati', 'per', 'cent@@', 'o@@', '.', '</s>']
2025-05-30 00:34:40,361 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:34:40,361 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:34:40,361 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per osservare che il ghiaccio arkto, che la chiamano <unk> Eisco, che aveva tre milioni di anni la dimensione dell<unk> Oceano per tre milioni di anni per la più grande del 48 stati per cento.
2025-05-30 00:34:40,361 - INFO - joeynmt.training - Example #1
2025-05-30 00:34:40,361 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:34:40,361 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:34:40,361 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'che', 'è', 'una', 'cosa', 'che', 'fa', 'la', 'prima', 'volta', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'accio', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:34:40,361 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:34:40,361 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:34:40,361 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, che è una cosa che fa la prima volta che non è il dibattito del ghiaccio dell<unk> Eises.
2025-05-30 00:34:40,361 - INFO - joeynmt.training - Example #2
2025-05-30 00:34:40,362 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:34:40,362 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:34:40,362 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'c@@', 'atti@@', 'mo', 'di', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:34:40,362 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:34:40,362 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:34:40,362 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cattimo di ghiaccio arktico globale.
2025-05-30 00:34:40,362 - INFO - joeynmt.training - Example #3
2025-05-30 00:34:40,362 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:34:40,362 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:34:40,362 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'cres@@', 'ce', 'nel', 'in@@', 'ver@@', 'no', 'e', 'si', 'sci@@', 'vol@@', 'ano', 'in', 'est@@', 'i@@', 'vo@@', '.', '</s>']
2025-05-30 00:34:40,362 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:34:40,362 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:34:40,362 - INFO - joeynmt.training - 	Hypothesis: Si cresce nel inverno e si scivolano in estivo.
2025-05-30 00:34:40,362 - INFO - joeynmt.training - Example #4
2025-05-30 00:34:40,362 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:34:40,362 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:34:40,362 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 'video', 'di', 'ri@@', 'fi@@', 'uto', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:34:40,363 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:34:40,363 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:34:40,363 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è un video di rifiuto che è successo negli ultimi 25 anni.
2025-05-30 00:34:43,759 - INFO - joeynmt.training - Epoch  10, Step:    88100, Batch Loss:     1.796803, Batch Acc: 0.513273, Tokens per Sec:    19893, Lr: 0.000147
2025-05-30 00:34:47,226 - INFO - joeynmt.training - Epoch  10, Step:    88200, Batch Loss:     1.614979, Batch Acc: 0.522602, Tokens per Sec:    19838, Lr: 0.000147
2025-05-30 00:34:50,693 - INFO - joeynmt.training - Epoch  10, Step:    88300, Batch Loss:     1.754808, Batch Acc: 0.513611, Tokens per Sec:    19638, Lr: 0.000147
2025-05-30 00:34:54,174 - INFO - joeynmt.training - Epoch  10, Step:    88400, Batch Loss:     1.817466, Batch Acc: 0.514932, Tokens per Sec:    20245, Lr: 0.000147
2025-05-30 00:34:57,622 - INFO - joeynmt.training - Epoch  10, Step:    88500, Batch Loss:     1.517435, Batch Acc: 0.519304, Tokens per Sec:    19603, Lr: 0.000147
2025-05-30 00:34:57,622 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:34:57,622 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:35:03,946 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.73, ppl:   5.62, acc:   0.52, generation: 6.3115[sec], evaluation: 0.0000[sec]
2025-05-30 00:35:03,957 - INFO - joeynmt.training - Example #0
2025-05-30 00:35:03,958 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:35:03,958 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:35:03,958 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'ch@@', 'ico', 'ar@@', 'ric@@', 'co', 'che', 'per', 'tre', 'milioni', 'di', 'anni', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'e@@', '<unk>', ',', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', 'del', '4@@', '8', 'st@@', 'ati@@', '.', '</s>']
2025-05-30 00:35:03,959 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:35:03,959 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:35:03,959 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per osservare che il ghiaccio archico arricco che per tre milioni di anni la dimensione dell<unk> Oce<unk> , che ha fatto per tre milioni di anni per la più grande del 48 stati.
2025-05-30 00:35:03,959 - INFO - joeynmt.training - Example #1
2025-05-30 00:35:03,959 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:35:03,959 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:35:03,959 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'che', 'è', 'molto', 'for@@', 'te@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:35:03,960 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:35:03,960 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:35:03,960 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, che è molto forte, perché non è il dibattito del ghiaccio.
2025-05-30 00:35:03,960 - INFO - joeynmt.training - Example #2
2025-05-30 00:35:03,960 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:35:03,960 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:35:03,960 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'c@@', 'atti@@', 'mo', 'di', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:35:03,961 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:35:03,961 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:35:03,961 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cattimo di ghiaccio arktico globale.
2025-05-30 00:35:03,961 - INFO - joeynmt.training - Example #3
2025-05-30 00:35:03,961 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:35:03,961 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:35:03,961 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'cominci@@', 'a', 'a', 'cresc@@', 'ere', 'e', 'si', 's@@', 'ente', 'n@@', 'ell@@', '<unk>', 'est@@', 'ate', 'in', 'est@@', 'i@@', 'pot@@', 'es@@', 'i@@', '.', '</s>']
2025-05-30 00:35:03,962 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:35:03,962 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:35:03,962 - INFO - joeynmt.training - 	Hypothesis: Si comincia a crescere e si sente nell<unk> estate in estipotesi.
2025-05-30 00:35:03,962 - INFO - joeynmt.training - Example #4
2025-05-30 00:35:03,962 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:35:03,962 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:35:03,962 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'un', 't@@', 'ale', 'ri@@', 'fi@@', 'ut@@', 'ato', 'di', 'ri@@', 'fi@@', 'uti', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:35:03,963 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:35:03,963 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:35:03,963 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è un tale rifiutato di rifiuti che è successo negli ultimi 25 anni.
2025-05-30 00:35:07,437 - INFO - joeynmt.training - Epoch  10, Step:    88600, Batch Loss:     1.785607, Batch Acc: 0.513070, Tokens per Sec:    19540, Lr: 0.000147
2025-05-30 00:35:10,878 - INFO - joeynmt.training - Epoch  10, Step:    88700, Batch Loss:     1.904566, Batch Acc: 0.518342, Tokens per Sec:    19130, Lr: 0.000147
2025-05-30 00:35:14,326 - INFO - joeynmt.training - Epoch  10, Step:    88800, Batch Loss:     1.628626, Batch Acc: 0.508061, Tokens per Sec:    20100, Lr: 0.000147
2025-05-30 00:35:17,777 - INFO - joeynmt.training - Epoch  10, Step:    88900, Batch Loss:     1.717949, Batch Acc: 0.520815, Tokens per Sec:    19847, Lr: 0.000147
2025-05-30 00:35:21,218 - INFO - joeynmt.training - Epoch  10, Step:    89000, Batch Loss:     1.756199, Batch Acc: 0.514963, Tokens per Sec:    19330, Lr: 0.000147
2025-05-30 00:35:21,219 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:35:21,219 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:35:28,684 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.72, ppl:   5.60, acc:   0.52, generation: 7.4552[sec], evaluation: 0.0000[sec]
2025-05-30 00:35:29,109 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/83000.ckpt
2025-05-30 00:35:29,138 - INFO - joeynmt.training - Example #0
2025-05-30 00:35:29,138 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:35:29,138 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:35:29,138 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'l@@', '<unk>', 'E@@', 'is@@', 'co@@', ',', 'che', 'l@@', '<unk>', 'ar@@', 'ric@@', 'ch@@', 'ezz@@', 'a@@', ',', 'che', 'ha', 'avuto', 'tre', 'milioni', 'di', 'anni', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'ci@@', 'dent@@', 'e@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'dimen@@', 'sioni', 'in', 'cui', 'si', 'è', 'ri@@', 'vel@@', 'ato', 'il', '4@@', '8', 'stati', 'per', 'cent@@', 'o@@', '.', '</s>']
2025-05-30 00:35:29,139 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:35:29,139 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:35:29,139 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per osservare che l<unk> Eisco, che l<unk> arricchezza, che ha avuto tre milioni di anni la dimensione dell<unk> Occidente, per tre milioni di anni di dimensioni in cui si è rivelato il 48 stati per cento.
2025-05-30 00:35:29,139 - INFO - joeynmt.training - Example #1
2025-05-30 00:35:29,139 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:35:29,139 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:35:29,139 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'la', 'più', 'grande', 'di', 'questo', 'speci@@', 'ale', 'problema', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:35:29,140 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:35:29,140 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:35:29,140 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, la più grande di questo speciale problema che non è il dibattito del ghiaccio.
2025-05-30 00:35:29,140 - INFO - joeynmt.training - Example #2
2025-05-30 00:35:29,140 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:35:29,140 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:35:29,140 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'l@@', '<unk>', 'ar@@', 'kt@@', 'ica', 'di', 'ghi@@', 'accio', 'è', 'il', 'cuore', 'del', 'nostro', 'sistema', 'c@@', 'lim@@', 'ati@@', 'vo@@', '.', '</s>']
2025-05-30 00:35:29,141 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:35:29,141 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:35:29,141 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, l<unk> arktica di ghiaccio è il cuore del nostro sistema climativo.
2025-05-30 00:35:29,141 - INFO - joeynmt.training - Example #3
2025-05-30 00:35:29,141 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:35:29,141 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:35:29,141 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'in', 'in@@', 'ver@@', 'no', 'e', 'si', 's@@', 'ente', 'n@@', 'ell@@', '<unk>', 'est@@', 'ate', 'e', 'si', 's@@', 'vol@@', 'ge', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:35:29,141 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:35:29,142 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:35:29,142 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo in inverno e si sente nell<unk> estate e si svolge in estate.
2025-05-30 00:35:29,142 - INFO - joeynmt.training - Example #4
2025-05-30 00:35:29,142 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:35:29,142 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:35:29,142 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'foto', 'di', 'un', 'att@@', 'eggi@@', 'amento', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:35:29,142 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:35:29,142 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:35:29,142 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una foto di un atteggiamento che è successo negli ultimi 25 anni.
2025-05-30 00:35:32,621 - INFO - joeynmt.training - Epoch  10, Step:    89100, Batch Loss:     1.740599, Batch Acc: 0.516424, Tokens per Sec:    17341, Lr: 0.000103
2025-05-30 00:35:36,105 - INFO - joeynmt.training - Epoch  10, Step:    89200, Batch Loss:     2.020231, Batch Acc: 0.514387, Tokens per Sec:    19545, Lr: 0.000103
2025-05-30 00:35:39,578 - INFO - joeynmt.training - Epoch  10, Step:    89300, Batch Loss:     1.586522, Batch Acc: 0.514242, Tokens per Sec:    19827, Lr: 0.000103
2025-05-30 00:35:43,046 - INFO - joeynmt.training - Epoch  10, Step:    89400, Batch Loss:     1.602200, Batch Acc: 0.522562, Tokens per Sec:    19576, Lr: 0.000103
2025-05-30 00:35:46,448 - INFO - joeynmt.training - Epoch  10, Step:    89500, Batch Loss:     1.830572, Batch Acc: 0.517427, Tokens per Sec:    19911, Lr: 0.000103
2025-05-30 00:35:46,448 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:35:46,449 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:35:53,372 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.72, ppl:   5.56, acc:   0.52, generation: 6.9133[sec], evaluation: 0.0000[sec]
2025-05-30 00:35:53,372 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:35:54,024 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/85000.ckpt
2025-05-30 00:35:54,052 - INFO - joeynmt.training - Example #0
2025-05-30 00:35:54,053 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:35:54,053 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:35:54,053 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'ar@@', 'ch@@', 'ico', 'che', 'chiam@@', 'ano', '<unk>', 'ghi@@', 'acci@@', 'ai', 'ar@@', 'c@@', 'ic@@', 'l@@', 'abili', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', 'quantità', 'di', 'n@@', 'azioni', 'in', 'cui', 'si', 'è', 'fer@@', 'm@@', 'ata', 'il', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cento', 'per', 'cento', 'del', '2@@', '5@@', '.', '</s>']
2025-05-30 00:35:54,054 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:35:54,054 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:35:54,054 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per osservare che il ghiaccio arktico archico che chiamano <unk> ghiacciai arciclabili per tre milioni di anni per la più grande quantità di nazioni in cui si è fermata il 48 stati per cento per cento per cento del 25.
2025-05-30 00:35:54,054 - INFO - joeynmt.training - Example #1
2025-05-30 00:35:54,054 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:35:54,054 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:35:54,054 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'accio', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:35:54,055 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:35:54,055 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:35:54,055 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, non è abbastanza forte, perché non è il dibattito del ghiaccio dell<unk> Eises.
2025-05-30 00:35:54,055 - INFO - joeynmt.training - Example #2
2025-05-30 00:35:54,055 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:35:54,055 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:35:54,055 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'att@@', 'ica', 'di', 'ghi@@', 'accio', 'è', 'il', 'cuore', 'del', 'nostro', 'sistema', 'c@@', 'lim@@', 'ati@@', 'che', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:35:54,056 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:35:54,056 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:35:54,056 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cattica di ghiaccio è il cuore del nostro sistema climatiche globale.
2025-05-30 00:35:54,056 - INFO - joeynmt.training - Example #3
2025-05-30 00:35:54,056 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:35:54,056 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:35:54,056 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'in', 'in@@', 'ver@@', 'no', 'e', 'sp@@', 'um@@', 'p@@', 'one', 'in', 'est@@', 'i@@', 'at@@', 'o@@', '.', '</s>']
2025-05-30 00:35:54,056 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:35:54,056 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:35:54,057 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo in inverno e spumpone in estiato.
2025-05-30 00:35:54,057 - INFO - joeynmt.training - Example #4
2025-05-30 00:35:54,057 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:35:54,057 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:35:54,057 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'foto', 'di', 'una', 'ri@@', 'presa', 'che', 'cosa', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:35:54,057 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:35:54,057 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:35:54,057 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una foto di una ripresa che cosa è successo negli ultimi 25 anni.
2025-05-30 00:35:57,568 - INFO - joeynmt.training - Epoch  10, Step:    89600, Batch Loss:     1.513053, Batch Acc: 0.519720, Tokens per Sec:    16866, Lr: 0.000103
2025-05-30 00:36:01,027 - INFO - joeynmt.training - Epoch  10, Step:    89700, Batch Loss:     1.501542, Batch Acc: 0.527272, Tokens per Sec:    19367, Lr: 0.000103
2025-05-30 00:36:04,509 - INFO - joeynmt.training - Epoch  10, Step:    89800, Batch Loss:     1.606663, Batch Acc: 0.522875, Tokens per Sec:    20131, Lr: 0.000103
2025-05-30 00:36:07,976 - INFO - joeynmt.training - Epoch  10, Step:    89900, Batch Loss:     1.896457, Batch Acc: 0.521972, Tokens per Sec:    20189, Lr: 0.000103
2025-05-30 00:36:11,447 - INFO - joeynmt.training - Epoch  10, Step:    90000, Batch Loss:     1.854441, Batch Acc: 0.521238, Tokens per Sec:    20321, Lr: 0.000103
2025-05-30 00:36:11,447 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:36:11,447 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:36:18,631 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.71, ppl:   5.55, acc:   0.52, generation: 7.1742[sec], evaluation: 0.0000[sec]
2025-05-30 00:36:18,632 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:36:19,306 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/83500.ckpt
2025-05-30 00:36:19,333 - INFO - joeynmt.training - Example #0
2025-05-30 00:36:19,334 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:36:19,334 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:36:19,334 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'l@@', '<unk>', 'E@@', 'is@@', 'co@@', ',', 'che', 'l@@', '<unk>', 'ar@@', 'ric@@', 'chi@@', 'ata', 'di', 'ghi@@', 'accio', 'che', 'aveva', 'circa', 'il', '40', 'per', 'cento', 'della', 'popolazione', 'in', 'basso', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cento', 'per', 'cento', 'di', 'anni', 'è', 'ri@@', 'ma@@', 'sto', 'per', 'ri@@', 'vel@@', 'are', 'il', '40', 'per', 'cento', 'della', 'popolazione', 'è', 'ri@@', 'd@@', 'ell@@', '<unk>', 'oce@@', 'ano', 'per', 'tre', 'milioni', 'di', 'anni', 'e', 'di', 'ri@@', 'fer@@', 'imento', 'di', 'un', 't@@', 'asso', 'di', 'g@@', 'all@@', 'eggi@@', 'amento', 'del', '4@@', '8', 'pa@@', 'es@@', 'e@@', '.', '</s>']
2025-05-30 00:36:19,334 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:36:19,335 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:36:19,335 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per osservare che l<unk> Eisco, che l<unk> arricchiata di ghiaccio che aveva circa il 40 per cento della popolazione in basso 48 stati per cento per cento per cento di anni è rimasto per rivelare il 40 per cento della popolazione è ridell<unk> oceano per tre milioni di anni e di riferimento di un tasso di galleggiamento del 48 paese.
2025-05-30 00:36:19,335 - INFO - joeynmt.training - Example #1
2025-05-30 00:36:19,335 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:36:19,335 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:36:19,335 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'forte', 'la', 'prima', 'volta', 'che', 'la', 'prima', 'volta', 'in', 'cui', 'non', 'è', 'il', 'vi@@', 'll@@', 'aggio', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:36:19,335 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:36:19,335 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:36:19,335 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte la prima volta che la prima volta in cui non è il villaggio del ghiaccio.
2025-05-30 00:36:19,336 - INFO - joeynmt.training - Example #2
2025-05-30 00:36:19,336 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:36:19,336 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:36:19,336 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'atti@@', 'va', 'ar@@', 't@@', 'ica', 'del', 'nostro', 'sistema', 'c@@', 'atti@@', 'vo', 'del', 'nostro', 'sistema', 'c@@', 'lim@@', 'ati@@', 'vo@@', '.', '</s>']
2025-05-30 00:36:19,336 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:36:19,336 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:36:19,336 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cattiva artica del nostro sistema cattivo del nostro sistema climativo.
2025-05-30 00:36:19,336 - INFO - joeynmt.training - Example #3
2025-05-30 00:36:19,337 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:36:19,337 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:36:19,337 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'cominci@@', 'a', 'a', 'a', 'cresc@@', 'ere', 'e', 'sp@@', 'ing@@', 'ere', 'in', 'est@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:36:19,337 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:36:19,337 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:36:19,337 - INFO - joeynmt.training - 	Hypothesis: Si comincia a a crescere e spingere in estate.
2025-05-30 00:36:19,337 - INFO - joeynmt.training - Example #4
2025-05-30 00:36:19,337 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:36:19,337 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:36:19,337 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'foto', 'di', 'una', 'ri@@', 'presa', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:36:19,338 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:36:19,338 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:36:19,338 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una foto di una ripresa che è successo negli ultimi 25 anni.
2025-05-30 00:36:22,822 - INFO - joeynmt.training - Epoch  10, Step:    90100, Batch Loss:     1.774982, Batch Acc: 0.513797, Tokens per Sec:    16345, Lr: 0.000103
2025-05-30 00:36:26,297 - INFO - joeynmt.training - Epoch  10, Step:    90200, Batch Loss:     1.708075, Batch Acc: 0.523117, Tokens per Sec:    20057, Lr: 0.000103
2025-05-30 00:36:29,759 - INFO - joeynmt.training - Epoch  10, Step:    90300, Batch Loss:     1.807715, Batch Acc: 0.520186, Tokens per Sec:    19364, Lr: 0.000103
2025-05-30 00:36:33,244 - INFO - joeynmt.training - Epoch  10, Step:    90400, Batch Loss:     1.484887, Batch Acc: 0.524285, Tokens per Sec:    19863, Lr: 0.000103
2025-05-30 00:36:36,710 - INFO - joeynmt.training - Epoch  10, Step:    90500, Batch Loss:     1.637575, Batch Acc: 0.519371, Tokens per Sec:    20090, Lr: 0.000103
2025-05-30 00:36:36,710 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:36:36,710 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:36:42,973 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.71, ppl:   5.53, acc:   0.52, generation: 6.2566[sec], evaluation: 0.0000[sec]
2025-05-30 00:36:42,973 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:36:43,514 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/84000.ckpt
2025-05-30 00:36:43,540 - INFO - joeynmt.training - Example #0
2025-05-30 00:36:43,541 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:36:43,541 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:36:43,541 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'che', 'la', 'chiam@@', 'ano', '<unk>', 'E@@', 'is@@', 'co@@', ',', 'che', 'chiam@@', 'ano', '<unk>', 'E@@', 'is@@', 'co@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cento', 'della', 'popolazione', 'in', 'cui', 'è', 'ri@@', 'ma@@', 'sto', 'per', 'tr@@', 'e@@', '.', '</s>']
2025-05-30 00:36:43,542 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:36:43,542 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:36:43,542 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per osservare che il ghiaccio arktico che la chiamano <unk> Eisco, che chiamano <unk> Eisco, per tre milioni di anni per la più grande 48 stati per cento per cento della popolazione in cui è rimasto per tre.
2025-05-30 00:36:43,542 - INFO - joeynmt.training - Example #1
2025-05-30 00:36:43,542 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:36:43,542 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:36:43,542 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'la', 'più', 'grande', 'cos@@', 'ci@@', 'enza', 'di', 'questo', 'problema', 'speci@@', 'al@@', 'e@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:36:43,542 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:36:43,543 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:36:43,543 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, la più grande coscienza di questo problema speciale, perché non è il dibattito del ghiaccio.
2025-05-30 00:36:43,543 - INFO - joeynmt.training - Example #2
2025-05-30 00:36:43,543 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:36:43,543 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:36:43,543 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'ris@@', 'i', 'ar@@', 't@@', 'ici', 'ar@@', 't@@', 'ici', 'ar@@', 't@@', 'ici', 'del', 'nostro', 'sistema', 'cli@@', 'ma@@', '.', '</s>']
2025-05-30 00:36:43,543 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:36:43,544 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:36:43,544 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la crisi artici artici artici del nostro sistema clima.
2025-05-30 00:36:43,544 - INFO - joeynmt.training - Example #3
2025-05-30 00:36:43,544 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:36:43,544 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:36:43,544 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'cres@@', 'ce', 'nel', 'vent@@', 'o@@', ',', 'e', 'si', 's@@', 'ente', 'n@@', 'ell@@', '<unk>', 'est@@', 'ate', 'in', 'est@@', 'i@@', '.', '</s>']
2025-05-30 00:36:43,544 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:36:43,544 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:36:43,544 - INFO - joeynmt.training - 	Hypothesis: Si cresce nel vento, e si sente nell<unk> estate in esti.
2025-05-30 00:36:43,544 - INFO - joeynmt.training - Example #4
2025-05-30 00:36:43,545 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:36:43,545 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:36:43,545 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'foto', 'di', 'una', 'ri@@', 'presa', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:36:43,545 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:36:43,545 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:36:43,545 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una foto di una ripresa che è successo negli ultimi 25 anni.
2025-05-30 00:36:46,978 - INFO - joeynmt.training - Epoch  10, Step:    90600, Batch Loss:     1.819644, Batch Acc: 0.519676, Tokens per Sec:    17310, Lr: 0.000103
2025-05-30 00:36:50,429 - INFO - joeynmt.training - Epoch  10, Step:    90700, Batch Loss:     1.618893, Batch Acc: 0.518127, Tokens per Sec:    20303, Lr: 0.000103
2025-05-30 00:36:53,898 - INFO - joeynmt.training - Epoch  10, Step:    90800, Batch Loss:     1.913910, Batch Acc: 0.518279, Tokens per Sec:    19952, Lr: 0.000103
2025-05-30 00:36:57,351 - INFO - joeynmt.training - Epoch  10, Step:    90900, Batch Loss:     1.762308, Batch Acc: 0.519212, Tokens per Sec:    19052, Lr: 0.000103
2025-05-30 00:37:00,821 - INFO - joeynmt.training - Epoch  10, Step:    91000, Batch Loss:     1.932393, Batch Acc: 0.518929, Tokens per Sec:    19669, Lr: 0.000103
2025-05-30 00:37:00,821 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:37:00,821 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:37:07,985 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.71, ppl:   5.53, acc:   0.52, generation: 7.1546[sec], evaluation: 0.0000[sec]
2025-05-30 00:37:07,986 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:37:08,611 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/89000.ckpt
2025-05-30 00:37:08,632 - INFO - joeynmt.training - Example #0
2025-05-30 00:37:08,633 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:37:08,633 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:37:08,633 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'la', 'p@@', 'om@@', 'p@@', 'a', 'ar@@', 'ic@@', 'ci@@', 'are', 'ar@@', 'ric@@', 'chi@@', ',', 'che', 'chiam@@', 'ano', '<unk>', 'E@@', 'is@@', 'co@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', 'quantità', 'di', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cento', 'è', 'ri@@', 'ma@@', 'sto', 'per', 'tre', 'milioni', 'di', 'anni', 'è', 'ri@@', 'ma@@', 'sto', 'per', 'ri@@', 'durre', 'il', '40', 'per', 'cento', 'è', 'ri@@', 'vel@@', 'ato', 'di', 'un', 't@@', 'asso', 'di', 'ri@@', 'fi@@', 'ut@@', 'o@@', '.', '</s>']
2025-05-30 00:37:08,633 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:37:08,633 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:37:08,633 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per osservare che la pompa aricciare arricchi, che chiamano <unk> Eisco, per tre milioni di anni per la più grande quantità di 48 stati per cento per cento è rimasto per tre milioni di anni è rimasto per ridurre il 40 per cento è rivelato di un tasso di rifiuto.
2025-05-30 00:37:08,633 - INFO - joeynmt.training - Example #1
2025-05-30 00:37:08,634 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:37:08,634 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:37:08,634 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'forte', 'la', 'prima', 'cosa', 'che', 'la', 'prima', 'cosa', 'che', 'si', 'fa', 'a', 'questo', 'particolare', 'problema', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:37:08,634 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:37:08,634 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:37:08,634 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte la prima cosa che la prima cosa che si fa a questo particolare problema perché non è il dibattito del ghiaccio.
2025-05-30 00:37:08,634 - INFO - joeynmt.training - Example #2
2025-05-30 00:37:08,635 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:37:08,635 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:37:08,635 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'ris@@', 'i', 'ar@@', 'kt@@', 'ico', 'ar@@', 'kt@@', 'ico', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:37:08,635 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:37:08,635 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:37:08,635 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la crisi arktico arktico globale.
2025-05-30 00:37:08,635 - INFO - joeynmt.training - Example #3
2025-05-30 00:37:08,635 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:37:08,635 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:37:08,635 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'in@@', 'ver@@', 'no', 'e', 'si', 's@@', 'ente', 'n@@', 'ell@@', '<unk>', 'est@@', 'ate', 'd@@', '<unk>', 'est@@', 'i@@', 'vo@@', '.', '</s>']
2025-05-30 00:37:08,636 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:37:08,636 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:37:08,636 - INFO - joeynmt.training - 	Hypothesis: L<unk> inverno e si sente nell<unk> estate d<unk> estivo.
2025-05-30 00:37:08,636 - INFO - joeynmt.training - Example #4
2025-05-30 00:37:08,636 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:37:08,636 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:37:08,636 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'un', 't@@', 'ale', 'di@@', 'seg@@', 'no', 'di', 'ri@@', 'fi@@', 'ut@@', 'are', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:37:08,636 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:37:08,637 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:37:08,637 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è un tale disegno di rifiutare ciò che è successo negli ultimi 25 anni.
2025-05-30 00:37:12,114 - INFO - joeynmt.training - Epoch  10, Step:    91100, Batch Loss:     1.695683, Batch Acc: 0.515016, Tokens per Sec:    16697, Lr: 0.000103
2025-05-30 00:37:15,601 - INFO - joeynmt.training - Epoch  10, Step:    91200, Batch Loss:     1.567078, Batch Acc: 0.516874, Tokens per Sec:    19739, Lr: 0.000103
2025-05-30 00:37:19,072 - INFO - joeynmt.training - Epoch  10, Step:    91300, Batch Loss:     1.854535, Batch Acc: 0.518421, Tokens per Sec:    19724, Lr: 0.000103
2025-05-30 00:37:22,549 - INFO - joeynmt.training - Epoch  10, Step:    91400, Batch Loss:     1.612534, Batch Acc: 0.519961, Tokens per Sec:    19279, Lr: 0.000103
2025-05-30 00:37:26,031 - INFO - joeynmt.training - Epoch  10, Step:    91500, Batch Loss:     1.731442, Batch Acc: 0.516532, Tokens per Sec:    19903, Lr: 0.000103
2025-05-30 00:37:26,031 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:37:26,031 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:37:33,356 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.71, ppl:   5.54, acc:   0.52, generation: 7.3148[sec], evaluation: 0.0000[sec]
2025-05-30 00:37:33,829 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/84500.ckpt
2025-05-30 00:37:33,858 - INFO - joeynmt.training - Example #0
2025-05-30 00:37:33,858 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:37:33,859 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:37:33,859 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'ar@@', 'ch@@', 'ico', 'che', 'chiam@@', 'ano', '<unk>', 'E@@', 'is@@', 'co@@', ',', 'che', 'chiam@@', 'ano', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', 'quantità', 'di', 'n@@', 'azioni', 'sott@@', 'op@@', 'ost@@', 'e', 'per', 'il', '40', 'per', 'cento', 'del', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cento', 'del', '2@@', '5@@', '.', '</s>']
2025-05-30 00:37:33,859 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:37:33,859 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:37:33,859 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per osservare che il ghiaccio arktico archico che chiamano <unk> Eisco, che chiamano per tre milioni di anni per la più grande quantità di nazioni sottoposte per il 40 per cento del 48 stati per cento per cento del 25.
2025-05-30 00:37:33,859 - INFO - joeynmt.training - Example #1
2025-05-30 00:37:33,860 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:37:33,860 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:37:33,860 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'che', 'è', 'una', 'cosa', 'che', 'è', 'la', 'più', 'grande', 'di', 'questo', 'problema', 'speci@@', 'al@@', 'e@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:37:33,860 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:37:33,860 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:37:33,860 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, che è una cosa che è la più grande di questo problema speciale, perché non è il dibattito del ghiaccio.
2025-05-30 00:37:33,860 - INFO - joeynmt.training - Example #2
2025-05-30 00:37:33,860 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:37:33,860 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:37:33,860 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'ar@@', 'kt@@', 'ico', 'ar@@', 't@@', 'ica', 'del', 'nostro', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:37:33,861 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:37:33,861 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:37:33,861 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cuore arktico artica del nostro sistema climatico globale.
2025-05-30 00:37:33,861 - INFO - joeynmt.training - Example #3
2025-05-30 00:37:33,861 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:37:33,861 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:37:33,861 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'in@@', 'ver@@', 'no', 'e', 'si', 'sp@@', 'um@@', 'p@@', 'one', 'in', 'est@@', 'i@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:37:33,862 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:37:33,862 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:37:33,862 - INFO - joeynmt.training - 	Hypothesis: L<unk> inverno e si spumpone in estiate.
2025-05-30 00:37:33,862 - INFO - joeynmt.training - Example #4
2025-05-30 00:37:33,862 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:37:33,862 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:37:33,862 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'foto', 'di', 'una', 'foto', 'di', 'una', 'ri@@', 'du@@', 'zione', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:37:33,862 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:37:33,863 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:37:33,863 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una foto di una foto di una riduzione di quello che è successo negli ultimi 25 anni.
2025-05-30 00:37:37,378 - INFO - joeynmt.training - Epoch  10, Step:    91600, Batch Loss:     1.893973, Batch Acc: 0.517214, Tokens per Sec:    17987, Lr: 0.000103
2025-05-30 00:37:40,860 - INFO - joeynmt.training - Epoch  10, Step:    91700, Batch Loss:     1.718956, Batch Acc: 0.522710, Tokens per Sec:    20044, Lr: 0.000103
2025-05-30 00:37:44,340 - INFO - joeynmt.training - Epoch  10, Step:    91800, Batch Loss:     1.736197, Batch Acc: 0.510767, Tokens per Sec:    19795, Lr: 0.000103
2025-05-30 00:37:47,814 - INFO - joeynmt.training - Epoch  10, Step:    91900, Batch Loss:     1.652500, Batch Acc: 0.513803, Tokens per Sec:    19757, Lr: 0.000103
2025-05-30 00:37:51,284 - INFO - joeynmt.training - Epoch  10, Step:    92000, Batch Loss:     1.799593, Batch Acc: 0.520737, Tokens per Sec:    19785, Lr: 0.000103
2025-05-30 00:37:51,284 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:37:51,285 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:37:57,707 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.71, ppl:   5.52, acc:   0.52, generation: 6.4134[sec], evaluation: 0.0000[sec]
2025-05-30 00:37:57,708 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:37:58,354 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/89500.ckpt
2025-05-30 00:37:58,382 - INFO - joeynmt.training - Example #0
2025-05-30 00:37:58,382 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:37:58,382 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:37:58,382 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'c@@', 'ci@@', 'one', 'ar@@', 't@@', 'ic@@', 'o@@', ',', 'che', 'si', 'chiam@@', 'ano', '<unk>', 'g@@', 'all@@', 'eggi@@', 'amento', 'di', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', 'del', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cento', 'della', 'popolazione', 'in', 'fon@@', 'do@@', '.', '</s>']
2025-05-30 00:37:58,383 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:37:58,383 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:37:58,383 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per osservare che il ghiaccio arccione artico, che si chiamano <unk> galleggiamento di tre milioni di anni per la più grande del 48 stati per cento per cento della popolazione in fondo.
2025-05-30 00:37:58,383 - INFO - joeynmt.training - Example #1
2025-05-30 00:37:58,383 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:37:58,383 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:37:58,383 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'ma', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'accio', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:37:58,384 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:37:58,384 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:37:58,384 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, ma non è abbastanza forte, perché non è il dibattito del ghiaccio dell<unk> Eises.
2025-05-30 00:37:58,384 - INFO - joeynmt.training - Example #2
2025-05-30 00:37:58,384 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:37:58,384 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:37:58,384 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'ris@@', 'i', 'ar@@', 't@@', 'ici', 'ar@@', 'c@@', 'ru@@', 'ci@@', 'ale', 'del', 'nostro', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:37:58,385 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:37:58,385 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:37:58,385 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la crisi artici arcruciale del nostro sistema climatico globale.
2025-05-30 00:37:58,385 - INFO - joeynmt.training - Example #3
2025-05-30 00:37:58,385 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:37:58,385 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:37:58,385 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'in', 'in@@', 'ver@@', 'no', 'e', 'sp@@', 'um@@', 'p@@', 'one', 'in', 'est@@', 'i@@', '.', '</s>']
2025-05-30 00:37:58,385 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:37:58,385 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:37:58,386 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo in inverno e spumpone in esti.
2025-05-30 00:37:58,386 - INFO - joeynmt.training - Example #4
2025-05-30 00:37:58,386 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:37:58,386 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:37:58,386 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'foto', 'di', 'un', 'di@@', 'seg@@', 'no', 'di', 'di@@', 'seg@@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:37:58,386 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:37:58,386 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:37:58,386 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una foto di un disegno di disegno di quello che è successo negli ultimi 25 anni.
2025-05-30 00:38:01,901 - INFO - joeynmt.training - Epoch  10, Step:    92100, Batch Loss:     1.701486, Batch Acc: 0.514593, Tokens per Sec:    16964, Lr: 0.000103
2025-05-30 00:38:05,361 - INFO - joeynmt.training - Epoch  10, Step:    92200, Batch Loss:     1.670083, Batch Acc: 0.514548, Tokens per Sec:    19566, Lr: 0.000103
2025-05-30 00:38:08,828 - INFO - joeynmt.training - Epoch  10, Step:    92300, Batch Loss:     1.701462, Batch Acc: 0.519617, Tokens per Sec:    19935, Lr: 0.000103
2025-05-30 00:38:12,283 - INFO - joeynmt.training - Epoch  10, Step:    92400, Batch Loss:     1.585439, Batch Acc: 0.519781, Tokens per Sec:    19753, Lr: 0.000103
2025-05-30 00:38:15,744 - INFO - joeynmt.training - Epoch  10, Step:    92500, Batch Loss:     1.607524, Batch Acc: 0.523650, Tokens per Sec:    19753, Lr: 0.000103
2025-05-30 00:38:15,744 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:38:15,744 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:38:22,066 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.71, ppl:   5.53, acc:   0.52, generation: 6.3122[sec], evaluation: 0.0000[sec]
2025-05-30 00:38:22,535 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/90000.ckpt
2025-05-30 00:38:22,564 - INFO - joeynmt.training - Example #0
2025-05-30 00:38:22,564 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:38:22,565 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:38:22,565 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'c@@', 'ci@@', 'one', 'ar@@', 'ch@@', 'ico', 'che', 'si', 'chiam@@', 'ano', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cento', 'di', 'anni', 'è', 'stato', 'ri@@', 'd@@', 'otto', 'per', 'cento', 'per', 'cento', 'di', 'questi', 'due', 's@@', 'ett@@', 'ori', 'di', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cent@@', 'o@@', '.', '</s>']
2025-05-30 00:38:22,565 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:38:22,565 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:38:22,565 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso ho mostrato queste due diapositive per osservare che il ghiaccio arccione archico che si chiamano per tre milioni di anni per la più grande 48 stati per cento per cento di anni è stato ridotto per cento per cento di questi due settori di 48 stati per cento per cento.
2025-05-30 00:38:22,565 - INFO - joeynmt.training - Example #1
2025-05-30 00:38:22,566 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:38:22,566 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:38:22,566 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'accio', 'd@@', 'ell@@', '<unk>', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:38:22,566 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:38:22,566 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:38:22,566 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte, non è abbastanza forte, perché non è il dibattito del ghiaccio dell<unk> ghiaccio.
2025-05-30 00:38:22,566 - INFO - joeynmt.training - Example #2
2025-05-30 00:38:22,566 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:38:22,566 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:38:22,566 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'atti@@', 'va', 'ar@@', 't@@', 'ica', 'del', 'nostro', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:38:22,567 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:38:22,567 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:38:22,567 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cattiva artica del nostro sistema climatico globale.
2025-05-30 00:38:22,567 - INFO - joeynmt.training - Example #3
2025-05-30 00:38:22,567 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:38:22,567 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:38:22,567 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'nel', 'in@@', 'ver@@', 'no', 'e', 'sp@@', 'um@@', 'ento', 'in', 'est@@', 'i@@', '.', '</s>']
2025-05-30 00:38:22,568 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:38:22,568 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:38:22,568 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo nel inverno e spumento in esti.
2025-05-30 00:38:22,568 - INFO - joeynmt.training - Example #4
2025-05-30 00:38:22,568 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:38:22,568 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:38:22,568 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'foto', 'di', 'una', 'ri@@', 'du@@', 'zione', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:38:22,569 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:38:22,569 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:38:22,569 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una foto di una riduzione di quello che è successo negli ultimi 25 anni.
2025-05-30 00:38:26,037 - INFO - joeynmt.training - Epoch  10, Step:    92600, Batch Loss:     1.746988, Batch Acc: 0.516526, Tokens per Sec:    16796, Lr: 0.000103
2025-05-30 00:38:29,494 - INFO - joeynmt.training - Epoch  10, Step:    92700, Batch Loss:     1.625844, Batch Acc: 0.523779, Tokens per Sec:    19650, Lr: 0.000103
2025-05-30 00:38:32,964 - INFO - joeynmt.training - Epoch  10, Step:    92800, Batch Loss:     1.793828, Batch Acc: 0.515245, Tokens per Sec:    19618, Lr: 0.000103
2025-05-30 00:38:36,426 - INFO - joeynmt.training - Epoch  10, Step:    92900, Batch Loss:     1.629726, Batch Acc: 0.517238, Tokens per Sec:    19669, Lr: 0.000103
2025-05-30 00:38:39,874 - INFO - joeynmt.training - Epoch  10, Step:    93000, Batch Loss:     1.696434, Batch Acc: 0.524402, Tokens per Sec:    19138, Lr: 0.000103
2025-05-30 00:38:39,874 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:38:39,874 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:38:46,023 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.71, ppl:   5.51, acc:   0.52, generation: 6.1392[sec], evaluation: 0.0000[sec]
2025-05-30 00:38:46,023 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:38:46,686 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/91500.ckpt
2025-05-30 00:38:46,716 - INFO - joeynmt.training - Example #0
2025-05-30 00:38:46,716 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:38:46,716 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:38:46,717 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'c@@', 'ci@@', 'one', 'ar@@', 'c@@', 'ic@@', 'o@@', ',', 'che', 'chiam@@', 'ano', '<unk>', 'E@@', 'is@@', 'co@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cento', 'per', 'cent@@', 'o@@', '.', '</s>']
2025-05-30 00:38:46,717 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:38:46,717 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:38:46,717 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per osservare che il ghiaccio arccione arcico, che chiamano <unk> Eisco, per tre milioni di anni per la più grande 48 stati per cento per cento per cento.
2025-05-30 00:38:46,717 - INFO - joeynmt.training - Example #1
2025-05-30 00:38:46,717 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:38:46,718 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:38:46,718 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:38:46,718 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:38:46,718 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:38:46,718 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, non è abbastanza forte, perché non è il dibattito del ghiaccio.
2025-05-30 00:38:46,718 - INFO - joeynmt.training - Example #2
2025-05-30 00:38:46,718 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:38:46,718 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:38:46,718 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'senso', 'è', 'il', 'cuore', 'ar@@', 'c@@', 'atti@@', 'vo', 'del', 'nostro', 'sistema', 'c@@', 'ru@@', 'ci@@', 'ale', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:38:46,719 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:38:46,719 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:38:46,719 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il senso è il cuore arcattivo del nostro sistema cruciale globale.
2025-05-30 00:38:46,719 - INFO - joeynmt.training - Example #3
2025-05-30 00:38:46,719 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:38:46,719 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:38:46,719 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', 'endo', 'in', 'in@@', 'ver@@', 'no', 'e', 'sp@@', 'um@@', 'ent@@', 'o@@', '.', '</s>']
2025-05-30 00:38:46,720 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:38:46,720 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:38:46,720 - INFO - joeynmt.training - 	Hypothesis: Si sta crescendo in inverno e spumento.
2025-05-30 00:38:46,720 - INFO - joeynmt.training - Example #4
2025-05-30 00:38:46,720 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:38:46,720 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:38:46,720 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'foto', 'di', 'un', 'att@@', 'eggi@@', 'amento', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:38:46,721 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:38:46,721 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:38:46,721 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una foto di un atteggiamento che è successo negli ultimi 25 anni.
2025-05-30 00:38:50,207 - INFO - joeynmt.training - Epoch  10, Step:    93100, Batch Loss:     1.624080, Batch Acc: 0.519829, Tokens per Sec:    16330, Lr: 0.000103
2025-05-30 00:38:53,670 - INFO - joeynmt.training - Epoch  10, Step:    93200, Batch Loss:     1.519573, Batch Acc: 0.521276, Tokens per Sec:    19826, Lr: 0.000103
2025-05-30 00:38:57,109 - INFO - joeynmt.training - Epoch  10, Step:    93300, Batch Loss:     1.713362, Batch Acc: 0.514700, Tokens per Sec:    19737, Lr: 0.000103
2025-05-30 00:39:00,532 - INFO - joeynmt.training - Epoch  10, Step:    93400, Batch Loss:     1.672223, Batch Acc: 0.521657, Tokens per Sec:    19703, Lr: 0.000103
2025-05-30 00:39:03,994 - INFO - joeynmt.training - Epoch  10, Step:    93500, Batch Loss:     1.450294, Batch Acc: 0.518588, Tokens per Sec:    20127, Lr: 0.000103
2025-05-30 00:39:03,995 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:39:03,995 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:39:10,830 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.70, ppl:   5.49, acc:   0.52, generation: 6.8255[sec], evaluation: 0.0000[sec]
2025-05-30 00:39:10,830 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:39:11,466 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/92500.ckpt
2025-05-30 00:39:11,497 - INFO - joeynmt.training - Example #0
2025-05-30 00:39:11,497 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:39:11,497 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:39:11,497 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'il', 'ghi@@', 'accio', 'ar@@', 'kt@@', 'ico', 'ar@@', 'c@@', 'ci@@', 'one', 'di', 'tre', 'milioni', 'di', 'anni', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'ci@@', 'dent@@', 'e@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'più', 'grande', '4@@', '8', 'stati', 'per', 'cento', 'per', 'cento', 'per', 'cent@@', 'o@@', '.', '</s>']
2025-05-30 00:39:11,498 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:39:11,498 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:39:11,498 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per osservare che il ghiaccio arktico arccione di tre milioni di anni la dimensione dell<unk> Occidente, per tre milioni di anni per la più grande 48 stati per cento per cento per cento.
2025-05-30 00:39:11,498 - INFO - joeynmt.training - Example #1
2025-05-30 00:39:11,498 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:39:11,498 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:39:11,498 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'ma', 'non', 'è', 'abbastanza', 'for@@', 'te@@', ',', 'perché', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'acci@@', 'o@@', '.', '</s>']
2025-05-30 00:39:11,499 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:39:11,499 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:39:11,499 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, ma non è abbastanza forte, perché non è il dibattito del ghiaccio.
2025-05-30 00:39:11,499 - INFO - joeynmt.training - Example #2
2025-05-30 00:39:11,499 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:39:11,499 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:39:11,499 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'la', 'c@@', 'atti@@', 'va', 'ar@@', 'kt@@', 'ico', 'del', 'nostro', 'sistema', 'c@@', 'ru@@', 'ci@@', 'ale', 'globale', 'di', 'c@@', 'atti@@', 'va', 'il', 'nostro', 'sistema', 'cli@@', 'mat@@', 'ic@@', 'o@@', '.', '</s>']
2025-05-30 00:39:11,500 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:39:11,500 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:39:11,500 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, la cattiva arktico del nostro sistema cruciale globale di cattiva il nostro sistema climatico.
2025-05-30 00:39:11,500 - INFO - joeynmt.training - Example #3
2025-05-30 00:39:11,500 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:39:11,500 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:39:11,500 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'cres@@', 'ce', 'nel', 'in@@', 'ver@@', 'no', 'e', 'si', 'sci@@', 'vol@@', 'a', 'in', 'est@@', 'i@@', 'at@@', 'e@@', '.', '</s>']
2025-05-30 00:39:11,500 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:39:11,500 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:39:11,501 - INFO - joeynmt.training - 	Hypothesis: Si cresce nel inverno e si scivola in estiate.
2025-05-30 00:39:11,501 - INFO - joeynmt.training - Example #4
2025-05-30 00:39:11,501 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:39:11,501 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:39:11,501 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'erò', 'è', 'una', 'foto', 'di', 'un', 'att@@', 'eggi@@', 'amento', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:39:11,501 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:39:11,501 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:39:11,501 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostrerò è una foto di un atteggiamento che è successo negli ultimi 25 anni.
2025-05-30 00:39:14,966 - INFO - joeynmt.training - Epoch  10, Step:    93600, Batch Loss:     1.799026, Batch Acc: 0.509725, Tokens per Sec:    16623, Lr: 0.000103
2025-05-30 00:39:18,434 - INFO - joeynmt.training - Epoch  10, Step:    93700, Batch Loss:     1.929826, Batch Acc: 0.514673, Tokens per Sec:    20011, Lr: 0.000103
2025-05-30 00:39:21,897 - INFO - joeynmt.training - Epoch  10, Step:    93800, Batch Loss:     1.714666, Batch Acc: 0.514761, Tokens per Sec:    19878, Lr: 0.000103
2025-05-30 00:39:25,352 - INFO - joeynmt.training - Epoch  10, Step:    93900, Batch Loss:     1.725132, Batch Acc: 0.518222, Tokens per Sec:    20139, Lr: 0.000103
2025-05-30 00:39:28,820 - INFO - joeynmt.training - Epoch  10, Step:    94000, Batch Loss:     1.561100, Batch Acc: 0.517937, Tokens per Sec:    20092, Lr: 0.000103
2025-05-30 00:39:28,820 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:39:28,820 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:39:35,477 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.70, ppl:   5.49, acc:   0.52, generation: 6.6470[sec], evaluation: 0.0000[sec]
2025-05-30 00:39:35,477 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-30 00:39:36,121 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/90500.ckpt
2025-05-30 00:39:36,150 - INFO - joeynmt.training - Example #0
2025-05-30 00:39:36,150 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', 'etz@@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', 'ien', 'ge@@', 'zei@@', 'g@@', 't@@', ',', 'um', 'zu', 'ver@@', 'an@@', 'schau@@', 'lich@@', 'en@@', ',', 'dass', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e@@', ',', 'die', 'für', 'ann@@', 'ä@@', 'her@@', 'n@@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', 'ös@@', 'se', 'der', 'unter@@', 'en', '4@@', '8', 'Staaten', 'h@@', 'att@@', 'e@@', ',', 'um', '40', 'Prozent', 'gesch@@', 'r@@', 'um@@', 'p@@', 'ft', 'ist@@', '.']
2025-05-30 00:39:36,150 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', "'@@", 'anno', 'scorso', 'ho', 'mostr@@', 'ato', 'queste', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'dimostr@@', 'are', 'che', 'la', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ic@@', 'a@@', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'Uniti', 'cont@@', 'in@@', 'ent@@', 'ali@@', ',', 'si', 'è', 'ri@@', 'str@@', 'etta', 'del', '4@@', '0@@', '%@@', '.']
2025-05-30 00:39:36,150 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', 'anno', 'scor@@', 'so@@', ',', 'ho', 'mostr@@', 'ato', 'queste', 'due', 'di@@', 'apos@@', 'iti@@', 've', 'per', 'osserv@@', 'are', 'che', 'l@@', '<unk>', 'ar@@', 'c@@', 'op@@', 'pia', 'ar@@', 'c@@', 'ica', 'che', 'ha', 'avuto', 'tre', 'milioni', 'di', 'anni', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'ci@@', 'dent@@', 'e@@', ',', 'per', 'tre', 'milioni', 'di', 'anni', 'per', 'la', 'dimen@@', 'sione', 'd@@', 'ell@@', '<unk>', 'O@@', 'c@@', 'ci@@', 'dent@@', 'e@@', '.', '</s>']
2025-05-30 00:39:36,151 - INFO - joeynmt.training - 	Source:     Letztes Jahr habe ich diese beiden Folien gezeigt, um zu veranschaulichen, dass die arktische Eiskappe, die für annähernd drei Millionen Jahre die Grösse der unteren 48 Staaten hatte, um 40 Prozent geschrumpft ist.
2025-05-30 00:39:36,151 - INFO - joeynmt.training - 	Reference:  L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2025-05-30 00:39:36,151 - INFO - joeynmt.training - 	Hypothesis: L<unk> anno scorso, ho mostrato queste due diapositive per osservare che l<unk> arcoppia arcica che ha avuto tre milioni di anni la dimensione dell<unk> Occidente, per tre milioni di anni per la dimensione dell<unk> Occidente.
2025-05-30 00:39:36,151 - INFO - joeynmt.training - Example #1
2025-05-30 00:39:36,151 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', 'ückt', 'nicht', 'star@@', 'k', 'genug', 'die', 'Er@@', 'n@@', 'st@@', 'ha@@', 'f@@', 'tigkeit', 'dieses', 'spezi@@', 'ellen', 'Probl@@', 'em@@', 's', 'aus@@', ',', 'da', 'es', 'nicht', 'die', 'D@@', 'ic@@', 'ke', 'des', 'E@@', 'is@@', 'es', 'zei@@', 'g@@', 't@@', '.']
2025-05-30 00:39:36,151 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', 'utt@@', 'a@@', 'via', 'questo', 'sott@@', 'ov@@', 'al@@', 'uta', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'ghi@@', 'acci@@', 'o@@', '.']
2025-05-30 00:39:36,151 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'molto', 'for@@', 'te@@', ',', 'che', 'è', 'molto', 'più', 'forte', 'di', 'questo', 'problema', 'che', 'non', 'è', 'il', 'di@@', 'batt@@', 'ito', 'del', 'ghi@@', 'accio', 'd@@', 'ell@@', '<unk>', 'E@@', 'is@@', 'es@@', '.', '</s>']
2025-05-30 00:39:36,152 - INFO - joeynmt.training - 	Source:     Aber dies drückt nicht stark genug die Ernsthaftigkeit dieses speziellen Problems aus, da es nicht die Dicke des Eises zeigt.
2025-05-30 00:39:36,152 - INFO - joeynmt.training - 	Reference:  Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2025-05-30 00:39:36,152 - INFO - joeynmt.training - 	Hypothesis: Ma non è molto forte, che è molto più forte di questo problema che non è il dibattito del ghiaccio dell<unk> Eises.
2025-05-30 00:39:36,152 - INFO - joeynmt.training - Example #2
2025-05-30 00:39:36,152 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', 'wis@@', 's@@', 'em', 'Sin@@', 'ne', 'ist', 'die', 'ar@@', 'kt@@', 'ische', 'E@@', 'is@@', 'k@@', 'app@@', 'e', 'das', 'schl@@', 'ag@@', 'ende', 'Her@@', 'z', 'unseres', 'glob@@', 'alen', 'K@@', 'lim@@', 'as@@', 'yst@@', 'em@@', 's@@', '.']
2025-05-30 00:39:36,152 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', 'otta', 'gl@@', 'a@@', 'ci@@', 'ale', 'art@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'cuore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'al@@', 'e@@', '.']
2025-05-30 00:39:36,152 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'sen@@', 'so@@', ',', 'il', 'c@@', 'atti@@', 'vo', 'ar@@', 'kt@@', 'ico', 'ar@@', 't@@', 'ico', 'glob@@', 'al@@', 'e@@', '.', '</s>']
2025-05-30 00:39:36,153 - INFO - joeynmt.training - 	Source:     In gewissem Sinne ist die arktische Eiskappe das schlagende Herz unseres globalen Klimasystems.
2025-05-30 00:39:36,153 - INFO - joeynmt.training - 	Reference:  La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2025-05-30 00:39:36,153 - INFO - joeynmt.training - 	Hypothesis: In un certo senso, il cattivo arktico artico globale.
2025-05-30 00:39:36,153 - INFO - joeynmt.training - Example #3
2025-05-30 00:39:36,153 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', 'ächst', 'im', 'Win@@', 'ter', 'und', 'sch@@', 'r@@', 'um@@', 'p@@', 'ft', 'im', 'S@@', 'om@@', 'mer@@', '.']
2025-05-30 00:39:36,153 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', 'pan@@', 'de', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 'si', 'rit@@', 'ir@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e@@', '.']
2025-05-30 00:39:36,153 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'cres@@', 'ce', 'nel', 'in@@', 'ver@@', 'no', 'e', 'si', 'sp@@', 'um@@', 'oli', 'in', 'est@@', 'i@@', '.', '</s>']
2025-05-30 00:39:36,154 - INFO - joeynmt.training - 	Source:     Sie wächst im Winter und schrumpft im Sommer.
2025-05-30 00:39:36,154 - INFO - joeynmt.training - 	Reference:  Si espande d'inverno e si ritira d'estate.
2025-05-30 00:39:36,154 - INFO - joeynmt.training - 	Hypothesis: Si cresce nel inverno e si spumoli in esti.
2025-05-30 00:39:36,154 - INFO - joeynmt.training - Example #4
2025-05-30 00:39:36,154 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', 'ie@@', ',', 'die', 'ich', 'Ihnen', 'zei@@', 'ge@@', ',', 'ist', 'eine', 'Zei@@', 'tra@@', 'ff@@', 'er@@', 'auf@@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist@@', '.']
2025-05-30 00:39:36,154 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', 'ma', 'di@@', 'apos@@', 'iti@@', 'va', 'sarà', 'una', 'rap@@', 'ida', 'car@@', 'r@@', 'ell@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '25', 'an@@', 'ni@@', '.']
2025-05-30 00:39:36,154 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', 'ma', 's@@', 'li@@', 'de', 'che', 'vi', 'mostr@@', 'o', 'è', 'una', 'foto', 'di', 'una', 'ri@@', 'du@@', 'zione', 'di', 'di@@', 'seg@@', 'no', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'an@@', 'ni@@', '.', '</s>']
2025-05-30 00:39:36,154 - INFO - joeynmt.training - 	Source:     Die nächste Folie, die ich Ihnen zeige, ist eine Zeitrafferaufnahme was in den letzten 25 Jahren passiert ist.
2025-05-30 00:39:36,154 - INFO - joeynmt.training - 	Reference:  La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2025-05-30 00:39:36,154 - INFO - joeynmt.training - 	Hypothesis: La prossima slide che vi mostro è una foto di una riduzione di disegno che è successo negli ultimi 25 anni.
2025-05-30 00:39:39,673 - INFO - joeynmt.training - Epoch  10, Step:    94100, Batch Loss:     1.797005, Batch Acc: 0.518726, Tokens per Sec:    17001, Lr: 0.000103
2025-05-30 00:39:43,142 - INFO - joeynmt.training - Epoch  10, Step:    94200, Batch Loss:     1.699735, Batch Acc: 0.519069, Tokens per Sec:    19957, Lr: 0.000103
2025-05-30 00:39:45,340 - INFO - joeynmt.training - Epoch  10: total training loss 16028.47
2025-05-30 00:39:45,340 - INFO - joeynmt.training - Training ended after  10 epochs.
2025-05-30 00:39:45,340 - INFO - joeynmt.training - Best validation result (greedy) at step    94000:   5.49 ppl.
2025-05-30 00:39:45,358 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-30 00:39:45,421 - INFO - joeynmt.model - Enc-dec model built.
2025-05-30 00:39:45,510 - INFO - joeynmt.helpers - Load model from /home/user/amsler/mt-exercise-4/models/bpe_5k_de_it/94000.ckpt.
2025-05-30 00:39:45,530 - INFO - joeynmt.prediction - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=4992),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=4992),
	loss_function=None)
2025-05-30 00:39:45,541 - INFO - joeynmt.prediction - Decoding on dev set...
2025-05-30 00:39:45,541 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:39:45,541 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:39:58,893 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 13.3444[sec], evaluation: 0.0000[sec]
2025-05-30 00:39:58,899 - INFO - joeynmt.prediction - Translations saved to: /home/user/amsler/mt-exercise-4/models/bpe_5k_de_it/00094000.hyps.dev.
2025-05-30 00:39:58,899 - INFO - joeynmt.prediction - Decoding on test set...
2025-05-30 00:39:58,899 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-30 00:39:58,899 - INFO - joeynmt.prediction - Predicting 1567 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-30 00:40:16,011 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 17.1059[sec], evaluation: 0.0000[sec]
2025-05-30 00:40:16,016 - INFO - joeynmt.prediction - Translations saved to: /home/user/amsler/mt-exercise-4/models/bpe_5k_de_it/00094000.hyps.test.
