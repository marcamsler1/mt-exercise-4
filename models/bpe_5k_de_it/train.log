2025-05-28 09:14:01,359 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-28 09:14:01,360 - INFO - joeynmt.helpers -                           cfg.name : bpe_5k_de_it
2025-05-28 09:14:01,360 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2025-05-28 09:14:01,360 - INFO - joeynmt.helpers -                     cfg.data.train : data/bpe_5k/train.bpe
2025-05-28 09:14:01,360 - INFO - joeynmt.helpers -                       cfg.data.dev : data/bpe_5k/dev.bpe
2025-05-28 09:14:01,360 - INFO - joeynmt.helpers -                      cfg.data.test : data/bpe_5k/test.bpe
2025-05-28 09:14:01,360 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain
2025-05-28 09:14:01,360 - INFO - joeynmt.helpers -                  cfg.data.src.lang : de
2025-05-28 09:14:01,360 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe
2025-05-28 09:14:01,360 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2025-05-28 09:14:01,360 - INFO - joeynmt.helpers -       cfg.data.src.max_sent_length : 100
2025-05-28 09:14:01,360 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : subword_models/5k/bpe_5k.joint_vocab
2025-05-28 09:14:01,360 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : subword-nmt
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -   cfg.data.src.tokenizer_cfg.codes : subword_models/5k/bpe_5k.codes
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : it
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -       cfg.data.trg.max_sent_length : 100
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : subword_models/5k/bpe_5k.joint_vocab
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : subword-nmt
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -   cfg.data.trg.tokenizer_cfg.codes : subword_models/5k/bpe_5k.codes
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -                  cfg.testing.alpha : 1.0
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -            cfg.training.batch_size : 2048
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 1024
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -              cfg.training.patience : 8
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -                cfg.training.epochs : 10
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 500
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -           cfg.training.eval_metric : bleu
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -             cfg.training.model_dir : models/bpe_5k_de_it
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -             cfg.training.overwrite : False
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -              cfg.training.use_cuda : True
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 100
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3, 4]
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.3
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier_uniform
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier_uniform
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2025-05-28 09:14:01,361 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True
2025-05-28 09:14:01,362 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2025-05-28 09:14:01,362 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2025-05-28 09:14:01,362 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 4
2025-05-28 09:14:01,362 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 2
2025-05-28 09:14:01,362 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2025-05-28 09:14:01,362 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2025-05-28 09:14:01,362 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0
2025-05-28 09:14:01,362 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2025-05-28 09:14:01,362 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 512
2025-05-28 09:14:01,362 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0
2025-05-28 09:14:01,362 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2025-05-28 09:14:01,362 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 1
2025-05-28 09:14:01,362 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 2
2025-05-28 09:14:01,362 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2025-05-28 09:14:01,362 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2025-05-28 09:14:01,362 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0
2025-05-28 09:14:01,362 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2025-05-28 09:14:01,362 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 512
2025-05-28 09:14:01,362 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0
2025-05-28 09:14:01,497 - INFO - joeynmt.data - Building tokenizer...
2025-05-28 09:14:01,516 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-28 09:14:01,516 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, -1), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-28 09:14:01,516 - INFO - joeynmt.data - Loading train set...
2025-05-28 09:14:01,686 - INFO - joeynmt.data - Building vocabulary...
2025-05-28 09:14:01,895 - INFO - joeynmt.data - Loading dev set...
2025-05-28 09:14:01,901 - INFO - joeynmt.data - Loading test set...
2025-05-28 09:14:01,908 - INFO - joeynmt.data - Data loaded.
2025-05-28 09:14:01,908 - INFO - joeynmt.data - Train dataset: PlaintextDataset(split=train, len=100000, src_lang=de, trg_lang=it, has_trg=True, random_subset=-1)
2025-05-28 09:14:01,908 - INFO - joeynmt.data - Valid dataset: PlaintextDataset(split=dev, len=923, src_lang=de, trg_lang=it, has_trg=True, random_subset=-1)
2025-05-28 09:14:01,908 - INFO - joeynmt.data -  Test dataset: PlaintextDataset(split=test, len=1567, src_lang=de, trg_lang=it, has_trg=True, random_subset=-1)
2025-05-28 09:14:01,909 - INFO - joeynmt.data - First training example:
	[SRC] Jetzt erinn@@ @@@ @ ern Sie sich , wir unter@@ @@@ @ suchen G@@ @@@ @ ene .
	[TRG] Ric@@ @@@ @ ord@@ @@@ @ ate che noi an@@ @@@ @ ali@@ @@@ @ z@@ @@@ @ zi@@ @@@ @ amo i gen@@ @@@ @ i .
2025-05-28 09:14:01,909 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ! (5) # (6) $ (7) % (8) &#91; (9) &#93;
2025-05-28 09:14:01,909 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) ! (5) # (6) $ (7) % (8) &#91; (9) &#93;
2025-05-28 09:14:01,909 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 4992
2025-05-28 09:14:01,909 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 4992
2025-05-28 09:14:01,918 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-28 09:14:02,001 - INFO - joeynmt.model - Enc-dec model built.
2025-05-28 09:14:02,020 - INFO - joeynmt.model - Total params: 4177152
2025-05-28 09:14:02,021 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'src_embed.lut.weight']
2025-05-28 09:14:02,022 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=4992),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=4992),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.3))
2025-05-28 09:14:05,513 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=(0.9, 0.999))
2025-05-28 09:14:05,514 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, threshold_mode=abs, eps=0.0, factor=0.7, patience=8)
2025-05-28 09:14:05,514 - INFO - joeynmt.training - Train stats:
	device: cuda
	n_gpu: 1
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 2048
	effective batch size (w. parallel & accumulation): 2048
2025-05-28 09:14:05,515 - INFO - joeynmt.training - EPOCH 1
2025-05-28 09:14:09,622 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     2.961700, Batch Acc: 0.303853, Tokens per Sec:    17611, Lr: 0.000300
2025-05-28 09:14:13,109 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     2.645480, Batch Acc: 0.390527, Tokens per Sec:    20440, Lr: 0.000300
2025-05-28 09:14:16,603 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     2.636791, Batch Acc: 0.406167, Tokens per Sec:    20571, Lr: 0.000300
2025-05-28 09:14:20,075 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     2.664561, Batch Acc: 0.421129, Tokens per Sec:    20920, Lr: 0.000300
2025-05-28 09:14:23,570 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     2.523987, Batch Acc: 0.422843, Tokens per Sec:    20844, Lr: 0.000300
2025-05-28 09:14:23,571 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:14:23,571 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:14:46,094 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.48, ppl:  11.96, acc:   0.43, generation: 22.5103[sec], evaluation: 0.0000[sec]
2025-05-28 09:14:46,096 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:14:46,635 - INFO - joeynmt.training - Example #0
2025-05-28 09:14:46,636 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:14:46,636 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:14:46,636 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'è', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un']
2025-05-28 09:14:46,637 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:14:46,637 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:14:46,637 - INFO - joeynmt.training - 	Hypothesis: E è è , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un
2025-05-28 09:14:46,637 - INFO - joeynmt.training - Example #1
2025-05-28 09:14:46,638 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:14:46,638 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:14:46,638 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'è', 'è', 'è', 'è', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un']
2025-05-28 09:14:46,638 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:14:46,638 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:14:46,638 - INFO - joeynmt.training - 	Hypothesis: E è è è è è , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un
2025-05-28 09:14:46,638 - INFO - joeynmt.training - Example #2
2025-05-28 09:14:46,639 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:14:46,639 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:14:46,639 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'è', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un', 'un']
2025-05-28 09:14:46,639 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:14:46,639 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:14:46,639 - INFO - joeynmt.training - 	Hypothesis: E è è , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un
2025-05-28 09:14:46,640 - INFO - joeynmt.training - Example #3
2025-05-28 09:14:46,640 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:14:46,640 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:14:46,640 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', '.', '</s>']
2025-05-28 09:14:46,640 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:14:46,640 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:14:46,640 - INFO - joeynmt.training - 	Hypothesis: E è è è è è è è è è è è è è è è è è è è è è , , , , , , , , , , , , , , , , , , , , , , , , , .
2025-05-28 09:14:46,640 - INFO - joeynmt.training - Example #4
2025-05-28 09:14:46,641 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:14:46,641 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:14:46,641 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', 'è', 'è', 'è', 'è', ',', '.', '</s>']
2025-05-28 09:14:46,641 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:14:46,641 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:14:46,641 - INFO - joeynmt.training - 	Hypothesis: E è è è è è è è è è è è è è è è è è è è è , , , , , , , , , , , , , , , , , , , , , è è è è , .
2025-05-28 09:14:50,077 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     2.525544, Batch Acc: 0.426591, Tokens per Sec:    17955, Lr: 0.000300
2025-05-28 09:14:53,547 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     2.457368, Batch Acc: 0.427781, Tokens per Sec:    20629, Lr: 0.000300
2025-05-28 09:14:57,020 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     2.371333, Batch Acc: 0.429129, Tokens per Sec:    20281, Lr: 0.000300
2025-05-28 09:15:00,505 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     2.555704, Batch Acc: 0.430021, Tokens per Sec:    20474, Lr: 0.000300
2025-05-28 09:15:03,986 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     2.442858, Batch Acc: 0.431885, Tokens per Sec:    20529, Lr: 0.000300
2025-05-28 09:15:03,987 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:15:03,987 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:15:28,976 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.44, ppl:  11.50, acc:   0.43, generation: 24.9700[sec], evaluation: 0.0000[sec]
2025-05-28 09:15:28,977 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:15:29,590 - INFO - joeynmt.training - Example #0
2025-05-28 09:15:29,591 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:15:29,591 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:15:29,591 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']
2025-05-28 09:15:29,592 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:15:29,592 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:15:29,592 - INFO - joeynmt.training - 	Hypothesis: E , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,
2025-05-28 09:15:29,592 - INFO - joeynmt.training - Example #1
2025-05-28 09:15:29,593 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:15:29,593 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:15:29,593 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',']
2025-05-28 09:15:29,593 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:15:29,593 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:15:29,593 - INFO - joeynmt.training - 	Hypothesis: E è è è è è è è è è è è è è è , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,
2025-05-28 09:15:29,594 - INFO - joeynmt.training - Example #2
2025-05-28 09:15:29,594 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:15:29,594 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:15:29,594 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', '.', '</s>']
2025-05-28 09:15:29,594 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:15:29,594 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:15:29,595 - INFO - joeynmt.training - 	Hypothesis: E , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , .
2025-05-28 09:15:29,595 - INFO - joeynmt.training - Example #3
2025-05-28 09:15:29,595 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:15:29,595 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:15:29,595 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', '.', '</s>']
2025-05-28 09:15:29,595 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:15:29,595 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:15:29,595 - INFO - joeynmt.training - 	Hypothesis: E è è è è è è è è è è è è è è è , , , , , , , , , , , , , , , , , , , , , , , , .
2025-05-28 09:15:29,596 - INFO - joeynmt.training - Example #4
2025-05-28 09:15:29,596 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:15:29,596 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:15:29,596 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', '.', '</s>']
2025-05-28 09:15:29,596 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:15:29,596 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:15:29,596 - INFO - joeynmt.training - 	Hypothesis: E , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , .
2025-05-28 09:15:33,094 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     2.535677, Batch Acc: 0.430403, Tokens per Sec:    17418, Lr: 0.000300
2025-05-28 09:15:36,591 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     2.455604, Batch Acc: 0.431114, Tokens per Sec:    21420, Lr: 0.000300
2025-05-28 09:15:40,036 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     2.443514, Batch Acc: 0.432411, Tokens per Sec:    20355, Lr: 0.000300
2025-05-28 09:15:43,505 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     2.294784, Batch Acc: 0.433676, Tokens per Sec:    20898, Lr: 0.000300
2025-05-28 09:15:46,953 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     2.368045, Batch Acc: 0.432570, Tokens per Sec:    20127, Lr: 0.000300
2025-05-28 09:15:46,954 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:15:46,954 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:16:06,936 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.41, ppl:  11.11, acc:   0.44, generation: 19.9714[sec], evaluation: 0.0000[sec]
2025-05-28 09:16:06,936 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:16:07,532 - INFO - joeynmt.training - Example #0
2025-05-28 09:16:07,533 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:16:07,533 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:16:07,533 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'che', 'il', 'mondo', '.', '</s>']
2025-05-28 09:16:07,534 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:16:07,534 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:16:07,534 - INFO - joeynmt.training - 	Hypothesis: E che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che che il mondo .
2025-05-28 09:16:07,534 - INFO - joeynmt.training - Example #1
2025-05-28 09:16:07,535 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:16:07,535 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:16:07,535 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'è', 'un', 'mondo', ',', 'la', 'mondo', ',', 'la', 'mondo', ',', 'la', 'mondo', ',', 'la', 'mondo', ',', 'la', 'mondo', ',', 'e', 'un', 'mondo', ',', 'e', 'un', 'mondo', ',', 'e', 'un', 'mondo', ',', 'e', 'un', 'mondo', '.', '</s>']
2025-05-28 09:16:07,535 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:16:07,535 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:16:07,535 - INFO - joeynmt.training - 	Hypothesis: E è è un mondo , la mondo , la mondo , la mondo , la mondo , la mondo , e un mondo , e un mondo , e un mondo , e un mondo .
2025-05-28 09:16:07,535 - INFO - joeynmt.training - Example #2
2025-05-28 09:16:07,536 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:16:07,536 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:16:07,536 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'un', 'mondo', ',', 'un', 'mondo', ',', 'un', 'mondo', ',', 'un', 'mondo', ',', 'e', 'un', 'mondo', ',', 'un', 'mondo', ',', 'e', 'un', 'mondo', ',', 'e', 'un', 'mondo', ',', 'e', 'un', 'mondo', ',', 'e', 'un', 'mondo', '.', '</s>']
2025-05-28 09:16:07,536 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:16:07,536 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:16:07,536 - INFO - joeynmt.training - 	Hypothesis: E è un mondo , un mondo , un mondo , un mondo , e un mondo , un mondo , e un mondo , e un mondo , e un mondo , e un mondo .
2025-05-28 09:16:07,537 - INFO - joeynmt.training - Example #3
2025-05-28 09:16:07,537 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:16:07,537 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:16:07,537 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'un', 'mondo', ',', 'un', 'mondo', ',', 'un', 'mondo', ',', 'un', 'mondo', ',', 'un', 'mondo', ',', 'un', 'mondo', '.', '</s>']
2025-05-28 09:16:07,537 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:16:07,537 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:16:07,537 - INFO - joeynmt.training - 	Hypothesis: E è un mondo , un mondo , un mondo , un mondo , un mondo , un mondo .
2025-05-28 09:16:07,537 - INFO - joeynmt.training - Example #4
2025-05-28 09:16:07,538 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:16:07,538 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:16:07,538 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'un', 'mondo', ',', 'la', 'mondo', ',', 'la', 'mondo', ',', 'la', 'mondo', ',', 'la', 'mondo', ',', 'un', 'mondo', '.', '</s>']
2025-05-28 09:16:07,538 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:16:07,538 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:16:07,538 - INFO - joeynmt.training - 	Hypothesis: E è un mondo , la mondo , la mondo , la mondo , la mondo , un mondo .
2025-05-28 09:16:11,050 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     2.554096, Batch Acc: 0.437400, Tokens per Sec:    17546, Lr: 0.000300
2025-05-28 09:16:14,522 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     2.399740, Batch Acc: 0.439179, Tokens per Sec:    20442, Lr: 0.000300
2025-05-28 09:16:17,995 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     2.334828, Batch Acc: 0.441856, Tokens per Sec:    20457, Lr: 0.000300
2025-05-28 09:16:21,491 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     2.425221, Batch Acc: 0.438117, Tokens per Sec:    20523, Lr: 0.000300
2025-05-28 09:16:24,972 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     2.432489, Batch Acc: 0.440890, Tokens per Sec:    20711, Lr: 0.000300
2025-05-28 09:16:24,973 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:16:24,973 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:16:36,176 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.37, ppl:  10.73, acc:   0.44, generation: 11.1940[sec], evaluation: 0.0000[sec]
2025-05-28 09:16:36,176 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:16:36,792 - INFO - joeynmt.training - Example #0
2025-05-28 09:16:36,793 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:16:36,793 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:16:36,793 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'la', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mondo', ',', 'e', 'un', 'mondo', ',', 'e', 'un', 'mondo', ',', 'e', 'un', 'mondo', ',', 'e', 'un', 'mondo', ',', 'e', 'un', 'mondo', '.', '</s>']
2025-05-28 09:16:36,793 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:16:36,793 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:16:36,794 - INFO - joeynmt.training - 	Hypothesis: E la mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mondo , e un mondo , e un mondo , e un mondo , e un mondo , e un mondo .
2025-05-28 09:16:36,794 - INFO - joeynmt.training - Example #1
2025-05-28 09:16:36,794 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:16:36,794 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:16:36,794 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'un', 'nostro', 'nostro', 'mondo', ',', 'e', 'la', 'mia', 'loro', ',', 'non', 'è', 'un', 'mondo', ',', 'non', 'è', 'un', 'mondo', ',', 'e', 'la', 'mia', 'mia', 'mia', 'loro', ',', 'e', 'la', 'loro', ',', 'e', 'la', 'mia', 'mia', 'mia', 'mia', 'loro', '.', '</s>']
2025-05-28 09:16:36,795 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:16:36,795 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:16:36,795 - INFO - joeynmt.training - 	Hypothesis: E è un nostro nostro mondo , e la mia loro , non è un mondo , non è un mondo , e la mia mia mia loro , e la loro , e la mia mia mia mia loro .
2025-05-28 09:16:36,795 - INFO - joeynmt.training - Example #2
2025-05-28 09:16:36,795 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:16:36,795 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:16:36,795 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'un', 'nostro', 'nostro', 'mondo', 'di', 'un', 'c@@', '<unk>', '@', 'o', ',', 'è', 'un', 'c@@', '<unk>', '@', 'o', ',', 'e', ',', 'è', 'un', 'mondo', '.', '</s>']
2025-05-28 09:16:36,796 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:16:36,796 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:16:36,796 - INFO - joeynmt.training - 	Hypothesis: E è un nostro nostro mondo di un c<unk> @ o , è un c<unk> @ o , e , è un mondo .
2025-05-28 09:16:36,796 - INFO - joeynmt.training - Example #3
2025-05-28 09:16:36,796 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:16:36,796 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:16:36,796 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'un', 'nostro', 'mondo', ',', 'la', 'loro', '.', '</s>']
2025-05-28 09:16:36,797 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:16:36,797 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:16:36,797 - INFO - joeynmt.training - 	Hypothesis: E è un nostro mondo , la loro .
2025-05-28 09:16:36,797 - INFO - joeynmt.training - Example #4
2025-05-28 09:16:36,797 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:16:36,797 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:16:36,797 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'un', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'mia', 'altro', '.', '</s>']
2025-05-28 09:16:36,797 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:16:36,798 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:16:36,798 - INFO - joeynmt.training - 	Hypothesis: E è un mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia mia altro .
2025-05-28 09:16:40,308 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     2.322253, Batch Acc: 0.441926, Tokens per Sec:    17551, Lr: 0.000300
2025-05-28 09:16:43,790 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     2.469867, Batch Acc: 0.442236, Tokens per Sec:    20701, Lr: 0.000300
2025-05-28 09:16:47,274 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     2.338009, Batch Acc: 0.444821, Tokens per Sec:    20924, Lr: 0.000300
2025-05-28 09:16:50,751 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     2.361157, Batch Acc: 0.447709, Tokens per Sec:    20706, Lr: 0.000300
2025-05-28 09:16:54,203 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     2.354455, Batch Acc: 0.449036, Tokens per Sec:    20795, Lr: 0.000300
2025-05-28 09:16:54,203 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:16:54,204 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:17:07,232 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.34, ppl:  10.34, acc:   0.45, generation: 13.0190[sec], evaluation: 0.0000[sec]
2025-05-28 09:17:07,233 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:17:07,860 - INFO - joeynmt.training - Example #0
2025-05-28 09:17:07,861 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:17:07,861 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:17:07,861 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'che', 'è', 'un', 'po', 'E', 'è', 'un', 'po', 'E', 'che', 'è', 'un', 'po', 'E', 'è', 'un', 'po', 'è', 'un', 'po', 'è', 'un', 'po', ',', 'e', 'la', 'mio', 'cosa', 'che', 'è', 'un', 'po', ',', 'e', 'la', 'mio', 'cosa', 'è', 'un', 'po', ',', 'e', 'il', 'mio', 'cosa', 'che', 'è', 'un', 'po', 'è', 'un', 'po', 'è', 'un', 'po', 'è', 'un', 'po', 'è', 'un', 'po', ',', 'e', 'la', 'mio', 'cosa', 'che', 'è', 'un', 'po', ',', 'e', 'il', 'mio', 'cosa', ',', 'e', 'il', 'mio', 'cosa', '.', '</s>']
2025-05-28 09:17:07,862 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:17:07,862 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:17:07,862 - INFO - joeynmt.training - 	Hypothesis: Ma che è un po E è un po E che è un po E è un po è un po è un po , e la mio cosa che è un po , e la mio cosa è un po , e il mio cosa che è un po è un po è un po è un po è un po , e la mio cosa che è un po , e il mio cosa , e il mio cosa .
2025-05-28 09:17:07,862 - INFO - joeynmt.training - Example #1
2025-05-28 09:17:07,862 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:17:07,862 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:17:07,863 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'un', 'cosa', 'è', 'un', 'po', 'E', 'non', 'è', 'un', 'po', 'E', 'non', 'è', 'un', 'po', 'è', 'un', 'mio', 'cosa', 'non', 'è', 'un', 'po', '&apos;', ',', 'ma', 'non', 'è', 'un', 'po', ',', 'ma', 'non', 'è', 'un', 'modo', ',', 'ma', 'non', 'è', 'un', 'mio', 'vita', '.', '</s>']
2025-05-28 09:17:07,863 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:17:07,863 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:17:07,863 - INFO - joeynmt.training - 	Hypothesis: Ma non è un cosa è un po E non è un po E non è un po è un mio cosa non è un po &apos; , ma non è un po , ma non è un modo , ma non è un mio vita .
2025-05-28 09:17:07,863 - INFO - joeynmt.training - Example #2
2025-05-28 09:17:07,863 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:17:07,864 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:17:07,864 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'mio', 'è', 'una', 'cosa', 'è', 'un', 'po', 'è', 'una', 'è', 'una', 'cosa', 'è', 'una', 'cosa', 'è', 'una', 'è', 'una', 'cosa', 'è', 'una', 'cosa', 'è', 'una', 'è', 'una', 'cosa', 'è', 'una', 'è', 'una', 'cosa', 'è', 'una', 's@@', '<unk>', '@', 'a', '.', '</s>']
2025-05-28 09:17:07,864 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:17:07,864 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:17:07,864 - INFO - joeynmt.training - 	Hypothesis: La mio è una cosa è un po è una è una cosa è una cosa è una è una cosa è una cosa è una è una cosa è una è una cosa è una s<unk> @ a .
2025-05-28 09:17:07,864 - INFO - joeynmt.training - Example #3
2025-05-28 09:17:07,864 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:17:07,864 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:17:07,865 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'mio', 'cosa', 'è', 'un', 'altro', 'di', 'un', 'altro', '.', '</s>']
2025-05-28 09:17:07,865 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:17:07,865 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:17:07,865 - INFO - joeynmt.training - 	Hypothesis: La mio cosa è un altro di un altro .
2025-05-28 09:17:07,865 - INFO - joeynmt.training - Example #4
2025-05-28 09:17:07,865 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:17:07,865 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:17:07,865 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'è', 'è', 'è', 'è', 'è', 'è', 'il', 'mio', 'cosa', 'che', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'è', 'che', 'è', 'che', 'è', 'il', 'mio', 'cosa', '.', '</s>']
2025-05-28 09:17:07,866 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:17:07,866 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:17:07,866 - INFO - joeynmt.training - 	Hypothesis: E è è è è è è il mio cosa che è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è è che è che è il mio cosa .
2025-05-28 09:17:11,342 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     2.297081, Batch Acc: 0.448693, Tokens per Sec:    16729, Lr: 0.000300
2025-05-28 09:17:14,825 - INFO - joeynmt.training - Epoch   1, Step:     2700, Batch Loss:     2.328634, Batch Acc: 0.451458, Tokens per Sec:    20818, Lr: 0.000300
2025-05-28 09:17:18,330 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     2.355487, Batch Acc: 0.448481, Tokens per Sec:    20413, Lr: 0.000300
2025-05-28 09:17:21,777 - INFO - joeynmt.training - Epoch   1, Step:     2900, Batch Loss:     2.439389, Batch Acc: 0.452779, Tokens per Sec:    20826, Lr: 0.000300
2025-05-28 09:17:24,430 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     2.328261, Batch Acc: 0.453399, Tokens per Sec:    27419, Lr: 0.000300
2025-05-28 09:17:24,430 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:17:24,430 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:17:30,983 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.30, ppl:   9.98, acc:   0.45, generation: 6.5456[sec], evaluation: 0.0000[sec]
2025-05-28 09:17:30,984 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:17:31,617 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/500.ckpt
2025-05-28 09:17:31,645 - INFO - joeynmt.training - Example #0
2025-05-28 09:17:31,645 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:17:31,645 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:17:31,645 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'la', 'mia', 'cosa', 'che', 'è', 'un', 'altro', 'che', 'è', 'un', 'altro', ',', 'e', 'la', 'nostra', 'storia', 'di', 'un', 'altro', ',', 'e', 'la', 'sua', 'storia', 'di', 'un', 'altro', ',', 'e', 'la', 'nostra', 'storia', 'di', 'un', 'altro', ',', 'e', 'la', 's@@', '<unk>', '@', 'ato', ',', 'e', 'la', 'nostra', 'storia', 'di', 'un', 'altro', '.', '</s>']
2025-05-28 09:17:31,646 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:17:31,646 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:17:31,646 - INFO - joeynmt.training - 	Hypothesis: E la mia cosa che è un altro che è un altro , e la nostra storia di un altro , e la sua storia di un altro , e la nostra storia di un altro , e la s<unk> @ ato , e la nostra storia di un altro .
2025-05-28 09:17:31,646 - INFO - joeynmt.training - Example #1
2025-05-28 09:17:31,646 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:17:31,646 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:17:31,647 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'un', 'altro', ',', 'ma', 'non', 'è', 'un', 'po', '&apos;', ',', 'ma', 'non', 'è', 'un', 'po', '&apos;', ',', 'ma', 'non', 'è', 'un', 'po', '&apos;', '.', '</s>']
2025-05-28 09:17:31,647 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:17:31,647 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:17:31,647 - INFO - joeynmt.training - 	Hypothesis: Ma non è un altro , ma non è un po &apos; , ma non è un po &apos; , ma non è un po &apos; .
2025-05-28 09:17:31,647 - INFO - joeynmt.training - Example #2
2025-05-28 09:17:31,647 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:17:31,648 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:17:31,648 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'sua', 'parte', 'di', 'un', 'altro', 'è', 'un', 'altro', 'è', 'un', 'altro', 'è', 'un', 'altro', 'è', 'un', 'in@@', '<unk>', '@', 'ato', '.', '</s>']
2025-05-28 09:17:31,648 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:17:31,648 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:17:31,648 - INFO - joeynmt.training - 	Hypothesis: La sua parte di un altro è un altro è un altro è un altro è un in<unk> @ ato .
2025-05-28 09:17:31,648 - INFO - joeynmt.training - Example #3
2025-05-28 09:17:31,648 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:17:31,649 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:17:31,649 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'sua', 'sua', 'storia', 'di', 'un', 'po', '&apos;', 'un', 'po', '&apos;', 'un', 'po', '&apos;', '.', '</s>']
2025-05-28 09:17:31,649 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:17:31,649 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:17:31,649 - INFO - joeynmt.training - 	Hypothesis: La sua sua storia di un po &apos; un po &apos; un po &apos; .
2025-05-28 09:17:31,649 - INFO - joeynmt.training - Example #4
2025-05-28 09:17:31,649 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:17:31,649 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:17:31,650 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&apos;', 'un', 'cosa', 'che', 'è', 'un', 'po', '&apos;', 'un', 'po', '&apos;', 'un', 'po', '&apos;', 'è', 'un', 'po', '&apos;', 'un', 'po', '&apos;', 'è', 'un', 'po', '&apos;', 'un', 'po', '&apos;', 'un', 'po', '&apos;', '.', '</s>']
2025-05-28 09:17:31,650 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:17:31,650 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:17:31,650 - INFO - joeynmt.training - 	Hypothesis: E &apos; un cosa che è un po &apos; un po &apos; un po &apos; è un po &apos; un po &apos; è un po &apos; un po &apos; un po &apos; .
2025-05-28 09:17:33,718 - INFO - joeynmt.training - Epoch   1, Step:     3100, Batch Loss:     2.313238, Batch Acc: 0.455946, Tokens per Sec:    26083, Lr: 0.000300
2025-05-28 09:17:35,440 - INFO - joeynmt.training - Epoch   1, Step:     3200, Batch Loss:     2.208359, Batch Acc: 0.456905, Tokens per Sec:    41930, Lr: 0.000300
2025-05-28 09:17:37,142 - INFO - joeynmt.training - Epoch   1, Step:     3300, Batch Loss:     2.218063, Batch Acc: 0.454718, Tokens per Sec:    41080, Lr: 0.000300
2025-05-28 09:17:39,543 - INFO - joeynmt.training - Epoch   1, Step:     3400, Batch Loss:     2.277390, Batch Acc: 0.457248, Tokens per Sec:    29140, Lr: 0.000300
2025-05-28 09:17:42,853 - INFO - joeynmt.training - Epoch   1, Step:     3500, Batch Loss:     2.234197, Batch Acc: 0.456585, Tokens per Sec:    20915, Lr: 0.000300
2025-05-28 09:17:42,854 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:17:42,854 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:18:03,203 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.24, ppl:   9.41, acc:   0.46, generation: 20.3409[sec], evaluation: 0.0000[sec]
2025-05-28 09:18:03,204 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:18:03,735 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/1000.ckpt
2025-05-28 09:18:03,761 - INFO - joeynmt.training - Example #0
2025-05-28 09:18:03,761 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:18:03,761 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:18:03,761 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'persone', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', '.', '</s>']
2025-05-28 09:18:03,762 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:18:03,762 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:18:03,762 - INFO - joeynmt.training - 	Hypothesis: In un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di persone di un po &apos; di un po &apos; .
2025-05-28 09:18:03,762 - INFO - joeynmt.training - Example #1
2025-05-28 09:18:03,762 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:18:03,762 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:18:03,762 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'un', 'po', '&apos;', ',', 'non', 'è', 'un', 'po', '&apos;', 'di', 'non', 'è', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'non', 'è', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'non', 'non', 'è', 'un', 'po', '&apos;', '.', '</s>']
2025-05-28 09:18:03,763 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:18:03,763 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:18:03,763 - INFO - joeynmt.training - 	Hypothesis: Ma non è un po &apos; di un po &apos; un po &apos; , non è un po &apos; di non è un po &apos; di un po &apos; di un po &apos; di non è un po &apos; di un po &apos; di un po &apos; di non non è un po &apos; .
2025-05-28 09:18:03,763 - INFO - joeynmt.training - Example #2
2025-05-28 09:18:03,763 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:18:03,763 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:18:03,763 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'cosa', 'è', 'un', 'po', '&apos;', 'un', 'po', '&apos;', 'un', 'po', '&apos;', 'un', 'po', '&apos;', 'un', 'po', '&apos;', 'un', 'po', '&apos;', 'un', 'po', '&apos;', 'un', 'po', '&apos;', 'un', 'po', '&apos;', 'un', 'po', '&apos;', 'un', 'po', '&apos;', 'un', 'po', '&apos;', 'un', 'po', '&apos;', 'un', 'po', '&apos;', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'un', 'po', '&apos;', '&apos;', '&apos;', '&apos;', 'un', 'po', '&apos;', '.', '</s>']
2025-05-28 09:18:03,763 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:18:03,763 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:18:03,764 - INFO - joeynmt.training - 	Hypothesis: La cosa è un po &apos; un po &apos; un po &apos; un po &apos; un po &apos; un po &apos; un po &apos; un po &apos; un po &apos; un po &apos; un po &apos; un po &apos; un po &apos; un po &apos; un po &apos; di un po &apos; un po &apos; &apos; &apos; &apos; un po &apos; .
2025-05-28 09:18:03,764 - INFO - joeynmt.training - Example #3
2025-05-28 09:18:03,764 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:18:03,764 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:18:03,764 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', '<unk>', '@', 't@@', '<unk>', '@', 'i@@', '<unk>', '@', 'i@@', '<unk>', '@', 'i@@', '<unk>', '@', 'i@@', '<unk>', '@', 'a', '.', '</s>']
2025-05-28 09:18:03,764 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:18:03,764 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:18:03,764 - INFO - joeynmt.training - 	Hypothesis: S<unk> @ t<unk> @ i<unk> @ i<unk> @ i<unk> @ i<unk> @ a .
2025-05-28 09:18:03,764 - INFO - joeynmt.training - Example #4
2025-05-28 09:18:03,765 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:18:03,765 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:18:03,765 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'mio', 'punto', ',', 'la', 'cosa', 'è', 'un', 'po', '&apos;', 'è', 'un', 'po', '&apos;', 'è', 'un', 'po', '&apos;', 'è', 'un', 'po', '&apos;', 'è', 'un', 'po', '&apos;', 'è', 'un', 'po', '&apos;', 'è', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'è', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', '.', '</s>']
2025-05-28 09:18:03,765 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:18:03,765 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:18:03,765 - INFO - joeynmt.training - 	Hypothesis: Il mio punto , la cosa è un po &apos; è un po &apos; è un po &apos; è un po &apos; è un po &apos; è un po &apos; è un po &apos; di un po &apos; di un po &apos; è un po &apos; di un po &apos; di un po &apos; .
2025-05-28 09:18:07,058 - INFO - joeynmt.training - Epoch   1, Step:     3600, Batch Loss:     2.193556, Batch Acc: 0.461522, Tokens per Sec:    18472, Lr: 0.000300
2025-05-28 09:18:10,328 - INFO - joeynmt.training - Epoch   1, Step:     3700, Batch Loss:     2.183064, Batch Acc: 0.463777, Tokens per Sec:    22322, Lr: 0.000300
2025-05-28 09:18:13,621 - INFO - joeynmt.training - Epoch   1, Step:     3800, Batch Loss:     2.117852, Batch Acc: 0.465755, Tokens per Sec:    22051, Lr: 0.000300
2025-05-28 09:18:16,944 - INFO - joeynmt.training - Epoch   1, Step:     3900, Batch Loss:     2.178051, Batch Acc: 0.467690, Tokens per Sec:    21609, Lr: 0.000300
2025-05-28 09:18:20,234 - INFO - joeynmt.training - Epoch   1, Step:     4000, Batch Loss:     2.170687, Batch Acc: 0.465472, Tokens per Sec:    22076, Lr: 0.000300
2025-05-28 09:18:20,235 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:18:20,235 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:18:30,651 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.16, ppl:   8.68, acc:   0.47, generation: 10.4102[sec], evaluation: 0.0000[sec]
2025-05-28 09:18:30,651 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:18:31,206 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/1500.ckpt
2025-05-28 09:18:31,231 - INFO - joeynmt.training - Example #0
2025-05-28 09:18:31,232 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:18:31,232 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:18:31,232 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'primo', ',', 'ho', 'iniziato', 'a', 'me', ',', 'ho', 'fatto', 'che', 'ho', 'fatto', 'che', 'ho', 'fatto', 'che', 'ho', 'fatto', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'anni', ',', 'e', 'la', 'sua', 'parte', 'di', 'persone', 'che', 'sono', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'anni', ',', 'che', 'sono', 'stati', 'stati', 'stati', 'stati', 'anni', ',', 'e', 'la', 'sua', 'parte', 'di', 'anni', ',', 'e', 'la', 'sua', 'volta', 'che', 'sono', 'stati', 'stati', 'stati', 'stati', 'stati', 'in', 'cui', 'le', 'persone', 'che', 'sono', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'anni', '.', '</s>']
2025-05-28 09:18:31,232 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:18:31,232 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:18:31,232 - INFO - joeynmt.training - 	Hypothesis: Il primo , ho iniziato a me , ho fatto che ho fatto che ho fatto che ho fatto un po &apos; di un po &apos; di anni , e la sua parte di persone che sono stati stati stati stati stati stati stati stati anni , che sono stati stati stati stati anni , e la sua parte di anni , e la sua volta che sono stati stati stati stati stati in cui le persone che sono stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati stati anni .
2025-05-28 09:18:31,233 - INFO - joeynmt.training - Example #1
2025-05-28 09:18:31,233 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:18:31,233 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:18:31,233 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'che', 'non', 'è', 'che', 'non', 'è', 'che', 'non', 'è', 'che', 'non', 'sono', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', ',', 'non', 'è', 'che', 'non', 'sono', 'stati', 'stati', 'stati', 'stati', 'in', 'cui', 'non', 'non', 'sono', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'stati', 'in', 'cui', 'non', 'si', 'può', 'essere', 'essere', 'essere', 'essere', 'essere', 'essere', '.', '</s>']
2025-05-28 09:18:31,233 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:18:31,233 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:18:31,234 - INFO - joeynmt.training - 	Hypothesis: Ma non è che non è che non è che non è che non sono stati stati stati stati stati stati , non è che non sono stati stati stati stati in cui non non sono stati stati stati stati stati stati stati in cui non si può essere essere essere essere essere essere .
2025-05-28 09:18:31,234 - INFO - joeynmt.training - Example #2
2025-05-28 09:18:31,234 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:18:31,234 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:18:31,234 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'primo', 'è', 'che', 'il', 'nostro', 'problema', 'è', 'che', 'il', 'nostro', 'problema', '.', '</s>']
2025-05-28 09:18:31,234 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:18:31,235 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:18:31,235 - INFO - joeynmt.training - 	Hypothesis: Il primo è che il nostro problema è che il nostro problema .
2025-05-28 09:18:31,235 - INFO - joeynmt.training - Example #3
2025-05-28 09:18:31,235 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:18:31,235 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:18:31,235 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Non', 'si', 'tratta', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', '.', '</s>']
2025-05-28 09:18:31,235 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:18:31,235 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:18:31,235 - INFO - joeynmt.training - 	Hypothesis: Non si tratta di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; di un po &apos; .
2025-05-28 09:18:31,235 - INFO - joeynmt.training - Example #4
2025-05-28 09:18:31,236 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:18:31,236 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:18:31,236 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'cosa', 'cosa', 'è', 'un', 'po', '&apos;', 'di', 'cui', 'ho', 'detto', ',', 'e', 'ho', 'detto', ',', 'ma', 'è', 'che', 'ho', 'detto', 'che', 'ho', 'detto', ',', 'ma', 'ho', 'detto', ',', 'ma', 'ho', 'detto', '.', '</s>']
2025-05-28 09:18:31,236 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:18:31,236 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:18:31,236 - INFO - joeynmt.training - 	Hypothesis: La cosa cosa è un po &apos; di cui ho detto , e ho detto , ma è che ho detto che ho detto , ma ho detto , ma ho detto .
2025-05-28 09:18:34,679 - INFO - joeynmt.training - Epoch   1, Step:     4100, Batch Loss:     2.147840, Batch Acc: 0.468003, Tokens per Sec:    17574, Lr: 0.000300
2025-05-28 09:18:38,138 - INFO - joeynmt.training - Epoch   1, Step:     4200, Batch Loss:     2.211648, Batch Acc: 0.474553, Tokens per Sec:    20723, Lr: 0.000300
2025-05-28 09:18:41,570 - INFO - joeynmt.training - Epoch   1, Step:     4300, Batch Loss:     2.090795, Batch Acc: 0.473958, Tokens per Sec:    20711, Lr: 0.000300
2025-05-28 09:18:45,010 - INFO - joeynmt.training - Epoch   1, Step:     4400, Batch Loss:     2.010767, Batch Acc: 0.474651, Tokens per Sec:    20670, Lr: 0.000300
2025-05-28 09:18:48,438 - INFO - joeynmt.training - Epoch   1, Step:     4500, Batch Loss:     2.147826, Batch Acc: 0.479412, Tokens per Sec:    20706, Lr: 0.000300
2025-05-28 09:18:48,438 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:18:48,438 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:19:02,925 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.08, ppl:   8.02, acc:   0.49, generation: 14.4797[sec], evaluation: 0.0000[sec]
2025-05-28 09:19:02,925 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:19:03,460 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/2000.ckpt
2025-05-28 09:19:03,482 - INFO - joeynmt.training - Example #0
2025-05-28 09:19:03,483 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:19:03,483 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:19:03,483 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'questo', 'modo', ',', 'ho', 'iniziato', 'a', 'circa', 'circa', 'circa', 'circa', 'circa', 'circa', 'circa', 'circa', 'circa', 'circa', 'circa', 'circa', '1@@', '<unk>', '@', '5', 'anni', ',', 'e', 'il', '200@@', '<unk>', '@', '7', 'anni', ',', 'e', 'il', '200@@', '<unk>', '@', '7', 'milioni', 'di', 'milioni', 'di', 'milioni', 'di', 'milioni', 'di', 'milioni', 'di', 'milioni', 'di', 'milioni', 'di', 'milioni', 'di', 'milioni', 'di', 'milioni', 'di', 'milioni', 'di', 'milioni', 'di', 'milioni', 'di', 'milioni', 'di', 'milioni', 'di', 'milioni', 'di', 'milioni', 'di', 'milioni', 'di', 'milioni', 'di', 'milioni', 'di', 'milioni', 'di', 'milioni', 'di', 'anni', '.', '</s>']
2025-05-28 09:19:03,484 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:19:03,484 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:19:03,484 - INFO - joeynmt.training - 	Hypothesis: In questo modo , ho iniziato a circa circa circa circa circa circa circa circa circa circa circa circa 1<unk> @ 5 anni , e il 200<unk> @ 7 anni , e il 200<unk> @ 7 milioni di milioni di milioni di milioni di milioni di milioni di milioni di milioni di milioni di milioni di milioni di milioni di milioni di milioni di milioni di milioni di milioni di milioni di milioni di milioni di milioni di milioni di anni .
2025-05-28 09:19:03,484 - INFO - joeynmt.training - Example #1
2025-05-28 09:19:03,484 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:19:03,484 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:19:03,484 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'la', 'nostra', 'parte', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', '&apos;', 'una', 'cosa', 'di', 'essere', 'essere', 'un', 'po', '&apos;', '&apos;', 'una', 'cosa', 'di', 'ri@@', '<unk>', '@', 'per@@', '<unk>', '@', 'dere', 'la', 'sua', 'vita', '.', '</s>']
2025-05-28 09:19:03,484 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:19:03,484 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:19:03,484 - INFO - joeynmt.training - 	Hypothesis: Ma la nostra parte di un po &apos; di un po &apos; di un po &apos; di un po &apos; &apos; una cosa di essere essere un po &apos; &apos; una cosa di ri<unk> @ per<unk> @ dere la sua vita .
2025-05-28 09:19:03,485 - INFO - joeynmt.training - Example #2
2025-05-28 09:19:03,485 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:19:03,485 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:19:03,485 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'realtà', 'è', 'la', 'sua', 'parte', 'della', 'vita', 'è', 'la', 'sua', 'parte', 'della', 'vita', '.', '</s>']
2025-05-28 09:19:03,485 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:19:03,485 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:19:03,485 - INFO - joeynmt.training - 	Hypothesis: In realtà è la sua parte della vita è la sua parte della vita .
2025-05-28 09:19:03,485 - INFO - joeynmt.training - Example #3
2025-05-28 09:19:03,486 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:19:03,486 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:19:03,486 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Hanno', 'fatto', 'il', 'mio', 'giorno', ',', 'e', 'si', 'può', 'ri@@', '<unk>', '@', 'per@@', '<unk>', '@', 'de', ',', 'e', 'un', 'p@@', '<unk>', '@', 'ett@@', '<unk>', '@', 'ato', '.', '</s>']
2025-05-28 09:19:03,486 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:19:03,486 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:19:03,486 - INFO - joeynmt.training - 	Hypothesis: Hanno fatto il mio giorno , e si può ri<unk> @ per<unk> @ de , e un p<unk> @ ett<unk> @ ato .
2025-05-28 09:19:03,486 - INFO - joeynmt.training - Example #4
2025-05-28 09:19:03,486 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:19:03,486 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:19:03,486 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'volta', 'che', 'ho', 'fatto', ',', 'è', 'una', 'volta', 'che', 'ho', 'fatto', ',', 'è', 'una', 'volta', 'che', 'è', 'una', 'volta', 'che', 'è', 'una', 'volta', 'che', 'è', 'un', 'po', '&apos;', 'un', 'po', '&apos;', 'un', 'po', '&apos;', 'una', 'di', 'un', 'po', '&apos;', 'una', 'di', 'un', 'po', '&apos;', 'di', 'anni', '.', '</s>']
2025-05-28 09:19:03,487 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:19:03,487 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:19:03,487 - INFO - joeynmt.training - 	Hypothesis: La prima volta che ho fatto , è una volta che ho fatto , è una volta che è una volta che è una volta che è un po &apos; un po &apos; un po &apos; una di un po &apos; una di un po &apos; di anni .
2025-05-28 09:19:06,795 - INFO - joeynmt.training - Epoch   1, Step:     4600, Batch Loss:     2.193179, Batch Acc: 0.481514, Tokens per Sec:    18314, Lr: 0.000300
2025-05-28 09:19:10,080 - INFO - joeynmt.training - Epoch   1, Step:     4700, Batch Loss:     2.006185, Batch Acc: 0.486121, Tokens per Sec:    21710, Lr: 0.000300
2025-05-28 09:19:13,369 - INFO - joeynmt.training - Epoch   1, Step:     4800, Batch Loss:     2.056288, Batch Acc: 0.486618, Tokens per Sec:    22389, Lr: 0.000300
2025-05-28 09:19:16,683 - INFO - joeynmt.training - Epoch   1, Step:     4900, Batch Loss:     2.026491, Batch Acc: 0.491232, Tokens per Sec:    22274, Lr: 0.000300
2025-05-28 09:19:19,965 - INFO - joeynmt.training - Epoch   1, Step:     5000, Batch Loss:     1.996455, Batch Acc: 0.493620, Tokens per Sec:    21852, Lr: 0.000300
2025-05-28 09:19:19,966 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:19:19,966 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:19:30,730 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.01, ppl:   7.44, acc:   0.50, generation: 10.7581[sec], evaluation: 0.0000[sec]
2025-05-28 09:19:30,731 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:19:31,266 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/2500.ckpt
2025-05-28 09:19:31,290 - INFO - joeynmt.training - Example #0
2025-05-28 09:19:31,291 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:19:31,291 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:19:31,291 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', '<unk>', '@', 'ei', 'due', 'anni', ',', 'ho', 'fatto', 'che', 'ho', 'fatto', ',', 'ho', 'fatto', 'che', 'ho', 'fatto', 'che', 'ho', 'fatto', 'che', 'la', 'prima', 'volta', 'che', 'la', 'prima', 'volta', 'che', 'la', 'prima', 'volta', 'che', 'la', 'prima', 'volta', 'che', 'la', 'prima', 'volta', 'che', 'la', 'prima', 'volta', 'che', 'la', 'prima', 'volta', 'che', 'la', 'prima', 'volta', 'che', 'la', 'prima', 'volta', 'che', 'la', 'prima', 'volta', 'che', 'si', 'sono', 'stati', 'in', 'un', 'anno', ',', 'un', 'anno', ',', 'un', 'anno', ',', 'un', 'anno', ',', 'un', 'anno', ',', 'un', 'anno', '.', '</s>']
2025-05-28 09:19:31,291 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:19:31,291 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:19:31,291 - INFO - joeynmt.training - 	Hypothesis: S<unk> @ ei due anni , ho fatto che ho fatto , ho fatto che ho fatto che ho fatto che la prima volta che la prima volta che la prima volta che la prima volta che la prima volta che la prima volta che la prima volta che la prima volta che la prima volta che la prima volta che si sono stati in un anno , un anno , un anno , un anno , un anno , un anno .
2025-05-28 09:19:31,291 - INFO - joeynmt.training - Example #1
2025-05-28 09:19:31,292 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:19:31,292 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:19:31,292 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'la', 'prima', 'volta', 'che', 'si', 'tratta', 'di', 'un', 'modo', 'di', 'essere', 'essere', 'essere', 'un', 'modo', 'di', 'questo', '.', '</s>']
2025-05-28 09:19:31,292 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:19:31,292 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:19:31,292 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è la prima volta che si tratta di un modo di essere essere essere un modo di questo .
2025-05-28 09:19:31,292 - INFO - joeynmt.training - Example #2
2025-05-28 09:19:31,292 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:19:31,292 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:19:31,293 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'realtà', ',', 'è', 'la', 'prima', 'cosa', 'è', 'la', 'stessa', 'cosa', '.', '</s>']
2025-05-28 09:19:31,293 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:19:31,293 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:19:31,293 - INFO - joeynmt.training - 	Hypothesis: In realtà , è la prima cosa è la stessa cosa .
2025-05-28 09:19:31,293 - INFO - joeynmt.training - Example #3
2025-05-28 09:19:31,293 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:19:31,293 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:19:31,293 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'tratta', 'di', 'un', 't@@', '<unk>', '@', 'om@@', '<unk>', '@', 'enti', '.', '</s>']
2025-05-28 09:19:31,294 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:19:31,294 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:19:31,294 - INFO - joeynmt.training - 	Hypothesis: Si tratta di un t<unk> @ om<unk> @ enti .
2025-05-28 09:19:31,294 - INFO - joeynmt.training - Example #4
2025-05-28 09:19:31,294 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:19:31,294 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:19:31,294 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'cosa', 'cosa', 'cosa', 'cosa', 'cosa', 'sia', 'un', 'po', '&apos;', 'un', 'po', '&apos;', 'un', 'po', '&apos;', 'di', 'un', 'po', '&apos;', 'anni', 'fa', '.', '</s>']
2025-05-28 09:19:31,294 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:19:31,294 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:19:31,294 - INFO - joeynmt.training - 	Hypothesis: La cosa cosa cosa cosa cosa sia un po &apos; un po &apos; un po &apos; di un po &apos; anni fa .
2025-05-28 09:19:34,637 - INFO - joeynmt.training - Epoch   1, Step:     5100, Batch Loss:     1.982336, Batch Acc: 0.493892, Tokens per Sec:    19299, Lr: 0.000300
2025-05-28 09:19:37,917 - INFO - joeynmt.training - Epoch   1, Step:     5200, Batch Loss:     1.945697, Batch Acc: 0.496572, Tokens per Sec:    21306, Lr: 0.000300
2025-05-28 09:19:41,184 - INFO - joeynmt.training - Epoch   1, Step:     5300, Batch Loss:     1.951867, Batch Acc: 0.497945, Tokens per Sec:    21904, Lr: 0.000300
2025-05-28 09:19:44,474 - INFO - joeynmt.training - Epoch   1, Step:     5400, Batch Loss:     1.960214, Batch Acc: 0.499993, Tokens per Sec:    22008, Lr: 0.000300
2025-05-28 09:19:47,760 - INFO - joeynmt.training - Epoch   1, Step:     5500, Batch Loss:     1.981525, Batch Acc: 0.503767, Tokens per Sec:    21538, Lr: 0.000300
2025-05-28 09:19:47,760 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:19:47,760 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:19:57,666 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.95, ppl:   7.00, acc:   0.51, generation: 9.8989[sec], evaluation: 0.0000[sec]
2025-05-28 09:19:57,666 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:19:58,348 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/3000.ckpt
2025-05-28 09:19:58,375 - INFO - joeynmt.training - Example #0
2025-05-28 09:19:58,375 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:19:58,375 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:19:58,375 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'due', 'milioni', 'di', 'anni', ',', 'che', 'ho', 'avuto', 'un', 'anno', ',', 'che', 'ho', 'avuto', 'un', 'anno', 'che', 'la', 'loro', 'volta', 'che', 'la', 'loro', 'delle', 'donne', 'che', 'le', 'persone', 'che', 'la', 'gente', 'che', 'la', 'gente', 'che', 'la', 'maggior', 'parte', 'di', 'un', 'gruppo', 'di', 'persone', 'che', 'la', 'gente', 'che', 'la', 'sua', 'specie', 'di', 'milioni', 'di', 'dollari', ',', 'la', 'Cina', ',', 'la', 'Cina', ',', 'la', 'prima', 'volta', 'che', 'la', 'sua', 'vita', ',', 'la', 'sua', 'volta', 'che', 'l&apos;', 'anno', ',', 'il', '1@@', '<unk>', '@', '5', '%', 'della', 'nostra', 'vita', '.', '</s>']
2025-05-28 09:19:58,376 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:19:58,376 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:19:58,376 - INFO - joeynmt.training - 	Hypothesis: In due milioni di anni , che ho avuto un anno , che ho avuto un anno che la loro volta che la loro delle donne che le persone che la gente che la gente che la maggior parte di un gruppo di persone che la gente che la sua specie di milioni di dollari , la Cina , la Cina , la prima volta che la sua vita , la sua volta che l&apos; anno , il 1<unk> @ 5 % della nostra vita .
2025-05-28 09:19:58,376 - INFO - joeynmt.training - Example #1
2025-05-28 09:19:58,376 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:19:58,376 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:19:58,376 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'il', 'suo', 'lavoro', 'di', 'una', 'b@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ato', 'la', 'sua', 'in@@', '<unk>', '@', 'cor@@', '<unk>', '@', 'por@@', '<unk>', '@', 'ti', ',', 'la', 'sua', 'parte', 'della', 'pol@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ale', '.', '</s>']
2025-05-28 09:19:58,377 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:19:58,377 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:19:58,377 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è il suo lavoro di una b<unk> @ ic<unk> @ ato la sua in<unk> @ cor<unk> @ por<unk> @ ti , la sua parte della pol<unk> @ i<unk> @ ale .
2025-05-28 09:19:58,377 - INFO - joeynmt.training - Example #2
2025-05-28 09:19:58,377 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:19:58,377 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:19:58,377 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'realtà', 'è', 'il', 'sistema', 'di', 'un', 'sistema', 'di', 'un', 'sistema', 'di', 'un', 'sistema', 'di', 'un', 'sistema', 'di', 'un', 'sistema', 'di', 'un', 'sistema', 'di', 'un', 'sistema', 'di', 'un', 'sistema', 'di', 'essere', 'la', 'nostra', 'tecnologia', 'della', 'nostra', 'nostra', 'nostra', 'nostra', 'nostra', '.', '</s>']
2025-05-28 09:19:58,378 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:19:58,378 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:19:58,378 - INFO - joeynmt.training - 	Hypothesis: In realtà è il sistema di un sistema di un sistema di un sistema di un sistema di un sistema di un sistema di un sistema di un sistema di essere la nostra tecnologia della nostra nostra nostra nostra nostra .
2025-05-28 09:19:58,378 - INFO - joeynmt.training - Example #3
2025-05-28 09:19:58,378 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:19:58,378 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:19:58,378 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'chiama', 'e', 'la', 'sua', 'f@@', '<unk>', '@', 'ate', 'e', 'la', 'sua', 'f@@', '<unk>', '@', 'o', 'e', 'il', 'suo', 'suo', 'lavoro', '.', '</s>']
2025-05-28 09:19:58,379 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:19:58,379 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:19:58,379 - INFO - joeynmt.training - 	Hypothesis: Si chiama e la sua f<unk> @ ate e la sua f<unk> @ o e il suo suo lavoro .
2025-05-28 09:19:58,379 - INFO - joeynmt.training - Example #4
2025-05-28 09:19:58,379 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:19:58,379 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:19:58,379 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'domanda', 'è', 'è', 'una', 'cosa', 'che', 'sia', 'una', 'specie', 'di', 'un', 'po', '&apos;', 'una', 'cosa', 'che', 'è', 'una', 'cosa', 'che', 'è', 'una', 'cosa', 'che', 'è', 'il', 'mio', 'padre', '.', '</s>']
2025-05-28 09:19:58,379 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:19:58,380 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:19:58,380 - INFO - joeynmt.training - 	Hypothesis: La domanda è è una cosa che sia una specie di un po &apos; una cosa che è una cosa che è una cosa che è il mio padre .
2025-05-28 09:20:01,753 - INFO - joeynmt.training - Epoch   1, Step:     5600, Batch Loss:     1.889875, Batch Acc: 0.502354, Tokens per Sec:    17407, Lr: 0.000300
2025-05-28 09:20:05,049 - INFO - joeynmt.training - Epoch   1, Step:     5700, Batch Loss:     2.039659, Batch Acc: 0.505742, Tokens per Sec:    21781, Lr: 0.000300
2025-05-28 09:20:08,328 - INFO - joeynmt.training - Epoch   1, Step:     5800, Batch Loss:     1.982712, Batch Acc: 0.505663, Tokens per Sec:    21786, Lr: 0.000300
2025-05-28 09:20:11,771 - INFO - joeynmt.training - Epoch   1, Step:     5900, Batch Loss:     1.895081, Batch Acc: 0.507451, Tokens per Sec:    20766, Lr: 0.000300
2025-05-28 09:20:15,226 - INFO - joeynmt.training - Epoch   1, Step:     6000, Batch Loss:     1.998514, Batch Acc: 0.513494, Tokens per Sec:    20351, Lr: 0.000300
2025-05-28 09:20:15,227 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:20:15,227 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:20:23,772 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.89, ppl:   6.61, acc:   0.51, generation: 8.5384[sec], evaluation: 0.0000[sec]
2025-05-28 09:20:23,772 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:20:24,342 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/3500.ckpt
2025-05-28 09:20:24,369 - INFO - joeynmt.training - Example #0
2025-05-28 09:20:24,369 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:20:24,369 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:20:24,369 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', '19@@', '<unk>', '@', '8@@', '<unk>', '@', '7', 'milioni', 'di', 'due', 'due', 'due', 'due', ',', 'che', 'ho', 'scoperto', 'che', 'la', 'stessa', 'cosa', 'che', 'la', 'st@@', '<unk>', '@', 'ica', 'è', 'che', 'la', 'guerra', 'è', 'che', 'la', 'guerra', 'è', 'che', 'i', 'primi', 'milioni', 'di', 'persone', 'che', 'hanno', 'scoperto', 'che', 'il', 'C@@', '<unk>', '@', 'er@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'di@@', '<unk>', '@', 'vi@@', '<unk>', '@', 'are', 'la', 'città', ',', 'che', 'è', 'stato', 'di', 'circa', 'cento', 'milioni', 'di', 'anni', ',', 'che', 'è', 'stato', 'di', '1@@', '<unk>', '@', '7', 'milioni', 'di', 'anni', '.', '</s>']
2025-05-28 09:20:24,370 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:20:24,370 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:20:24,370 - INFO - joeynmt.training - 	Hypothesis: Il 19<unk> @ 8<unk> @ 7 milioni di due due due due , che ho scoperto che la stessa cosa che la st<unk> @ ica è che la guerra è che la guerra è che i primi milioni di persone che hanno scoperto che il C<unk> @ er<unk> @ ri<unk> @ di<unk> @ vi<unk> @ are la città , che è stato di circa cento milioni di anni , che è stato di 1<unk> @ 7 milioni di anni .
2025-05-28 09:20:24,370 - INFO - joeynmt.training - Example #1
2025-05-28 09:20:24,370 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:20:24,371 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:20:24,371 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'la', 'cosa', 'di', 'essere', 'in', 'cui', 'non', 'è', 'la', 'tecnologia', ',', 'che', 'non', 'è', 'la', 'stessa', 'cosa', 'che', 'non', 'è', 'la', 'tecnologia', ',', 'non', 'è', 'la', 'tecnologia', '.', '</s>']
2025-05-28 09:20:24,371 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:20:24,371 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:20:24,371 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è la cosa di essere in cui non è la tecnologia , che non è la stessa cosa che non è la tecnologia , non è la tecnologia .
2025-05-28 09:20:24,371 - INFO - joeynmt.training - Example #2
2025-05-28 09:20:24,371 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:20:24,371 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:20:24,371 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'realtà', ',', 'la', 'cosa', 'è', 'la', 'tecnologia', 'è', 'la', 'tecnologia', 'della', 'nostra', 'tecnologia', '.', '</s>']
2025-05-28 09:20:24,372 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:20:24,372 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:20:24,372 - INFO - joeynmt.training - 	Hypothesis: In realtà , la cosa è la tecnologia è la tecnologia della nostra tecnologia .
2025-05-28 09:20:24,372 - INFO - joeynmt.training - Example #3
2025-05-28 09:20:24,372 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:20:24,372 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:20:24,372 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'chiama', ',', 'e', 'la', 'vostra', 'parte', 'e', 'la', 'p@@', '<unk>', '@', 'ace', 'e', 'la', 'p@@', '<unk>', '@', 'ace', '.', '</s>']
2025-05-28 09:20:24,373 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:20:24,373 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:20:24,373 - INFO - joeynmt.training - 	Hypothesis: Si chiama , e la vostra parte e la p<unk> @ ace e la p<unk> @ ace .
2025-05-28 09:20:24,373 - INFO - joeynmt.training - Example #4
2025-05-28 09:20:24,373 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:20:24,373 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:20:24,373 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'volta', ',', 'ho', 'fatto', ',', 'è', 'una', 'cosa', 'che', 'è', 'una', 'cosa', 'che', 'è', 'una', 'cosa', 'che', 'è', 'che', 'è', 'che', 'questo', 'è', 'che', 'è', 'che', 'succede', '.', '</s>']
2025-05-28 09:20:24,373 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:20:24,373 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:20:24,373 - INFO - joeynmt.training - 	Hypothesis: La prima volta , ho fatto , è una cosa che è una cosa che è una cosa che è che è che questo è che è che succede .
2025-05-28 09:20:27,683 - INFO - joeynmt.training - Epoch   1, Step:     6100, Batch Loss:     1.897679, Batch Acc: 0.514179, Tokens per Sec:    18150, Lr: 0.000300
2025-05-28 09:20:30,985 - INFO - joeynmt.training - Epoch   1, Step:     6200, Batch Loss:     1.876536, Batch Acc: 0.514020, Tokens per Sec:    21505, Lr: 0.000300
2025-05-28 09:20:34,038 - INFO - joeynmt.training - Epoch   1: total training loss 14234.76
2025-05-28 09:20:34,038 - INFO - joeynmt.training - EPOCH 2
2025-05-28 09:20:34,301 - INFO - joeynmt.training - Epoch   2, Step:     6300, Batch Loss:     1.816573, Batch Acc: 0.525415, Tokens per Sec:    21894, Lr: 0.000300
2025-05-28 09:20:37,598 - INFO - joeynmt.training - Epoch   2, Step:     6400, Batch Loss:     1.837936, Batch Acc: 0.518771, Tokens per Sec:    21516, Lr: 0.000300
2025-05-28 09:20:40,900 - INFO - joeynmt.training - Epoch   2, Step:     6500, Batch Loss:     1.910788, Batch Acc: 0.516962, Tokens per Sec:    21558, Lr: 0.000300
2025-05-28 09:20:40,900 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:20:40,900 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:20:50,797 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.84, ppl:   6.31, acc:   0.52, generation: 9.8892[sec], evaluation: 0.0000[sec]
2025-05-28 09:20:50,798 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:20:51,348 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/4000.ckpt
2025-05-28 09:20:51,374 - INFO - joeynmt.training - Example #0
2025-05-28 09:20:51,374 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:20:51,374 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:20:51,374 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'primo', 'anno', 'scorso', 'scorso', 'scorso', 'scorso', 'scorso', 'scorso', 'scorso', 'scorso', 'scorso', 'scorso', 'scorso', 'scorso', ',', 'che', 'è', 'che', 'il', 'governo', 'che', 'è', 'che', 'il', 'governo', 'del', 'governo', ',', 'che', 'il', 'governo', ',', 'che', 'il', 'governo', ',', 'che', 'il', 'governo', ',', 'che', 'i', 'primi', 'milioni', 'di', 'anni', ',', 'che', 'il', 'governo', 'del', 'C@@', '<unk>', '@', 'er@@', '<unk>', '@', 'o@@', '<unk>', '@', 'p', ',', 'il', '2@@', '<unk>', '@', '000', 'milioni', 'di', 'anni', ',', 'il', '2@@', '<unk>', '@', '7', 'milioni', 'di', 'anni', ',', 'che', 'è', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', 'stato', ',', 'il', '2@@', '<unk>', '@', '7', 'milioni', 'di', 'anni', '.', '</s>']
2025-05-28 09:20:51,375 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:20:51,375 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:20:51,375 - INFO - joeynmt.training - 	Hypothesis: Il primo anno scorso scorso scorso scorso scorso scorso scorso scorso scorso scorso scorso scorso , che è che il governo che è che il governo del governo , che il governo , che il governo , che il governo , che i primi milioni di anni , che il governo del C<unk> @ er<unk> @ o<unk> @ p , il 2<unk> @ 000 milioni di anni , il 2<unk> @ 7 milioni di anni , che è stato stato stato stato stato stato stato , il 2<unk> @ 7 milioni di anni .
2025-05-28 09:20:51,375 - INFO - joeynmt.training - Example #1
2025-05-28 09:20:51,375 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:20:51,376 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:20:51,376 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'il', 'modo', 'che', 'non', 'è', 'il', 'fatto', 'che', 'la', 'stessa', 'cosa', 'che', 'è', 'il', 'sistema', 'che', 'non', 'è', 'il', 'sistema', 'che', 'non', 'è', 'il', 'fatto', 'che', 'non', 'è', 'il', 't@@', '<unk>', '@', 'ale', ',', 'non', 'è', 'il', 't@@', '<unk>', '@', 'ur@@', '<unk>', '@', 'ato', '.', '</s>']
2025-05-28 09:20:51,376 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:20:51,376 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:20:51,376 - INFO - joeynmt.training - 	Hypothesis: Ma non è il modo che non è il fatto che la stessa cosa che è il sistema che non è il sistema che non è il fatto che non è il t<unk> @ ale , non è il t<unk> @ ur<unk> @ ato .
2025-05-28 09:20:51,376 - INFO - joeynmt.training - Example #2
2025-05-28 09:20:51,376 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:20:51,376 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:20:51,377 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'effetti', ',', 'è', 'la', 'cosa', 'che', 'è', 'la', 'natura', 'è', 'la', 'natura', 'della', 'natura', '.', '</s>']
2025-05-28 09:20:51,377 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:20:51,377 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:20:51,377 - INFO - joeynmt.training - 	Hypothesis: In effetti , è la cosa che è la natura è la natura della natura .
2025-05-28 09:20:51,377 - INFO - joeynmt.training - Example #3
2025-05-28 09:20:51,377 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:20:51,377 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:20:51,377 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'tratta', 'di', 'quello', 'che', 'si', 'è', 'il', 'suo', 'lavoro', 'e', 'il', 'suo', 'lavoro', '.', '</s>']
2025-05-28 09:20:51,378 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:20:51,378 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:20:51,378 - INFO - joeynmt.training - 	Hypothesis: Si tratta di quello che si è il suo lavoro e il suo lavoro .
2025-05-28 09:20:51,378 - INFO - joeynmt.training - Example #4
2025-05-28 09:20:51,378 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:20:51,378 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:20:51,378 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'mo', ',', 'vi', 'vi', 'vi', 'mostr@@', '<unk>', '@', 'erò', ',', 'è', 'una', 'cosa', 'che', 'è', 'una', 'cosa', 'che', 'è', 'il', 'primo', '.', '</s>']
2025-05-28 09:20:51,378 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:20:51,378 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:20:51,379 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ mo , vi vi vi mostr<unk> @ erò , è una cosa che è una cosa che è il primo .
2025-05-28 09:20:54,716 - INFO - joeynmt.training - Epoch   2, Step:     6600, Batch Loss:     1.856952, Batch Acc: 0.527448, Tokens per Sec:    18041, Lr: 0.000300
2025-05-28 09:20:58,014 - INFO - joeynmt.training - Epoch   2, Step:     6700, Batch Loss:     1.865538, Batch Acc: 0.527245, Tokens per Sec:    21442, Lr: 0.000300
2025-05-28 09:21:01,342 - INFO - joeynmt.training - Epoch   2, Step:     6800, Batch Loss:     1.821813, Batch Acc: 0.528886, Tokens per Sec:    21812, Lr: 0.000300
2025-05-28 09:21:04,638 - INFO - joeynmt.training - Epoch   2, Step:     6900, Batch Loss:     1.775259, Batch Acc: 0.528085, Tokens per Sec:    22114, Lr: 0.000300
2025-05-28 09:21:07,947 - INFO - joeynmt.training - Epoch   2, Step:     7000, Batch Loss:     1.766856, Batch Acc: 0.533585, Tokens per Sec:    21273, Lr: 0.000300
2025-05-28 09:21:07,947 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:21:07,947 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:21:17,305 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.79, ppl:   6.01, acc:   0.53, generation: 9.3471[sec], evaluation: 0.0000[sec]
2025-05-28 09:21:17,306 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:21:17,929 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/4500.ckpt
2025-05-28 09:21:17,956 - INFO - joeynmt.training - Example #0
2025-05-28 09:21:17,957 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:21:17,957 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:21:17,957 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', ',', 'ho', 'fatto', 'questi', 'due', 'milioni', 'di', 'anni', ',', 'ho', 'fatto', 'che', 'i', 'due', 'milioni', 'di', 'anni', ',', 'che', 'i', 'sono', 'i', 'paesi', 'che', 'i', 'sono', 'i', 'i', 'paesi', 'che', 'i', 'paesi', 'che', 'i', 'hanno', 'i', 'paesi', 'che', 'i', 'paesi', 'che', 'i', 'paesi', 'in', 'cui', 'i', 'paesi', 'in', 'un', 'anno', ',', 'il', '2@@', '<unk>', '@', '7', 'milioni', 'di', 'anni', ',', 'il', '2@@', '<unk>', '@', '7', 'milioni', 'di', 'anni', '.', '</s>']
2025-05-28 09:21:17,958 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:21:17,958 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:21:17,958 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno , ho fatto questi due milioni di anni , ho fatto che i due milioni di anni , che i sono i paesi che i sono i i paesi che i paesi che i hanno i paesi che i paesi che i paesi in cui i paesi in un anno , il 2<unk> @ 7 milioni di anni , il 2<unk> @ 7 milioni di anni .
2025-05-28 09:21:17,958 - INFO - joeynmt.training - Example #1
2025-05-28 09:21:17,958 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:21:17,958 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:21:17,958 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'un', 'po', '&apos;', 'di', 'questo', ',', 'non', 'è', 'il', 'nostro', 'sistema', 'di', 'essere', 'in', 'cui', 'non', 'è', 'la', 'tecnologia', ',', 'non', 'è', 'la', 'causa', 'di', 'un', 'sistema', 'di', 'questo', ',', 'non', 'è', 'il', 'p@@', '<unk>', '@', 'ur@@', '<unk>', '@', 'ale', '.', '</s>']
2025-05-28 09:21:17,959 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:21:17,959 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:21:17,959 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è un po &apos; di questo , non è il nostro sistema di essere in cui non è la tecnologia , non è la causa di un sistema di questo , non è il p<unk> @ ur<unk> @ ale .
2025-05-28 09:21:17,959 - INFO - joeynmt.training - Example #2
2025-05-28 09:21:17,959 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:21:17,960 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:21:17,960 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'effetti', ',', 'la', 'nostra', 'società', 'è', 'la', 'nostra', 'società', '.', '</s>']
2025-05-28 09:21:17,960 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:21:17,960 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:21:17,960 - INFO - joeynmt.training - 	Hypothesis: In effetti , la nostra società è la nostra società .
2025-05-28 09:21:17,960 - INFO - joeynmt.training - Example #3
2025-05-28 09:21:17,960 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:21:17,960 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:21:17,960 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'ogli@@', '<unk>', '@', 'amo', 'a', 'fare', 'in', 'modo', 'in', 'cui', 'il', 'suo', 's@@', '<unk>', '@', 'ettore', '.', '</s>']
2025-05-28 09:21:17,961 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:21:17,961 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:21:17,961 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ ogli<unk> @ amo a fare in modo in cui il suo s<unk> @ ettore .
2025-05-28 09:21:17,961 - INFO - joeynmt.training - Example #4
2025-05-28 09:21:17,961 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:21:17,961 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:21:17,961 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'mo', 'di', 'voi', ',', 'vi', 'vi', 'mostr@@', '<unk>', '@', 'arvi', ',', 'è', 'un', 'po', '&apos;', 'di', 'nuovo', '.', '</s>']
2025-05-28 09:21:17,962 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:21:17,962 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:21:17,962 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ mo di voi , vi vi mostr<unk> @ arvi , è un po &apos; di nuovo .
2025-05-28 09:21:21,460 - INFO - joeynmt.training - Epoch   2, Step:     7100, Batch Loss:     1.795797, Batch Acc: 0.531367, Tokens per Sec:    17502, Lr: 0.000300
2025-05-28 09:21:24,943 - INFO - joeynmt.training - Epoch   2, Step:     7200, Batch Loss:     1.748989, Batch Acc: 0.534794, Tokens per Sec:    21006, Lr: 0.000300
2025-05-28 09:21:28,423 - INFO - joeynmt.training - Epoch   2, Step:     7300, Batch Loss:     1.784664, Batch Acc: 0.534744, Tokens per Sec:    20305, Lr: 0.000300
2025-05-28 09:21:31,902 - INFO - joeynmt.training - Epoch   2, Step:     7400, Batch Loss:     1.741865, Batch Acc: 0.539799, Tokens per Sec:    20814, Lr: 0.000300
2025-05-28 09:21:35,370 - INFO - joeynmt.training - Epoch   2, Step:     7500, Batch Loss:     1.700259, Batch Acc: 0.534751, Tokens per Sec:    20333, Lr: 0.000300
2025-05-28 09:21:35,371 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:21:35,371 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:21:47,000 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.76, ppl:   5.83, acc:   0.54, generation: 11.6215[sec], evaluation: 0.0000[sec]
2025-05-28 09:21:47,000 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:21:47,570 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/5000.ckpt
2025-05-28 09:21:47,596 - INFO - joeynmt.training - Example #0
2025-05-28 09:21:47,597 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:21:47,597 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:21:47,597 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ho', 'fatto', 'questa', 'foto', 'di', 'questi', 'due', 'milioni', 'di', 'dati', 'che', 'ho', 'fatto', 'questi', 'due', 'milioni', 'di', 'dati', 'per', 'fare', 'il', 'primo', 'in@@', '<unk>', '@', 'fan@@', '<unk>', '@', 'ti@@', '<unk>', '@', 'ene', 'che', 'la', 'c@@', '<unk>', '@', 'lin@@', '<unk>', '@', 'ica', 'per', 'il', 'c@@', '<unk>', '@', 'att@@', '<unk>', '@', 'ur@@', '<unk>', '@', 'are', 'il', 'numero', 'di', 'milioni', 'di', 'anni', ',', 'il', 'numero', 'di', 'milioni', 'di', 'anni', ',', 'il', '9@@', '<unk>', '@', '00', 'milioni', 'di', 'anni', '.', '</s>']
2025-05-28 09:21:47,597 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:21:47,598 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:21:47,598 - INFO - joeynmt.training - 	Hypothesis: Ho fatto questa foto di questi due milioni di dati che ho fatto questi due milioni di dati per fare il primo in<unk> @ fan<unk> @ ti<unk> @ ene che la c<unk> @ lin<unk> @ ica per il c<unk> @ att<unk> @ ur<unk> @ are il numero di milioni di anni , il numero di milioni di anni , il 9<unk> @ 00 milioni di anni .
2025-05-28 09:21:47,598 - INFO - joeynmt.training - Example #1
2025-05-28 09:21:47,598 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:21:47,598 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:21:47,598 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'un', 'modo', 'di', 'cui', 'la', 's@@', '<unk>', '@', 'quad@@', '<unk>', '@', 'ro', 'di', 'questo', 'tipo', 'di', 'vita', 'di', 'questo', 'tipo', 'di', 'vita', ',', 'non', 'è', 'la', 'com@@', '<unk>', '@', 'plic@@', '<unk>', '@', 'azione', 'di', 'questo', '.', '</s>']
2025-05-28 09:21:47,598 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:21:47,598 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:21:47,599 - INFO - joeynmt.training - 	Hypothesis: Ma non è un modo di cui la s<unk> @ quad<unk> @ ro di questo tipo di vita di questo tipo di vita , non è la com<unk> @ plic<unk> @ azione di questo .
2025-05-28 09:21:47,599 - INFO - joeynmt.training - Example #2
2025-05-28 09:21:47,599 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:21:47,599 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:21:47,599 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'questo', 'modo', 'di', 'cui', 'è', 'la', 'nostra', 'in@@', '<unk>', '@', 'ti@@', '<unk>', '@', 'r@@', '<unk>', '@', 'ità', 'è', 'la', 'nostra', 'in@@', '<unk>', '@', 'azione', 'di', 'un', 'sistema', 'di', 'c@@', '<unk>', '@', 'lin@@', '<unk>', '@', 'ica', '.', '</s>']
2025-05-28 09:21:47,599 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:21:47,599 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:21:47,599 - INFO - joeynmt.training - 	Hypothesis: In questo modo di cui è la nostra in<unk> @ ti<unk> @ r<unk> @ ità è la nostra in<unk> @ azione di un sistema di c<unk> @ lin<unk> @ ica .
2025-05-28 09:21:47,600 - INFO - joeynmt.training - Example #3
2025-05-28 09:21:47,600 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:21:47,600 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:21:47,600 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'chiama', 's@@', '<unk>', '@', 'ett@@', '<unk>', '@', 'em@@', '<unk>', '@', 'bre', 'e', 'il', 's@@', '<unk>', '@', 'ett@@', '<unk>', '@', 'em@@', '<unk>', '@', 'bre', '.', '</s>']
2025-05-28 09:21:47,600 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:21:47,600 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:21:47,600 - INFO - joeynmt.training - 	Hypothesis: Si chiama s<unk> @ ett<unk> @ em<unk> @ bre e il s<unk> @ ett<unk> @ em<unk> @ bre .
2025-05-28 09:21:47,600 - INFO - joeynmt.training - Example #4
2025-05-28 09:21:47,601 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:21:47,601 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:21:47,601 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di', 'voi', 'vi', 'mostr@@', '<unk>', '@', 'o', 'di', 'voi', 'è', 'una', 'cosa', 'che', 'è', 'una', 'cosa', 'che', 'è', 'in', 'cui', 'è', 'il', 'primo', 'primo', '.', '</s>']
2025-05-28 09:21:47,601 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:21:47,601 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:21:47,601 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di voi vi mostr<unk> @ o di voi è una cosa che è una cosa che è in cui è il primo primo .
2025-05-28 09:21:50,925 - INFO - joeynmt.training - Epoch   2, Step:     7600, Batch Loss:     1.765854, Batch Acc: 0.542491, Tokens per Sec:    18171, Lr: 0.000300
2025-05-28 09:21:54,271 - INFO - joeynmt.training - Epoch   2, Step:     7700, Batch Loss:     1.729391, Batch Acc: 0.541820, Tokens per Sec:    21467, Lr: 0.000300
2025-05-28 09:21:57,705 - INFO - joeynmt.training - Epoch   2, Step:     7800, Batch Loss:     1.771281, Batch Acc: 0.544637, Tokens per Sec:    20926, Lr: 0.000300
2025-05-28 09:22:01,249 - INFO - joeynmt.training - Epoch   2, Step:     7900, Batch Loss:     1.664158, Batch Acc: 0.544970, Tokens per Sec:    20597, Lr: 0.000300
2025-05-28 09:22:04,802 - INFO - joeynmt.training - Epoch   2, Step:     8000, Batch Loss:     1.694962, Batch Acc: 0.547701, Tokens per Sec:    19893, Lr: 0.000300
2025-05-28 09:22:04,802 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:22:04,803 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:22:14,309 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.73, ppl:   5.66, acc:   0.55, generation: 9.4960[sec], evaluation: 0.0000[sec]
2025-05-28 09:22:14,310 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:22:14,910 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/5500.ckpt
2025-05-28 09:22:14,937 - INFO - joeynmt.training - Example #0
2025-05-28 09:22:14,938 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:22:14,938 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:22:14,938 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Il', 'anno', ',', 'ho', 'fatto', 'che', 'questi', 'due', 'due', 'due', 'due', ',', 'ho', 'fatto', 'che', 'questi', 'due', 'due', ',', 'per', 'cento', 'che', 'i', 'fi@@', '<unk>', '@', 'che', 'che', 'le', 'st@@', '<unk>', '@', 'avano', 'che', 'le', 'st@@', '<unk>', '@', 'ati@@', '<unk>', '@', 'che', 'che', 'che', 'è', 'stato', 'per', 'cento', 'per', 'cento', 'di', '3@@', '<unk>', '@', '5', 'milioni', 'di', 'anni', ',', 'il', '2@@', '<unk>', '@', '000', 'milioni', 'di', 'anni', ',', 'il', '20', ',', 'il', '20', 'anni', ',', 'il', '9@@', '<unk>', '@', '5', '%', 'della', 'popolazione', '.', '</s>']
2025-05-28 09:22:14,938 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:22:14,938 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:22:14,939 - INFO - joeynmt.training - 	Hypothesis: Il anno , ho fatto che questi due due due due , ho fatto che questi due due , per cento che i fi<unk> @ che che le st<unk> @ avano che le st<unk> @ ati<unk> @ che che che è stato per cento per cento di 3<unk> @ 5 milioni di anni , il 2<unk> @ 000 milioni di anni , il 20 , il 20 anni , il 9<unk> @ 5 % della popolazione .
2025-05-28 09:22:14,939 - INFO - joeynmt.training - Example #1
2025-05-28 09:22:14,939 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:22:14,939 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:22:14,939 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'un', 'certo', 'senso', ',', 'non', 'è', 'un', 'senso', 'di', 'essere', 'più', 'grande', ',', 'che', 'non', 'è', 'un', 'problema', 'di', 'tecnologia', ',', 'ma', 'non', 'è', 'la', 'tecnologia', ',', 'non', 'è', 'la', 'tecnologia', ',', 'non', 'è', 'la', 'tecnologia', '.', '</s>']
2025-05-28 09:22:14,939 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:22:14,939 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:22:14,939 - INFO - joeynmt.training - 	Hypothesis: Ma non è un certo senso , non è un senso di essere più grande , che non è un problema di tecnologia , ma non è la tecnologia , non è la tecnologia , non è la tecnologia .
2025-05-28 09:22:14,940 - INFO - joeynmt.training - Example #2
2025-05-28 09:22:14,940 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:22:14,940 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:22:14,940 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'questo', 'caso', ',', 'la', 'cosa', 'di', 'cui', 'è', 'la', 'struttura', 'di', 'cui', 'il', 'nostro', 's@@', '<unk>', '@', 'ettore', 'della', 'nostra', 'struttura', 'della', 'nostra', 'c@@', '<unk>', '@', 'av@@', '<unk>', '@', 'anz@@', '<unk>', '@', 'o', 'della', 'nostra', 'società', '.', '</s>']
2025-05-28 09:22:14,940 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:22:14,940 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:22:14,940 - INFO - joeynmt.training - 	Hypothesis: In questo caso , la cosa di cui è la struttura di cui il nostro s<unk> @ ettore della nostra struttura della nostra c<unk> @ av<unk> @ anz<unk> @ o della nostra società .
2025-05-28 09:22:14,940 - INFO - joeynmt.training - Example #3
2025-05-28 09:22:14,941 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:22:14,941 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:22:14,941 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'tratta', 'di', 'un', 'modo', 'in', 'cui', 'si', 'tratta', 'di', 'ri@@', '<unk>', '@', 'stor@@', '<unk>', '@', 'ica', 'e', 'la', 's@@', '<unk>', '@', 'essu@@', '<unk>', '@', 'ale', '.', '</s>']
2025-05-28 09:22:14,941 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:22:14,941 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:22:14,941 - INFO - joeynmt.training - 	Hypothesis: Si tratta di un modo in cui si tratta di ri<unk> @ stor<unk> @ ica e la s<unk> @ essu<unk> @ ale .
2025-05-28 09:22:14,941 - INFO - joeynmt.training - Example #4
2025-05-28 09:22:14,941 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:22:14,942 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:22:14,942 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'la', 'prossi@@', '<unk>', '@', 'ma', 'è', 'una', 'cosa', 'che', 'vi', 'mostr@@', '<unk>', '@', 'arvi', ',', 'che', 'è', 'una', 'cosa', 'che', 'è', 'successo', '.', '</s>']
2025-05-28 09:22:14,942 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:22:14,942 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:22:14,942 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma la prossi<unk> @ ma è una cosa che vi mostr<unk> @ arvi , che è una cosa che è successo .
2025-05-28 09:22:18,427 - INFO - joeynmt.training - Epoch   2, Step:     8100, Batch Loss:     1.704638, Batch Acc: 0.550034, Tokens per Sec:    17735, Lr: 0.000300
2025-05-28 09:22:21,911 - INFO - joeynmt.training - Epoch   2, Step:     8200, Batch Loss:     1.645332, Batch Acc: 0.548279, Tokens per Sec:    19575, Lr: 0.000300
2025-05-28 09:22:25,393 - INFO - joeynmt.training - Epoch   2, Step:     8300, Batch Loss:     1.666752, Batch Acc: 0.547913, Tokens per Sec:    19413, Lr: 0.000300
2025-05-28 09:22:28,927 - INFO - joeynmt.training - Epoch   2, Step:     8400, Batch Loss:     1.811200, Batch Acc: 0.550369, Tokens per Sec:    20576, Lr: 0.000300
2025-05-28 09:22:32,446 - INFO - joeynmt.training - Epoch   2, Step:     8500, Batch Loss:     1.804078, Batch Acc: 0.551774, Tokens per Sec:    20917, Lr: 0.000300
2025-05-28 09:22:32,447 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:22:32,447 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:22:45,703 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.71, ppl:   5.51, acc:   0.55, generation: 13.2444[sec], evaluation: 0.0000[sec]
2025-05-28 09:22:45,704 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:22:46,397 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/6000.ckpt
2025-05-28 09:22:46,428 - INFO - joeynmt.training - Example #0
2025-05-28 09:22:46,429 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:22:46,429 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:22:46,429 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', ',', 'ho', 'fatto', 'queste', 'due', 'due', 'due', 'due', 'due', ',', 'per', 'esempio', ',', 'per', 'esempio', ',', 'che', 'si', 'sono', 'ri@@', '<unk>', '@', 'ma@@', '<unk>', '@', 'sti', 'che', 'i', 'm@@', '<unk>', '@', 'essi', 'di', 'essere', 'il', '2@@', '<unk>', '@', '7', 'milioni', 'di', 'anni', ',', 'che', 'il', '2@@', '<unk>', '@', '1', 'milioni', 'di', 'anni', ',', 'che', 'il', '2@@', '<unk>', '@', '7', 'milioni', 'di', 'anni', ',', 'il', '3@@', '<unk>', '@', '8', 'milioni', 'di', 'anni', ',', 'il', '3@@', '<unk>', '@', '8', 'milioni', 'di', 'anni', ',', 'per', 'cento', ',', 'il', '3@@', '<unk>', '@', '8', 'milioni', 'di', 'anni', '.', '</s>']
2025-05-28 09:22:46,430 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:22:46,430 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:22:46,430 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno , ho fatto queste due due due due due , per esempio , per esempio , che si sono ri<unk> @ ma<unk> @ sti che i m<unk> @ essi di essere il 2<unk> @ 7 milioni di anni , che il 2<unk> @ 1 milioni di anni , che il 2<unk> @ 7 milioni di anni , il 3<unk> @ 8 milioni di anni , il 3<unk> @ 8 milioni di anni , per cento , il 3<unk> @ 8 milioni di anni .
2025-05-28 09:22:46,430 - INFO - joeynmt.training - Example #1
2025-05-28 09:22:46,430 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:22:46,430 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:22:46,430 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'un', 'po', '&apos;', 'di', 'essere', 'più', 'difficile', ',', 'che', 'non', 'è', 'molto', 'difficile', ',', 'che', 'non', 'è', 'molto', 'semplice', ',', 'che', 'non', 'è', 'la', 'tecnologia', ',', 'non', 'è', 'il', 'problema', 'che', 'non', 'è', 'il', 'problema', 'che', 'non', 'è', 'il', 'problema', '.', '</s>']
2025-05-28 09:22:46,431 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:22:46,431 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:22:46,431 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è un po &apos; di essere più difficile , che non è molto difficile , che non è molto semplice , che non è la tecnologia , non è il problema che non è il problema che non è il problema .
2025-05-28 09:22:46,431 - INFO - joeynmt.training - Example #2
2025-05-28 09:22:46,431 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:22:46,431 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:22:46,432 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'realtà', ',', 'la', 'cosa', 'più', 'importante', 'è', 'la', 'nostra', 'in@@', '<unk>', '@', 'azione', 'di', 'E@@', '<unk>', '@', 'b@@', '<unk>', '@', 'bi@@', '<unk>', '@', '-@@', '<unk>', '@', 'si@@', '<unk>', '@', 'bile', ',', 'il', 'nostro', 'sistema', 'di', 'ri@@', '<unk>', '@', 'bile', '.', '</s>']
2025-05-28 09:22:46,432 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:22:46,432 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:22:46,432 - INFO - joeynmt.training - 	Hypothesis: In realtà , la cosa più importante è la nostra in<unk> @ azione di E<unk> @ b<unk> @ bi<unk> @ -<unk> @ si<unk> @ bile , il nostro sistema di ri<unk> @ bile .
2025-05-28 09:22:46,432 - INFO - joeynmt.training - Example #3
2025-05-28 09:22:46,432 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:22:46,432 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:22:46,432 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'oro', 'che', 'si', 'trova', 'in', 'modo', 'in', 'cui', 'si', 'è', 'ri@@', '<unk>', '@', 'ma@@', '<unk>', '@', 'sto', '.', '</s>']
2025-05-28 09:22:46,433 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:22:46,433 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:22:46,433 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ oro che si trova in modo in cui si è ri<unk> @ ma<unk> @ sto .
2025-05-28 09:22:46,433 - INFO - joeynmt.training - Example #4
2025-05-28 09:22:46,433 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:22:46,433 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:22:46,433 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'un', 'po', '&apos;', 'di', 'voi', ',', 'è', 'una', 'cosa', 'che', 'vi', 'mostr@@', '<unk>', '@', 'ato', 'in', 'un', 'dec@@', '<unk>', '@', 'en@@', '<unk>', '@', 'uto', 'di', 'cosa', 'succede', '.', '</s>']
2025-05-28 09:22:46,434 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:22:46,434 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:22:46,434 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma vi mostr<unk> @ erò un po &apos; di voi , è una cosa che vi mostr<unk> @ ato in un dec<unk> @ en<unk> @ uto di cosa succede .
2025-05-28 09:22:50,016 - INFO - joeynmt.training - Epoch   2, Step:     8600, Batch Loss:     1.658334, Batch Acc: 0.552178, Tokens per Sec:    16344, Lr: 0.000300
2025-05-28 09:22:53,557 - INFO - joeynmt.training - Epoch   2, Step:     8700, Batch Loss:     1.768061, Batch Acc: 0.549079, Tokens per Sec:    20282, Lr: 0.000300
2025-05-28 09:22:57,088 - INFO - joeynmt.training - Epoch   2, Step:     8800, Batch Loss:     1.736887, Batch Acc: 0.555498, Tokens per Sec:    19818, Lr: 0.000300
2025-05-28 09:23:00,606 - INFO - joeynmt.training - Epoch   2, Step:     8900, Batch Loss:     1.638907, Batch Acc: 0.555184, Tokens per Sec:    20301, Lr: 0.000300
2025-05-28 09:23:04,150 - INFO - joeynmt.training - Epoch   2, Step:     9000, Batch Loss:     1.653959, Batch Acc: 0.557851, Tokens per Sec:    20540, Lr: 0.000300
2025-05-28 09:23:04,150 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:23:04,150 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:23:18,285 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.68, ppl:   5.38, acc:   0.56, generation: 14.1187[sec], evaluation: 0.0000[sec]
2025-05-28 09:23:18,286 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:23:18,970 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/6500.ckpt
2025-05-28 09:23:19,001 - INFO - joeynmt.training - Example #0
2025-05-28 09:23:19,001 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:23:19,001 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:23:19,001 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'fatto', 'questi', 'due', 'due', 'due', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'che', 'i', 'primi', 'per', 'l&apos;', 'inter@@', '<unk>', '@', 'faccia', 'di', 'un', 't@@', '<unk>', '@', 'on@@', '<unk>', '@', 'ico', 'che', 'il', 'C@@', '<unk>', '@', 'is@@', '<unk>', '@', 'o', ',', 'che', 'il', '9@@', '<unk>', '@', '8', 'milioni', 'di', 'anni', ',', 'per', 'il', '9@@', '<unk>', '@', '8', 'milioni', 'di', 'anni', ',', 'il', '9@@', '<unk>', '@', '8', 'milioni', 'di', 'anni', ',', 'il', '9@@', '<unk>', '@', '8', 'milioni', 'di', 'anni', ',', 'il', '9@@', '<unk>', '@', '8', 'milioni', 'di', 'anni', '.', '</s>']
2025-05-28 09:23:19,002 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:23:19,002 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:23:19,002 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho fatto questi due due due , ho mostr<unk> @ ato che i primi per l&apos; inter<unk> @ faccia di un t<unk> @ on<unk> @ ico che il C<unk> @ is<unk> @ o , che il 9<unk> @ 8 milioni di anni , per il 9<unk> @ 8 milioni di anni , il 9<unk> @ 8 milioni di anni , il 9<unk> @ 8 milioni di anni , il 9<unk> @ 8 milioni di anni .
2025-05-28 09:23:19,002 - INFO - joeynmt.training - Example #1
2025-05-28 09:23:19,002 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:23:19,003 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:23:19,003 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'un', 'certo', 'senso', ',', 'che', 'non', 'è', 'un', 'problema', 'di', 'essere', 'un', 'problema', 'di', 'problemi', 'di', 'problemi', 'di', 'questo', 'problema', ',', 'che', 'non', 'non', 'è', 'il', 'problema', 'di', 'questa', 'è', 'che', 'non', 'è', 'la', 'differenza', '.', '</s>']
2025-05-28 09:23:19,003 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:23:19,003 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:23:19,003 - INFO - joeynmt.training - 	Hypothesis: Ma non è un certo senso , che non è un problema di essere un problema di problemi di problemi di questo problema , che non non è il problema di questa è che non è la differenza .
2025-05-28 09:23:19,003 - INFO - joeynmt.training - Example #2
2025-05-28 09:23:19,004 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:23:19,004 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:23:19,004 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'effetti', ',', 'il', 'punto', 'di', 'vista', 'è', 'il', 't@@', '<unk>', '@', 'oc@@', '<unk>', '@', 'chio', 'di', 'carb@@', '<unk>', '@', 'on@@', '<unk>', '@', 'io', 'del', 'nostro', 'sistema', 'di', 's@@', '<unk>', '@', 'essu@@', '<unk>', '@', 'ale', '.', '</s>']
2025-05-28 09:23:19,004 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:23:19,004 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:23:19,004 - INFO - joeynmt.training - 	Hypothesis: In effetti , il punto di vista è il t<unk> @ oc<unk> @ chio di carb<unk> @ on<unk> @ io del nostro sistema di s<unk> @ essu<unk> @ ale .
2025-05-28 09:23:19,004 - INFO - joeynmt.training - Example #3
2025-05-28 09:23:19,005 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:23:19,005 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:23:19,005 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'ei', 'si', 'ri@@', '<unk>', '@', 'vel@@', '<unk>', '@', 'a', 'e', 'il', 't@@', '<unk>', '@', 'om@@', '<unk>', '@', 'eno', 'di', 's@@', '<unk>', '@', 'ov@@', '<unk>', '@', 'i', '.', '</s>']
2025-05-28 09:23:19,005 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:23:19,005 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:23:19,005 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ ei si ri<unk> @ vel<unk> @ a e il t<unk> @ om<unk> @ eno di s<unk> @ ov<unk> @ i .
2025-05-28 09:23:19,005 - INFO - joeynmt.training - Example #4
2025-05-28 09:23:19,005 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:23:19,006 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:23:19,006 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'un', 'po', '&apos;', 'di', 'mostr@@', '<unk>', '@', 'o', 'un', 'po', '&apos;', 'di', 'fatto', 'che', 'è', 'successo', '.', '</s>']
2025-05-28 09:23:19,006 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:23:19,006 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:23:19,006 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma vi mostr<unk> @ erò un po &apos; di mostr<unk> @ o un po &apos; di fatto che è successo .
2025-05-28 09:23:22,598 - INFO - joeynmt.training - Epoch   2, Step:     9100, Batch Loss:     1.692916, Batch Acc: 0.560122, Tokens per Sec:    16212, Lr: 0.000300
2025-05-28 09:23:26,141 - INFO - joeynmt.training - Epoch   2, Step:     9200, Batch Loss:     1.651945, Batch Acc: 0.559534, Tokens per Sec:    20567, Lr: 0.000300
2025-05-28 09:23:29,688 - INFO - joeynmt.training - Epoch   2, Step:     9300, Batch Loss:     1.604336, Batch Acc: 0.559692, Tokens per Sec:    20684, Lr: 0.000300
2025-05-28 09:23:33,237 - INFO - joeynmt.training - Epoch   2, Step:     9400, Batch Loss:     1.629757, Batch Acc: 0.559088, Tokens per Sec:    20029, Lr: 0.000300
2025-05-28 09:23:36,792 - INFO - joeynmt.training - Epoch   2, Step:     9500, Batch Loss:     1.735863, Batch Acc: 0.561809, Tokens per Sec:    20140, Lr: 0.000300
2025-05-28 09:23:36,793 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:23:36,793 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:23:47,530 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.66, ppl:   5.28, acc:   0.56, generation: 10.7274[sec], evaluation: 0.0000[sec]
2025-05-28 09:23:47,532 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:23:48,337 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/7000.ckpt
2025-05-28 09:23:48,367 - INFO - joeynmt.training - Example #0
2025-05-28 09:23:48,368 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:23:48,368 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:23:48,368 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'in', 'due', 'due', 'giorni', 'per', 'creare', 'un', 'anno', 'per', 'creare', 'un', 'anno', 'per', 'esempio', 'di', 'cui', 'i', 'due', 'anni', ',', 'che', 'i', 'media', 'è', 'che', 'i', 's@@', '<unk>', '@', 'essu@@', '<unk>', '@', 'ali', 'per', 'il', 't@@', '<unk>', '@', 'oc@@', '<unk>', '@', 'chio', 'per', 'cento', 'di', 'tre', 'anni', ',', 'per', 'cento', 'di', 'tre', 'anni', ',', 'per', 'cento', 'di', 'tre', 'anni', ',', 'per', 'cento', 'di', 'tre', 'anni', ',', 'per', 'cento', 'di', 'tre', 'anni', '.', '</s>']
2025-05-28 09:23:48,368 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:23:48,369 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:23:48,369 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato in due due giorni per creare un anno per creare un anno per esempio di cui i due anni , che i media è che i s<unk> @ essu<unk> @ ali per il t<unk> @ oc<unk> @ chio per cento di tre anni , per cento di tre anni , per cento di tre anni , per cento di tre anni , per cento di tre anni .
2025-05-28 09:23:48,369 - INFO - joeynmt.training - Example #1
2025-05-28 09:23:48,369 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:23:48,369 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:23:48,369 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'si', 'è', 'ri@@', '<unk>', '@', 'era', ',', 'la', 'cosa', 'che', 'è', 'più', 'grande', ',', 'che', 'non', 'ha', 'un', 'problema', 'di', 'vita', 'di', 'questa', 'tecnologia', ',', 'che', 'non', 'ha', 'mostr@@', '<unk>', '@', 'ato', 'la', 'tecnologia', ',', 'che', 'non', 'ha', 'spieg@@', '<unk>', '@', 'ato', 'il', 'problema', '.', '</s>']
2025-05-28 09:23:48,369 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:23:48,370 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:23:48,370 - INFO - joeynmt.training - 	Hypothesis: Ma non si è ri<unk> @ era , la cosa che è più grande , che non ha un problema di vita di questa tecnologia , che non ha mostr<unk> @ ato la tecnologia , che non ha spieg<unk> @ ato il problema .
2025-05-28 09:23:48,370 - INFO - joeynmt.training - Example #2
2025-05-28 09:23:48,370 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:23:48,370 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:23:48,370 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'realtà', ',', 'il', 'punto', 'di', 'vista', 'è', 'il', 't@@', '<unk>', '@', 'avolo', 'di', 'ar@@', '<unk>', '@', 'ab@@', '<unk>', '@', 'ar@@', '<unk>', '@', 'ne', 'di', 'un', 'eff@@', '<unk>', '@', 'etto', 'di', 'b@@', '<unk>', '@', 'al@@', '<unk>', '@', 'ista', 'del', 'nostro', 'sistema', 'globale', '.', '</s>']
2025-05-28 09:23:48,371 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:23:48,371 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:23:48,371 - INFO - joeynmt.training - 	Hypothesis: In realtà , il punto di vista è il t<unk> @ avolo di ar<unk> @ ab<unk> @ ar<unk> @ ne di un eff<unk> @ etto di b<unk> @ al<unk> @ ista del nostro sistema globale .
2025-05-28 09:23:48,371 - INFO - joeynmt.training - Example #3
2025-05-28 09:23:48,371 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:23:48,371 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:23:48,371 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['S@@', '<unk>', '@', 'ta', ',', 'in', 'tutto', 'il', 'suo', 's@@', '<unk>', '@', 'ettore', 'e', 'il', 's@@', '<unk>', '@', 'ettore', '.', '</s>']
2025-05-28 09:23:48,372 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:23:48,372 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:23:48,372 - INFO - joeynmt.training - 	Hypothesis: S<unk> @ ta , in tutto il suo s<unk> @ ettore e il s<unk> @ ettore .
2025-05-28 09:23:48,372 - INFO - joeynmt.training - Example #4
2025-05-28 09:23:48,372 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:23:48,372 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:23:48,372 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'la', 'prima', 'cosa', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'un', 'esempio', 'di', 'una', 'cosa', 'che', 'è', 'una', 'cosa', 'che', 'è', 'successo', '.', '</s>']
2025-05-28 09:23:48,372 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:23:48,372 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:23:48,372 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma la prima cosa che vi mostr<unk> @ erò è un esempio di una cosa che è una cosa che è successo .
2025-05-28 09:23:51,969 - INFO - joeynmt.training - Epoch   2, Step:     9600, Batch Loss:     1.462315, Batch Acc: 0.564567, Tokens per Sec:    16974, Lr: 0.000300
2025-05-28 09:23:55,516 - INFO - joeynmt.training - Epoch   2, Step:     9700, Batch Loss:     1.733129, Batch Acc: 0.562926, Tokens per Sec:    20149, Lr: 0.000300
2025-05-28 09:23:59,062 - INFO - joeynmt.training - Epoch   2, Step:     9800, Batch Loss:     1.598253, Batch Acc: 0.567071, Tokens per Sec:    20009, Lr: 0.000300
2025-05-28 09:24:02,547 - INFO - joeynmt.training - Epoch   2, Step:     9900, Batch Loss:     1.715773, Batch Acc: 0.566745, Tokens per Sec:    20854, Lr: 0.000300
2025-05-28 09:24:06,061 - INFO - joeynmt.training - Epoch   2, Step:    10000, Batch Loss:     1.711090, Batch Acc: 0.566507, Tokens per Sec:    20409, Lr: 0.000300
2025-05-28 09:24:06,061 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:24:06,061 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:24:20,043 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.64, ppl:   5.14, acc:   0.57, generation: 13.9709[sec], evaluation: 0.0000[sec]
2025-05-28 09:24:20,044 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:24:20,722 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/7500.ckpt
2025-05-28 09:24:20,749 - INFO - joeynmt.training - Example #0
2025-05-28 09:24:20,750 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:24:20,750 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:24:20,750 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'che', 'la', 'prima', 'volta', 'che', 'la', 'legge', 'è', 'che', 'la', 'legge', 'di', 'questi', 'due', 'anni', ',', 'per', 'la', 'legge', ',', 'che', 'è', 'stato', 'un', 'ap@@', '<unk>', '@', 'proc@@', '<unk>', '@', 'cio', ',', 'per', 'gli', 'ultimi', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'gli', 'ultimi', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'gli', 'ultimi', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'gli', 'Stati', 'Uniti', ',', 'per', 'gli', 'Stati', 'Uniti', ',', 'per', 'gli', 'Stati', 'Uniti', '.', '</s>']
2025-05-28 09:24:20,751 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:24:20,751 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:24:20,751 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato che la prima volta che la legge è che la legge di questi due anni , per la legge , che è stato un ap<unk> @ proc<unk> @ cio , per gli ultimi tre milioni di anni , per gli ultimi tre milioni di anni , per gli ultimi tre milioni di anni , per gli Stati Uniti , per gli Stati Uniti , per gli Stati Uniti .
2025-05-28 09:24:20,751 - INFO - joeynmt.training - Example #1
2025-05-28 09:24:20,752 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:24:20,752 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:24:20,752 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'molto', 'più', 'facile', ',', 'che', 'è', 'molto', 'più', 'facile', ',', 'che', 'non', 'è', 'molto', 'più', 'importante', ',', 'che', 'è', 'la', 'prima', 'cosa', 'che', 'non', 'è', 'la', 'realtà', ',', 'non', 'è', 'la', 'gente', 'che', 'ha', 'mostr@@', '<unk>', '@', 'ato', '.', '</s>']
2025-05-28 09:24:20,752 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:24:20,752 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:24:20,752 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è molto più facile , che è molto più facile , che non è molto più importante , che è la prima cosa che non è la realtà , non è la gente che ha mostr<unk> @ ato .
2025-05-28 09:24:20,752 - INFO - joeynmt.training - Example #2
2025-05-28 09:24:20,753 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:24:20,753 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:24:20,753 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'realtà', ',', 'la', 'realtà', 'è', 'la', 'realtà', 'è', 'la', 'nostra', 'uman@@', '<unk>', '@', 'ità', 'del', 'nostro', 'sistema', 'di', 'c@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'l@@', '<unk>', '@', 'is@@', '<unk>', '@', 'e', 'del', 'nostro', 'sistema', 'del', 'nostro', 'sistema', '.', '</s>']
2025-05-28 09:24:20,753 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:24:20,753 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:24:20,753 - INFO - joeynmt.training - 	Hypothesis: In realtà , la realtà è la realtà è la nostra uman<unk> @ ità del nostro sistema di c<unk> @ ic<unk> @ l<unk> @ is<unk> @ e del nostro sistema del nostro sistema .
2025-05-28 09:24:20,753 - INFO - joeynmt.training - Example #3
2025-05-28 09:24:20,754 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:24:20,754 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:24:20,754 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&apos;', 'quello', 'che', 'è', 'nel', 'tempo', ',', 'e', 'si', 'è', 'ri@@', '<unk>', '@', 'vel@@', '<unk>', '@', 'a', '.', '</s>']
2025-05-28 09:24:20,754 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:24:20,754 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:24:20,754 - INFO - joeynmt.training - 	Hypothesis: E &apos; quello che è nel tempo , e si è ri<unk> @ vel<unk> @ a .
2025-05-28 09:24:20,754 - INFO - joeynmt.training - Example #4
2025-05-28 09:24:20,754 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:24:20,754 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:24:20,755 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'la', 'prossi@@', '<unk>', '@', 'ma', 'di', 'voi', ',', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'un', 'esempio', 'di', 'voi', '.', '</s>']
2025-05-28 09:24:20,755 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:24:20,755 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:24:20,755 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma la prossi<unk> @ ma di voi , vi mostr<unk> @ erò un esempio di voi .
2025-05-28 09:24:24,321 - INFO - joeynmt.training - Epoch   2, Step:    10100, Batch Loss:     1.547150, Batch Acc: 0.572763, Tokens per Sec:    17168, Lr: 0.000300
2025-05-28 09:24:27,885 - INFO - joeynmt.training - Epoch   2, Step:    10200, Batch Loss:     1.627965, Batch Acc: 0.576241, Tokens per Sec:    20394, Lr: 0.000300
2025-05-28 09:24:31,438 - INFO - joeynmt.training - Epoch   2, Step:    10300, Batch Loss:     1.575857, Batch Acc: 0.568127, Tokens per Sec:    20130, Lr: 0.000300
2025-05-28 09:24:34,972 - INFO - joeynmt.training - Epoch   2, Step:    10400, Batch Loss:     1.667717, Batch Acc: 0.567719, Tokens per Sec:    19838, Lr: 0.000300
2025-05-28 09:24:38,500 - INFO - joeynmt.training - Epoch   2, Step:    10500, Batch Loss:     1.505612, Batch Acc: 0.574968, Tokens per Sec:    20323, Lr: 0.000300
2025-05-28 09:24:38,501 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:24:38,501 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:24:51,270 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.61, ppl:   5.02, acc:   0.57, generation: 12.7585[sec], evaluation: 0.0000[sec]
2025-05-28 09:24:51,271 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:24:51,916 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/8000.ckpt
2025-05-28 09:24:51,946 - INFO - joeynmt.training - Example #0
2025-05-28 09:24:51,947 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:24:51,947 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:24:51,947 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'scorso', 'scorso', 'scorso', 'anno', ',', 'ho', 'fatto', 'che', 'il', 'numero', 'di', 'due', 'due', 'due', 'anni', ',', 'per', 'esempio', ',', 'che', 'la', 'cor@@', '<unk>', '@', 'sa', 'che', 'la', 'cor@@', '<unk>', '@', 'sa', 'di', 'E@@', '<unk>', '@', 'k@@', '<unk>', '@', 'et@@', '<unk>', '@', 'er@@', '<unk>', '@', 'o@@', '<unk>', '@', 'ica', 'per', 'la', 'C@@', '<unk>', '@', 'aliforni@@', '<unk>', '@', 'a', ',', 'per', 'il', 'più', 'di', 'circa', '4@@', '<unk>', '@', '8', 'milioni', 'di', 'anni', ',', 'che', 'è', 'stato', 'stato', 'stato', 'per', 'il', '20', 'anni', '.', '</s>']
2025-05-28 09:24:51,948 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:24:51,948 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:24:51,948 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso scorso scorso scorso anno , ho fatto che il numero di due due due anni , per esempio , che la cor<unk> @ sa che la cor<unk> @ sa di E<unk> @ k<unk> @ et<unk> @ er<unk> @ o<unk> @ ica per la C<unk> @ aliforni<unk> @ a , per il più di circa 4<unk> @ 8 milioni di anni , che è stato stato stato per il 20 anni .
2025-05-28 09:24:51,948 - INFO - joeynmt.training - Example #1
2025-05-28 09:24:51,948 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:24:51,949 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:24:51,949 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'abbastanza', ',', 'ma', 'la', 'com@@', '<unk>', '@', 'passione', ',', 'il', 'problema', 'di', 'questa', 're@@', '<unk>', '@', 'azione', ',', 'il', 'problema', 'di', 'questa', 'con@@', '<unk>', '@', 'sap@@', '<unk>', '@', 'evol@@', '<unk>', '@', 'uzione', 'del', 'problema', ',', 'non', 'è', 'il', 'problema', '.', '</s>']
2025-05-28 09:24:51,949 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:24:51,949 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:24:51,949 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza abbastanza , ma la com<unk> @ passione , il problema di questa re<unk> @ azione , il problema di questa con<unk> @ sap<unk> @ evol<unk> @ uzione del problema , non è il problema .
2025-05-28 09:24:51,949 - INFO - joeynmt.training - Example #2
2025-05-28 09:24:51,950 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:24:51,950 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:24:51,950 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'realtà', ',', 'il', 'senso', 'è', 'il', 'più', 'grande', 'è', 'il', 'sistema', 'di', 'cui', 'è', 'il', 'sistema', 'di', 'b@@', '<unk>', '@', 'otti@@', '<unk>', '@', 'gli@@', '<unk>', '@', 'o', 'dell&apos;', 'E@@', '<unk>', '@', 'k@@', '<unk>', '@', 'et@@', '<unk>', '@', 'ico', '.', '</s>']
2025-05-28 09:24:51,950 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:24:51,950 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:24:51,950 - INFO - joeynmt.training - 	Hypothesis: In realtà , il senso è il più grande è il sistema di cui è il sistema di b<unk> @ otti<unk> @ gli<unk> @ o dell&apos; E<unk> @ k<unk> @ et<unk> @ ico .
2025-05-28 09:24:51,950 - INFO - joeynmt.training - Example #3
2025-05-28 09:24:51,951 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:24:51,951 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:24:51,951 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'si', 'ri@@', '<unk>', '@', 'ma@@', '<unk>', '@', 'va', 'e', 'in', 'grado', 'di', 'ri@@', '<unk>', '@', 'fer@@', '<unk>', '@', 'imento', '.', '</s>']
2025-05-28 09:24:51,951 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:24:51,951 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:24:51,951 - INFO - joeynmt.training - 	Hypothesis: E si ri<unk> @ ma<unk> @ va e in grado di ri<unk> @ fer<unk> @ imento .
2025-05-28 09:24:51,951 - INFO - joeynmt.training - Example #4
2025-05-28 09:24:51,951 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:24:51,951 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:24:51,952 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'prossi@@', '<unk>', '@', 'ma', 'è', 'una', 'volta', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'un', 'di@@', '<unk>', '@', 'str@@', '<unk>', '@', 'etto', 'di', 'quello', 'che', 'sta', 'succe@@', '<unk>', '@', 'dendo', '.', '</s>']
2025-05-28 09:24:51,952 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:24:51,952 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:24:51,952 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma prossi<unk> @ ma è una volta che vi mostr<unk> @ erò un di<unk> @ str<unk> @ etto di quello che sta succe<unk> @ dendo .
2025-05-28 09:24:55,536 - INFO - joeynmt.training - Epoch   2, Step:    10600, Batch Loss:     1.688096, Batch Acc: 0.571633, Tokens per Sec:    16703, Lr: 0.000300
2025-05-28 09:24:59,074 - INFO - joeynmt.training - Epoch   2, Step:    10700, Batch Loss:     1.652135, Batch Acc: 0.576290, Tokens per Sec:    20405, Lr: 0.000300
2025-05-28 09:25:02,603 - INFO - joeynmt.training - Epoch   2, Step:    10800, Batch Loss:     1.684029, Batch Acc: 0.577399, Tokens per Sec:    20432, Lr: 0.000300
2025-05-28 09:25:06,107 - INFO - joeynmt.training - Epoch   2, Step:    10900, Batch Loss:     1.578392, Batch Acc: 0.577708, Tokens per Sec:    20067, Lr: 0.000300
2025-05-28 09:25:09,629 - INFO - joeynmt.training - Epoch   2, Step:    11000, Batch Loss:     1.624312, Batch Acc: 0.577046, Tokens per Sec:    20193, Lr: 0.000300
2025-05-28 09:25:09,630 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:25:09,630 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:25:18,554 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.60, ppl:   4.94, acc:   0.58, generation: 8.9177[sec], evaluation: 0.0000[sec]
2025-05-28 09:25:18,555 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:25:19,123 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/8500.ckpt
2025-05-28 09:25:19,151 - INFO - joeynmt.training - Example #0
2025-05-28 09:25:19,152 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:25:19,152 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:25:19,152 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'che', 'i', 'due', 'due', 'due', 'anni', ',', 'per', 'creare', 'un', 'certo', 'certo', 'di', 'cui', 'i', 'd@@', '<unk>', '@', 'unque', ',', 'che', 'la', 'maggior', 'parte', 'di', 'cui', 'i', 'c@@', '<unk>', '@', 'lin@@', '<unk>', '@', 'ee', 'di', 'fatto', 'che', 'la', 'maggior', 'parte', 'di', 'cui', 'i', 'tre', 'milioni', 'di', 'anni', ',', 'che', 'è', 'stato', 'stato', 'fatto', 'per', 'il', '4@@', '<unk>', '@', '8', 'milioni', 'di', 'anni', ',', 'che', 'è', 'stato', 'stato', 'stato', 'fatto', 'per', 'cento', 'di', 'questi', 'stati', ',', 'per', 'cento', 'di', 'tre', 'milioni', 'di', 'anni', ',', 'è', 'stato', 'stato', 'stato', 'il', '40', '%', '.', '</s>']
2025-05-28 09:25:19,153 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:25:19,153 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:25:19,153 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno , ho mostr<unk> @ ato che i due due due anni , per creare un certo certo di cui i d<unk> @ unque , che la maggior parte di cui i c<unk> @ lin<unk> @ ee di fatto che la maggior parte di cui i tre milioni di anni , che è stato stato fatto per il 4<unk> @ 8 milioni di anni , che è stato stato stato fatto per cento di questi stati , per cento di tre milioni di anni , è stato stato stato il 40 % .
2025-05-28 09:25:19,153 - INFO - joeynmt.training - Example #1
2025-05-28 09:25:19,153 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:25:19,153 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:25:19,153 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', ',', 'la', 'per@@', '<unk>', '@', 'dita', 'di', 'questa', 'idea', 'di', 'questo', 'problema', 'di', 'questo', 'problema', 'di', 'cui', 'è', 'un', 'problema', 'di', 'questo', 'problema', 'di', 'cui', 'non', 'è', 'il', 'sistema', 'di', 'essere', 'il', 'problema', 'di', 'questo', 'problema', ',', 'non', 'è', 'il', 'sistema', 'di', 'essere', 'del', 'sistema', 'di', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'h@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ca', '.', '</s>']
2025-05-28 09:25:19,154 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:25:19,154 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:25:19,154 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza , la per<unk> @ dita di questa idea di questo problema di questo problema di cui è un problema di questo problema di cui non è il sistema di essere il problema di questo problema , non è il sistema di essere del sistema di E<unk> @ is<unk> @ h<unk> @ is<unk> @ ca .
2025-05-28 09:25:19,154 - INFO - joeynmt.training - Example #2
2025-05-28 09:25:19,154 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:25:19,155 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:25:19,155 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'realtà', ',', 'la', 'cosa', 'più', 'in@@', '<unk>', '@', 'for@@', '<unk>', '@', 'mat@@', '<unk>', '@', 'ica', 'è', 'il', 'sistema', 'di', 'mat@@', '<unk>', '@', 'ico', '.', '</s>']
2025-05-28 09:25:19,155 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:25:19,155 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:25:19,155 - INFO - joeynmt.training - 	Hypothesis: In realtà , la cosa più in<unk> @ for<unk> @ mat<unk> @ ica è il sistema di mat<unk> @ ico .
2025-05-28 09:25:19,155 - INFO - joeynmt.training - Example #3
2025-05-28 09:25:19,155 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:25:19,155 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:25:19,155 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'ei', ',', 'prima', 'di', 'un', 'po', '&apos;', 'di', 's@@', '<unk>', '@', 'for@@', '<unk>', '@', 'mat@@', '<unk>', '@', 'ico', '.', '</s>']
2025-05-28 09:25:19,156 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:25:19,156 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:25:19,156 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ ei , prima di un po &apos; di s<unk> @ for<unk> @ mat<unk> @ ico .
2025-05-28 09:25:19,156 - INFO - joeynmt.training - Example #4
2025-05-28 09:25:19,156 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:25:19,156 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:25:19,156 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'la', 'prossi@@', '<unk>', '@', 'ma', 'è', 'una', 'cosa', 'che', 'è', 'una', 'cosa', 'che', 'sta', 'succe@@', '<unk>', '@', 'dendo', '.', '</s>']
2025-05-28 09:25:19,157 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:25:19,157 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:25:19,157 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma la prossi<unk> @ ma è una cosa che è una cosa che sta succe<unk> @ dendo .
2025-05-28 09:25:22,733 - INFO - joeynmt.training - Epoch   2, Step:    11100, Batch Loss:     1.653217, Batch Acc: 0.579633, Tokens per Sec:    17018, Lr: 0.000300
2025-05-28 09:25:26,323 - INFO - joeynmt.training - Epoch   2, Step:    11200, Batch Loss:     1.605749, Batch Acc: 0.579857, Tokens per Sec:    20730, Lr: 0.000300
2025-05-28 09:25:29,859 - INFO - joeynmt.training - Epoch   2, Step:    11300, Batch Loss:     1.599518, Batch Acc: 0.579681, Tokens per Sec:    20464, Lr: 0.000300
2025-05-28 09:25:33,416 - INFO - joeynmt.training - Epoch   2, Step:    11400, Batch Loss:     1.531622, Batch Acc: 0.582664, Tokens per Sec:    20485, Lr: 0.000300
2025-05-28 09:25:36,938 - INFO - joeynmt.training - Epoch   2, Step:    11500, Batch Loss:     1.514483, Batch Acc: 0.583756, Tokens per Sec:    20604, Lr: 0.000300
2025-05-28 09:25:36,938 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:25:36,939 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:25:51,840 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.58, ppl:   4.83, acc:   0.58, generation: 14.8892[sec], evaluation: 0.0000[sec]
2025-05-28 09:25:51,841 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:25:52,460 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/9000.ckpt
2025-05-28 09:25:52,483 - INFO - joeynmt.training - Example #0
2025-05-28 09:25:52,484 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:25:52,484 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:25:52,484 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'questa', 'foto', 'per', 'vedere', 'che', 'la', 'gente', 'si', 'è', 'ri@@', '<unk>', '@', 'ma@@', '<unk>', '@', 'i@@', '<unk>', '@', 'fic@@', '<unk>', '@', 'ato', 'che', 'la', 'c@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ione', 'di', 'E@@', '<unk>', '@', 'g@@', '<unk>', '@', 'amb@@', '<unk>', '@', 'a', ',', 'che', 'i', 'tre', 'anni', ',', 'per', 'i', 'tre', 'anni', ',', 'per', 'i', 'tre', 'anni', ',', 'che', 'ha', 'fatto', ',', 'per', 'i', 'tre', 'anni', ',', 'in', 'realtà', ',', 'è', 'stato', 'stato', 'stato', 'stato', 'un', 'altro', 'per', 'cento', 'di', 'circa', '40', '%', 'dei', 'tre', '.', '</s>']
2025-05-28 09:25:52,485 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:25:52,485 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:25:52,485 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato questa foto per vedere che la gente si è ri<unk> @ ma<unk> @ i<unk> @ fic<unk> @ ato che la c<unk> @ is<unk> @ ione di E<unk> @ g<unk> @ amb<unk> @ a , che i tre anni , per i tre anni , per i tre anni , che ha fatto , per i tre anni , in realtà , è stato stato stato stato un altro per cento di circa 40 % dei tre .
2025-05-28 09:25:52,485 - INFO - joeynmt.training - Example #1
2025-05-28 09:25:52,485 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:25:52,485 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:25:52,485 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'facile', ',', 'che', 'la', 'gente', 'non', 'è', 'abbastanza', 'facile', ',', 'che', 'la', 'cos@@', '<unk>', '@', 'ci@@', '<unk>', '@', 'enza', 'di', 'questo', 'problema', ',', 'non', 'è', 'un', 'problema', 'di', 'questo', 'problema', ',', 'che', 'non', 'è', 'il', 'problema', 'di', 'D@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'is@@', '<unk>', '@', 'o', '.', '</s>']
2025-05-28 09:25:52,486 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:25:52,486 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:25:52,486 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza facile , che la gente non è abbastanza facile , che la cos<unk> @ ci<unk> @ enza di questo problema , non è un problema di questo problema , che non è il problema di D<unk> @ ic<unk> @ is<unk> @ o .
2025-05-28 09:25:52,486 - INFO - joeynmt.training - Example #2
2025-05-28 09:25:52,486 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:25:52,486 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:25:52,486 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'realtà', ',', 'il', 'senso', 'di', 'E@@', '<unk>', '@', 'h@@', '<unk>', '@', 'an', 'è', 'la', 'realtà', 'di', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'e', '&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'e', 'il', 'nostro', 'sistema', 'di', 'c@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'l@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ale', '.', '</s>']
2025-05-28 09:25:52,487 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:25:52,487 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:25:52,487 - INFO - joeynmt.training - 	Hypothesis: In realtà , il senso di E<unk> @ h<unk> @ an è la realtà di E<unk> @ is<unk> @ e &apos; E<unk> @ is<unk> @ e il nostro sistema di c<unk> @ ic<unk> @ l<unk> @ ic<unk> @ ale .
2025-05-28 09:25:52,487 - INFO - joeynmt.training - Example #3
2025-05-28 09:25:52,487 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:25:52,487 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:25:52,487 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'ei', ',', 'in', 'realtà', ',', 'in', 'un', 'c@@', '<unk>', '@', 'em@@', '<unk>', '@', 'po', ',', 'e', 'poi', 'in', 'un', 's@@', '<unk>', '@', 'ett@@', '<unk>', '@', 'em@@', '<unk>', '@', 'io', '.', '</s>']
2025-05-28 09:25:52,488 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:25:52,488 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:25:52,488 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ ei , in realtà , in un c<unk> @ em<unk> @ po , e poi in un s<unk> @ ett<unk> @ em<unk> @ io .
2025-05-28 09:25:52,488 - INFO - joeynmt.training - Example #4
2025-05-28 09:25:52,488 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:25:52,488 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:25:52,488 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'prossi@@', '<unk>', '@', 'ma', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'una', 'serie', 'di', 'due', 'anni', '.', '</s>']
2025-05-28 09:25:52,488 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:25:52,488 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:25:52,488 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma prossi<unk> @ ma che vi mostr<unk> @ erò è una serie di due anni .
2025-05-28 09:25:56,083 - INFO - joeynmt.training - Epoch   2, Step:    11600, Batch Loss:     1.539679, Batch Acc: 0.582298, Tokens per Sec:    16738, Lr: 0.000300
2025-05-28 09:25:59,656 - INFO - joeynmt.training - Epoch   2, Step:    11700, Batch Loss:     1.615276, Batch Acc: 0.584803, Tokens per Sec:    20107, Lr: 0.000300
2025-05-28 09:26:03,206 - INFO - joeynmt.training - Epoch   2, Step:    11800, Batch Loss:     1.654080, Batch Acc: 0.583899, Tokens per Sec:    20753, Lr: 0.000300
2025-05-28 09:26:06,740 - INFO - joeynmt.training - Epoch   2, Step:    11900, Batch Loss:     1.430415, Batch Acc: 0.583710, Tokens per Sec:    20233, Lr: 0.000300
2025-05-28 09:26:10,261 - INFO - joeynmt.training - Epoch   2, Step:    12000, Batch Loss:     1.486101, Batch Acc: 0.584927, Tokens per Sec:    20318, Lr: 0.000300
2025-05-28 09:26:10,262 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:26:10,262 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:26:20,222 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.55, ppl:   4.73, acc:   0.59, generation: 9.9522[sec], evaluation: 0.0000[sec]
2025-05-28 09:26:20,224 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:26:20,774 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/9500.ckpt
2025-05-28 09:26:20,801 - INFO - joeynmt.training - Example #0
2025-05-28 09:26:20,802 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:26:20,802 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:26:20,802 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'che', 'mostr@@', '<unk>', '@', 'o', 'che', 'mostr@@', '<unk>', '@', 'o', 'che', 'i', 'giovani', 'per', 'creare', 'un', 'paio', 'di', 'anni', 'per', 'la', 'gente', 'che', 'i', 'pol@@', '<unk>', '@', 'izi@@', '<unk>', '@', 'otti', 'per', 'la', 'c@@', '<unk>', '@', 'lasse', 'di', 'tre', 'anni', ',', 'per', 'i', 'tre', 'tre', 'anni', ',', 'per', 'i', 'tre', 'anni', ',', 'per', 'i', 'primi', 'anni', ',', 'per', 'tre', 'anni', ',', 'per', 'tre', 'anni', ',', 'per', 'i', '40', '%', ',', 'per', 'i', '40', '%', '.', '</s>']
2025-05-28 09:26:20,803 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:26:20,803 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:26:20,803 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato che mostr<unk> @ o che mostr<unk> @ o che i giovani per creare un paio di anni per la gente che i pol<unk> @ izi<unk> @ otti per la c<unk> @ lasse di tre anni , per i tre tre anni , per i tre anni , per i primi anni , per tre anni , per tre anni , per i 40 % , per i 40 % .
2025-05-28 09:26:20,803 - INFO - joeynmt.training - Example #1
2025-05-28 09:26:20,803 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:26:20,803 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:26:20,803 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', ',', 'non', 'è', 'abbastanza', ',', 'che', 'ha', 'un', 'problema', 'di', 'con@@', '<unk>', '@', 'sap@@', '<unk>', '@', 'evol@@', '<unk>', '@', 'ezza', 'di', 'questo', 'problema', ',', 'non', 'è', 'che', 'si', 'vede', 'il', 'fatto', 'di', 'un', 'problema', 'di', 'queste', 'cose', 'che', 'non', 'è', 'il', 'D@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'is@@', '<unk>', '@', 'o', '.', '</s>']
2025-05-28 09:26:20,804 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:26:20,804 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:26:20,804 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza , non è abbastanza , che ha un problema di con<unk> @ sap<unk> @ evol<unk> @ ezza di questo problema , non è che si vede il fatto di un problema di queste cose che non è il D<unk> @ ic<unk> @ is<unk> @ o .
2025-05-28 09:26:20,804 - INFO - joeynmt.training - Example #2
2025-05-28 09:26:20,804 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:26:20,804 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:26:20,804 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'effetti', ',', 'il', 'punto', 'di', 'vista', 'è', 'la', 'co@@', '<unk>', '@', 'per@@', '<unk>', '@', 'dita', 'di', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'as@@', '<unk>', '@', 'ci@@', '<unk>', '@', 'ma', '.', '</s>']
2025-05-28 09:26:20,805 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:26:20,805 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:26:20,805 - INFO - joeynmt.training - 	Hypothesis: In effetti , il punto di vista è la co<unk> @ per<unk> @ dita di E<unk> @ is<unk> @ k<unk> @ as<unk> @ ci<unk> @ ma .
2025-05-28 09:26:20,805 - INFO - joeynmt.training - Example #3
2025-05-28 09:26:20,805 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:26:20,805 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:26:20,805 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'ei', 'si', 'ri@@', '<unk>', '@', 'ma@@', '<unk>', '@', 'sto', 'e', 's@@', '<unk>', '@', 'ov@@', '<unk>', '@', 'o', '.', '</s>']
2025-05-28 09:26:20,805 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:26:20,805 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:26:20,805 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ ei si ri<unk> @ ma<unk> @ sto e s<unk> @ ov<unk> @ o .
2025-05-28 09:26:20,805 - INFO - joeynmt.training - Example #4
2025-05-28 09:26:20,806 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:26:20,806 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:26:20,806 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'prossi@@', '<unk>', '@', 'ma', 'che', 'vi', 'mostr@@', '<unk>', '@', 'o', 'è', 'una', 'cosa', 'che', 'sta', 'succe@@', '<unk>', '@', 'dendo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:26:20,806 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:26:20,806 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:26:20,806 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma prossi<unk> @ ma che vi mostr<unk> @ o è una cosa che sta succe<unk> @ dendo negli ultimi 25 anni .
2025-05-28 09:26:24,156 - INFO - joeynmt.training - Epoch   2, Step:    12100, Batch Loss:     1.544697, Batch Acc: 0.591808, Tokens per Sec:    18348, Lr: 0.000300
2025-05-28 09:26:27,424 - INFO - joeynmt.training - Epoch   2, Step:    12200, Batch Loss:     1.382614, Batch Acc: 0.590307, Tokens per Sec:    21750, Lr: 0.000300
2025-05-28 09:26:30,731 - INFO - joeynmt.training - Epoch   2, Step:    12300, Batch Loss:     1.668389, Batch Acc: 0.587848, Tokens per Sec:    22026, Lr: 0.000300
2025-05-28 09:26:34,132 - INFO - joeynmt.training - Epoch   2, Step:    12400, Batch Loss:     1.640299, Batch Acc: 0.593148, Tokens per Sec:    20604, Lr: 0.000300
2025-05-28 09:26:37,492 - INFO - joeynmt.training - Epoch   2, Step:    12500, Batch Loss:     1.532765, Batch Acc: 0.588258, Tokens per Sec:    22114, Lr: 0.000300
2025-05-28 09:26:37,493 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:26:37,493 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:26:49,471 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.54, ppl:   4.66, acc:   0.59, generation: 11.9706[sec], evaluation: 0.0000[sec]
2025-05-28 09:26:49,472 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:26:50,005 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/10000.ckpt
2025-05-28 09:26:50,031 - INFO - joeynmt.training - Example #0
2025-05-28 09:26:50,032 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:26:50,032 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:26:50,032 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'due', 'due', 'due', 'due', 'due', 'due', 'volte', 'per', 'essere', 'ven@@', '<unk>', '@', 'di@@', '<unk>', '@', 't@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ici', 'che', 'i', 'c@@', '<unk>', '@', 'ani', 'è', 'che', 'i', 'c@@', '<unk>', '@', 'att@@', '<unk>', '@', 'ac@@', '<unk>', '@', 'chi', 'di', 'tre', 'milioni', 'di', 'anni', ',', 'il', 't@@', '<unk>', '@', 'asso', 'di', 't@@', '<unk>', '@', 'ur@@', '<unk>', '@', 'no', ',', 'per', 'cento', 'di', 'tre', 'milioni', 'di', 'anni', ',', 'il', 't@@', '<unk>', '@', 'essu@@', '<unk>', '@', 'to', '.', '</s>']
2025-05-28 09:26:50,033 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:26:50,033 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:26:50,033 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno ho mostr<unk> @ ato queste due due due due due due due volte per essere ven<unk> @ di<unk> @ t<unk> @ ant<unk> @ ici che i c<unk> @ ani è che i c<unk> @ att<unk> @ ac<unk> @ chi di tre milioni di anni , il t<unk> @ asso di t<unk> @ ur<unk> @ no , per cento di tre milioni di anni , il t<unk> @ essu<unk> @ to .
2025-05-28 09:26:50,033 - INFO - joeynmt.training - Example #1
2025-05-28 09:26:50,033 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:26:50,033 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:26:50,033 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', ',', 'la', 'cosa', 'che', 'non', 'è', 'abbastanza', ',', 'che', 'non', 'è', 'abbastanza', 'compl@@', '<unk>', '@', 'essa', ',', 'non', 'è', 'un', 'problema', 'di', 'una', 'realtà', 'di', 'questo', 'problema', ',', 'non', 'è', 'il', 'che', 'non', 'è', 'il', 'D@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 09:26:50,033 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:26:50,034 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:26:50,034 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza , la cosa che non è abbastanza , che non è abbastanza compl<unk> @ essa , non è un problema di una realtà di questo problema , non è il che non è il D<unk> @ ic<unk> @ is<unk> @ es .
2025-05-28 09:26:50,034 - INFO - joeynmt.training - Example #2
2025-05-28 09:26:50,034 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:26:50,034 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:26:50,034 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'realtà', ',', 'il', 'senso', 'di', 'cosa', 'è', 'il', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ico', 'di', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'ing', ',', 'che', 'è', 'il', 'nostro', 'sistema', 'globale', '.', '</s>']
2025-05-28 09:26:50,034 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:26:50,034 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:26:50,034 - INFO - joeynmt.training - 	Hypothesis: In realtà , il senso di cosa è il c<unk> @ ant<unk> @ ico di E<unk> @ is<unk> @ k<unk> @ ing , che è il nostro sistema globale .
2025-05-28 09:26:50,035 - INFO - joeynmt.training - Example #3
2025-05-28 09:26:50,035 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:26:50,035 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:26:50,035 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'en@@', '<unk>', '@', 'dono', 'in', 'una', 'c@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'l@@', '<unk>', '@', 'etta', 'e', 'poi', 'poi', 'si', 'ri@@', '<unk>', '@', 'un@@', '<unk>', '@', 'isce', '.', '</s>']
2025-05-28 09:26:50,035 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:26:50,035 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:26:50,035 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ en<unk> @ dono in una c<unk> @ ic<unk> @ l<unk> @ etta e poi poi si ri<unk> @ un<unk> @ isce .
2025-05-28 09:26:50,035 - INFO - joeynmt.training - Example #4
2025-05-28 09:26:50,035 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:26:50,035 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:26:50,035 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'prossi@@', '<unk>', '@', 'ma', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'un', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'ret@@', '<unk>', '@', 'o', 'in', 'quello', 'che', 'succede', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:26:50,036 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:26:50,036 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:26:50,036 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma prossi<unk> @ ma che vi mostr<unk> @ erò un di<unk> @ seg<unk> @ ret<unk> @ o in quello che succede negli ultimi 25 anni .
2025-05-28 09:26:51,289 - INFO - joeynmt.training - Epoch   2: total training loss 10457.22
2025-05-28 09:26:51,289 - INFO - joeynmt.training - EPOCH 3
2025-05-28 09:26:51,779 - INFO - joeynmt.training - Epoch   3, Step:    12600, Batch Loss:     1.541603, Batch Acc: 0.603067, Tokens per Sec:    41143, Lr: 0.000300
2025-05-28 09:26:53,514 - INFO - joeynmt.training - Epoch   3, Step:    12700, Batch Loss:     1.458333, Batch Acc: 0.600899, Tokens per Sec:    42095, Lr: 0.000300
2025-05-28 09:26:55,209 - INFO - joeynmt.training - Epoch   3, Step:    12800, Batch Loss:     1.436939, Batch Acc: 0.605789, Tokens per Sec:    42235, Lr: 0.000300
2025-05-28 09:26:56,853 - INFO - joeynmt.training - Epoch   3, Step:    12900, Batch Loss:     1.448123, Batch Acc: 0.599425, Tokens per Sec:    42973, Lr: 0.000300
2025-05-28 09:26:58,558 - INFO - joeynmt.training - Epoch   3, Step:    13000, Batch Loss:     1.651098, Batch Acc: 0.603161, Tokens per Sec:    41597, Lr: 0.000300
2025-05-28 09:26:58,558 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:26:58,558 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:27:06,847 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.52, ppl:   4.59, acc:   0.60, generation: 8.2813[sec], evaluation: 0.0000[sec]
2025-05-28 09:27:06,848 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:27:07,341 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/10500.ckpt
2025-05-28 09:27:07,361 - INFO - joeynmt.training - Example #0
2025-05-28 09:27:07,361 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:27:07,361 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:27:07,361 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'questo', 'due', 'due', 'due', 'anni', 'per', 'creare', 'che', 'la', 'maggior', 'parte', 'del', 'genere', 'di', 'cose', 'che', 'si', 'chiama', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ale', 'che', 'i', 'c@@', '<unk>', '@', 'att@@', '<unk>', '@', 'ac@@', '<unk>', '@', 'i', ',', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'il', 'S@@', '<unk>', '@', 'olo', 'di', 'tre', 'anni', ',', 'il', 'S@@', '<unk>', '@', 'olo', 'di', 'tre', 'milioni', 'di', 'anni', ',', 'il', 'S@@', '<unk>', '@', 'olo', 'per@@', '<unk>', '@', 'cento', '.', '</s>']
2025-05-28 09:27:07,362 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:27:07,362 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:27:07,362 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato questo due due due anni per creare che la maggior parte del genere di cose che si chiama E<unk> @ is<unk> @ k<unk> @ i<unk> @ ale che i c<unk> @ att<unk> @ ac<unk> @ i , per tre milioni di anni , il S<unk> @ olo di tre anni , il S<unk> @ olo di tre milioni di anni , il S<unk> @ olo per<unk> @ cento .
2025-05-28 09:27:07,362 - INFO - joeynmt.training - Example #1
2025-05-28 09:27:07,362 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:27:07,362 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:27:07,362 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'abbastanza', ',', 'la', 'nostra', 'in@@', '<unk>', '@', 'fan@@', '<unk>', '@', 'zia', ',', 'la', 'cre@@', '<unk>', '@', 'azione', 'della', 'nostra', 'conosc@@', '<unk>', '@', 'enza', ',', 'non', 'è', 'la', 'c@@', '<unk>', '@', 'lasse', 'di', 'una', 'cosa', 'che', 'non', 'è', 'la', 'c@@', '<unk>', '@', 'lasse', 'di', 'D@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 09:27:07,362 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:27:07,363 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:27:07,363 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza abbastanza , la nostra in<unk> @ fan<unk> @ zia , la cre<unk> @ azione della nostra conosc<unk> @ enza , non è la c<unk> @ lasse di una cosa che non è la c<unk> @ lasse di D<unk> @ ic<unk> @ is<unk> @ es .
2025-05-28 09:27:07,363 - INFO - joeynmt.training - Example #2
2025-05-28 09:27:07,363 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:27:07,363 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:27:07,363 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'realtà', ',', 'il', 'punto', 'è', 'la', 'realtà', 'è', 'la', 'c@@', '<unk>', '@', 'att@@', '<unk>', '@', 'uale', 'di', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ale', ',', 'il', 'nostro', 'sistema', 'globale', 'globale', 'globale', '.', '</s>']
2025-05-28 09:27:07,363 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:27:07,363 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:27:07,363 - INFO - joeynmt.training - 	Hypothesis: In realtà , il punto è la realtà è la c<unk> @ att<unk> @ uale di E<unk> @ is<unk> @ k<unk> @ i<unk> @ ale , il nostro sistema globale globale globale .
2025-05-28 09:27:07,364 - INFO - joeynmt.training - Example #3
2025-05-28 09:27:07,364 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:27:07,364 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:27:07,364 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'en@@', '<unk>', '@', 'gono', ',', 'in', 'un', 'vento', 'e', 'poi', 'si', 'è', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'and@@', '<unk>', '@', 'ata', '.', '</s>']
2025-05-28 09:27:07,364 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:27:07,364 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:27:07,364 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ en<unk> @ gono , in un vento e poi si è ri<unk> @ m<unk> @ and<unk> @ ata .
2025-05-28 09:27:07,364 - INFO - joeynmt.training - Example #4
2025-05-28 09:27:07,364 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:27:07,365 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:27:07,365 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'un', 'di@@', '<unk>', '@', 'stri@@', '<unk>', '@', 'bu@@', '<unk>', '@', 'io', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'quello', 'che', 'succede', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:27:07,365 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:27:07,365 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:27:07,365 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma che vi mostr<unk> @ erò è un di<unk> @ stri<unk> @ bu<unk> @ io che vi mostr<unk> @ erò quello che succede negli ultimi 25 anni .
2025-05-28 09:27:09,107 - INFO - joeynmt.training - Epoch   3, Step:    13100, Batch Loss:     1.449666, Batch Acc: 0.602566, Tokens per Sec:    31560, Lr: 0.000300
2025-05-28 09:27:10,785 - INFO - joeynmt.training - Epoch   3, Step:    13200, Batch Loss:     1.410261, Batch Acc: 0.600414, Tokens per Sec:    42029, Lr: 0.000300
2025-05-28 09:27:12,453 - INFO - joeynmt.training - Epoch   3, Step:    13300, Batch Loss:     1.455682, Batch Acc: 0.599750, Tokens per Sec:    43143, Lr: 0.000300
2025-05-28 09:27:14,142 - INFO - joeynmt.training - Epoch   3, Step:    13400, Batch Loss:     1.428044, Batch Acc: 0.607377, Tokens per Sec:    41972, Lr: 0.000300
2025-05-28 09:27:15,871 - INFO - joeynmt.training - Epoch   3, Step:    13500, Batch Loss:     1.372822, Batch Acc: 0.604591, Tokens per Sec:    41293, Lr: 0.000300
2025-05-28 09:27:15,871 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:27:15,871 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:27:25,817 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.51, ppl:   4.55, acc:   0.60, generation: 9.9404[sec], evaluation: 0.0000[sec]
2025-05-28 09:27:25,818 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:27:26,331 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/11000.ckpt
2025-05-28 09:27:26,357 - INFO - joeynmt.training - Example #0
2025-05-28 09:27:26,357 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:27:26,357 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:27:26,357 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'questa', 'immagine', 'di', 'un', 'punto', 'di', 'vista', 'per', 'far@@', '<unk>', '@', 'la', 'che', 'la', 'maggior', 'parte', 'di', 'un', 'po', '&apos;', 'di', 'più', 'di', 'un', 'po', '&apos;', 'di', 'più', 't@@', '<unk>', '@', 'avolo', ',', 'il', 'che', 'i', 'p@@', '<unk>', '@', 'ezzi', 'di', 'tre', 'milioni', 'di', 'anni', ',', 'il', 'che', 'è', 'stato', 'stato', 'stato', 'fatto', 'per', 'la', 'con@@', '<unk>', '@', 'qu@@', '<unk>', '@', 'ist@@', '<unk>', '@', 'itu@@', '<unk>', '@', 'd@@', '<unk>', '@', 'ine', 'di', 'tre', 'milioni', 'di', 'anni', ',', 'il', '30', '%', 'di', 'più', 'di', 'un', 'paese', '.', '</s>']
2025-05-28 09:27:26,358 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:27:26,358 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:27:26,358 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato questa immagine di un punto di vista per far<unk> @ la che la maggior parte di un po &apos; di più di un po &apos; di più t<unk> @ avolo , il che i p<unk> @ ezzi di tre milioni di anni , il che è stato stato stato fatto per la con<unk> @ qu<unk> @ ist<unk> @ itu<unk> @ d<unk> @ ine di tre milioni di anni , il 30 % di più di un paese .
2025-05-28 09:27:26,358 - INFO - joeynmt.training - Example #1
2025-05-28 09:27:26,358 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:27:26,358 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:27:26,358 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', ',', 'non', 'è', 'abbastanza', ',', 'per', 'la', 'prima', 'cosa', 'di', 'cui', 'la', 'cosa', 'più', 'profon@@', '<unk>', '@', 'di@@', '<unk>', '@', 'tà', 'di', 'questo', 'problema', ',', 'non', 'è', 'l&apos;', 'l&apos;', 'eff@@', '<unk>', '@', 'etto', 'di', 'questo', 'problema', '.', '</s>']
2025-05-28 09:27:26,359 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:27:26,359 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:27:26,359 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza , non è abbastanza , per la prima cosa di cui la cosa più profon<unk> @ di<unk> @ tà di questo problema , non è l&apos; l&apos; eff<unk> @ etto di questo problema .
2025-05-28 09:27:26,359 - INFO - joeynmt.training - Example #2
2025-05-28 09:27:26,359 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:27:26,359 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:27:26,359 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'effetti', ',', 'il', 'punto', 'di', 'vista', 'è', 'la', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'ca', 'è', 'la', 'rete', 'di', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ale', '.', '</s>']
2025-05-28 09:27:26,360 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:27:26,360 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:27:26,360 - INFO - joeynmt.training - 	Hypothesis: In effetti , il punto di vista è la E<unk> @ is<unk> @ ca è la rete di E<unk> @ is<unk> @ k<unk> @ i<unk> @ ale .
2025-05-28 09:27:26,360 - INFO - joeynmt.training - Example #3
2025-05-28 09:27:26,360 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:27:26,360 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:27:26,360 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&apos;', 'il', 'vento', 'di', 'cui', 'si', 'è', 'ri@@', '<unk>', '@', 'ma@@', '<unk>', '@', 'sto', 'e', 'di', 's@@', '<unk>', '@', 'om@@', '<unk>', '@', 'ent@@', '<unk>', '@', 'emente', '.', '</s>']
2025-05-28 09:27:26,360 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:27:26,360 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:27:26,361 - INFO - joeynmt.training - 	Hypothesis: E &apos; il vento di cui si è ri<unk> @ ma<unk> @ sto e di s<unk> @ om<unk> @ ent<unk> @ emente .
2025-05-28 09:27:26,361 - INFO - joeynmt.training - Example #4
2025-05-28 09:27:26,361 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:27:26,361 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:27:26,361 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'è', 'la', 'prossi@@', '<unk>', '@', 'ma', 'è', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'una', 'cosa', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:27:26,361 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:27:26,361 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:27:26,361 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma è la prossi<unk> @ ma è che vi mostr<unk> @ erò è una cosa che è successo negli ultimi 25 anni .
2025-05-28 09:27:28,103 - INFO - joeynmt.training - Epoch   3, Step:    13600, Batch Loss:     1.397892, Batch Acc: 0.603536, Tokens per Sec:    31239, Lr: 0.000300
2025-05-28 09:27:29,807 - INFO - joeynmt.training - Epoch   3, Step:    13700, Batch Loss:     1.357680, Batch Acc: 0.604565, Tokens per Sec:    41748, Lr: 0.000300
2025-05-28 09:27:31,483 - INFO - joeynmt.training - Epoch   3, Step:    13800, Batch Loss:     1.541626, Batch Acc: 0.604238, Tokens per Sec:    42510, Lr: 0.000300
2025-05-28 09:27:33,204 - INFO - joeynmt.training - Epoch   3, Step:    13900, Batch Loss:     1.487208, Batch Acc: 0.604762, Tokens per Sec:    42467, Lr: 0.000300
2025-05-28 09:27:34,985 - INFO - joeynmt.training - Epoch   3, Step:    14000, Batch Loss:     1.495573, Batch Acc: 0.604830, Tokens per Sec:    41050, Lr: 0.000300
2025-05-28 09:27:34,985 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:27:34,985 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:27:45,979 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.50, ppl:   4.49, acc:   0.60, generation: 10.9875[sec], evaluation: 0.0000[sec]
2025-05-28 09:27:45,980 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:27:46,550 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/11500.ckpt
2025-05-28 09:27:46,574 - INFO - joeynmt.training - Example #0
2025-05-28 09:27:46,574 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:27:46,574 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:27:46,574 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'questa', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'per', 'poter', 'vedere', 'l&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'h@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'h@@', '<unk>', '@', 'ev@@', '<unk>', '@', 'a', 'che', 'il', 'risultato', 'è', 'stato', 'per', 'gli', 'ucc@@', '<unk>', '@', 'is@@', '<unk>', '@', 'o', ',', 'per', 'il', '40', 'milioni', 'di', 'anni', ',', 'e', 'il', 'più', 'stato', 'in', 'grado', 'di', 'salv@@', '<unk>', '@', 'are', 'il', '40', 'per', 'cento', 'di', 'molti', 'anni', ',', 'è', 'stato', 'stato', 'stato', 'un', '3@@', '<unk>', '@', '4@@', '<unk>', '@', '8', ',', 'il', '40', '%', '.', '</s>']
2025-05-28 09:27:46,575 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:27:46,575 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:27:46,575 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato questa di<unk> @ apos<unk> @ iti<unk> @ va per poter vedere l&apos; E<unk> @ is<unk> @ h<unk> @ is<unk> @ k<unk> @ h<unk> @ ev<unk> @ a che il risultato è stato per gli ucc<unk> @ is<unk> @ o , per il 40 milioni di anni , e il più stato in grado di salv<unk> @ are il 40 per cento di molti anni , è stato stato stato un 3<unk> @ 4<unk> @ 8 , il 40 % .
2025-05-28 09:27:46,575 - INFO - joeynmt.training - Example #1
2025-05-28 09:27:46,576 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:27:46,576 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:27:46,576 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'facile', 'da', 'fare', 'una', 'sor@@', '<unk>', '@', 'ta', 'di', 'cre@@', '<unk>', '@', 'azione', 'di', 'questa', 'tecnologia', ',', 'non', 'è', 'abbastanza', 'speci@@', '<unk>', '@', 'fico', 'che', 'non', 'è', 'il', 'fatto', 'che', 'l&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 09:27:46,576 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:27:46,576 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:27:46,576 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza facile da fare una sor<unk> @ ta di cre<unk> @ azione di questa tecnologia , non è abbastanza speci<unk> @ fico che non è il fatto che l&apos; E<unk> @ is<unk> @ is<unk> @ es .
2025-05-28 09:27:46,576 - INFO - joeynmt.training - Example #2
2025-05-28 09:27:46,576 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:27:46,577 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:27:46,577 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'realtà', ',', 'in', 'realtà', ',', 'il', 'punto', 'è', 'la', 'nostra', 'al@@', '<unk>', '@', 'te@@', '<unk>', '@', 'zza', 'dell&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'sia', 'il', 'nostro', 'sistema', 'globale', 'globale', 'globale', '.', '</s>']
2025-05-28 09:27:46,577 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:27:46,577 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:27:46,577 - INFO - joeynmt.training - 	Hypothesis: In realtà , in realtà , il punto è la nostra al<unk> @ te<unk> @ zza dell&apos; E<unk> @ is<unk> @ k<unk> @ sia il nostro sistema globale globale globale .
2025-05-28 09:27:46,577 - INFO - joeynmt.training - Example #3
2025-05-28 09:27:46,577 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:27:46,577 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:27:46,577 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&apos;', 'arriv@@', '<unk>', '@', 'ato', 'al', 'vento', 'e', 'poi', 'si', 'è', 'ri@@', '<unk>', '@', 'ma@@', '<unk>', '@', 'sta', '.', '</s>']
2025-05-28 09:27:46,578 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:27:46,578 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:27:46,578 - INFO - joeynmt.training - 	Hypothesis: E &apos; arriv<unk> @ ato al vento e poi si è ri<unk> @ ma<unk> @ sta .
2025-05-28 09:27:46,578 - INFO - joeynmt.training - Example #4
2025-05-28 09:27:46,578 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:27:46,578 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:27:46,578 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'questa', 'è', 'la', 'prossi@@', '<unk>', '@', 'ma', 'è', 'una', 'cosa', 'che', 'sta', 'acca@@', '<unk>', '@', 'dendo', 'negli', 'ultimi', '25', 'anni', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', 'è', 'successo', '.', '</s>']
2025-05-28 09:27:46,578 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:27:46,578 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:27:46,579 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma questa è la prossi<unk> @ ma è una cosa che sta acca<unk> @ dendo negli ultimi 25 anni è successo negli ultimi 25 anni è successo .
2025-05-28 09:27:48,275 - INFO - joeynmt.training - Epoch   3, Step:    14100, Batch Loss:     1.371133, Batch Acc: 0.608246, Tokens per Sec:    30553, Lr: 0.000300
2025-05-28 09:27:49,987 - INFO - joeynmt.training - Epoch   3, Step:    14200, Batch Loss:     1.514719, Batch Acc: 0.604204, Tokens per Sec:    43177, Lr: 0.000300
2025-05-28 09:27:51,663 - INFO - joeynmt.training - Epoch   3, Step:    14300, Batch Loss:     1.522360, Batch Acc: 0.610198, Tokens per Sec:    43210, Lr: 0.000300
2025-05-28 09:27:53,386 - INFO - joeynmt.training - Epoch   3, Step:    14400, Batch Loss:     1.456288, Batch Acc: 0.608158, Tokens per Sec:    40383, Lr: 0.000300
2025-05-28 09:27:55,040 - INFO - joeynmt.training - Epoch   3, Step:    14500, Batch Loss:     1.529043, Batch Acc: 0.607247, Tokens per Sec:    42210, Lr: 0.000300
2025-05-28 09:27:55,040 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:27:55,040 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:28:04,322 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.49, ppl:   4.44, acc:   0.60, generation: 9.2759[sec], evaluation: 0.0000[sec]
2025-05-28 09:28:04,323 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:28:05,009 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/12000.ckpt
2025-05-28 09:28:05,035 - INFO - joeynmt.training - Example #0
2025-05-28 09:28:05,036 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:28:05,036 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:28:05,036 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'questa', 'due', 'due', 'di@@', '<unk>', '@', 'mostra', 'che', 'i', 'd@@', '<unk>', '@', 'ott@@', '<unk>', '@', 'ori', ',', 'che', 'le', 'cose', 'si', 'è', 'ri@@', '<unk>', '@', 'vel@@', '<unk>', '@', 'ate', 'per', 'la', 'c@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'l@@', '<unk>', '@', 'ità', ',', 'per', 'la', 'prima', 'volta', ',', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'il', '40', '%', ',', 'il', '40', '%', ',', 'che', 'ha', 'fatto', ',', '40', '%', 'dei', '4@@', '<unk>', '@', '8', '%', ',', '40', '%', ',', '40', '%', 'dei', '40', '%', ',', 'il', '40', '%', 'dei', '4@@', '%', ',', '40', '%', ',', 'il', '40', '%', '.', '</s>']
2025-05-28 09:28:05,037 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:28:05,037 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:28:05,037 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato questa due due di<unk> @ mostra che i d<unk> @ ott<unk> @ ori , che le cose si è ri<unk> @ vel<unk> @ ate per la c<unk> @ ic<unk> @ l<unk> @ ità , per la prima volta , per tre milioni di anni , il 40 % , il 40 % , che ha fatto , 40 % dei 4<unk> @ 8 % , 40 % , 40 % dei 40 % , il 40 % dei 4% , 40 % , il 40 % .
2025-05-28 09:28:05,037 - INFO - joeynmt.training - Example #1
2025-05-28 09:28:05,037 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:28:05,037 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:28:05,037 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'facile', ',', 'non', 'è', 'abbastanza', 'abbastanza', 'per', 'la', 'con@@', '<unk>', '@', 'sap@@', '<unk>', '@', 'evol@@', '<unk>', '@', 'ezza', 'che', 'questo', 'problema', 'è', 'che', 'non', 'è', 'una', 'delle', 'delle', 'cose', 'che', 'non', 'è', 'il', 'D@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ke', '.', '</s>']
2025-05-28 09:28:05,038 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:28:05,038 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:28:05,038 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza facile , non è abbastanza abbastanza per la con<unk> @ sap<unk> @ evol<unk> @ ezza che questo problema è che non è una delle delle cose che non è il D<unk> @ ic<unk> @ ke .
2025-05-28 09:28:05,038 - INFO - joeynmt.training - Example #2
2025-05-28 09:28:05,038 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:28:05,038 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:28:05,038 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'certo', 'senso', ',', 'è', 'la', 'cosa', 'più', 'b@@', '<unk>', '@', 'ella', 'di', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'it', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'it', ',', 'il', 'nostro', 'sistema', 'globale', 'globale', 'globale', 'globale', 'globale', 'globale', 'globale', 'globale', 'globale', 'globale', 'globale', '.', '</s>']
2025-05-28 09:28:05,038 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:28:05,039 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:28:05,039 - INFO - joeynmt.training - 	Hypothesis: In certo senso , è la cosa più b<unk> @ ella di E<unk> @ is<unk> @ k<unk> @ it E<unk> @ is<unk> @ k<unk> @ it , il nostro sistema globale globale globale globale globale globale globale globale globale globale globale .
2025-05-28 09:28:05,039 - INFO - joeynmt.training - Example #3
2025-05-28 09:28:05,039 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:28:05,039 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:28:05,039 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&apos;', 'un', 'po', '&apos;', 'e', 'si', 'ri@@', '<unk>', '@', 'vel@@', '<unk>', '@', 'a', 'in', 'un', 'p@@', '<unk>', '@', 'om@@', '<unk>', '@', 'eno', '.', '</s>']
2025-05-28 09:28:05,039 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:28:05,040 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:28:05,040 - INFO - joeynmt.training - 	Hypothesis: E &apos; un po &apos; e si ri<unk> @ vel<unk> @ a in un p<unk> @ om<unk> @ eno .
2025-05-28 09:28:05,040 - INFO - joeynmt.training - Example #4
2025-05-28 09:28:05,040 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:28:05,040 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:28:05,040 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'prossi@@', '<unk>', '@', 'ma', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'una', 'cosa', 'succede', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:28:05,040 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:28:05,040 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:28:05,040 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma prossi<unk> @ ma che vi mostr<unk> @ erò una cosa succede negli ultimi 25 anni .
2025-05-28 09:28:08,397 - INFO - joeynmt.training - Epoch   3, Step:    14600, Batch Loss:     1.469101, Batch Acc: 0.611804, Tokens per Sec:    17521, Lr: 0.000300
2025-05-28 09:28:11,739 - INFO - joeynmt.training - Epoch   3, Step:    14700, Batch Loss:     1.467986, Batch Acc: 0.609697, Tokens per Sec:    21585, Lr: 0.000300
2025-05-28 09:28:15,056 - INFO - joeynmt.training - Epoch   3, Step:    14800, Batch Loss:     1.357844, Batch Acc: 0.610802, Tokens per Sec:    20957, Lr: 0.000300
2025-05-28 09:28:18,371 - INFO - joeynmt.training - Epoch   3, Step:    14900, Batch Loss:     1.433847, Batch Acc: 0.607071, Tokens per Sec:    21908, Lr: 0.000300
2025-05-28 09:28:21,709 - INFO - joeynmt.training - Epoch   3, Step:    15000, Batch Loss:     1.530165, Batch Acc: 0.608742, Tokens per Sec:    22226, Lr: 0.000300
2025-05-28 09:28:21,710 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:28:21,710 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:28:32,234 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.47, ppl:   4.35, acc:   0.61, generation: 10.5161[sec], evaluation: 0.0000[sec]
2025-05-28 09:28:32,234 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:28:32,822 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/12500.ckpt
2025-05-28 09:28:32,848 - INFO - joeynmt.training - Example #0
2025-05-28 09:28:32,849 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:28:32,849 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:28:32,849 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'questa', 'due', 'due', 'di@@', '<unk>', '@', 'mostra', 'che', 'i', 'p@@', '<unk>', '@', 'ezzi', 'di', 'ghi@@', '<unk>', '@', 'accio', ',', 'per', 'il', 'ghi@@', '<unk>', '@', 'accio', ',', 'che', 'i', 'p@@', '<unk>', '@', 'om@@', '<unk>', '@', 'ani', ',', 'per', 'il', 'p@@', '<unk>', '@', 'ane', ',', 'per', 'il', '30', '%', 'di', 'tre', 'anni', ',', 'il', '30', '%', 'di', 'tre', 'anni', ',', 'il', '40', '%', 'di', 'anni', ',', 'il', '40', '%', 'di', 'tre', 'anni', ',', 'per', 'il', '40', 'per', 'cento', '.', '</s>']
2025-05-28 09:28:32,850 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:28:32,850 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:28:32,850 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato questa due due di<unk> @ mostra che i p<unk> @ ezzi di ghi<unk> @ accio , per il ghi<unk> @ accio , che i p<unk> @ om<unk> @ ani , per il p<unk> @ ane , per il 30 % di tre anni , il 30 % di tre anni , il 40 % di anni , il 40 % di tre anni , per il 40 per cento .
2025-05-28 09:28:32,850 - INFO - joeynmt.training - Example #1
2025-05-28 09:28:32,850 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:28:32,850 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:28:32,850 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'facile', ',', 'non', 'è', 'abbastanza', 'abbastanza', 'difficile', 'da', 'fare', 'la', 'prima', 'cosa', 'di', 'questo', 'problema', 'della', 'nostra', 'mente', ',', 'non', 'è', 'abbastanza', 'da', 'risolvere', 'il', 'problema', 'dell&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 09:28:32,851 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:28:32,851 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:28:32,851 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza facile , non è abbastanza abbastanza difficile da fare la prima cosa di questo problema della nostra mente , non è abbastanza da risolvere il problema dell&apos; E<unk> @ is<unk> @ es .
2025-05-28 09:28:32,851 - INFO - joeynmt.training - Example #2
2025-05-28 09:28:32,851 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:28:32,851 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:28:32,851 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'qualche', 'certo', 'senso', ',', 'il', 'vero', 'vero', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'it', 'è', 'il', 'nostro', 'sistema', 'di', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'it', 'globale', '.', '</s>']
2025-05-28 09:28:32,852 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:28:32,852 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:28:32,852 - INFO - joeynmt.training - 	Hypothesis: In qualche certo senso , il vero vero E<unk> @ is<unk> @ k<unk> @ it è il nostro sistema di E<unk> @ is<unk> @ k<unk> @ it globale .
2025-05-28 09:28:32,852 - INFO - joeynmt.training - Example #3
2025-05-28 09:28:32,852 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:28:32,852 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:28:32,852 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Pri@@', '<unk>', '@', 'ma', 'di', 'tutto', ',', 'e', 'poi', ',', 'il', 'vento', '.', '</s>']
2025-05-28 09:28:32,853 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:28:32,853 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:28:32,853 - INFO - joeynmt.training - 	Hypothesis: Pri<unk> @ ma di tutto , e poi , il vento .
2025-05-28 09:28:32,853 - INFO - joeynmt.training - Example #4
2025-05-28 09:28:32,853 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:28:32,853 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:28:32,853 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'prossi@@', '<unk>', '@', 'mo', ',', 'vi', 'mostr@@', '<unk>', '@', 'o', 'un', 'seg@@', '<unk>', '@', 'ret@@', '<unk>', '@', 'o', ',', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:28:32,853 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:28:32,853 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:28:32,853 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma prossi<unk> @ mo , vi mostr<unk> @ o un seg<unk> @ ret<unk> @ o , che è successo negli ultimi 25 anni .
2025-05-28 09:28:36,183 - INFO - joeynmt.training - Epoch   3, Step:    15100, Batch Loss:     1.349750, Batch Acc: 0.608856, Tokens per Sec:    17529, Lr: 0.000300
2025-05-28 09:28:39,480 - INFO - joeynmt.training - Epoch   3, Step:    15200, Batch Loss:     1.400487, Batch Acc: 0.611386, Tokens per Sec:    21734, Lr: 0.000300
2025-05-28 09:28:42,802 - INFO - joeynmt.training - Epoch   3, Step:    15300, Batch Loss:     1.364597, Batch Acc: 0.609597, Tokens per Sec:    21672, Lr: 0.000300
2025-05-28 09:28:46,255 - INFO - joeynmt.training - Epoch   3, Step:    15400, Batch Loss:     1.352440, Batch Acc: 0.612365, Tokens per Sec:    21273, Lr: 0.000300
2025-05-28 09:28:49,586 - INFO - joeynmt.training - Epoch   3, Step:    15500, Batch Loss:     1.283570, Batch Acc: 0.614737, Tokens per Sec:    22274, Lr: 0.000300
2025-05-28 09:28:49,586 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:28:49,586 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:29:01,275 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.46, ppl:   4.31, acc:   0.61, generation: 11.6820[sec], evaluation: 0.0000[sec]
2025-05-28 09:29:01,276 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:29:01,852 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/13000.ckpt
2025-05-28 09:29:01,878 - INFO - joeynmt.training - Example #0
2025-05-28 09:29:01,879 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:29:01,879 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:29:01,879 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'questa', 'due', 'due', 'due', 'di@@', '<unk>', '@', 'mostra', 'che', 'i', 'c@@', '<unk>', '@', 'ani', 'per', 'ri@@', '<unk>', '@', 'vel@@', '<unk>', '@', 'are', 'l&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'h@@', '<unk>', '@', 'ab@@', '<unk>', '@', 'us@@', '<unk>', '@', 'k@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ana', ',', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'il', '30', 'per', 'cento', 'di', 'tre', 'anni', ',', 'per', 'cento', 'di', 'più', 'di', '4@@', '<unk>', '@', '8', 'anni', ',', 'per', 'cento', ',', 'per', 'cento', 'di', 'tre', 'anni', ',', 'per', 'il', '40', 'per', 'cento', '.', '</s>']
2025-05-28 09:29:01,879 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:29:01,879 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:29:01,879 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato questa due due due di<unk> @ mostra che i c<unk> @ ani per ri<unk> @ vel<unk> @ are l&apos; E<unk> @ is<unk> @ k<unk> @ h<unk> @ ab<unk> @ us<unk> @ k<unk> @ i<unk> @ ana , per tre milioni di anni , il 30 per cento di tre anni , per cento di più di 4<unk> @ 8 anni , per cento , per cento di tre anni , per il 40 per cento .
2025-05-28 09:29:01,879 - INFO - joeynmt.training - Example #1
2025-05-28 09:29:01,880 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:29:01,880 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:29:01,880 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'difficile', 'da', 'fare', 'la', 'st@@', '<unk>', '@', 'ab@@', '<unk>', '@', 'i@@', '<unk>', '@', 'fic@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ale', ',', 'che', 'non', 'è', 'il', 'D@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'eo', '.', '</s>']
2025-05-28 09:29:01,880 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:29:01,880 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:29:01,880 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza difficile da fare la st<unk> @ ab<unk> @ i<unk> @ fic<unk> @ i<unk> @ ale , che non è il D<unk> @ ic<unk> @ eo .
2025-05-28 09:29:01,880 - INFO - joeynmt.training - Example #2
2025-05-28 09:29:01,880 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:29:01,880 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:29:01,880 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'certo', 'senso', ',', 'il', 'senso', 'di', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'it', 'è', 'il', 'nostro', 'sistema', 'fis@@', '<unk>', '@', 'ico', 'globale', 'globale', 'globale', 'globale', ',', 'il', 'c@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'io', 'del', 'nostro', 'sistema', 'globale', 'globale', 'globale', 'globale', 'globale', 'globale', '.', '</s>']
2025-05-28 09:29:01,881 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:29:01,881 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:29:01,881 - INFO - joeynmt.training - 	Hypothesis: In certo senso , il senso di E<unk> @ is<unk> @ k<unk> @ it è il nostro sistema fis<unk> @ ico globale globale globale globale , il c<unk> @ ic<unk> @ io del nostro sistema globale globale globale globale globale globale .
2025-05-28 09:29:01,881 - INFO - joeynmt.training - Example #3
2025-05-28 09:29:01,881 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:29:01,881 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:29:01,881 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'ei', 'entr@@', '<unk>', '@', 'a', 'e', 'il', 'vento', '.', '</s>']
2025-05-28 09:29:01,881 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:29:01,881 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:29:01,881 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ ei entr<unk> @ a e il vento .
2025-05-28 09:29:01,882 - INFO - joeynmt.training - Example #4
2025-05-28 09:29:01,882 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:29:01,882 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:29:01,882 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'la', 'prossi@@', '<unk>', '@', 'ma', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'una', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'na', 'di', 'cosa', 'succede', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:29:01,882 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:29:01,882 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:29:01,882 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma la prossi<unk> @ ma vi mostr<unk> @ erò una di<unk> @ seg<unk> @ na di cosa succede negli ultimi 25 anni .
2025-05-28 09:29:05,281 - INFO - joeynmt.training - Epoch   3, Step:    15600, Batch Loss:     1.433387, Batch Acc: 0.613079, Tokens per Sec:    18069, Lr: 0.000300
2025-05-28 09:29:08,712 - INFO - joeynmt.training - Epoch   3, Step:    15700, Batch Loss:     1.629798, Batch Acc: 0.611430, Tokens per Sec:    20826, Lr: 0.000300
2025-05-28 09:29:12,123 - INFO - joeynmt.training - Epoch   3, Step:    15800, Batch Loss:     1.500139, Batch Acc: 0.612491, Tokens per Sec:    20973, Lr: 0.000300
2025-05-28 09:29:15,541 - INFO - joeynmt.training - Epoch   3, Step:    15900, Batch Loss:     1.502190, Batch Acc: 0.618012, Tokens per Sec:    20858, Lr: 0.000300
2025-05-28 09:29:18,979 - INFO - joeynmt.training - Epoch   3, Step:    16000, Batch Loss:     1.361303, Batch Acc: 0.613252, Tokens per Sec:    20824, Lr: 0.000300
2025-05-28 09:29:18,979 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:29:18,979 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:29:29,817 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.45, ppl:   4.27, acc:   0.61, generation: 10.8300[sec], evaluation: 0.0000[sec]
2025-05-28 09:29:29,817 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:29:30,533 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/13500.ckpt
2025-05-28 09:29:30,560 - INFO - joeynmt.training - Example #0
2025-05-28 09:29:30,560 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:29:30,560 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:29:30,560 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'questa', 'immagine', 'di', 'due', 'due', 'anni', 'per', 'la', 'prima', 'cosa', 'che', 'il', 'suo', 's@@', '<unk>', '@', 'ettore', 'fis@@', '<unk>', '@', 'ico', 'che', 'il', 'ghi@@', '<unk>', '@', 'accio', 'fis@@', '<unk>', '@', 'ico', 'per', 'la', 'maggior', 'parte', 'dei', 'tre', 'milioni', 'di', 'anni', ',', 'il', '4@@', '<unk>', '@', '8', ',', 'il', '40', 'per', 'cento', 'di', 'tre', 'anni', ',', 'il', '4@@', '<unk>', '@', '8', ',', 'per', 'cento', 'di', 'tre', 'anni', ',', 'per', 'cento', '.', '</s>']
2025-05-28 09:29:30,561 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:29:30,561 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:29:30,561 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno ho mostr<unk> @ ato questa immagine di due due anni per la prima cosa che il suo s<unk> @ ettore fis<unk> @ ico che il ghi<unk> @ accio fis<unk> @ ico per la maggior parte dei tre milioni di anni , il 4<unk> @ 8 , il 40 per cento di tre anni , il 4<unk> @ 8 , per cento di tre anni , per cento .
2025-05-28 09:29:30,561 - INFO - joeynmt.training - Example #1
2025-05-28 09:29:30,561 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:29:30,561 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:29:30,562 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'forte', ',', 'la', 'cos@@', '<unk>', '@', 'ci@@', '<unk>', '@', 'enza', 'di', 'questo', 'problema', ',', 'che', 'non', 'è', 'il', 'problema', 'della', 'cos@@', '<unk>', '@', 'ci@@', '<unk>', '@', 'enza', '.', '</s>']
2025-05-28 09:29:30,562 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:29:30,562 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:29:30,562 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza forte , la cos<unk> @ ci<unk> @ enza di questo problema , che non è il problema della cos<unk> @ ci<unk> @ enza .
2025-05-28 09:29:30,562 - INFO - joeynmt.training - Example #2
2025-05-28 09:29:30,562 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:29:30,563 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:29:30,563 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'realtà', ',', 'il', 'senso', 'di', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'it', 'è', 'il', 'cuore', 'di', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'it', ',', 'il', 'cuore', 'globale', 'globale', 'globale', 'globale', '.', '</s>']
2025-05-28 09:29:30,563 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:29:30,563 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:29:30,563 - INFO - joeynmt.training - 	Hypothesis: In realtà , il senso di E<unk> @ is<unk> @ k<unk> @ it è il cuore di E<unk> @ is<unk> @ k<unk> @ it , il cuore globale globale globale globale .
2025-05-28 09:29:30,563 - INFO - joeynmt.training - Example #3
2025-05-28 09:29:30,563 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:29:30,564 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:29:30,564 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'ei', 'si', 'ri@@', '<unk>', '@', 'd@@', '<unk>', '@', 'à', 'e', 'il', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 09:29:30,564 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:29:30,564 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:29:30,564 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ ei si ri<unk> @ d<unk> @ à e il ghi<unk> @ accio .
2025-05-28 09:29:30,564 - INFO - joeynmt.training - Example #4
2025-05-28 09:29:30,564 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:29:30,565 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:29:30,565 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'una', 'cosa', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'una', 'cosa', 'che', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:29:30,565 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:29:30,565 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:29:30,565 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma vi mostr<unk> @ erò è una cosa che vi mostr<unk> @ erò è una cosa che negli ultimi 25 anni .
2025-05-28 09:29:33,950 - INFO - joeynmt.training - Epoch   3, Step:    16100, Batch Loss:     1.279807, Batch Acc: 0.615342, Tokens per Sec:    17366, Lr: 0.000300
2025-05-28 09:29:37,358 - INFO - joeynmt.training - Epoch   3, Step:    16200, Batch Loss:     1.429226, Batch Acc: 0.614133, Tokens per Sec:    20767, Lr: 0.000300
2025-05-28 09:29:40,766 - INFO - joeynmt.training - Epoch   3, Step:    16300, Batch Loss:     1.460569, Batch Acc: 0.612095, Tokens per Sec:    21053, Lr: 0.000300
2025-05-28 09:29:44,192 - INFO - joeynmt.training - Epoch   3, Step:    16400, Batch Loss:     1.522179, Batch Acc: 0.613323, Tokens per Sec:    21120, Lr: 0.000300
2025-05-28 09:29:47,611 - INFO - joeynmt.training - Epoch   3, Step:    16500, Batch Loss:     1.464122, Batch Acc: 0.614279, Tokens per Sec:    20798, Lr: 0.000300
2025-05-28 09:29:47,611 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:29:47,612 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:29:57,140 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.45, ppl:   4.24, acc:   0.61, generation: 9.5185[sec], evaluation: 0.0000[sec]
2025-05-28 09:29:57,141 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:29:57,776 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/14000.ckpt
2025-05-28 09:29:57,804 - INFO - joeynmt.training - Example #0
2025-05-28 09:29:57,804 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:29:57,805 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:29:57,805 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'due', 'due', 'anni', 'per', 'far', 'ri@@', '<unk>', '@', 'durre', 'per', 'ri@@', '<unk>', '@', 'pro@@', '<unk>', '@', 'durre', 'il', 't@@', '<unk>', '@', 'asso', 'di', 'al@@', '<unk>', '@', 'te@@', '<unk>', '@', 'zza', 'che', 'i', 'tre', 'milioni', 'di', 'anni', ',', 'che', 'è', 'stato', 'in', 'grado', 'di', 'ri@@', '<unk>', '@', 'durre', 'la', 'Gr@@', '<unk>', '@', 'an', 'B@@', '<unk>', '@', 'ret@@', '<unk>', '@', 'd', ',', 'tre', 'milioni', 'di', 'anni', ',', 'che', 'ha', 'fatto', 'la', 'Gr@@', '<unk>', '@', 'an', 'B@@', '<unk>', '@', 'ret@@', '<unk>', '@', 'ag@@', '<unk>', '@', 'na', ',', 'per', 'il', '40', '%', '.', '</s>']
2025-05-28 09:29:57,805 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:29:57,805 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:29:57,805 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due due due anni per far ri<unk> @ durre per ri<unk> @ pro<unk> @ durre il t<unk> @ asso di al<unk> @ te<unk> @ zza che i tre milioni di anni , che è stato in grado di ri<unk> @ durre la Gr<unk> @ an B<unk> @ ret<unk> @ d , tre milioni di anni , che ha fatto la Gr<unk> @ an B<unk> @ ret<unk> @ ag<unk> @ na , per il 40 % .
2025-05-28 09:29:57,805 - INFO - joeynmt.training - Example #1
2025-05-28 09:29:57,806 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:29:57,806 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:29:57,806 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'forte', ',', 'la', 'giu@@', '<unk>', '@', 'st@@', '<unk>', '@', 'izia', ',', 'la', 'giu@@', '<unk>', '@', 'st@@', '<unk>', '@', 'izia', ',', 'non', 'è', 'abbastanza', 'speci@@', '<unk>', '@', 'fico', ',', 'non', 'è', 'la', 'giu@@', '<unk>', '@', 'st@@', '<unk>', '@', 'izia', '.', '</s>']
2025-05-28 09:29:57,806 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:29:57,806 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:29:57,806 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte , la giu<unk> @ st<unk> @ izia , la giu<unk> @ st<unk> @ izia , non è abbastanza speci<unk> @ fico , non è la giu<unk> @ st<unk> @ izia .
2025-05-28 09:29:57,806 - INFO - joeynmt.training - Example #2
2025-05-28 09:29:57,807 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:29:57,807 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:29:57,807 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'realtà', ',', 'il', 'senso', 'di', 'in@@', '<unk>', '@', 'cor@@', '<unk>', '@', 'aggio', 'è', 'la', 'c@@', '<unk>', '@', 'lasse', 'di', 'un', 'sistema', 'di', 'c@@', '<unk>', '@', 'ru@@', '<unk>', '@', 'olo', '.', '</s>']
2025-05-28 09:29:57,807 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:29:57,807 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:29:57,807 - INFO - joeynmt.training - 	Hypothesis: In realtà , il senso di in<unk> @ cor<unk> @ aggio è la c<unk> @ lasse di un sistema di c<unk> @ ru<unk> @ olo .
2025-05-28 09:29:57,808 - INFO - joeynmt.training - Example #3
2025-05-28 09:29:57,808 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:29:57,808 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:29:57,808 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'oro', 'si', 'sta', 'entr@@', '<unk>', '@', 'ando', 'in', 'un', 'vento', 'e', 'il', 'vento', '.', '</s>']
2025-05-28 09:29:57,808 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:29:57,808 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:29:57,808 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ oro si sta entr<unk> @ ando in un vento e il vento .
2025-05-28 09:29:57,808 - INFO - joeynmt.training - Example #4
2025-05-28 09:29:57,809 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:29:57,809 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:29:57,809 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'prossi@@', '<unk>', '@', 'ma', ',', 'è', 'una', 'cosa', 'che', 'mostr@@', '<unk>', '@', 'o', 'è', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:29:57,809 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:29:57,809 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:29:57,809 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma prossi<unk> @ ma , è una cosa che mostr<unk> @ o è che è successo negli ultimi 25 anni .
2025-05-28 09:30:01,238 - INFO - joeynmt.training - Epoch   3, Step:    16600, Batch Loss:     1.398743, Batch Acc: 0.617657, Tokens per Sec:    17298, Lr: 0.000300
2025-05-28 09:30:04,640 - INFO - joeynmt.training - Epoch   3, Step:    16700, Batch Loss:     1.488775, Batch Acc: 0.616519, Tokens per Sec:    20742, Lr: 0.000300
2025-05-28 09:30:08,033 - INFO - joeynmt.training - Epoch   3, Step:    16800, Batch Loss:     1.525088, Batch Acc: 0.615094, Tokens per Sec:    20773, Lr: 0.000300
2025-05-28 09:30:11,417 - INFO - joeynmt.training - Epoch   3, Step:    16900, Batch Loss:     1.499619, Batch Acc: 0.617814, Tokens per Sec:    20297, Lr: 0.000300
2025-05-28 09:30:14,829 - INFO - joeynmt.training - Epoch   3, Step:    17000, Batch Loss:     1.430862, Batch Acc: 0.617134, Tokens per Sec:    20509, Lr: 0.000300
2025-05-28 09:30:14,829 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:30:14,829 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:30:26,372 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.44, ppl:   4.21, acc:   0.61, generation: 11.5322[sec], evaluation: 0.0000[sec]
2025-05-28 09:30:26,373 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:30:26,982 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/14500.ckpt
2025-05-28 09:30:27,010 - INFO - joeynmt.training - Example #0
2025-05-28 09:30:27,011 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:30:27,011 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:30:27,011 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'questa', 'foto', ',', 'per', 'far', 'vedere', 'che', 'il', '4@@', '<unk>', '@', '8', 'anni', ',', 'per', 'ri@@', '<unk>', '@', 'durre', 'il', 't@@', '<unk>', '@', 'opo', 'di', 'essere', 'il', '4@@', '<unk>', '@', '8', 'anni', ',', 'che', 'ha', 'fatto', 'il', '4@@', '<unk>', '@', '8', 'anni', ',', 'il', '40', '%', 'dei', 'tre', 'anni', ',', 'che', 'ha', 'fatto', 'il', '4@@', '<unk>', '@', '8', 'anni', ',', 'per', 'il', '40', '%', 'di', 'più', 'di', 'circa', 'il', '40', '%', '.', '</s>']
2025-05-28 09:30:27,012 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:30:27,012 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:30:27,012 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato questa foto , per far vedere che il 4<unk> @ 8 anni , per ri<unk> @ durre il t<unk> @ opo di essere il 4<unk> @ 8 anni , che ha fatto il 4<unk> @ 8 anni , il 40 % dei tre anni , che ha fatto il 4<unk> @ 8 anni , per il 40 % di più di circa il 40 % .
2025-05-28 09:30:27,012 - INFO - joeynmt.training - Example #1
2025-05-28 09:30:27,012 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:30:27,012 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:30:27,012 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', ',', 'per', 'la', 'giu@@', '<unk>', '@', 'st@@', '<unk>', '@', 'izia', 'di', 'questo', 'problema', ',', 'che', 'non', 'è', 'abbastanza', ',', 'che', 'non', 'è', 'un', 'problema', 'che', 'non', 'è', 'il', 'fatto', 'che', 'non', 'è', 'il', 'D@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ke', '.', '</s>']
2025-05-28 09:30:27,013 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:30:27,013 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:30:27,013 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza , per la giu<unk> @ st<unk> @ izia di questo problema , che non è abbastanza , che non è un problema che non è il fatto che non è il D<unk> @ ic<unk> @ ke .
2025-05-28 09:30:27,013 - INFO - joeynmt.training - Example #2
2025-05-28 09:30:27,013 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:30:27,013 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:30:27,013 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'è', 'la', 'c@@', '<unk>', '@', 'assa', 'di', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'it', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'c@@', '<unk>', '@', 'ru@@', '<unk>', '@', 'ot@@', '<unk>', '@', 'ico', '.', '</s>']
2025-05-28 09:30:27,014 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:30:27,014 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:30:27,014 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , è la c<unk> @ assa di E<unk> @ is<unk> @ k<unk> @ it , il cuore del nostro sistema c<unk> @ ru<unk> @ ot<unk> @ ico .
2025-05-28 09:30:27,014 - INFO - joeynmt.training - Example #3
2025-05-28 09:30:27,014 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:30:27,014 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:30:27,014 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&apos;', 'in', 'un', 'ang@@', '<unk>', '@', 'olo', 'e', 'il', 'vento', 'in', 'un', 'ang@@', '<unk>', '@', 'olo', '.', '</s>']
2025-05-28 09:30:27,015 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:30:27,015 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:30:27,015 - INFO - joeynmt.training - 	Hypothesis: E &apos; in un ang<unk> @ olo e il vento in un ang<unk> @ olo .
2025-05-28 09:30:27,015 - INFO - joeynmt.training - Example #4
2025-05-28 09:30:27,015 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:30:27,015 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:30:27,015 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', ',', 'la', 'prossi@@', '<unk>', '@', 'ma', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'una', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'ret@@', '<unk>', '@', 'ta', 'che', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:30:27,016 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:30:27,016 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:30:27,016 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma , la prossi<unk> @ ma che vi mostr<unk> @ erò è una di<unk> @ seg<unk> @ ret<unk> @ ta che negli ultimi 25 anni .
2025-05-28 09:30:30,433 - INFO - joeynmt.training - Epoch   3, Step:    17100, Batch Loss:     1.443577, Batch Acc: 0.616362, Tokens per Sec:    17367, Lr: 0.000300
2025-05-28 09:30:33,847 - INFO - joeynmt.training - Epoch   3, Step:    17200, Batch Loss:     1.430813, Batch Acc: 0.622116, Tokens per Sec:    21423, Lr: 0.000300
2025-05-28 09:30:37,247 - INFO - joeynmt.training - Epoch   3, Step:    17300, Batch Loss:     1.368389, Batch Acc: 0.620194, Tokens per Sec:    20628, Lr: 0.000300
2025-05-28 09:30:40,650 - INFO - joeynmt.training - Epoch   3, Step:    17400, Batch Loss:     1.347899, Batch Acc: 0.616103, Tokens per Sec:    20830, Lr: 0.000300
2025-05-28 09:30:44,064 - INFO - joeynmt.training - Epoch   3, Step:    17500, Batch Loss:     1.525092, Batch Acc: 0.622047, Tokens per Sec:    20903, Lr: 0.000300
2025-05-28 09:30:44,065 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:30:44,065 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:30:54,052 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.43, ppl:   4.18, acc:   0.62, generation: 9.9803[sec], evaluation: 0.0000[sec]
2025-05-28 09:30:54,052 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:30:54,625 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/15000.ckpt
2025-05-28 09:30:54,646 - INFO - joeynmt.training - Example #0
2025-05-28 09:30:54,647 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:30:54,647 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:30:54,647 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'due', 'due', 'cose', 'per', 'vedere', 'che', 'i', 'più', 'cost@@', '<unk>', '@', 'i', 'di', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ani', 'per', 'l&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'i@@', '<unk>', '@', 'f@@', '<unk>', '@', 'o', 'di', 'tre', 'anni', ',', 'il', '40', '%', ',', 'il', '40', '%', ',', 'il', '40', '%', ',', 'il', '40', '%', ',', 'il', '40', '%', ',', 'il', '40', '%', ',', 'il', '40', '%', ',', 'il', '40', '%', ',', 'per', 'cento', '.', '</s>']
2025-05-28 09:30:54,647 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:30:54,647 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:30:54,647 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato queste due due due cose per vedere che i più cost<unk> @ i di E<unk> @ is<unk> @ k<unk> @ i<unk> @ ani per l&apos; E<unk> @ is<unk> @ k<unk> @ i<unk> @ f<unk> @ o di tre anni , il 40 % , il 40 % , il 40 % , il 40 % , il 40 % , il 40 % , il 40 % , il 40 % , per cento .
2025-05-28 09:30:54,648 - INFO - joeynmt.training - Example #1
2025-05-28 09:30:54,648 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:30:54,648 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:30:54,648 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'difficile', 'da', 'fare', 'la', 'cosa', 'più', 'della', 'giu@@', '<unk>', '@', 'st@@', '<unk>', '@', 'izia', 'di', 'questo', 'problema', 'della', 'nostra', 'sfida', ',', 'non', 'è', 'che', 'non', 'è', 'abbastanza', 'da', 'un', 'problema', 'di', 'speci@@', '<unk>', '@', 'fico', ',', 'che', 'non', 'è', 'l&apos;', 'eff@@', '<unk>', '@', 'etto', 'della', 'ghi@@', '<unk>', '@', 'acci@@', '<unk>', '@', 'ata', '.', '</s>']
2025-05-28 09:30:54,648 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:30:54,649 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:30:54,649 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza difficile da fare la cosa più della giu<unk> @ st<unk> @ izia di questo problema della nostra sfida , non è che non è abbastanza da un problema di speci<unk> @ fico , che non è l&apos; eff<unk> @ etto della ghi<unk> @ acci<unk> @ ata .
2025-05-28 09:30:54,649 - INFO - joeynmt.training - Example #2
2025-05-28 09:30:54,649 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:30:54,649 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:30:54,649 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'qualche', 'certo', 'senso', ',', 'è', 'la', 'più', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'è', 'la', 'più', 'cost@@', '<unk>', '@', 'ante', 'dell&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ale', ',', 'il', 'cuore', 'globale', 'del', 'nostro', 'sistema', 'globale', 'globale', 'globale', 'globale', '.', '</s>']
2025-05-28 09:30:54,649 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:30:54,650 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:30:54,650 - INFO - joeynmt.training - 	Hypothesis: In qualche certo senso , è la più ar<unk> @ t<unk> @ ica è la più cost<unk> @ ante dell&apos; E<unk> @ is<unk> @ k<unk> @ i<unk> @ ale , il cuore globale del nostro sistema globale globale globale globale .
2025-05-28 09:30:54,650 - INFO - joeynmt.training - Example #3
2025-05-28 09:30:54,650 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:30:54,650 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:30:54,650 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', '@', 'erto', ',', 'in', 'vento', 'e', 'si', 'è', 'ri@@', '<unk>', '@', 'ma@@', '<unk>', '@', 'sto', '.', '</s>']
2025-05-28 09:30:54,650 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:30:54,650 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:30:54,650 - INFO - joeynmt.training - 	Hypothesis: C<unk> @ erto , in vento e si è ri<unk> @ ma<unk> @ sto .
2025-05-28 09:30:54,650 - INFO - joeynmt.training - Example #4
2025-05-28 09:30:54,651 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:30:54,651 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:30:54,651 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'questa', 'è', 'la', 'prossi@@', '<unk>', '@', 'ma', ',', 'è', 'una', 'di@@', '<unk>', '@', 'stri@@', '<unk>', '@', 'bu@@', '<unk>', '@', 'zione', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'in', 'più', 'di', '25', 'anni', '.', '</s>']
2025-05-28 09:30:54,651 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:30:54,651 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:30:54,651 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma questa è la prossi<unk> @ ma , è una di<unk> @ stri<unk> @ bu<unk> @ zione che vi mostr<unk> @ erò in più di 25 anni .
2025-05-28 09:30:57,985 - INFO - joeynmt.training - Epoch   3, Step:    17600, Batch Loss:     1.184709, Batch Acc: 0.618991, Tokens per Sec:    18090, Lr: 0.000300
2025-05-28 09:31:01,370 - INFO - joeynmt.training - Epoch   3, Step:    17700, Batch Loss:     1.544555, Batch Acc: 0.620115, Tokens per Sec:    21245, Lr: 0.000300
2025-05-28 09:31:04,791 - INFO - joeynmt.training - Epoch   3, Step:    17800, Batch Loss:     1.340805, Batch Acc: 0.620635, Tokens per Sec:    21552, Lr: 0.000300
2025-05-28 09:31:08,204 - INFO - joeynmt.training - Epoch   3, Step:    17900, Batch Loss:     1.368002, Batch Acc: 0.619359, Tokens per Sec:    21096, Lr: 0.000300
2025-05-28 09:31:11,611 - INFO - joeynmt.training - Epoch   3, Step:    18000, Batch Loss:     1.369340, Batch Acc: 0.616599, Tokens per Sec:    20938, Lr: 0.000300
2025-05-28 09:31:11,611 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:31:11,611 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:31:23,158 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.42, ppl:   4.14, acc:   0.62, generation: 11.5388[sec], evaluation: 0.0000[sec]
2025-05-28 09:31:23,158 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:31:23,852 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/15500.ckpt
2025-05-28 09:31:23,879 - INFO - joeynmt.training - Example #0
2025-05-28 09:31:23,880 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:31:23,880 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:31:23,880 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'due', 'due', 'due', 'anni', 'per', 'cercare', 'di', 'vedere', 'che', 'la', 'gr@@', '<unk>', '@', 'av@@', '<unk>', '@', 'ità', 'di', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'fi@@', '<unk>', '@', 'che', 'che', 'per', 'i', 'tre', 'milioni', 'di', 'anni', ',', 'che', 'ha', 'fatto', 'per', 'la', 'Gr@@', '<unk>', '@', 'um@@', '<unk>', '@', 'ag@@', '<unk>', '@', 'na', 'ha', 'fatto', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'cento', '.', '</s>']
2025-05-28 09:31:23,881 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:31:23,881 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:31:23,881 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato queste due due due due anni per cercare di vedere che la gr<unk> @ av<unk> @ ità di E<unk> @ is<unk> @ k<unk> @ fi<unk> @ che che per i tre milioni di anni , che ha fatto per la Gr<unk> @ um<unk> @ ag<unk> @ na ha fatto tre milioni di anni , per cento .
2025-05-28 09:31:23,881 - INFO - joeynmt.training - Example #1
2025-05-28 09:31:23,881 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:31:23,881 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:31:23,881 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', ',', 'in', 'grado', 'di', 'fare', 'il', 'problema', 'della', 'nostra', 'esperienza', 'speci@@', '<unk>', '@', 'ale', ',', 'non', 'è', 'abbastanza', 'per', 'la', 'prima', 'cosa', 'che', 'non', 'è', 'il', 'problema', 'della', 'parte', 'del', 'problema', '.', '</s>']
2025-05-28 09:31:23,882 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:31:23,882 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:31:23,882 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza , in grado di fare il problema della nostra esperienza speci<unk> @ ale , non è abbastanza per la prima cosa che non è il problema della parte del problema .
2025-05-28 09:31:23,882 - INFO - joeynmt.training - Example #2
2025-05-28 09:31:23,882 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:31:23,882 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:31:23,882 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'qualche', 'modo', ',', 'il', 'senso', 'di', 'in@@', '<unk>', '@', 'contro', 'è', 'la', 'c@@', '<unk>', '@', 'au@@', '<unk>', '@', 'mento', 'di', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'it', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'globale', 'globale', '.', '</s>']
2025-05-28 09:31:23,883 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:31:23,883 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:31:23,883 - INFO - joeynmt.training - 	Hypothesis: In qualche modo , il senso di in<unk> @ contro è la c<unk> @ au<unk> @ mento di E<unk> @ is<unk> @ k<unk> @ it , il cuore del nostro sistema globale globale .
2025-05-28 09:31:23,883 - INFO - joeynmt.training - Example #3
2025-05-28 09:31:23,883 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:31:23,883 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:31:23,883 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'prima', 'di', 'arrivare', 'al', 'vento', 'e', 'poi', 'si', 'è', 'ri@@', '<unk>', '@', 'fer@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ore', 'in', 'un', 'in@@', '<unk>', '@', 'vi@@', '<unk>', '@', 'o', '.', '</s>']
2025-05-28 09:31:23,884 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:31:23,884 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:31:23,884 - INFO - joeynmt.training - 	Hypothesis: La prima prima di arrivare al vento e poi si è ri<unk> @ fer<unk> @ i<unk> @ ore in un in<unk> @ vi<unk> @ o .
2025-05-28 09:31:23,884 - INFO - joeynmt.training - Example #4
2025-05-28 09:31:23,884 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:31:23,884 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:31:23,884 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', ',', 'la', 'prossi@@', '<unk>', '@', 'ma', ',', 'è', 'una', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'ret@@', '<unk>', '@', 'ta', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'in', '25', 'anni', '.', '</s>']
2025-05-28 09:31:23,885 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:31:23,885 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:31:23,885 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma , la prossi<unk> @ ma , è una di<unk> @ seg<unk> @ ret<unk> @ ta che vi mostr<unk> @ erò in 25 anni .
2025-05-28 09:31:27,262 - INFO - joeynmt.training - Epoch   3, Step:    18100, Batch Loss:     1.384098, Batch Acc: 0.617568, Tokens per Sec:    17413, Lr: 0.000300
2025-05-28 09:31:30,672 - INFO - joeynmt.training - Epoch   3, Step:    18200, Batch Loss:     1.239958, Batch Acc: 0.618889, Tokens per Sec:    20744, Lr: 0.000300
2025-05-28 09:31:34,086 - INFO - joeynmt.training - Epoch   3, Step:    18300, Batch Loss:     1.439061, Batch Acc: 0.619604, Tokens per Sec:    20681, Lr: 0.000300
2025-05-28 09:31:37,490 - INFO - joeynmt.training - Epoch   3, Step:    18400, Batch Loss:     1.323443, Batch Acc: 0.621503, Tokens per Sec:    20953, Lr: 0.000300
2025-05-28 09:31:40,888 - INFO - joeynmt.training - Epoch   3, Step:    18500, Batch Loss:     1.313148, Batch Acc: 0.619327, Tokens per Sec:    21050, Lr: 0.000300
2025-05-28 09:31:40,888 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:31:40,888 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:31:53,784 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.41, ppl:   4.11, acc:   0.62, generation: 12.8845[sec], evaluation: 0.0000[sec]
2025-05-28 09:31:53,784 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:31:54,359 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/16000.ckpt
2025-05-28 09:31:54,384 - INFO - joeynmt.training - Example #0
2025-05-28 09:31:54,385 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:31:54,385 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:31:54,385 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'due', 'cose', 'che', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'per', 'fare', 'per', 'ri@@', '<unk>', '@', 'durre', 'che', 'le', 'epi@@', '<unk>', '@', 'te', 'di', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'f@@', '<unk>', '@', 'i', 'che', 'per', 'l&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'f@@', '<unk>', '@', 'i', ',', 'per', 'il', '40', '%', 'delle', 'gr@@', '<unk>', '@', 'azioni', ',', 'per', 'il', '40', '%', '.', '</s>']
2025-05-28 09:31:54,385 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:31:54,385 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:31:54,385 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due due cose che ho mostr<unk> @ ato per fare per ri<unk> @ durre che le epi<unk> @ te di E<unk> @ is<unk> @ k<unk> @ f<unk> @ i che per l&apos; E<unk> @ is<unk> @ k<unk> @ f<unk> @ i , per il 40 % delle gr<unk> @ azioni , per il 40 % .
2025-05-28 09:31:54,385 - INFO - joeynmt.training - Example #1
2025-05-28 09:31:54,386 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:31:54,386 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:31:54,386 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'mor@@', '<unk>', '@', 't@@', '<unk>', '@', 'ale', 'di', 'questo', 'problema', 'di', 'più', 'di', 'questo', 'problema', 'di', 'questo', 'problema', 'di', 'questo', 'problema', 'di', 'questo', 'punto', 'di', 'vista', 'speci@@', '<unk>', '@', 'fico', ',', 'non', 'è', 'l&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 09:31:54,386 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:31:54,386 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:31:54,386 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza mor<unk> @ t<unk> @ ale di questo problema di più di questo problema di questo problema di questo problema di questo punto di vista speci<unk> @ fico , non è l&apos; E<unk> @ is<unk> @ es .
2025-05-28 09:31:54,386 - INFO - joeynmt.training - Example #2
2025-05-28 09:31:54,387 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:31:54,387 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:31:54,387 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'qualche', 'qualche', 'qualche', 'qualche', 'idea', 'è', 'la', 'c@@', '<unk>', '@', 'att@@', '<unk>', '@', 'esa', 'di', 'ghi@@', '<unk>', '@', 'accio', 'fis@@', '<unk>', '@', 'ico', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'c@@', '<unk>', '@', 'ru@@', '<unk>', '@', 'g@@', '<unk>', '@', 'ale', '.', '</s>']
2025-05-28 09:31:54,387 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:31:54,387 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:31:54,387 - INFO - joeynmt.training - 	Hypothesis: In qualche qualche qualche qualche idea è la c<unk> @ att<unk> @ esa di ghi<unk> @ accio fis<unk> @ ico , il cuore del nostro sistema c<unk> @ ru<unk> @ g<unk> @ ale .
2025-05-28 09:31:54,387 - INFO - joeynmt.training - Example #3
2025-05-28 09:31:54,388 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:31:54,388 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:31:54,388 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'cosa', 'che', 'è', 'in', 'vento', 'e', 'il', 'vento', 'in', 'un', 'al@@', '<unk>', '@', 'bero', '.', '</s>']
2025-05-28 09:31:54,388 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:31:54,388 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:31:54,388 - INFO - joeynmt.training - 	Hypothesis: La prima cosa che è in vento e il vento in un al<unk> @ bero .
2025-05-28 09:31:54,388 - INFO - joeynmt.training - Example #4
2025-05-28 09:31:54,389 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:31:54,389 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:31:54,389 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', ',', 'la', 'prossi@@', '<unk>', '@', 'ma', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'una', 'seg@@', '<unk>', '@', 'ret@@', '<unk>', '@', 'ta', 'di', 'cosa', 'succede', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:31:54,389 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:31:54,389 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:31:54,389 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma , la prossi<unk> @ ma che vi mostr<unk> @ erò è una seg<unk> @ ret<unk> @ ta di cosa succede negli ultimi 25 anni .
2025-05-28 09:31:57,768 - INFO - joeynmt.training - Epoch   3, Step:    18600, Batch Loss:     1.329594, Batch Acc: 0.620352, Tokens per Sec:    17950, Lr: 0.000300
2025-05-28 09:32:01,161 - INFO - joeynmt.training - Epoch   3, Step:    18700, Batch Loss:     1.332546, Batch Acc: 0.622425, Tokens per Sec:    20403, Lr: 0.000300
2025-05-28 09:32:04,545 - INFO - joeynmt.training - Epoch   3, Step:    18800, Batch Loss:     1.374808, Batch Acc: 0.623155, Tokens per Sec:    20505, Lr: 0.000300
2025-05-28 09:32:07,434 - INFO - joeynmt.training - Epoch   3: total training loss 9105.23
2025-05-28 09:32:07,434 - INFO - joeynmt.training - EPOCH 4
2025-05-28 09:32:07,951 - INFO - joeynmt.training - Epoch   4, Step:    18900, Batch Loss:     1.395045, Batch Acc: 0.633493, Tokens per Sec:    21520, Lr: 0.000300
2025-05-28 09:32:11,341 - INFO - joeynmt.training - Epoch   4, Step:    19000, Batch Loss:     1.375632, Batch Acc: 0.637944, Tokens per Sec:    21252, Lr: 0.000300
2025-05-28 09:32:11,342 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:32:11,342 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:32:22,918 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.40, ppl:   4.05, acc:   0.62, generation: 11.5653[sec], evaluation: 0.0000[sec]
2025-05-28 09:32:22,919 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:32:23,508 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/16500.ckpt
2025-05-28 09:32:23,537 - INFO - joeynmt.training - Example #0
2025-05-28 09:32:23,538 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:32:23,538 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:32:23,538 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'questi', 'due', 'due', 'due', 'due', 'due', 'anni', 'per', 'la', 'Gr@@', '<unk>', '@', 'an', 'B@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'et', 'che', 'che', 'i', 't@@', '<unk>', '@', 'av@@', '<unk>', '@', 'ranno', 'un', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'et', 'per', 'gli', 'Stati', 'Uniti', ',', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'il', '40', ',', 'per', 'cento', 'cento', '.', '</s>']
2025-05-28 09:32:23,539 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:32:23,539 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:32:23,539 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato questi due due due due due anni per la Gr<unk> @ an B<unk> @ is<unk> @ k<unk> @ et che che i t<unk> @ av<unk> @ ranno un E<unk> @ is<unk> @ k<unk> @ et per gli Stati Uniti , per tre milioni di anni , per il 40 , per cento cento .
2025-05-28 09:32:23,539 - INFO - joeynmt.training - Example #1
2025-05-28 09:32:23,539 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:32:23,539 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:32:23,539 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'mor@@', '<unk>', '@', 'ti@@', '<unk>', '@', 'vo', 'di', 'un', 'processo', 'di', 'consegu@@', '<unk>', '@', 'enza', 'di', 'questo', 'problema', 'speci@@', '<unk>', '@', 'ale', ',', 'non', 'è', 'il', 'ter@@', '<unk>', '@', 'ren@@', '<unk>', '@', 'o', 'del', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 09:32:23,540 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:32:23,540 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:32:23,540 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza mor<unk> @ ti<unk> @ vo di un processo di consegu<unk> @ enza di questo problema speci<unk> @ ale , non è il ter<unk> @ ren<unk> @ o del ghi<unk> @ accio .
2025-05-28 09:32:23,540 - INFO - joeynmt.training - Example #2
2025-05-28 09:32:23,540 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:32:23,540 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:32:23,540 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'qualche', 'qualche', 'certo', 'senso', ',', 'è', 'il', 'cuore', 'di', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'et', ',', 'il', 'cuore', 'della', 'cor@@', '<unk>', '@', 'sa', 'c@@', '<unk>', '@', 'ru@@', '<unk>', '@', 'ci@@', '<unk>', '@', 'ale', 'del', 'nostro', 'sistema', 'globale', '.', '</s>']
2025-05-28 09:32:23,541 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:32:23,541 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:32:23,541 - INFO - joeynmt.training - 	Hypothesis: In qualche qualche certo senso , è il cuore di E<unk> @ is<unk> @ k<unk> @ et , il cuore della cor<unk> @ sa c<unk> @ ru<unk> @ ci<unk> @ ale del nostro sistema globale .
2025-05-28 09:32:23,541 - INFO - joeynmt.training - Example #3
2025-05-28 09:32:23,541 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:32:23,541 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:32:23,541 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'ei', 'cres@@', '<unk>', '@', 'ci@@', '<unk>', '@', 'uta', 'in', 'piedi', ',', 'e', 'si', 's@@', '<unk>', '@', 'com@@', '<unk>', '@', 'pi@@', '<unk>', '@', 'ato', 'in', 'S@@', '<unk>', '@', 'om@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 09:32:23,542 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:32:23,542 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:32:23,542 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ ei cres<unk> @ ci<unk> @ uta in piedi , e si s<unk> @ com<unk> @ pi<unk> @ ato in S<unk> @ om<unk> @ ate .
2025-05-28 09:32:23,542 - INFO - joeynmt.training - Example #4
2025-05-28 09:32:23,542 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:32:23,542 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:32:23,542 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'eci', ',', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'un', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:32:23,543 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:32:23,543 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:32:23,543 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ eci , vi mostr<unk> @ erò è un di<unk> @ seg<unk> @ no di quello che è successo negli ultimi 25 anni .
2025-05-28 09:32:26,961 - INFO - joeynmt.training - Epoch   4, Step:    19100, Batch Loss:     1.309546, Batch Acc: 0.636098, Tokens per Sec:    17730, Lr: 0.000300
2025-05-28 09:32:30,438 - INFO - joeynmt.training - Epoch   4, Step:    19200, Batch Loss:     1.302887, Batch Acc: 0.634985, Tokens per Sec:    20410, Lr: 0.000300
2025-05-28 09:32:33,822 - INFO - joeynmt.training - Epoch   4, Step:    19300, Batch Loss:     1.318587, Batch Acc: 0.639274, Tokens per Sec:    20155, Lr: 0.000300
2025-05-28 09:32:37,235 - INFO - joeynmt.training - Epoch   4, Step:    19400, Batch Loss:     1.337209, Batch Acc: 0.634309, Tokens per Sec:    21483, Lr: 0.000300
2025-05-28 09:32:40,635 - INFO - joeynmt.training - Epoch   4, Step:    19500, Batch Loss:     1.232314, Batch Acc: 0.631988, Tokens per Sec:    20921, Lr: 0.000300
2025-05-28 09:32:40,635 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:32:40,635 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:32:52,289 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.40, ppl:   4.06, acc:   0.63, generation: 11.6433[sec], evaluation: 0.0000[sec]
2025-05-28 09:32:52,695 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/17000.ckpt
2025-05-28 09:32:52,722 - INFO - joeynmt.training - Example #0
2025-05-28 09:32:52,723 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:32:52,723 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:32:52,723 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'due', 'cose', 'che', 'sono', 'stati', 'mostr@@', '<unk>', '@', 'i', 'che', 'i', 't@@', '<unk>', '@', 'ali', 'per', 'vedere', 'che', 'i', 't@@', '<unk>', '@', 'ali', 'di', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'la', 'Gr@@', '<unk>', '@', 'an', 'B@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ana', ',', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'il', '40', ',', '40', '%', 'di', 'più', '.', '</s>']
2025-05-28 09:32:52,724 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:32:52,724 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:32:52,724 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato queste due due cose che sono stati mostr<unk> @ i che i t<unk> @ ali per vedere che i t<unk> @ ali di tre milioni di anni , per la Gr<unk> @ an B<unk> @ is<unk> @ k<unk> @ i<unk> @ ana , per tre milioni di anni , per il 40 , 40 % di più .
2025-05-28 09:32:52,724 - INFO - joeynmt.training - Example #1
2025-05-28 09:32:52,724 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:32:52,724 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:32:52,724 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'difficile', 'da', 'fare', 'abbastanza', 'la', 'nostra', 'immagin@@', '<unk>', '@', 'azione', 'di', 'questo', 'problema', ',', 'che', 'non', 'è', 'abbastanza', 'in@@', '<unk>', '@', 'vi@@', '<unk>', '@', 'dente', ',', 'non', 'è', 'il', 'D@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ke', 'del', 'D@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ke', '.', '</s>']
2025-05-28 09:32:52,724 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:32:52,725 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:32:52,725 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza difficile da fare abbastanza la nostra immagin<unk> @ azione di questo problema , che non è abbastanza in<unk> @ vi<unk> @ dente , non è il D<unk> @ ic<unk> @ ke del D<unk> @ ic<unk> @ ke .
2025-05-28 09:32:52,725 - INFO - joeynmt.training - Example #2
2025-05-28 09:32:52,725 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:32:52,725 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:32:52,725 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'qualche', 'certo', 'senso', ',', 'il', 'cuore', 'è', 'il', 'cuore', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'di', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ana', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'globale', '.', '</s>']
2025-05-28 09:32:52,725 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:32:52,725 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:32:52,726 - INFO - joeynmt.training - 	Hypothesis: In qualche certo senso , il cuore è il cuore ar<unk> @ t<unk> @ ico di E<unk> @ is<unk> @ k<unk> @ i<unk> @ ana , il cuore del nostro sistema globale .
2025-05-28 09:32:52,726 - INFO - joeynmt.training - Example #3
2025-05-28 09:32:52,726 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:32:52,726 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:32:52,726 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'tratta', 'di', 'un', 'po', '&apos;', 'e', 'di', 'più', '.', '</s>']
2025-05-28 09:32:52,726 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:32:52,726 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:32:52,726 - INFO - joeynmt.training - 	Hypothesis: Si tratta di un po &apos; e di più .
2025-05-28 09:32:52,726 - INFO - joeynmt.training - Example #4
2025-05-28 09:32:52,727 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:32:52,727 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:32:52,727 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', ',', 'è', 'successo', 'in', 'questo', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:32:52,727 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:32:52,727 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:32:52,727 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va , è successo in questo ultimi 25 anni .
2025-05-28 09:32:56,149 - INFO - joeynmt.training - Epoch   4, Step:    19600, Batch Loss:     1.270109, Batch Acc: 0.635913, Tokens per Sec:    18333, Lr: 0.000300
2025-05-28 09:32:59,550 - INFO - joeynmt.training - Epoch   4, Step:    19700, Batch Loss:     1.235824, Batch Acc: 0.632635, Tokens per Sec:    21096, Lr: 0.000300
2025-05-28 09:33:02,933 - INFO - joeynmt.training - Epoch   4, Step:    19800, Batch Loss:     1.305333, Batch Acc: 0.634418, Tokens per Sec:    20526, Lr: 0.000300
2025-05-28 09:33:06,331 - INFO - joeynmt.training - Epoch   4, Step:    19900, Batch Loss:     1.347620, Batch Acc: 0.631752, Tokens per Sec:    21000, Lr: 0.000300
2025-05-28 09:33:09,732 - INFO - joeynmt.training - Epoch   4, Step:    20000, Batch Loss:     1.277833, Batch Acc: 0.633249, Tokens per Sec:    21131, Lr: 0.000300
2025-05-28 09:33:09,733 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:33:09,733 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:33:20,762 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.39, ppl:   4.03, acc:   0.63, generation: 11.0224[sec], evaluation: 0.0000[sec]
2025-05-28 09:33:20,763 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:33:21,374 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/17500.ckpt
2025-05-28 09:33:21,404 - INFO - joeynmt.training - Example #0
2025-05-28 09:33:21,404 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:33:21,404 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:33:21,404 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', ',', 'per', 'ri@@', '<unk>', '@', 'pro@@', '<unk>', '@', 'durre', 'la', 's@@', '<unk>', '@', 'edia', ',', 'per', 'la', 'c@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'atric@@', '<unk>', '@', 'e', ',', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'la', 'prima', 'volta', 'che', 'il', '40', 'milioni', 'di', 'anni', ',', 'per', 'la', 'prima', 'volta', 'che', 'il', '40', 'per', 'cento', 'di', 'tutti', 'i', 'tre', 'anni', ',', 'per', 'il', '40', ',', 'per', 'cento', 'degli', 'Stati', 'Uniti', ',', 'per', 'cento', 'degli', 'Stati', 'Uniti', ',', 'per', 'il', '40', '.', '</s>']
2025-05-28 09:33:21,405 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:33:21,405 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:33:21,405 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato queste due , per ri<unk> @ pro<unk> @ durre la s<unk> @ edia , per la c<unk> @ ic<unk> @ atric<unk> @ e , per tre milioni di anni , per la prima volta che il 40 milioni di anni , per la prima volta che il 40 per cento di tutti i tre anni , per il 40 , per cento degli Stati Uniti , per cento degli Stati Uniti , per il 40 .
2025-05-28 09:33:21,405 - INFO - joeynmt.training - Example #1
2025-05-28 09:33:21,405 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:33:21,405 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:33:21,406 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', ',', 'abbastanza', ',', 'la', 'con@@', '<unk>', '@', 'sap@@', '<unk>', '@', 'evol@@', '<unk>', '@', 'ezza', ',', 'la', 'con@@', '<unk>', '@', 'vers@@', '<unk>', '@', 'azione', 'di', 'questo', 'problema', ',', 'non', 'è', 'il', 'ter@@', '<unk>', '@', 'zo', 'dell&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 09:33:21,406 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:33:21,406 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:33:21,406 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza , abbastanza , la con<unk> @ sap<unk> @ evol<unk> @ ezza , la con<unk> @ vers<unk> @ azione di questo problema , non è il ter<unk> @ zo dell&apos; E<unk> @ is<unk> @ es .
2025-05-28 09:33:21,406 - INFO - joeynmt.training - Example #2
2025-05-28 09:33:21,406 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:33:21,407 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:33:21,407 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'qualche', 'certo', 'senso', ',', 'è', 'la', 'co@@', '<unk>', '@', 'per@@', '<unk>', '@', 'dita', 'di', 'os@@', '<unk>', '@', 'sa', ',', 'la', 'p@@', '<unk>', '@', 'ica', 'è', 'la', 'co@@', '<unk>', '@', 'per@@', '<unk>', '@', 'dita', 'di', 'c@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'izia', 'globale', '.', '</s>']
2025-05-28 09:33:21,407 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:33:21,407 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:33:21,407 - INFO - joeynmt.training - 	Hypothesis: In qualche certo senso , è la co<unk> @ per<unk> @ dita di os<unk> @ sa , la p<unk> @ ica è la co<unk> @ per<unk> @ dita di c<unk> @ ic<unk> @ izia globale .
2025-05-28 09:33:21,407 - INFO - joeynmt.training - Example #3
2025-05-28 09:33:21,407 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:33:21,407 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:33:21,408 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&apos;', 'cres@@', '<unk>', '@', 'ciuto', 'nel', 'vento', 'e', 'si', 'ri@@', '<unk>', '@', 'fi@@', '<unk>', '@', 'ut@@', '<unk>', '@', 'ta', 'nel', 'vento', '.', '</s>']
2025-05-28 09:33:21,408 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:33:21,408 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:33:21,408 - INFO - joeynmt.training - 	Hypothesis: E &apos; cres<unk> @ ciuto nel vento e si ri<unk> @ fi<unk> @ ut<unk> @ ta nel vento .
2025-05-28 09:33:21,408 - INFO - joeynmt.training - Example #4
2025-05-28 09:33:21,408 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:33:21,408 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:33:21,409 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', ',', 'la', 'prossi@@', '<unk>', '@', 'ma', ',', 'è', 'un', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'no', 'di', 'un', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'no', 'di', 'quello', 'che', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:33:21,409 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:33:21,409 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:33:21,409 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma , la prossi<unk> @ ma , è un di<unk> @ seg<unk> @ no di un di<unk> @ seg<unk> @ no di quello che negli ultimi 25 anni .
2025-05-28 09:33:24,852 - INFO - joeynmt.training - Epoch   4, Step:    20100, Batch Loss:     1.354941, Batch Acc: 0.631365, Tokens per Sec:    17567, Lr: 0.000300
2025-05-28 09:33:28,257 - INFO - joeynmt.training - Epoch   4, Step:    20200, Batch Loss:     1.320505, Batch Acc: 0.634549, Tokens per Sec:    20306, Lr: 0.000300
2025-05-28 09:33:31,653 - INFO - joeynmt.training - Epoch   4, Step:    20300, Batch Loss:     1.231894, Batch Acc: 0.637796, Tokens per Sec:    21469, Lr: 0.000300
2025-05-28 09:33:35,064 - INFO - joeynmt.training - Epoch   4, Step:    20400, Batch Loss:     1.226834, Batch Acc: 0.638044, Tokens per Sec:    20911, Lr: 0.000300
2025-05-28 09:33:38,456 - INFO - joeynmt.training - Epoch   4, Step:    20500, Batch Loss:     1.377327, Batch Acc: 0.639257, Tokens per Sec:    21286, Lr: 0.000300
2025-05-28 09:33:38,456 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:33:38,456 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:33:48,804 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.39, ppl:   4.02, acc:   0.63, generation: 10.3372[sec], evaluation: 0.0000[sec]
2025-05-28 09:33:48,804 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:33:49,434 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/18000.ckpt
2025-05-28 09:33:49,461 - INFO - joeynmt.training - Example #0
2025-05-28 09:33:49,462 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:33:49,462 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:33:49,462 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'questa', 'due', 'due', 'per@@', '<unk>', '@', 'cento', 'di', 'questi', 'due', 'anni', 'per', 'cercare', 'di', 'osserv@@', '<unk>', '@', 'are', 'che', 'la', 'gr@@', '<unk>', '@', 'at@@', '<unk>', '@', 'ola', 'che', 'i', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ani', 'per', 'la', 'maggior', 'parte', 'dei', 'tre', 'milioni', 'di', 'anni', ',', 'che', 'ha', 'fatto', 'il', '4@@', '<unk>', '@', '8', '%', 'della', 'Gr@@', '<unk>', '@', 'um@@', '<unk>', '@', 'p@@', '<unk>', '@', 'a', '.', '</s>']
2025-05-28 09:33:49,463 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:33:49,463 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:33:49,463 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato questa due due per<unk> @ cento di questi due anni per cercare di osserv<unk> @ are che la gr<unk> @ at<unk> @ ola che i t<unk> @ ic<unk> @ ani per la maggior parte dei tre milioni di anni , che ha fatto il 4<unk> @ 8 % della Gr<unk> @ um<unk> @ p<unk> @ a .
2025-05-28 09:33:49,463 - INFO - joeynmt.training - Example #1
2025-05-28 09:33:49,463 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:33:49,463 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:33:49,463 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'forte', 'abbastanza', 'forte', 'per', 'la', 'prima', 'cosa', 'che', 'è', 'che', 'il', 'problema', 'di', 'questo', 'problema', ',', 'non', 'è', 'abbastanza', 'speci@@', '<unk>', '@', 'alizz@@', '<unk>', '@', 'ato', 'da', 'questo', 'problema', ',', 'non', 'è', 'il', 'D@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ke', 'del', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 09:33:49,464 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:33:49,464 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:33:49,464 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte abbastanza forte per la prima cosa che è che il problema di questo problema , non è abbastanza speci<unk> @ alizz<unk> @ ato da questo problema , non è il D<unk> @ ic<unk> @ ke del ghi<unk> @ accio .
2025-05-28 09:33:49,464 - INFO - joeynmt.training - Example #2
2025-05-28 09:33:49,464 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:33:49,464 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:33:49,464 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'certo', 'certo', 'senso', ',', 'la', 'cosa', 'più', 'profon@@', '<unk>', '@', 'da', 'è', 'il', 'ghi@@', '<unk>', '@', 'accio', 'fis@@', '<unk>', '@', 'ico', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'c@@', '<unk>', '@', 'ru@@', '<unk>', '@', 'olo', 'globale', '.', '</s>']
2025-05-28 09:33:49,465 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:33:49,465 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:33:49,465 - INFO - joeynmt.training - 	Hypothesis: In certo certo senso , la cosa più profon<unk> @ da è il ghi<unk> @ accio fis<unk> @ ico , il cuore del nostro sistema c<unk> @ ru<unk> @ olo globale .
2025-05-28 09:33:49,465 - INFO - joeynmt.training - Example #3
2025-05-28 09:33:49,465 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:33:49,465 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:33:49,465 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&apos;', 'un', 'in@@', '<unk>', '@', 'ver@@', '<unk>', '@', 'no', 'e', 'si', 's@@', '<unk>', '@', 'ov@@', '<unk>', '@', 'a', 'in', 'S@@', '<unk>', '@', 'om@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 09:33:49,466 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:33:49,466 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:33:49,466 - INFO - joeynmt.training - 	Hypothesis: E &apos; un in<unk> @ ver<unk> @ no e si s<unk> @ ov<unk> @ a in S<unk> @ om<unk> @ ate .
2025-05-28 09:33:49,466 - INFO - joeynmt.training - Example #4
2025-05-28 09:33:49,466 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:33:49,466 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:33:49,466 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'consegu@@', '<unk>', '@', 'enza', 'che', 'vi', 'mostr@@', '<unk>', '@', 'o', 'è', 'una', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'ret@@', '<unk>', '@', 'a', 'che', 'negli', 'ultimi', '25', 'anni', 'è', 'acca@@', '<unk>', '@', 'duto', 'in', 'questo', '25', 'anni', '.', '</s>']
2025-05-28 09:33:49,466 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:33:49,466 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:33:49,466 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma consegu<unk> @ enza che vi mostr<unk> @ o è una di<unk> @ seg<unk> @ ret<unk> @ a che negli ultimi 25 anni è acca<unk> @ duto in questo 25 anni .
2025-05-28 09:33:52,888 - INFO - joeynmt.training - Epoch   4, Step:    20600, Batch Loss:     1.581508, Batch Acc: 0.636623, Tokens per Sec:    17768, Lr: 0.000300
2025-05-28 09:33:56,315 - INFO - joeynmt.training - Epoch   4, Step:    20700, Batch Loss:     1.258659, Batch Acc: 0.636970, Tokens per Sec:    21500, Lr: 0.000300
2025-05-28 09:33:59,746 - INFO - joeynmt.training - Epoch   4, Step:    20800, Batch Loss:     1.486968, Batch Acc: 0.636296, Tokens per Sec:    21194, Lr: 0.000300
2025-05-28 09:34:03,129 - INFO - joeynmt.training - Epoch   4, Step:    20900, Batch Loss:     1.289888, Batch Acc: 0.635608, Tokens per Sec:    21248, Lr: 0.000300
2025-05-28 09:34:06,535 - INFO - joeynmt.training - Epoch   4, Step:    21000, Batch Loss:     1.480905, Batch Acc: 0.630832, Tokens per Sec:    21177, Lr: 0.000300
2025-05-28 09:34:06,535 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:34:06,535 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:34:18,765 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.39, ppl:   4.01, acc:   0.63, generation: 12.2184[sec], evaluation: 0.0000[sec]
2025-05-28 09:34:18,766 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:34:19,371 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/18500.ckpt
2025-05-28 09:34:19,394 - INFO - joeynmt.training - Example #0
2025-05-28 09:34:19,394 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:34:19,394 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:34:19,394 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'questi', 'due', 'due', 'due', 'anni', 'per', 'ri@@', '<unk>', '@', 'ver@@', '<unk>', '@', 'si', 'che', 'la', 'p@@', '<unk>', '@', 'it@@', '<unk>', '@', 'ana', 'per', 'la', 'c@@', '<unk>', '@', 'accia', 'di', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'app@@', '<unk>', '@', 'e', 'per', 'la', 'maggior', 'parte', 'dei', 'tre', 'anni', ',', 'per', 'cento', ',', 'per', 'cento', 'dei', 'paesi', ',', 'per', 'cento', '.', '</s>']
2025-05-28 09:34:19,395 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:34:19,395 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:34:19,395 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato questi due due due anni per ri<unk> @ ver<unk> @ si che la p<unk> @ it<unk> @ ana per la c<unk> @ accia di E<unk> @ is<unk> @ k<unk> @ app<unk> @ e per la maggior parte dei tre anni , per cento , per cento dei paesi , per cento .
2025-05-28 09:34:19,395 - INFO - joeynmt.training - Example #1
2025-05-28 09:34:19,395 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:34:19,395 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:34:19,395 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', ',', 'che', 'non', 'è', 'abbastanza', 'per', 'l&apos;', 'in@@', '<unk>', '@', 'ver@@', '<unk>', '@', 'i@@', '<unk>', '@', 'fic@@', '<unk>', '@', 'azione', 'di', 'questo', 'problema', ',', 'non', 'è', 'che', 'non', 'è', 'il', 'vi@@', '<unk>', '@', 'll@@', '<unk>', '@', 'aggio', 'che', 'non', 'è', 'l&apos;', 'el@@', '<unk>', '@', 'im@@', '<unk>', '@', 'a', '.', '</s>']
2025-05-28 09:34:19,396 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:34:19,396 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:34:19,396 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza , che non è abbastanza per l&apos; in<unk> @ ver<unk> @ i<unk> @ fic<unk> @ azione di questo problema , non è che non è il vi<unk> @ ll<unk> @ aggio che non è l&apos; el<unk> @ im<unk> @ a .
2025-05-28 09:34:19,396 - INFO - joeynmt.training - Example #2
2025-05-28 09:34:19,396 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:34:19,396 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:34:19,396 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'è', 'il', 'cuore', 'di', 'un', 'eff@@', '<unk>', '@', 'etto', 't@@', '<unk>', '@', 'ico', 'di', 'ghi@@', '<unk>', '@', 'accio', 'globale', 'del', 'nostro', 'sistema', 'c@@', '<unk>', '@', 'ru@@', '<unk>', '@', 'ci@@', '<unk>', '@', 'ale', '.', '</s>']
2025-05-28 09:34:19,397 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:34:19,397 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:34:19,397 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , è il cuore di un eff<unk> @ etto t<unk> @ ico di ghi<unk> @ accio globale del nostro sistema c<unk> @ ru<unk> @ ci<unk> @ ale .
2025-05-28 09:34:19,397 - INFO - joeynmt.training - Example #3
2025-05-28 09:34:19,397 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:34:19,397 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:34:19,397 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', '<unk>', '@', 'ei', 'cres@@', '<unk>', '@', 'ci@@', '<unk>', '@', 'uta', 'in', 'un', 'vento', 'e', 'gi@@', '<unk>', '@', 'ù', '.', '</s>']
2025-05-28 09:34:19,398 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:34:19,398 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:34:19,398 - INFO - joeynmt.training - 	Hypothesis: L<unk> @ ei cres<unk> @ ci<unk> @ uta in un vento e gi<unk> @ ù .
2025-05-28 09:34:19,398 - INFO - joeynmt.training - Example #4
2025-05-28 09:34:19,398 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:34:19,398 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:34:19,398 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', ',', 'la', 'prossi@@', '<unk>', '@', 'ma', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'un', 'seg@@', '<unk>', '@', 'no', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:34:19,398 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:34:19,398 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:34:19,398 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma , la prossi<unk> @ ma che vi mostr<unk> @ erò un seg<unk> @ no è successo negli ultimi 25 anni .
2025-05-28 09:34:22,827 - INFO - joeynmt.training - Epoch   4, Step:    21100, Batch Loss:     1.285573, Batch Acc: 0.635183, Tokens per Sec:    17955, Lr: 0.000300
2025-05-28 09:34:26,247 - INFO - joeynmt.training - Epoch   4, Step:    21200, Batch Loss:     1.411378, Batch Acc: 0.637753, Tokens per Sec:    21048, Lr: 0.000300
2025-05-28 09:34:29,637 - INFO - joeynmt.training - Epoch   4, Step:    21300, Batch Loss:     1.317147, Batch Acc: 0.639177, Tokens per Sec:    21726, Lr: 0.000300
2025-05-28 09:34:33,016 - INFO - joeynmt.training - Epoch   4, Step:    21400, Batch Loss:     1.345841, Batch Acc: 0.635893, Tokens per Sec:    20772, Lr: 0.000300
2025-05-28 09:34:36,397 - INFO - joeynmt.training - Epoch   4, Step:    21500, Batch Loss:     1.400891, Batch Acc: 0.633029, Tokens per Sec:    21148, Lr: 0.000300
2025-05-28 09:34:36,397 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:34:36,397 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:34:46,501 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.38, ppl:   3.97, acc:   0.63, generation: 10.0939[sec], evaluation: 0.0000[sec]
2025-05-28 09:34:46,502 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:34:47,110 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/19500.ckpt
2025-05-28 09:34:47,135 - INFO - joeynmt.training - Example #0
2025-05-28 09:34:47,135 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:34:47,135 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:34:47,135 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'questa', 'due', 'due', 'due', 'anni', 'per', 'ri@@', '<unk>', '@', 'pro@@', '<unk>', '@', 'durre', 'per', 'ri@@', '<unk>', '@', 'durre', 'il', 't@@', '<unk>', '@', 'avolo', 'di', 'p@@', '<unk>', '@', 'ezzi', 'di', 'questi', 'due', 'anni', ',', 'che', 'sono', 'stati', 'stati', 's@@', '<unk>', '@', 'ot@@', '<unk>', '@', 'ici', 'per', 'la', 'maggior', 'parte', 'dei', 'tre', 'anni', 'di', 'questi', 'sono', 'stati', 'stati', 'stati', 'stati', '.', '</s>']
2025-05-28 09:34:47,136 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:34:47,136 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:34:47,136 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato questa due due due anni per ri<unk> @ pro<unk> @ durre per ri<unk> @ durre il t<unk> @ avolo di p<unk> @ ezzi di questi due anni , che sono stati stati s<unk> @ ot<unk> @ ici per la maggior parte dei tre anni di questi sono stati stati stati stati .
2025-05-28 09:34:47,136 - INFO - joeynmt.training - Example #1
2025-05-28 09:34:47,136 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:34:47,137 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:34:47,137 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'difficile', 'da', 'fare', 'abbastanza', 'la', 'st@@', '<unk>', '@', 'anza', 'di', 'questo', 'problema', ',', 'che', 'non', 'è', 'abbastanza', 'la', 'min@@', '<unk>', '@', 'im@@', '<unk>', '@', 'a', 'di', 'questo', 'problema', ',', 'non', 'è', 'la', 'de@@', '<unk>', '@', 'st@@', '<unk>', '@', 'izia', '.', '</s>']
2025-05-28 09:34:47,137 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:34:47,137 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:34:47,137 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza difficile da fare abbastanza la st<unk> @ anza di questo problema , che non è abbastanza la min<unk> @ im<unk> @ a di questo problema , non è la de<unk> @ st<unk> @ izia .
2025-05-28 09:34:47,137 - INFO - joeynmt.training - Example #2
2025-05-28 09:34:47,138 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:34:47,138 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:34:47,138 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'qualche', 'qualche', 'certo', 'senso', ',', 'è', 'la', 'c@@', '<unk>', '@', 'ena', 'di', 'ghi@@', '<unk>', '@', 'accio', ',', 'il', 'nostro', 'cuore', 'del', 'nostro', 'sistema', 'c@@', '<unk>', '@', 'ru@@', '<unk>', '@', 'olo', 'globale', '.', '</s>']
2025-05-28 09:34:47,138 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:34:47,138 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:34:47,138 - INFO - joeynmt.training - 	Hypothesis: In qualche qualche certo senso , è la c<unk> @ ena di ghi<unk> @ accio , il nostro cuore del nostro sistema c<unk> @ ru<unk> @ olo globale .
2025-05-28 09:34:47,138 - INFO - joeynmt.training - Example #3
2025-05-28 09:34:47,138 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:34:47,139 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:34:47,139 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&apos;', 'cres@@', '<unk>', '@', 'ce', 'e', 'si', 'è', 'in', 'in@@', '<unk>', '@', 'ver@@', '<unk>', '@', 'no', '.', '</s>']
2025-05-28 09:34:47,139 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:34:47,139 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:34:47,139 - INFO - joeynmt.training - 	Hypothesis: E &apos; cres<unk> @ ce e si è in in<unk> @ ver<unk> @ no .
2025-05-28 09:34:47,139 - INFO - joeynmt.training - Example #4
2025-05-28 09:34:47,139 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:34:47,139 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:34:47,139 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'una', 'disc@@', '<unk>', '@', 'us@@', '<unk>', '@', 'sione', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:34:47,140 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:34:47,140 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:34:47,140 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ erò è una disc<unk> @ us<unk> @ sione che è successo negli ultimi 25 anni .
2025-05-28 09:34:50,577 - INFO - joeynmt.training - Epoch   4, Step:    21600, Batch Loss:     1.209163, Batch Acc: 0.636480, Tokens per Sec:    17771, Lr: 0.000300
2025-05-28 09:34:53,983 - INFO - joeynmt.training - Epoch   4, Step:    21700, Batch Loss:     1.310232, Batch Acc: 0.633900, Tokens per Sec:    21024, Lr: 0.000300
2025-05-28 09:34:57,389 - INFO - joeynmt.training - Epoch   4, Step:    21800, Batch Loss:     1.233280, Batch Acc: 0.642930, Tokens per Sec:    21741, Lr: 0.000300
2025-05-28 09:35:00,761 - INFO - joeynmt.training - Epoch   4, Step:    21900, Batch Loss:     1.344427, Batch Acc: 0.633413, Tokens per Sec:    20873, Lr: 0.000300
2025-05-28 09:35:04,181 - INFO - joeynmt.training - Epoch   4, Step:    22000, Batch Loss:     1.301807, Batch Acc: 0.636536, Tokens per Sec:    21253, Lr: 0.000300
2025-05-28 09:35:04,182 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:35:04,182 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:35:14,507 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.37, ppl:   3.95, acc:   0.63, generation: 10.3148[sec], evaluation: 0.0000[sec]
2025-05-28 09:35:14,508 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:35:15,111 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/19000.ckpt
2025-05-28 09:35:15,139 - INFO - joeynmt.training - Example #0
2025-05-28 09:35:15,139 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:35:15,139 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:35:15,140 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'vedere', 'che', 'il', 't@@', '<unk>', '@', 'asso', 'di', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'ab', ',', 'che', 'il', 'che', 'il', 't@@', '<unk>', '@', 'asso', 'di', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'ab', ',', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'che', 'ha', 'avuto', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'la', 'Gr@@', '<unk>', '@', 'an', 'B@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'ab', '.', '</s>']
2025-05-28 09:35:15,140 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:35:15,140 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:35:15,140 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato queste due due di<unk> @ apos<unk> @ iti<unk> @ ve per vedere che il t<unk> @ asso di E<unk> @ is<unk> @ k<unk> @ ab , che il che il t<unk> @ asso di E<unk> @ is<unk> @ k<unk> @ ab , per tre milioni di anni , che ha avuto tre milioni di anni , per la Gr<unk> @ an B<unk> @ is<unk> @ k<unk> @ ab .
2025-05-28 09:35:15,140 - INFO - joeynmt.training - Example #1
2025-05-28 09:35:15,141 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:35:15,141 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:35:15,141 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'forte', 'che', 'la', 'nostra', 'not@@', '<unk>', '@', 'izia', 'è', 'abbastanza', 'per', 'la', 'Terra', ',', 'perché', 'non', 'è', 'l&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', 'mostra', 'il', 'problema', 'di', 'cui', 'non', 'è', 'l&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', 'mostra', '.', '</s>']
2025-05-28 09:35:15,141 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:35:15,141 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:35:15,141 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza forte che la nostra not<unk> @ izia è abbastanza per la Terra , perché non è l&apos; E<unk> @ is<unk> @ es mostra il problema di cui non è l&apos; E<unk> @ is<unk> @ es mostra .
2025-05-28 09:35:15,141 - INFO - joeynmt.training - Example #2
2025-05-28 09:35:15,142 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:35:15,142 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:35:15,142 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'qualche', 'modo', ',', 'il', 'cuore', 'è', 'il', 'cuore', 'del', 'nostro', 'sistema', 'c@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ale', '.', '</s>']
2025-05-28 09:35:15,142 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:35:15,142 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:35:15,142 - INFO - joeynmt.training - 	Hypothesis: In qualche modo , il cuore è il cuore del nostro sistema c<unk> @ ic<unk> @ ale .
2025-05-28 09:35:15,142 - INFO - joeynmt.training - Example #3
2025-05-28 09:35:15,142 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:35:15,143 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:35:15,143 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'è', 'cres@@', '<unk>', '@', 'ce', 'in', 'un', 'ang@@', '<unk>', '@', 'olo', 'e', 'il', 'p@@', '<unk>', '@', 'om@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 09:35:15,143 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:35:15,143 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:35:15,143 - INFO - joeynmt.training - 	Hypothesis: Si è cres<unk> @ ce in un ang<unk> @ olo e il p<unk> @ om<unk> @ ate .
2025-05-28 09:35:15,143 - INFO - joeynmt.training - Example #4
2025-05-28 09:35:15,143 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:35:15,144 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:35:15,144 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', ',', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', ',', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:35:15,144 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:35:15,144 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:35:15,144 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va , che vi mostr<unk> @ erò , che è successo negli ultimi 25 anni .
2025-05-28 09:35:18,560 - INFO - joeynmt.training - Epoch   4, Step:    22100, Batch Loss:     1.320048, Batch Acc: 0.634809, Tokens per Sec:    17408, Lr: 0.000300
2025-05-28 09:35:21,973 - INFO - joeynmt.training - Epoch   4, Step:    22200, Batch Loss:     1.287270, Batch Acc: 0.630892, Tokens per Sec:    21298, Lr: 0.000300
2025-05-28 09:35:25,336 - INFO - joeynmt.training - Epoch   4, Step:    22300, Batch Loss:     1.412062, Batch Acc: 0.637253, Tokens per Sec:    21100, Lr: 0.000300
2025-05-28 09:35:28,699 - INFO - joeynmt.training - Epoch   4, Step:    22400, Batch Loss:     1.308422, Batch Acc: 0.637576, Tokens per Sec:    20717, Lr: 0.000300
2025-05-28 09:35:32,100 - INFO - joeynmt.training - Epoch   4, Step:    22500, Batch Loss:     1.290442, Batch Acc: 0.638111, Tokens per Sec:    21047, Lr: 0.000300
2025-05-28 09:35:32,101 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:35:32,101 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:35:42,616 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.36, ppl:   3.92, acc:   0.63, generation: 10.5055[sec], evaluation: 0.0000[sec]
2025-05-28 09:35:42,617 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:35:43,205 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/20000.ckpt
2025-05-28 09:35:43,233 - INFO - joeynmt.training - Example #0
2025-05-28 09:35:43,233 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:35:43,233 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:35:43,233 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'vedere', 'che', 'i', 't@@', '<unk>', '@', 'av@@', '<unk>', '@', 'vi@@', '<unk>', '@', 'are', 'i', 't@@', '<unk>', '@', 'um@@', '<unk>', '@', 'ori', 'di', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'aggi', 'di', 'tre', 'milioni', 'di', 'anni', ',', 'che', 'ha', 'avuto', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'la', 'maggior', 'parte', 'dei', 'tre', 'anni', ',', 'per', 'cento', '.', '</s>']
2025-05-28 09:35:43,234 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:35:43,234 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:35:43,234 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per vedere che i t<unk> @ av<unk> @ vi<unk> @ are i t<unk> @ um<unk> @ ori di c<unk> @ ant<unk> @ aggi di tre milioni di anni , che ha avuto tre milioni di anni , per la maggior parte dei tre anni , per cento .
2025-05-28 09:35:43,234 - INFO - joeynmt.training - Example #1
2025-05-28 09:35:43,234 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:35:43,234 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:35:43,235 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'difficile', 'da', 'fare', 'la', 'nostra', 'immagin@@', '<unk>', '@', 'azione', 'di', 'questo', 'problema', 'speci@@', '<unk>', '@', 'ale', ',', 'perché', 'non', 'è', 'il', 't@@', '<unk>', '@', 'asso', 'di', 'el@@', '<unk>', '@', 'im@@', '<unk>', '@', 'in@@', '<unk>', '@', 'ità', '.', '</s>']
2025-05-28 09:35:43,235 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:35:43,235 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:35:43,235 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza difficile da fare la nostra immagin<unk> @ azione di questo problema speci<unk> @ ale , perché non è il t<unk> @ asso di el<unk> @ im<unk> @ in<unk> @ ità .
2025-05-28 09:35:43,235 - INFO - joeynmt.training - Example #2
2025-05-28 09:35:43,235 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:35:43,236 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:35:43,236 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'qualche', 'certo', 'senso', ',', 'la', 'c@@', '<unk>', '@', 'lasse', 'è', 'il', 't@@', '<unk>', '@', 'asso', 'di', 'ghi@@', '<unk>', '@', 'accio', 'globale', '.', '</s>']
2025-05-28 09:35:43,236 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:35:43,236 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:35:43,236 - INFO - joeynmt.training - 	Hypothesis: In qualche certo senso , la c<unk> @ lasse è il t<unk> @ asso di ghi<unk> @ accio globale .
2025-05-28 09:35:43,236 - INFO - joeynmt.training - Example #3
2025-05-28 09:35:43,236 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:35:43,236 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:35:43,237 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'en@@', '<unk>', '@', 'gono', 'in', 'un', 'in@@', '<unk>', '@', 'ver@@', '<unk>', '@', 'no', 'e', 's@@', '<unk>', '@', 'om@@', '<unk>', '@', 'p@@', '<unk>', '@', 'a', '.', '</s>']
2025-05-28 09:35:43,237 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:35:43,237 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:35:43,237 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ en<unk> @ gono in un in<unk> @ ver<unk> @ no e s<unk> @ om<unk> @ p<unk> @ a .
2025-05-28 09:35:43,237 - INFO - joeynmt.training - Example #4
2025-05-28 09:35:43,237 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:35:43,237 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:35:43,237 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'cosa', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:35:43,238 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:35:43,238 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:35:43,238 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ erò è un seg<unk> @ no di cosa negli ultimi 25 anni .
2025-05-28 09:35:46,666 - INFO - joeynmt.training - Epoch   4, Step:    22600, Batch Loss:     1.392711, Batch Acc: 0.632526, Tokens per Sec:    17810, Lr: 0.000300
2025-05-28 09:35:50,064 - INFO - joeynmt.training - Epoch   4, Step:    22700, Batch Loss:     1.332871, Batch Acc: 0.635858, Tokens per Sec:    20571, Lr: 0.000300
2025-05-28 09:35:53,482 - INFO - joeynmt.training - Epoch   4, Step:    22800, Batch Loss:     1.325504, Batch Acc: 0.637791, Tokens per Sec:    21621, Lr: 0.000300
2025-05-28 09:35:56,874 - INFO - joeynmt.training - Epoch   4, Step:    22900, Batch Loss:     1.296274, Batch Acc: 0.636697, Tokens per Sec:    21132, Lr: 0.000300
2025-05-28 09:36:00,263 - INFO - joeynmt.training - Epoch   4, Step:    23000, Batch Loss:     1.377342, Batch Acc: 0.634758, Tokens per Sec:    20836, Lr: 0.000300
2025-05-28 09:36:00,263 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:36:00,263 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:36:09,947 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.36, ppl:   3.91, acc:   0.63, generation: 9.6764[sec], evaluation: 0.0000[sec]
2025-05-28 09:36:09,947 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:36:10,490 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/20500.ckpt
2025-05-28 09:36:10,515 - INFO - joeynmt.training - Example #0
2025-05-28 09:36:10,515 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:36:10,516 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:36:10,516 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'questi', 'due', 'due', 'di@@', '<unk>', '@', 'ci', 'per', 'ri@@', '<unk>', '@', 'durre', 'il', '4@@', '<unk>', '@', '8', '%', 'per', 'il', '4@@', '<unk>', '@', '8', 'anni', ',', 'che', 'i', 'c@@', '<unk>', '@', 'ani', 'sono', 'i', 'c@@', '<unk>', '@', 'avi', 'per', 'la', 'con@@', '<unk>', '@', 'vers@@', '<unk>', '@', 'azione', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'per', 'tre', 'anni', ',', 'il', '40', '%', '.', '</s>']
2025-05-28 09:36:10,516 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:36:10,516 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:36:10,516 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato questi due due di<unk> @ ci per ri<unk> @ durre il 4<unk> @ 8 % per il 4<unk> @ 8 anni , che i c<unk> @ ani sono i c<unk> @ avi per la con<unk> @ vers<unk> @ azione ar<unk> @ t<unk> @ ica per tre anni , il 40 % .
2025-05-28 09:36:10,516 - INFO - joeynmt.training - Example #1
2025-05-28 09:36:10,516 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:36:10,517 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:36:10,517 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'difficile', 'da', 'fare', 'il', 'problema', 'della', 'st@@', '<unk>', '@', 'izia', ',', 'che', 'non', 'è', 'più', 'il', 'problema', 'di', 'questo', 'problema', ',', 'non', 'è', 'il', 'problema', 'del', 'problema', ',', 'non', 'è', 'che', 'non', 'è', 'il', 'di@@', '<unk>', '@', 'mostra', 'il', 'problema', 'dell&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 09:36:10,517 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:36:10,517 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:36:10,517 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza difficile da fare il problema della st<unk> @ izia , che non è più il problema di questo problema , non è il problema del problema , non è che non è il di<unk> @ mostra il problema dell&apos; E<unk> @ is<unk> @ es .
2025-05-28 09:36:10,517 - INFO - joeynmt.training - Example #2
2025-05-28 09:36:10,518 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:36:10,518 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:36:10,518 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'qualche', 'certo', 'senso', ',', 'è', 'la', 'c@@', '<unk>', '@', 'att@@', '<unk>', '@', 'uale', ',', 'è', 'il', 'cuore', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'del', 'nostro', 'sistema', 'c@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'lo', 'del', 'nostro', 'sistema', 'c@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'lo', '.', '</s>']
2025-05-28 09:36:10,518 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:36:10,518 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:36:10,518 - INFO - joeynmt.training - 	Hypothesis: In qualche certo senso , è la c<unk> @ att<unk> @ uale , è il cuore ar<unk> @ t<unk> @ ico del nostro sistema c<unk> @ ic<unk> @ lo del nostro sistema c<unk> @ ic<unk> @ lo .
2025-05-28 09:36:10,518 - INFO - joeynmt.training - Example #3
2025-05-28 09:36:10,518 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:36:10,519 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:36:10,519 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', '@', 'ambi@@', '<unk>', '@', 'ano', 'in', 'vento', 'e', 'lo', 's@@', '<unk>', '@', 'fondo', '.', '</s>']
2025-05-28 09:36:10,519 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:36:10,519 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:36:10,519 - INFO - joeynmt.training - 	Hypothesis: C<unk> @ ambi<unk> @ ano in vento e lo s<unk> @ fondo .
2025-05-28 09:36:10,519 - INFO - joeynmt.training - Example #4
2025-05-28 09:36:10,519 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:36:10,519 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:36:10,519 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', ',', 'è', 'una', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'ret@@', '<unk>', '@', 'a', 'che', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:36:10,520 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:36:10,520 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:36:10,520 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va , è una di<unk> @ seg<unk> @ ret<unk> @ a che negli ultimi 25 anni .
2025-05-28 09:36:13,928 - INFO - joeynmt.training - Epoch   4, Step:    23100, Batch Loss:     1.310335, Batch Acc: 0.634728, Tokens per Sec:    18155, Lr: 0.000300
2025-05-28 09:36:17,330 - INFO - joeynmt.training - Epoch   4, Step:    23200, Batch Loss:     1.359983, Batch Acc: 0.635843, Tokens per Sec:    21363, Lr: 0.000300
2025-05-28 09:36:20,746 - INFO - joeynmt.training - Epoch   4, Step:    23300, Batch Loss:     1.277481, Batch Acc: 0.640384, Tokens per Sec:    21537, Lr: 0.000300
2025-05-28 09:36:24,153 - INFO - joeynmt.training - Epoch   4, Step:    23400, Batch Loss:     1.348072, Batch Acc: 0.637389, Tokens per Sec:    20940, Lr: 0.000300
2025-05-28 09:36:27,580 - INFO - joeynmt.training - Epoch   4, Step:    23500, Batch Loss:     1.311506, Batch Acc: 0.637525, Tokens per Sec:    19903, Lr: 0.000300
2025-05-28 09:36:27,580 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:36:27,581 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:36:39,176 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.36, ppl:   3.89, acc:   0.63, generation: 11.5844[sec], evaluation: 0.0000[sec]
2025-05-28 09:36:39,177 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:36:39,811 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/21000.ckpt
2025-05-28 09:36:39,838 - INFO - joeynmt.training - Example #0
2025-05-28 09:36:39,838 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:36:39,838 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:36:39,838 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'questi', 'due', 'due', 'di@@', '<unk>', '@', 'ci', 'per', 'ri@@', '<unk>', '@', 'durre', 'il', '40', '%', 'di', 'petrolio', ',', 'che', 'i', 'ghi@@', '<unk>', '@', 'acci@@', '<unk>', '@', 'aio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', ',', 'che', 'i', 'c@@', '<unk>', '@', 'att@@', '<unk>', '@', 'ori', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ici', ',', 'che', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'il', '4@@', '<unk>', '@', '8', '%', 'degli', 'stati', ',', 'per', 'il', '40', '%', 'dei', 'paesi', 'che', 'hanno', 'fatto', 'il', '40', '%', '.', '</s>']
2025-05-28 09:36:39,839 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:36:39,839 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:36:39,839 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato questi due due di<unk> @ ci per ri<unk> @ durre il 40 % di petrolio , che i ghi<unk> @ acci<unk> @ aio ar<unk> @ t<unk> @ ico , che i c<unk> @ att<unk> @ ori ar<unk> @ t<unk> @ ici , che per tre milioni di anni , il 4<unk> @ 8 % degli stati , per il 40 % dei paesi che hanno fatto il 40 % .
2025-05-28 09:36:39,839 - INFO - joeynmt.training - Example #1
2025-05-28 09:36:39,840 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:36:39,840 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:36:39,840 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'difficile', 'da', 'fare', 'la', 'st@@', '<unk>', '@', 'izia', ',', 'non', 'è', 'abbastanza', 'est@@', '<unk>', '@', 'i@@', '<unk>', '@', 'os@@', '<unk>', '@', 'ità', 'di', 'questo', 'problema', ',', 'non', 'è', 'il', 'dop@@', '<unk>', '@', 'pio', 'della', 'ghi@@', '<unk>', '@', 'acci@@', '<unk>', '@', 'aio', '.', '</s>']
2025-05-28 09:36:39,840 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:36:39,840 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:36:39,840 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza difficile da fare la st<unk> @ izia , non è abbastanza est<unk> @ i<unk> @ os<unk> @ ità di questo problema , non è il dop<unk> @ pio della ghi<unk> @ acci<unk> @ aio .
2025-05-28 09:36:39,840 - INFO - joeynmt.training - Example #2
2025-05-28 09:36:39,841 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:36:39,841 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:36:39,841 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'è', 'il', 'cuore', 'di', 'un', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', ',', 'il', 'cuore', 'del', 'cuore', 'del', 'nostro', 'c@@', '<unk>', '@', 'ru@@', '<unk>', '@', 'g@@', '<unk>', '@', 'olo', 'globale', '.', '</s>']
2025-05-28 09:36:39,841 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:36:39,841 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:36:39,841 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , è il cuore di un ghi<unk> @ accio ar<unk> @ t<unk> @ ico , il cuore del cuore del nostro c<unk> @ ru<unk> @ g<unk> @ olo globale .
2025-05-28 09:36:39,841 - INFO - joeynmt.training - Example #3
2025-05-28 09:36:39,842 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:36:39,842 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:36:39,842 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'è', 'ri@@', '<unk>', '@', 'fer@@', '<unk>', '@', 'ma', ',', 'in', 'un', 'in@@', '<unk>', '@', 'ver@@', '<unk>', '@', 'no', '.', '</s>']
2025-05-28 09:36:39,842 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:36:39,842 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:36:39,842 - INFO - joeynmt.training - 	Hypothesis: Si è ri<unk> @ fer<unk> @ ma , in un in<unk> @ ver<unk> @ no .
2025-05-28 09:36:39,842 - INFO - joeynmt.training - Example #4
2025-05-28 09:36:39,842 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:36:39,843 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:36:39,843 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'una', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'na', 'che', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:36:39,843 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:36:39,843 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:36:39,843 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ erò è una di<unk> @ seg<unk> @ na che negli ultimi 25 anni .
2025-05-28 09:36:43,277 - INFO - joeynmt.training - Epoch   4, Step:    23600, Batch Loss:     1.399955, Batch Acc: 0.639727, Tokens per Sec:    17636, Lr: 0.000300
2025-05-28 09:36:46,690 - INFO - joeynmt.training - Epoch   4, Step:    23700, Batch Loss:     1.319420, Batch Acc: 0.641681, Tokens per Sec:    21195, Lr: 0.000300
2025-05-28 09:36:50,084 - INFO - joeynmt.training - Epoch   4, Step:    23800, Batch Loss:     1.518388, Batch Acc: 0.636190, Tokens per Sec:    21235, Lr: 0.000300
2025-05-28 09:36:53,468 - INFO - joeynmt.training - Epoch   4, Step:    23900, Batch Loss:     1.429163, Batch Acc: 0.637411, Tokens per Sec:    20623, Lr: 0.000300
2025-05-28 09:36:56,849 - INFO - joeynmt.training - Epoch   4, Step:    24000, Batch Loss:     1.228798, Batch Acc: 0.639140, Tokens per Sec:    20617, Lr: 0.000300
2025-05-28 09:36:56,849 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:36:56,849 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:37:07,360 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.36, ppl:   3.88, acc:   0.63, generation: 10.5026[sec], evaluation: 0.0000[sec]
2025-05-28 09:37:07,360 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:37:07,877 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/21500.ckpt
2025-05-28 09:37:07,902 - INFO - joeynmt.training - Example #0
2025-05-28 09:37:07,902 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:37:07,902 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:37:07,902 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'questa', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'per', 'la', 'con@@', '<unk>', '@', 'vers@@', '<unk>', '@', 'azione', 'che', 'la', 'man@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ava', 'per', 'la', 'sua', 's@@', '<unk>', '@', 'quad@@', '<unk>', '@', 'ra', 'di', 'un', 't@@', '<unk>', '@', 'avolo', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', ',', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'l&apos;', 'ep@@', '<unk>', '@', 'oca', 'del', '4@@', '<unk>', '@', '8', '%', 'degli', 'stati', ',', 'per', 'il', '40', '%', '.', '</s>']
2025-05-28 09:37:07,903 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:37:07,903 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:37:07,903 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato questa di<unk> @ apos<unk> @ iti<unk> @ va per la con<unk> @ vers<unk> @ azione che la man<unk> @ c<unk> @ ava per la sua s<unk> @ quad<unk> @ ra di un t<unk> @ avolo ar<unk> @ t<unk> @ ico , per tre milioni di anni , l&apos; ep<unk> @ oca del 4<unk> @ 8 % degli stati , per il 40 % .
2025-05-28 09:37:07,903 - INFO - joeynmt.training - Example #1
2025-05-28 09:37:07,903 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:37:07,903 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:37:07,903 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'difficile', 'da', 'fare', 'la', 'st@@', '<unk>', '@', 'ha@@', '<unk>', '@', 'f@@', '<unk>', '@', 'amili@@', '<unk>', '@', 'are', ',', 'perché', 'non', 'è', 'il', 'D@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ke', 'del', 'problema', ',', 'non', 'è', 'l&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 09:37:07,904 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:37:07,904 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:37:07,904 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza difficile da fare la st<unk> @ ha<unk> @ f<unk> @ amili<unk> @ are , perché non è il D<unk> @ ic<unk> @ ke del problema , non è l&apos; E<unk> @ is<unk> @ es .
2025-05-28 09:37:07,904 - INFO - joeynmt.training - Example #2
2025-05-28 09:37:07,904 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:37:07,904 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:37:07,904 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'qualche', 'certo', 'senso', ',', 'è', 'la', 'sua', 'b@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'l@@', '<unk>', '@', 'etta', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'del', 'nostro', 'sistema', 'c@@', '<unk>', '@', 'ru@@', '<unk>', '@', 'ci@@', '<unk>', '@', 'ale', '.', '</s>']
2025-05-28 09:37:07,905 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:37:07,905 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:37:07,905 - INFO - joeynmt.training - 	Hypothesis: In qualche certo senso , è la sua b<unk> @ ic<unk> @ l<unk> @ etta ar<unk> @ t<unk> @ ica del nostro sistema c<unk> @ ru<unk> @ ci<unk> @ ale .
2025-05-28 09:37:07,905 - INFO - joeynmt.training - Example #3
2025-05-28 09:37:07,905 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:37:07,905 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:37:07,905 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'può', 'essere', 'in', 'grado', 'di', 'fer@@', '<unk>', '@', 'mare', 'e', 's@@', '<unk>', '@', 'v@@', '<unk>', '@', 'inc@@', '<unk>', '@', 'ere', 'nel', 'vento', '.', '</s>']
2025-05-28 09:37:07,906 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:37:07,906 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:37:07,906 - INFO - joeynmt.training - 	Hypothesis: Si può essere in grado di fer<unk> @ mare e s<unk> @ v<unk> @ inc<unk> @ ere nel vento .
2025-05-28 09:37:07,906 - INFO - joeynmt.training - Example #4
2025-05-28 09:37:07,906 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:37:07,906 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:37:07,906 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'una', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'ret@@', '<unk>', '@', 'ta', 'che', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:37:07,906 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:37:07,906 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:37:07,906 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ erò è una di<unk> @ seg<unk> @ ret<unk> @ ta che negli ultimi 25 anni .
2025-05-28 09:37:11,292 - INFO - joeynmt.training - Epoch   4, Step:    24100, Batch Loss:     1.314182, Batch Acc: 0.637399, Tokens per Sec:    18495, Lr: 0.000300
2025-05-28 09:37:14,652 - INFO - joeynmt.training - Epoch   4, Step:    24200, Batch Loss:     1.421049, Batch Acc: 0.639994, Tokens per Sec:    21045, Lr: 0.000300
2025-05-28 09:37:17,990 - INFO - joeynmt.training - Epoch   4, Step:    24300, Batch Loss:     1.399996, Batch Acc: 0.638132, Tokens per Sec:    21555, Lr: 0.000300
2025-05-28 09:37:21,336 - INFO - joeynmt.training - Epoch   4, Step:    24400, Batch Loss:     1.216210, Batch Acc: 0.637537, Tokens per Sec:    21025, Lr: 0.000300
2025-05-28 09:37:24,677 - INFO - joeynmt.training - Epoch   4, Step:    24500, Batch Loss:     1.273754, Batch Acc: 0.641562, Tokens per Sec:    21249, Lr: 0.000300
2025-05-28 09:37:24,678 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:37:24,678 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:37:34,613 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.35, ppl:   3.85, acc:   0.64, generation: 9.9284[sec], evaluation: 0.0000[sec]
2025-05-28 09:37:34,613 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:37:35,163 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/22000.ckpt
2025-05-28 09:37:35,183 - INFO - joeynmt.training - Example #0
2025-05-28 09:37:35,184 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:37:35,184 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:37:35,184 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'questi', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'per', 'far', 'sì', 'che', 'il', 't@@', '<unk>', '@', 'avolo', 'di', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'are', 'il', 't@@', '<unk>', '@', 'ed@@', '<unk>', '@', 'es@@', '<unk>', '@', 'co', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'che', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'il', '40', 'milioni', 'di', 'anni', ',', 'che', 'ha', 'fatto', 'fatto', 'fatto', 'il', '40', 'milioni', 'di', 'anni', ',', 'per', 'cento', '.', '</s>']
2025-05-28 09:37:35,184 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:37:35,184 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:37:35,184 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato questi due di<unk> @ apos<unk> @ iti<unk> @ va per far sì che il t<unk> @ avolo di ar<unk> @ t<unk> @ ic<unk> @ are il t<unk> @ ed<unk> @ es<unk> @ co ar<unk> @ t<unk> @ ico che per tre milioni di anni , il 40 milioni di anni , che ha fatto fatto fatto il 40 milioni di anni , per cento .
2025-05-28 09:37:35,184 - INFO - joeynmt.training - Example #1
2025-05-28 09:37:35,184 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:37:35,184 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:37:35,185 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'forte', ',', 'non', 'è', 'abbastanza', 'abbastanza', 'forte', ',', 'la', 'st@@', '<unk>', '@', 'av@@', '<unk>', '@', 'ità', 'di', 'questo', 'problema', ',', 'non', 'è', 'il', 'punto', 'di', 'vista', 'del', 'problema', ',', 'non', 'è', 'il', 'livello', 'dell&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', 'mostra', 'il', 'fatto', '.', '</s>']
2025-05-28 09:37:35,185 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:37:35,185 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:37:35,185 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte , non è abbastanza abbastanza forte , la st<unk> @ av<unk> @ ità di questo problema , non è il punto di vista del problema , non è il livello dell&apos; E<unk> @ is<unk> @ es mostra il fatto .
2025-05-28 09:37:35,185 - INFO - joeynmt.training - Example #2
2025-05-28 09:37:35,185 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:37:35,185 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:37:35,185 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'effetti', ',', 'la', 'cer@@', '<unk>', '@', 'ta', 'è', 'la', 'p@@', '<unk>', '@', 'app@@', '<unk>', '@', 'ola', 'che', 'il', 'cuore', 'della', 'nostra', 'c@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ona', 'più', 'cal@@', '<unk>', '@', 'do', 'del', 'nostro', 'sistema', '.', '</s>']
2025-05-28 09:37:35,186 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:37:35,186 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:37:35,186 - INFO - joeynmt.training - 	Hypothesis: In effetti , la cer<unk> @ ta è la p<unk> @ app<unk> @ ola che il cuore della nostra c<unk> @ ic<unk> @ ona più cal<unk> @ do del nostro sistema .
2025-05-28 09:37:35,186 - INFO - joeynmt.training - Example #3
2025-05-28 09:37:35,186 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:37:35,186 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:37:35,186 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'a', 'cresc@@', '<unk>', '@', 'ere', 'in', 'ver@@', '<unk>', '@', 'no', 'e', 'il', 'v@@', '<unk>', '@', 'oto', 'in', 'est@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 09:37:35,187 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:37:35,187 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:37:35,187 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ a cresc<unk> @ ere in ver<unk> @ no e il v<unk> @ oto in est<unk> @ ate .
2025-05-28 09:37:35,187 - INFO - joeynmt.training - Example #4
2025-05-28 09:37:35,187 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:37:35,187 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:37:35,187 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:37:35,188 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:37:35,188 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:37:35,188 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ erò è un seg<unk> @ no di quello che è successo negli ultimi 25 anni .
2025-05-28 09:37:38,493 - INFO - joeynmt.training - Epoch   4, Step:    24600, Batch Loss:     1.298607, Batch Acc: 0.641971, Tokens per Sec:    18676, Lr: 0.000300
2025-05-28 09:37:41,793 - INFO - joeynmt.training - Epoch   4, Step:    24700, Batch Loss:     1.299444, Batch Acc: 0.638070, Tokens per Sec:    21627, Lr: 0.000300
2025-05-28 09:37:45,096 - INFO - joeynmt.training - Epoch   4, Step:    24800, Batch Loss:     1.344481, Batch Acc: 0.636347, Tokens per Sec:    22131, Lr: 0.000300
2025-05-28 09:37:48,457 - INFO - joeynmt.training - Epoch   4, Step:    24900, Batch Loss:     1.414152, Batch Acc: 0.640105, Tokens per Sec:    21459, Lr: 0.000300
2025-05-28 09:37:51,881 - INFO - joeynmt.training - Epoch   4, Step:    25000, Batch Loss:     1.281762, Batch Acc: 0.636770, Tokens per Sec:    20586, Lr: 0.000300
2025-05-28 09:37:51,881 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:37:51,881 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:38:03,619 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.35, ppl:   3.85, acc:   0.63, generation: 11.7277[sec], evaluation: 0.0000[sec]
2025-05-28 09:38:03,620 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:38:04,263 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/22500.ckpt
2025-05-28 09:38:04,290 - INFO - joeynmt.training - Example #0
2025-05-28 09:38:04,291 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:38:04,291 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:38:04,291 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'due', 'due', 'due', ',', 'per', 'la', 'l@@', '<unk>', '@', 'arg@@', '<unk>', '@', 'a', 'per', 'la', 'ri@@', '<unk>', '@', 'vista', ',', 'che', 'i', 'p@@', '<unk>', '@', 'ezzi', 'di', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', ',', 'per', 'il', '40', 'milioni', 'di', 'anni', ',', 'e', 'il', '40', 'milioni', 'di', 'anni', ',', 'e', 'il', '40', 'anni', ',', 'per', 'cento', '.', '</s>']
2025-05-28 09:38:04,291 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:38:04,292 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:38:04,292 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due due due due , per la l<unk> @ arg<unk> @ a per la ri<unk> @ vista , che i p<unk> @ ezzi di ghi<unk> @ accio ar<unk> @ t<unk> @ ico , per il 40 milioni di anni , e il 40 milioni di anni , e il 40 anni , per cento .
2025-05-28 09:38:04,292 - INFO - joeynmt.training - Example #1
2025-05-28 09:38:04,292 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:38:04,292 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:38:04,292 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', ',', 'non', 'è', 'abbastanza', ',', 'abbastanza', 'la', 'pot@@', '<unk>', '@', 'enza', 'di', 'questo', 'problema', ',', 'perché', 'non', 'è', 'il', 'di@@', '<unk>', '@', 'batt@@', '<unk>', '@', 'ito', 'non', 'è', 'il', 'D@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ke', 'del', 'ghi@@', '<unk>', '@', 'accio', 'del', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 09:38:04,293 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:38:04,293 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:38:04,293 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza , non è abbastanza , abbastanza la pot<unk> @ enza di questo problema , perché non è il di<unk> @ batt<unk> @ ito non è il D<unk> @ ic<unk> @ ke del ghi<unk> @ accio del ghi<unk> @ accio .
2025-05-28 09:38:04,293 - INFO - joeynmt.training - Example #2
2025-05-28 09:38:04,293 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:38:04,293 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:38:04,293 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'è', 'il', 'cuore', 'di', 'un', 'po', '&apos;', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', ',', 'il', 'c@@', '<unk>', '@', 'att@@', '<unk>', '@', 'uale', 'globale', '.', '</s>']
2025-05-28 09:38:04,294 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:38:04,294 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:38:04,294 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , è il cuore di un po &apos; ar<unk> @ t<unk> @ ico , il c<unk> @ att<unk> @ uale globale .
2025-05-28 09:38:04,294 - INFO - joeynmt.training - Example #3
2025-05-28 09:38:04,294 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:38:04,294 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:38:04,294 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'avvic@@', '<unk>', '@', 'ina', 'al', 'vento', 'e', 'si', 's@@', '<unk>', '@', 'ale', 'in', 'vento', '.', '</s>']
2025-05-28 09:38:04,294 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:38:04,295 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:38:04,295 - INFO - joeynmt.training - 	Hypothesis: Si avvic<unk> @ ina al vento e si s<unk> @ ale in vento .
2025-05-28 09:38:04,295 - INFO - joeynmt.training - Example #4
2025-05-28 09:38:04,295 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:38:04,295 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:38:04,295 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'una', 'serie', 'di', 'cosa', 'che', 'sta', 'succe@@', '<unk>', '@', 'dendo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:38:04,295 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:38:04,295 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:38:04,295 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ erò una serie di cosa che sta succe<unk> @ dendo negli ultimi 25 anni .
2025-05-28 09:38:07,724 - INFO - joeynmt.training - Epoch   4, Step:    25100, Batch Loss:     1.216131, Batch Acc: 0.643107, Tokens per Sec:    17175, Lr: 0.000300
2025-05-28 09:38:10,513 - INFO - joeynmt.training - Epoch   4: total training loss 8420.16
2025-05-28 09:38:10,513 - INFO - joeynmt.training - EPOCH 5
2025-05-28 09:38:11,133 - INFO - joeynmt.training - Epoch   5, Step:    25200, Batch Loss:     1.246842, Batch Acc: 0.653944, Tokens per Sec:    20381, Lr: 0.000300
2025-05-28 09:38:14,578 - INFO - joeynmt.training - Epoch   5, Step:    25300, Batch Loss:     1.384865, Batch Acc: 0.654297, Tokens per Sec:    21549, Lr: 0.000300
2025-05-28 09:38:17,984 - INFO - joeynmt.training - Epoch   5, Step:    25400, Batch Loss:     1.311310, Batch Acc: 0.655108, Tokens per Sec:    20705, Lr: 0.000300
2025-05-28 09:38:21,391 - INFO - joeynmt.training - Epoch   5, Step:    25500, Batch Loss:     1.134555, Batch Acc: 0.658009, Tokens per Sec:    20963, Lr: 0.000300
2025-05-28 09:38:21,391 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:38:21,391 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:38:30,204 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.34, ppl:   3.81, acc:   0.64, generation: 8.8058[sec], evaluation: 0.0000[sec]
2025-05-28 09:38:30,204 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:38:30,754 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/23000.ckpt
2025-05-28 09:38:30,779 - INFO - joeynmt.training - Example #0
2025-05-28 09:38:30,780 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:38:30,780 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:38:30,780 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'due', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'per', 'ri@@', '<unk>', '@', 'durre', 'il', '40', 'anni', ',', 'che', 'il', 'fu@@', '<unk>', '@', 'ori@@', '<unk>', '@', 'entale', 'che', 'per', 'la', 'fr@@', '<unk>', '@', 'anc@@', '<unk>', '@', 'ese', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'che', 'è', 'stato', 'acc@@', '<unk>', '@', 'esso', 'a', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'cento', '.', '</s>']
2025-05-28 09:38:30,780 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:38:30,780 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:38:30,780 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due due due di<unk> @ apos<unk> @ iti<unk> @ va per ri<unk> @ durre il 40 anni , che il fu<unk> @ ori<unk> @ entale che per la fr<unk> @ anc<unk> @ ese che ha fatto per tre milioni di anni , che è stato acc<unk> @ esso a tre milioni di anni , per cento .
2025-05-28 09:38:30,780 - INFO - joeynmt.training - Example #1
2025-05-28 09:38:30,781 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:38:30,781 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:38:30,781 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'abbastanza', ',', 'il', 'fatto', 'di', 'questo', 'problema', ',', 'la', 'giu@@', '<unk>', '@', 'st@@', '<unk>', '@', 'izia', ',', 'perché', 'non', 'è', 'il', 'di@@', '<unk>', '@', 'stri@@', '<unk>', '@', 'bu@@', '<unk>', '@', 'ito', 'del', 'problema', ',', 'perché', 'non', 'è', 'il', 'd@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'co', 'dell&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 09:38:30,781 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:38:30,781 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:38:30,781 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza abbastanza , il fatto di questo problema , la giu<unk> @ st<unk> @ izia , perché non è il di<unk> @ stri<unk> @ bu<unk> @ ito del problema , perché non è il d<unk> @ ic<unk> @ co dell&apos; E<unk> @ is<unk> @ es .
2025-05-28 09:38:30,781 - INFO - joeynmt.training - Example #2
2025-05-28 09:38:30,782 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:38:30,782 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:38:30,782 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'è', 'il', 'cuore', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', ',', 'il', 'cuore', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'globale', '.', '</s>']
2025-05-28 09:38:30,782 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:38:30,782 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:38:30,782 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , è il cuore ar<unk> @ t<unk> @ t<unk> @ ico , il cuore ar<unk> @ t<unk> @ t<unk> @ ico globale .
2025-05-28 09:38:30,782 - INFO - joeynmt.training - Example #3
2025-05-28 09:38:30,782 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:38:30,782 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:38:30,783 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'volta', ',', 'è', 'il', 'vento', 'e', 'lo', 'sc@@', '<unk>', '@', 'al@@', '<unk>', '@', 'bero', '.', '</s>']
2025-05-28 09:38:30,783 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:38:30,783 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:38:30,783 - INFO - joeynmt.training - 	Hypothesis: La prima volta , è il vento e lo sc<unk> @ al<unk> @ bero .
2025-05-28 09:38:30,783 - INFO - joeynmt.training - Example #4
2025-05-28 09:38:30,783 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:38:30,783 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:38:30,783 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'o', 'è', 'una', 'di@@', '<unk>', '@', 'stri@@', '<unk>', '@', 'bu@@', '<unk>', '@', 'zione', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:38:30,784 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:38:30,784 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:38:30,784 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ o è una di<unk> @ stri<unk> @ bu<unk> @ zione che è successo negli ultimi 25 anni .
2025-05-28 09:38:34,168 - INFO - joeynmt.training - Epoch   5, Step:    25600, Batch Loss:     1.303526, Batch Acc: 0.652060, Tokens per Sec:    18243, Lr: 0.000300
2025-05-28 09:38:37,614 - INFO - joeynmt.training - Epoch   5, Step:    25700, Batch Loss:     1.255105, Batch Acc: 0.655026, Tokens per Sec:    21310, Lr: 0.000300
2025-05-28 09:38:41,010 - INFO - joeynmt.training - Epoch   5, Step:    25800, Batch Loss:     1.291930, Batch Acc: 0.651869, Tokens per Sec:    20285, Lr: 0.000300
2025-05-28 09:38:44,418 - INFO - joeynmt.training - Epoch   5, Step:    25900, Batch Loss:     1.319166, Batch Acc: 0.651758, Tokens per Sec:    20970, Lr: 0.000300
2025-05-28 09:38:47,810 - INFO - joeynmt.training - Epoch   5, Step:    26000, Batch Loss:     1.258451, Batch Acc: 0.653863, Tokens per Sec:    21310, Lr: 0.000300
2025-05-28 09:38:47,811 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:38:47,811 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:38:58,894 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.33, ppl:   3.80, acc:   0.64, generation: 11.0726[sec], evaluation: 0.0000[sec]
2025-05-28 09:38:58,894 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:38:59,477 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/23500.ckpt
2025-05-28 09:38:59,502 - INFO - joeynmt.training - Example #0
2025-05-28 09:38:59,503 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:38:59,503 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:38:59,503 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'questa', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', ',', 'per', 'la', 'quale', ',', 'l&apos;', 'epi@@', '<unk>', '@', 'de@@', '<unk>', '@', 'termin@@', '<unk>', '@', 'azione', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'che', 'l&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'app@@', '<unk>', '@', 'e', 'che', 'per', 'il', '4@@', '<unk>', '@', '8', 'milioni', 'di', 'anni', ',', 'il', 'li@@', '<unk>', '@', 'bero', 'di', 'anni', ',', 'il', 'fu@@', '<unk>', '@', 'oco', 'di', 'circa', 'il', '40', '%', '.', '</s>']
2025-05-28 09:38:59,503 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:38:59,503 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:38:59,503 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato questa di<unk> @ apos<unk> @ iti<unk> @ va , per la quale , l&apos; epi<unk> @ de<unk> @ termin<unk> @ azione ar<unk> @ t<unk> @ ica che l&apos; E<unk> @ is<unk> @ k<unk> @ app<unk> @ e che per il 4<unk> @ 8 milioni di anni , il li<unk> @ bero di anni , il fu<unk> @ oco di circa il 40 % .
2025-05-28 09:38:59,503 - INFO - joeynmt.training - Example #1
2025-05-28 09:38:59,504 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:38:59,504 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:38:59,504 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'mor@@', '<unk>', '@', 'to', 'la', 'st@@', '<unk>', '@', 'izia', 'di', 'questo', 'problema', ',', 'non', 'è', 'l&apos;', 'er@@', '<unk>', '@', 'n@@', '<unk>', '@', 'arr@@', '<unk>', '@', 'azione', 'di', 'questo', 'particolare', 'problema', ',', 'non', 'è', 'l&apos;', 'idea', 'di', 'D@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ke', 'del', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 09:38:59,504 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:38:59,504 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:38:59,504 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza mor<unk> @ to la st<unk> @ izia di questo problema , non è l&apos; er<unk> @ n<unk> @ arr<unk> @ azione di questo particolare problema , non è l&apos; idea di D<unk> @ ic<unk> @ ke del E<unk> @ is<unk> @ es .
2025-05-28 09:38:59,504 - INFO - joeynmt.training - Example #2
2025-05-28 09:38:59,505 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:38:59,505 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:38:59,505 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'qualche', 'certo', 'senso', ',', 'la', 'cosa', 'più', 'cal@@', '<unk>', '@', 'da', 'è', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'di', 'g@@', '<unk>', '@', 'amb@@', '<unk>', '@', 'a', 'globale', '.', '</s>']
2025-05-28 09:38:59,505 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:38:59,505 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:38:59,505 - INFO - joeynmt.training - 	Hypothesis: In qualche certo senso , la cosa più cal<unk> @ da è ar<unk> @ t<unk> @ ico ar<unk> @ t<unk> @ ico di g<unk> @ amb<unk> @ a globale .
2025-05-28 09:38:59,505 - INFO - joeynmt.training - Example #3
2025-05-28 09:38:59,505 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:38:59,505 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:38:59,505 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'en@@', '<unk>', '@', 'gono', 'in', 'ver@@', '<unk>', '@', 'no', 'e', 'lo', 's@@', '<unk>', '@', 'om@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 09:38:59,506 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:38:59,506 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:38:59,506 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ en<unk> @ gono in ver<unk> @ no e lo s<unk> @ om<unk> @ ate .
2025-05-28 09:38:59,506 - INFO - joeynmt.training - Example #4
2025-05-28 09:38:59,506 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:38:59,506 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:38:59,506 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'una', 'serie', 'di', 'sc@@', '<unk>', '@', 'at@@', '<unk>', '@', 'u@@', '<unk>', '@', 'ali', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:38:59,507 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:38:59,507 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:38:59,507 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ erò è una serie di sc<unk> @ at<unk> @ u<unk> @ ali che è successo negli ultimi 25 anni .
2025-05-28 09:39:02,890 - INFO - joeynmt.training - Epoch   5, Step:    26100, Batch Loss:     1.216878, Batch Acc: 0.654409, Tokens per Sec:    17472, Lr: 0.000300
2025-05-28 09:39:06,306 - INFO - joeynmt.training - Epoch   5, Step:    26200, Batch Loss:     1.340502, Batch Acc: 0.648825, Tokens per Sec:    21311, Lr: 0.000300
2025-05-28 09:39:09,700 - INFO - joeynmt.training - Epoch   5, Step:    26300, Batch Loss:     1.213824, Batch Acc: 0.646420, Tokens per Sec:    20310, Lr: 0.000300
2025-05-28 09:39:13,085 - INFO - joeynmt.training - Epoch   5, Step:    26400, Batch Loss:     1.287021, Batch Acc: 0.649336, Tokens per Sec:    20224, Lr: 0.000300
2025-05-28 09:39:16,517 - INFO - joeynmt.training - Epoch   5, Step:    26500, Batch Loss:     1.364028, Batch Acc: 0.655472, Tokens per Sec:    20807, Lr: 0.000300
2025-05-28 09:39:16,517 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:39:16,517 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:39:28,850 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.34, ppl:   3.81, acc:   0.64, generation: 12.3220[sec], evaluation: 0.0000[sec]
2025-05-28 09:39:29,230 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/24000.ckpt
2025-05-28 09:39:29,258 - INFO - joeynmt.training - Example #0
2025-05-28 09:39:29,258 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:39:29,258 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:39:29,258 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'due', 'due', 'cose', 'che', 'hanno', 'mostr@@', '<unk>', '@', 'ato', 'per', 'ri@@', '<unk>', '@', 'durre', 'la', 'stessa', 'cosa', 'per', 'ri@@', '<unk>', '@', 'durre', 'la', 'c@@', '<unk>', '@', 'ena', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'che', 'è', 'il', 't@@', '<unk>', '@', 'avolo', 'di', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'per', 'il', '4@@', '<unk>', '@', '8', 'milioni', 'di', 'anni', ',', 'il', '40', ',', 'il', '40', 'per@@', '<unk>', '@', 'cento', 'di', 'queste', 'persone', ',', 'e', 'la', 'Gr@@', '<unk>', '@', 'um@@', '<unk>', '@', 'or@@', '<unk>', '@', 'n@@', '<unk>', '@', 'd', 'è', 'stato', '.', '</s>']
2025-05-28 09:39:29,259 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:39:29,259 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:39:29,259 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato queste due due due cose che hanno mostr<unk> @ ato per ri<unk> @ durre la stessa cosa per ri<unk> @ durre la c<unk> @ ena ar<unk> @ t<unk> @ ica che è il t<unk> @ avolo di ghi<unk> @ accio ar<unk> @ t<unk> @ ico per il 4<unk> @ 8 milioni di anni , il 40 , il 40 per<unk> @ cento di queste persone , e la Gr<unk> @ um<unk> @ or<unk> @ n<unk> @ d è stato .
2025-05-28 09:39:29,259 - INFO - joeynmt.training - Example #1
2025-05-28 09:39:29,259 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:39:29,259 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:39:29,259 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'la', 'di@@', '<unk>', '@', 'chiar@@', '<unk>', '@', 'azione', 'di', 'questo', 'problema', ',', 'non', 'è', 'la', 'di@@', '<unk>', '@', 'chiar@@', '<unk>', '@', 'ezza', 'di', 'questo', 'problema', ',', 'non', 'è', 'il', 'del@@', '<unk>', '@', 'eg@@', '<unk>', '@', 'no', 'dell&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 09:39:29,260 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:39:29,260 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:39:29,260 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza la di<unk> @ chiar<unk> @ azione di questo problema , non è la di<unk> @ chiar<unk> @ ezza di questo problema , non è il del<unk> @ eg<unk> @ no dell&apos; E<unk> @ is<unk> @ es .
2025-05-28 09:39:29,260 - INFO - joeynmt.training - Example #2
2025-05-28 09:39:29,260 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:39:29,260 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:39:29,260 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'c@@', '<unk>', '@', 'att@@', '<unk>', '@', 'uale', 'è', 'la', 'c@@', '<unk>', '@', 'app@@', '<unk>', '@', 'a', 'di', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'are', 'la', 'nostra', 'c@@', '<unk>', '@', 'ena', 'globale', '.', '</s>']
2025-05-28 09:39:29,261 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:39:29,261 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:39:29,261 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la c<unk> @ att<unk> @ uale è la c<unk> @ app<unk> @ a di ar<unk> @ t<unk> @ ic<unk> @ are la nostra c<unk> @ ena globale .
2025-05-28 09:39:29,261 - INFO - joeynmt.training - Example #3
2025-05-28 09:39:29,261 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:39:29,261 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:39:29,261 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'è', 'in', 'ver@@', '<unk>', '@', 'no', ',', 'in', 'un', 'in@@', '<unk>', '@', 'ver@@', '<unk>', '@', 'no', 'e', 'il', 'b@@', '<unk>', '@', 'ag@@', '<unk>', '@', 'o', '.', '</s>']
2025-05-28 09:39:29,262 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:39:29,262 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:39:29,262 - INFO - joeynmt.training - 	Hypothesis: Si è in ver<unk> @ no , in un in<unk> @ ver<unk> @ no e il b<unk> @ ag<unk> @ o .
2025-05-28 09:39:29,262 - INFO - joeynmt.training - Example #4
2025-05-28 09:39:29,262 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:39:29,262 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:39:29,262 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', ',', 'è', 'quello', 'che', 'sta', 'acca@@', '<unk>', '@', 'dendo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:39:29,262 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:39:29,262 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:39:29,263 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va , è quello che sta acca<unk> @ dendo negli ultimi 25 anni .
2025-05-28 09:39:32,695 - INFO - joeynmt.training - Epoch   5, Step:    26600, Batch Loss:     1.446123, Batch Acc: 0.651301, Tokens per Sec:    18984, Lr: 0.000300
2025-05-28 09:39:36,113 - INFO - joeynmt.training - Epoch   5, Step:    26700, Batch Loss:     1.363147, Batch Acc: 0.654641, Tokens per Sec:    20931, Lr: 0.000300
2025-05-28 09:39:39,551 - INFO - joeynmt.training - Epoch   5, Step:    26800, Batch Loss:     1.298963, Batch Acc: 0.649108, Tokens per Sec:    21120, Lr: 0.000300
2025-05-28 09:39:42,947 - INFO - joeynmt.training - Epoch   5, Step:    26900, Batch Loss:     1.380491, Batch Acc: 0.647528, Tokens per Sec:    20772, Lr: 0.000300
2025-05-28 09:39:46,365 - INFO - joeynmt.training - Epoch   5, Step:    27000, Batch Loss:     1.334318, Batch Acc: 0.651478, Tokens per Sec:    21213, Lr: 0.000300
2025-05-28 09:39:46,365 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:39:46,365 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:39:57,468 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.33, ppl:   3.79, acc:   0.64, generation: 11.0950[sec], evaluation: 0.0000[sec]
2025-05-28 09:39:57,468 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:39:58,147 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/24500.ckpt
2025-05-28 09:39:58,166 - INFO - joeynmt.training - Example #0
2025-05-28 09:39:58,166 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:39:58,166 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:39:58,167 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'questa', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', ',', 'per', 'ri@@', '<unk>', '@', 'durre', 'l&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'app@@', '<unk>', '@', 'a', ',', 'che', 'il', 'livello', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'è', 'il', '4@@', '<unk>', '@', '8', 'milioni', 'di', 'anni', '.', '</s>']
2025-05-28 09:39:58,167 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:39:58,167 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:39:58,167 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno ho mostr<unk> @ ato questa di<unk> @ apos<unk> @ iti<unk> @ va , per ri<unk> @ durre l&apos; E<unk> @ is<unk> @ k<unk> @ app<unk> @ a , che il livello ar<unk> @ t<unk> @ ico è il 4<unk> @ 8 milioni di anni .
2025-05-28 09:39:58,167 - INFO - joeynmt.training - Example #1
2025-05-28 09:39:58,167 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:39:58,167 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:39:58,167 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'er@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'bile', ',', 'che', 'non', 'è', 'abbastanza', 'er@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'm@@', '<unk>', '@', 'ità', 'del', 'problema', ',', 'non', 'è', 'il', 'del@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'io', '.', '</s>']
2025-05-28 09:39:58,168 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:39:58,168 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:39:58,168 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza er<unk> @ ri<unk> @ bile , che non è abbastanza er<unk> @ ri<unk> @ m<unk> @ ità del problema , non è il del<unk> @ ic<unk> @ io .
2025-05-28 09:39:58,168 - INFO - joeynmt.training - Example #2
2025-05-28 09:39:58,168 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:39:58,168 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:39:58,168 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'cer@@', '<unk>', '@', 't@@', '<unk>', '@', 'amente', ',', 'il', 'livello', 'di', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'è', 'il', 'cuore', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', '.', '</s>']
2025-05-28 09:39:58,169 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:39:58,169 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:39:58,169 - INFO - joeynmt.training - 	Hypothesis: In cer<unk> @ t<unk> @ amente , il livello di ghi<unk> @ accio ar<unk> @ t<unk> @ ico è il cuore ar<unk> @ t<unk> @ ico .
2025-05-28 09:39:58,169 - INFO - joeynmt.training - Example #3
2025-05-28 09:39:58,169 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:39:58,169 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:39:58,169 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', '@', 'erto', ',', 'il', 'vento', 'e', 'lo', 's@@', '<unk>', '@', 'guar@@', '<unk>', '@', 'do', 'in', 'ci@@', '<unk>', '@', 'ma', '.', '</s>']
2025-05-28 09:39:58,170 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:39:58,170 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:39:58,170 - INFO - joeynmt.training - 	Hypothesis: C<unk> @ erto , il vento e lo s<unk> @ guar<unk> @ do in ci<unk> @ ma .
2025-05-28 09:39:58,170 - INFO - joeynmt.training - Example #4
2025-05-28 09:39:58,170 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:39:58,170 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:39:58,170 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', ',', 'è', 'un', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'no', 'di', 'cosa', 'succede', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:39:58,171 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:39:58,171 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:39:58,171 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va , è un di<unk> @ seg<unk> @ no di cosa succede negli ultimi 25 anni .
2025-05-28 09:40:01,546 - INFO - joeynmt.training - Epoch   5, Step:    27100, Batch Loss:     1.339611, Batch Acc: 0.652815, Tokens per Sec:    17539, Lr: 0.000300
2025-05-28 09:40:04,964 - INFO - joeynmt.training - Epoch   5, Step:    27200, Batch Loss:     1.145680, Batch Acc: 0.654845, Tokens per Sec:    21227, Lr: 0.000300
2025-05-28 09:40:08,403 - INFO - joeynmt.training - Epoch   5, Step:    27300, Batch Loss:     1.246314, Batch Acc: 0.647523, Tokens per Sec:    21142, Lr: 0.000300
2025-05-28 09:40:11,810 - INFO - joeynmt.training - Epoch   5, Step:    27400, Batch Loss:     1.261957, Batch Acc: 0.649943, Tokens per Sec:    20851, Lr: 0.000300
2025-05-28 09:40:15,218 - INFO - joeynmt.training - Epoch   5, Step:    27500, Batch Loss:     1.258353, Batch Acc: 0.653906, Tokens per Sec:    20300, Lr: 0.000300
2025-05-28 09:40:15,219 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:40:15,219 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:40:26,459 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.33, ppl:   3.79, acc:   0.64, generation: 11.2299[sec], evaluation: 0.0000[sec]
2025-05-28 09:40:26,460 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:40:27,085 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/25000.ckpt
2025-05-28 09:40:27,113 - INFO - joeynmt.training - Example #0
2025-05-28 09:40:27,114 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:40:27,114 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:40:27,114 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'questa', 'due', 'due', 'ri@@', '<unk>', '@', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'per', 'ri@@', '<unk>', '@', 'durre', 'che', 'i', 'ghi@@', '<unk>', '@', 'acci@@', '<unk>', '@', 'ai', ',', 'per', 'la', 'Gr@@', '<unk>', '@', 'an', 'B@@', '<unk>', '@', 'ret@@', '<unk>', '@', 'n@@', '<unk>', '@', 'd', ',', 'per', 'il', '40', 'milioni', 'di', 'anni', ',', 'per', 'cento', '.', '</s>']
2025-05-28 09:40:27,115 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:40:27,115 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:40:27,115 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato questa due due ri<unk> @ di<unk> @ apos<unk> @ iti<unk> @ va per ri<unk> @ durre che i ghi<unk> @ acci<unk> @ ai , per la Gr<unk> @ an B<unk> @ ret<unk> @ n<unk> @ d , per il 40 milioni di anni , per cento .
2025-05-28 09:40:27,115 - INFO - joeynmt.training - Example #1
2025-05-28 09:40:27,115 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:40:27,116 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:40:27,116 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'or@@', '<unk>', '@', 'mai', 'il', 'tr@@', '<unk>', '@', 'atto', 'di', 'questo', 'problema', ',', 'perché', 'non', 'è', 'un', 'ti@@', '<unk>', '@', 'zio', 'di', 'questo', 'problema', ',', 'non', 'è', 'il', 'del@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'io', '.', '</s>']
2025-05-28 09:40:27,116 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:40:27,116 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:40:27,116 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza or<unk> @ mai il tr<unk> @ atto di questo problema , perché non è un ti<unk> @ zio di questo problema , non è il del<unk> @ ic<unk> @ io .
2025-05-28 09:40:27,116 - INFO - joeynmt.training - Example #2
2025-05-28 09:40:27,116 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:40:27,117 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:40:27,117 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'è', 'il', 'cuore', 'di', 'un', 'eff@@', '<unk>', '@', 'etto', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'di', 'un', 'livello', 'di', 'ris@@', '<unk>', '@', 'chi@@', '<unk>', '@', 'amento', 'globale', '.', '</s>']
2025-05-28 09:40:27,117 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:40:27,117 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:40:27,117 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , è il cuore di un eff<unk> @ etto ar<unk> @ t<unk> @ ico di un livello di ris<unk> @ chi<unk> @ amento globale .
2025-05-28 09:40:27,117 - INFO - joeynmt.training - Example #3
2025-05-28 09:40:27,117 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:40:27,117 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:40:27,118 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'a', 'cres@@', '<unk>', '@', 'ce', 'e', 'si', 's@@', '<unk>', '@', 'v@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'ano', 'in', 'est@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 09:40:27,118 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:40:27,118 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:40:27,118 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ a cres<unk> @ ce e si s<unk> @ v<unk> @ ant<unk> @ ano in est<unk> @ ate .
2025-05-28 09:40:27,118 - INFO - joeynmt.training - Example #4
2025-05-28 09:40:27,118 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:40:27,118 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:40:27,118 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'una', 'serie', 'di', 'cose', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:40:27,119 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:40:27,119 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:40:27,119 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ erò è una serie di cose che è successo negli ultimi 25 anni .
2025-05-28 09:40:30,513 - INFO - joeynmt.training - Epoch   5, Step:    27600, Batch Loss:     1.236434, Batch Acc: 0.656275, Tokens per Sec:    17209, Lr: 0.000300
2025-05-28 09:40:33,920 - INFO - joeynmt.training - Epoch   5, Step:    27700, Batch Loss:     1.146950, Batch Acc: 0.648104, Tokens per Sec:    21734, Lr: 0.000300
2025-05-28 09:40:37,313 - INFO - joeynmt.training - Epoch   5, Step:    27800, Batch Loss:     1.268123, Batch Acc: 0.651788, Tokens per Sec:    21460, Lr: 0.000300
2025-05-28 09:40:40,711 - INFO - joeynmt.training - Epoch   5, Step:    27900, Batch Loss:     1.182863, Batch Acc: 0.651023, Tokens per Sec:    21243, Lr: 0.000300
2025-05-28 09:40:44,094 - INFO - joeynmt.training - Epoch   5, Step:    28000, Batch Loss:     1.231695, Batch Acc: 0.650993, Tokens per Sec:    21386, Lr: 0.000300
2025-05-28 09:40:44,094 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:40:44,094 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:40:55,829 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.32, ppl:   3.75, acc:   0.64, generation: 11.7245[sec], evaluation: 0.0000[sec]
2025-05-28 09:40:55,829 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:40:56,426 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/25500.ckpt
2025-05-28 09:40:56,456 - INFO - joeynmt.training - Example #0
2025-05-28 09:40:56,457 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:40:56,457 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:40:56,457 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'due', 'due', 'cose', 'che', 'sono', 'stati', 'in@@', '<unk>', '@', 'vi@@', '<unk>', '@', 'sti', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ali', 'per', 'ri@@', '<unk>', '@', 'durre', 'il', 'più', 'ar@@', '<unk>', '@', 'kt@@', '<unk>', '@', 'ico', 'che', 'i', 'c@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'aggi', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ali', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'cento', '.', '</s>']
2025-05-28 09:40:56,457 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:40:56,457 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:40:56,457 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due due due cose che sono stati in<unk> @ vi<unk> @ sti ar<unk> @ t<unk> @ ali per ri<unk> @ durre il più ar<unk> @ kt<unk> @ ico che i c<unk> @ ant<unk> @ aggi ar<unk> @ t<unk> @ ali per tre milioni di anni , per cento .
2025-05-28 09:40:56,458 - INFO - joeynmt.training - Example #1
2025-05-28 09:40:56,458 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:40:56,458 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:40:56,458 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'or@@', '<unk>', '@', 'mai', ',', 'perché', 'non', 'è', 'abbastanza', 'la', 'di@@', '<unk>', '@', 'mostra', 'la', 'di@@', '<unk>', '@', 'mostra', 'che', 'non', 'è', 'l&apos;', 'em@@', '<unk>', '@', 'a@@', '<unk>', '@', 'sia', 'del', 'fatto', '.', '</s>']
2025-05-28 09:40:56,458 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:40:56,458 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:40:56,459 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza or<unk> @ mai , perché non è abbastanza la di<unk> @ mostra la di<unk> @ mostra che non è l&apos; em<unk> @ a<unk> @ sia del fatto .
2025-05-28 09:40:56,459 - INFO - joeynmt.training - Example #2
2025-05-28 09:40:56,459 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:40:56,459 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:40:56,459 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'c@@', '<unk>', '@', 'ena', 'è', 'la', 'c@@', '<unk>', '@', 'ena', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'es@@', '<unk>', '@', 'ca', 'di', 'un', 'cuore', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ale', '.', '</s>']
2025-05-28 09:40:56,459 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:40:56,460 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:40:56,460 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la c<unk> @ ena è la c<unk> @ ena ar<unk> @ t<unk> @ es<unk> @ ca di un cuore ar<unk> @ t<unk> @ ale .
2025-05-28 09:40:56,460 - INFO - joeynmt.training - Example #3
2025-05-28 09:40:56,460 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:40:56,460 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:40:56,460 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'a', 'cres@@', '<unk>', '@', 'ci@@', '<unk>', '@', 'ce', 'e', 'il', 'v@@', '<unk>', '@', 'ino', 'della', 's@@', '<unk>', '@', 'om@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 09:40:56,460 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:40:56,460 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:40:56,461 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ a cres<unk> @ ci<unk> @ ce e il v<unk> @ ino della s<unk> @ om<unk> @ ate .
2025-05-28 09:40:56,461 - INFO - joeynmt.training - Example #4
2025-05-28 09:40:56,461 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:40:56,461 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:40:56,461 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 's@@', '<unk>', '@', 'li@@', '<unk>', '@', 'de', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'una', 'ri@@', '<unk>', '@', 'ma@@', '<unk>', '@', 'zione', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:40:56,461 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:40:56,461 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:40:56,461 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma s<unk> @ li<unk> @ de che vi mostr<unk> @ erò è una ri<unk> @ ma<unk> @ zione che è successo negli ultimi 25 anni .
2025-05-28 09:40:59,819 - INFO - joeynmt.training - Epoch   5, Step:    28100, Batch Loss:     1.207423, Batch Acc: 0.650587, Tokens per Sec:    17969, Lr: 0.000300
2025-05-28 09:41:03,182 - INFO - joeynmt.training - Epoch   5, Step:    28200, Batch Loss:     1.241202, Batch Acc: 0.649936, Tokens per Sec:    21937, Lr: 0.000300
2025-05-28 09:41:06,532 - INFO - joeynmt.training - Epoch   5, Step:    28300, Batch Loss:     1.255250, Batch Acc: 0.653595, Tokens per Sec:    21144, Lr: 0.000300
2025-05-28 09:41:09,851 - INFO - joeynmt.training - Epoch   5, Step:    28400, Batch Loss:     1.163020, Batch Acc: 0.650013, Tokens per Sec:    21470, Lr: 0.000300
2025-05-28 09:41:13,201 - INFO - joeynmt.training - Epoch   5, Step:    28500, Batch Loss:     1.155647, Batch Acc: 0.649262, Tokens per Sec:    20989, Lr: 0.000300
2025-05-28 09:41:13,202 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:41:13,202 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:41:24,421 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.32, ppl:   3.75, acc:   0.64, generation: 11.2094[sec], evaluation: 0.0000[sec]
2025-05-28 09:41:24,986 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/26500.ckpt
2025-05-28 09:41:25,015 - INFO - joeynmt.training - Example #0
2025-05-28 09:41:25,015 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:41:25,016 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:41:25,016 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 's@@', '<unk>', '@', 'li@@', '<unk>', '@', 'de', 'per', 'far', 'sì', 'che', 'il', 'livello', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'es@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ale', 'che', 'il', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'per', 'il', '4@@', '<unk>', '@', '8', '8', 'anni', ',', 'il', '40', ',', 'per', 'cento', '.', '</s>']
2025-05-28 09:41:25,016 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:41:25,016 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:41:25,016 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due s<unk> @ li<unk> @ de per far sì che il livello ar<unk> @ t<unk> @ es<unk> @ i<unk> @ ale che il ghi<unk> @ accio ar<unk> @ t<unk> @ ico per il 4<unk> @ 8 8 anni , il 40 , per cento .
2025-05-28 09:41:25,016 - INFO - joeynmt.training - Example #1
2025-05-28 09:41:25,017 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:41:25,017 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:41:25,017 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'che', 'la', 'nostra', 'immagin@@', '<unk>', '@', 'azione', 'è', 'abbastanza', 'che', 'è', 'una', 'pros@@', '<unk>', '@', 'p@@', '<unk>', '@', 'ettiva', ',', 'perché', 'non', 'è', 'l&apos;', 'idea', 'di', 'questo', 'problema', ',', 'non', 'è', 'l&apos;', 'idea', 'dell&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 09:41:25,017 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:41:25,017 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:41:25,017 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza che la nostra immagin<unk> @ azione è abbastanza che è una pros<unk> @ p<unk> @ ettiva , perché non è l&apos; idea di questo problema , non è l&apos; idea dell&apos; E<unk> @ is<unk> @ es .
2025-05-28 09:41:25,017 - INFO - joeynmt.training - Example #2
2025-05-28 09:41:25,018 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:41:25,018 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:41:25,018 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'sc@@', '<unk>', '@', 'at@@', '<unk>', '@', 'ola', 'è', 'la', 'mente', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'es@@', '<unk>', '@', 'ca', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'amente', 'il', 'cuore', 'globale', '.', '</s>']
2025-05-28 09:41:25,018 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:41:25,018 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:41:25,018 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la sc<unk> @ at<unk> @ ola è la mente ar<unk> @ t<unk> @ es<unk> @ ca ar<unk> @ t<unk> @ amente il cuore globale .
2025-05-28 09:41:25,019 - INFO - joeynmt.training - Example #3
2025-05-28 09:41:25,019 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:41:25,019 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:41:25,019 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'a', 'cresc@@', '<unk>', '@', 'ere', 'e', 's@@', '<unk>', '@', 'v@@', '<unk>', '@', 'egli@@', '<unk>', '@', 'ere', 'nel', 'vento', '.', '</s>']
2025-05-28 09:41:25,019 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:41:25,019 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:41:25,019 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ a cresc<unk> @ ere e s<unk> @ v<unk> @ egli<unk> @ ere nel vento .
2025-05-28 09:41:25,019 - INFO - joeynmt.training - Example #4
2025-05-28 09:41:25,020 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:41:25,020 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:41:25,020 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'consegu@@', '<unk>', '@', 'enza', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'più', 'di', '25', 'anni', '.', '</s>']
2025-05-28 09:41:25,020 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:41:25,020 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:41:25,020 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma consegu<unk> @ enza che vi mostr<unk> @ erò è un seg<unk> @ no di un seg<unk> @ no di più di 25 anni .
2025-05-28 09:41:28,410 - INFO - joeynmt.training - Epoch   5, Step:    28600, Batch Loss:     1.288577, Batch Acc: 0.647467, Tokens per Sec:    18196, Lr: 0.000300
2025-05-28 09:41:31,799 - INFO - joeynmt.training - Epoch   5, Step:    28700, Batch Loss:     1.235729, Batch Acc: 0.646942, Tokens per Sec:    21207, Lr: 0.000300
2025-05-28 09:41:35,176 - INFO - joeynmt.training - Epoch   5, Step:    28800, Batch Loss:     1.208312, Batch Acc: 0.649384, Tokens per Sec:    21074, Lr: 0.000300
2025-05-28 09:41:38,567 - INFO - joeynmt.training - Epoch   5, Step:    28900, Batch Loss:     1.282397, Batch Acc: 0.655718, Tokens per Sec:    20619, Lr: 0.000300
2025-05-28 09:41:41,951 - INFO - joeynmt.training - Epoch   5, Step:    29000, Batch Loss:     1.407633, Batch Acc: 0.652724, Tokens per Sec:    21217, Lr: 0.000300
2025-05-28 09:41:41,952 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:41:41,952 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:41:53,478 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.32, ppl:   3.75, acc:   0.64, generation: 11.5147[sec], evaluation: 0.0000[sec]
2025-05-28 09:41:53,478 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:41:54,111 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/26000.ckpt
2025-05-28 09:41:54,140 - INFO - joeynmt.training - Example #0
2025-05-28 09:41:54,141 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:41:54,141 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:41:54,141 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'questa', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'per', 'vedere', 'che', 'i', 'due', 's@@', '<unk>', '@', 'for@@', '<unk>', '@', 'zi', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ali', 'che', 'si', 'sono', 's@@', '<unk>', '@', 'ov@@', '<unk>', '@', 'rap@@', '<unk>', '@', 'ol@@', '<unk>', '@', 'ari', 'che', 'hanno', 'fatto', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'la', 'gr@@', '<unk>', '@', 'av@@', '<unk>', '@', 'ità', '.', '</s>']
2025-05-28 09:41:54,141 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:41:54,141 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:41:54,141 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato questa di<unk> @ apos<unk> @ iti<unk> @ va per vedere che i due s<unk> @ for<unk> @ zi ar<unk> @ t<unk> @ ic<unk> @ ali che si sono s<unk> @ ov<unk> @ rap<unk> @ ol<unk> @ ari che hanno fatto tre milioni di anni , per la gr<unk> @ av<unk> @ ità .
2025-05-28 09:41:54,141 - INFO - joeynmt.training - Example #1
2025-05-28 09:41:54,142 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:41:54,142 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:41:54,142 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'difficile', 'da', 'essere', 'un', 'punto', 'di', 'st@@', '<unk>', '@', 'ile', 'di', 'in@@', '<unk>', '@', 'qu@@', '<unk>', '@', 'in@@', '<unk>', '@', 'azione', ',', 'perché', 'non', 'è', 'il', 'fatto', 'che', 'questo', 'problema', 'speci@@', '<unk>', '@', 'ale', ',', 'non', 'è', 'il', 'fatto', 'che', 'l&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 09:41:54,142 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:41:54,142 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:41:54,142 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza difficile da essere un punto di st<unk> @ ile di in<unk> @ qu<unk> @ in<unk> @ azione , perché non è il fatto che questo problema speci<unk> @ ale , non è il fatto che l&apos; E<unk> @ is<unk> @ es .
2025-05-28 09:41:54,142 - INFO - joeynmt.training - Example #2
2025-05-28 09:41:54,143 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:41:54,143 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:41:54,143 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'c@@', '<unk>', '@', 'att@@', '<unk>', '@', 'uale', 'è', 'la', 'c@@', '<unk>', '@', 'att@@', '<unk>', '@', 'uale', 'del', 'nostro', 'sistema', 'globale', '.', '</s>']
2025-05-28 09:41:54,143 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:41:54,143 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:41:54,143 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la c<unk> @ att<unk> @ uale è la c<unk> @ att<unk> @ uale del nostro sistema globale .
2025-05-28 09:41:54,144 - INFO - joeynmt.training - Example #3
2025-05-28 09:41:54,144 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:41:54,144 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:41:54,144 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'a', 'cres@@', '<unk>', '@', 'ce', 'e', 'in', 'un', 'in@@', '<unk>', '@', 'ver@@', '<unk>', '@', 'no', '.', '</s>']
2025-05-28 09:41:54,144 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:41:54,144 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:41:54,144 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ a cres<unk> @ ce e in un in<unk> @ ver<unk> @ no .
2025-05-28 09:41:54,144 - INFO - joeynmt.training - Example #4
2025-05-28 09:41:54,145 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:41:54,145 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:41:54,145 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'un', 'segn@@', '<unk>', '@', 'ale', 'che', 'cosa', 'succede', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:41:54,145 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:41:54,145 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:41:54,145 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ erò è un segn<unk> @ ale che cosa succede negli ultimi 25 anni .
2025-05-28 09:41:57,575 - INFO - joeynmt.training - Epoch   5, Step:    29100, Batch Loss:     1.163007, Batch Acc: 0.652815, Tokens per Sec:    17300, Lr: 0.000300
2025-05-28 09:42:00,997 - INFO - joeynmt.training - Epoch   5, Step:    29200, Batch Loss:     1.288850, Batch Acc: 0.649139, Tokens per Sec:    20411, Lr: 0.000300
2025-05-28 09:42:04,403 - INFO - joeynmt.training - Epoch   5, Step:    29300, Batch Loss:     1.287492, Batch Acc: 0.649264, Tokens per Sec:    20937, Lr: 0.000300
2025-05-28 09:42:07,811 - INFO - joeynmt.training - Epoch   5, Step:    29400, Batch Loss:     1.354472, Batch Acc: 0.648188, Tokens per Sec:    21338, Lr: 0.000300
2025-05-28 09:42:11,222 - INFO - joeynmt.training - Epoch   5, Step:    29500, Batch Loss:     1.306632, Batch Acc: 0.651932, Tokens per Sec:    21386, Lr: 0.000300
2025-05-28 09:42:11,223 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:42:11,223 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:42:23,369 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.32, ppl:   3.73, acc:   0.64, generation: 12.1362[sec], evaluation: 0.0000[sec]
2025-05-28 09:42:23,370 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:42:24,150 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/27000.ckpt
2025-05-28 09:42:24,178 - INFO - joeynmt.training - Example #0
2025-05-28 09:42:24,178 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:42:24,178 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:42:24,178 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'questo', 'due', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'per', 'vedere', 'che', 'i', 'ghi@@', '<unk>', '@', 'acci@@', '<unk>', '@', 'aio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'che', 'è', 'stato', 'chiamato', '4@@', '<unk>', '@', '8', '8', 'milioni', 'di', 'anni', ',', 'per', 'la', 'Gr@@', '<unk>', '@', 'an', 'B@@', '<unk>', '@', 'ret@@', '<unk>', '@', 'ag@@', '<unk>', '@', 'na', '.', '</s>']
2025-05-28 09:42:24,179 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:42:24,179 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:42:24,179 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato questo due due di<unk> @ apos<unk> @ iti<unk> @ va per vedere che i ghi<unk> @ acci<unk> @ aio ar<unk> @ t<unk> @ ico che ha fatto per tre milioni di anni , che è stato chiamato 4<unk> @ 8 8 milioni di anni , per la Gr<unk> @ an B<unk> @ ret<unk> @ ag<unk> @ na .
2025-05-28 09:42:24,179 - INFO - joeynmt.training - Example #1
2025-05-28 09:42:24,180 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:42:24,180 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:42:24,180 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', ',', 'che', 'è', 'abbastanza', 'la', 'sost@@', '<unk>', '@', 'anza', 'di', 'questo', 'problema', ',', 'non', 'è', 'che', 'non', 'lo', 'fa', 'il', 'problema', 'di', 'questo', 'problema', ',', 'non', 'è', 'che', 'mostra', 'il', 'punto', 'di', 'vista', 'dell&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 09:42:24,180 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:42:24,180 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:42:24,180 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza , che è abbastanza la sost<unk> @ anza di questo problema , non è che non lo fa il problema di questo problema , non è che mostra il punto di vista dell&apos; E<unk> @ is<unk> @ es .
2025-05-28 09:42:24,180 - INFO - joeynmt.training - Example #2
2025-05-28 09:42:24,180 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:42:24,181 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:42:24,181 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'c@@', '<unk>', '@', 'ass@@', '<unk>', '@', 'ita', 'di', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'globale', '.', '</s>']
2025-05-28 09:42:24,181 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:42:24,181 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:42:24,181 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la c<unk> @ ass<unk> @ ita di ghi<unk> @ accio ar<unk> @ t<unk> @ ico , il cuore del nostro sistema globale .
2025-05-28 09:42:24,181 - INFO - joeynmt.training - Example #3
2025-05-28 09:42:24,181 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:42:24,181 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:42:24,181 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&apos;', 'una', 'volta', 'che', 'si', 'è', 's@@', '<unk>', '@', 'v@@', '<unk>', '@', 'egli@@', '<unk>', '@', 'ato', 'in', 'est@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 09:42:24,182 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:42:24,182 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:42:24,182 - INFO - joeynmt.training - 	Hypothesis: E &apos; una volta che si è s<unk> @ v<unk> @ egli<unk> @ ato in est<unk> @ ate .
2025-05-28 09:42:24,182 - INFO - joeynmt.training - Example #4
2025-05-28 09:42:24,182 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:42:24,182 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:42:24,182 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'consegu@@', '<unk>', '@', 'enza', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'cosa', 'succede', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:42:24,183 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:42:24,183 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:42:24,183 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma consegu<unk> @ enza che vi mostr<unk> @ erò è un seg<unk> @ no di cosa succede negli ultimi 25 anni .
2025-05-28 09:42:27,587 - INFO - joeynmt.training - Epoch   5, Step:    29600, Batch Loss:     1.355397, Batch Acc: 0.651686, Tokens per Sec:    16938, Lr: 0.000300
2025-05-28 09:42:30,983 - INFO - joeynmt.training - Epoch   5, Step:    29700, Batch Loss:     1.368701, Batch Acc: 0.653013, Tokens per Sec:    21084, Lr: 0.000300
2025-05-28 09:42:34,360 - INFO - joeynmt.training - Epoch   5, Step:    29800, Batch Loss:     1.182070, Batch Acc: 0.650378, Tokens per Sec:    20748, Lr: 0.000300
2025-05-28 09:42:37,760 - INFO - joeynmt.training - Epoch   5, Step:    29900, Batch Loss:     1.206711, Batch Acc: 0.655134, Tokens per Sec:    21335, Lr: 0.000300
2025-05-28 09:42:41,160 - INFO - joeynmt.training - Epoch   5, Step:    30000, Batch Loss:     1.253966, Batch Acc: 0.655224, Tokens per Sec:    20461, Lr: 0.000300
2025-05-28 09:42:41,160 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:42:41,160 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:42:53,444 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.32, ppl:   3.74, acc:   0.64, generation: 12.2727[sec], evaluation: 0.0000[sec]
2025-05-28 09:42:53,857 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/27500.ckpt
2025-05-28 09:42:53,885 - INFO - joeynmt.training - Example #0
2025-05-28 09:42:53,886 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:42:53,886 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:42:53,886 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', ',', 'per', 'essere', 'in', 'grado', 'di', 'vedere', 'che', 'i', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', ',', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'la', 'Gr@@', '<unk>', '@', 'an', 'B@@', '<unk>', '@', 'ret@@', '<unk>', '@', 'ag@@', '<unk>', '@', 'na', 'ha', 'fatto', 'tre', 'milioni', 'di', 'anni', '.', '</s>']
2025-05-28 09:42:53,886 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:42:53,886 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:42:53,886 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due due di<unk> @ apos<unk> @ iti<unk> @ ve , per essere in grado di vedere che i ghi<unk> @ accio ar<unk> @ t<unk> @ ico , che ha fatto per tre milioni di anni , la Gr<unk> @ an B<unk> @ ret<unk> @ ag<unk> @ na ha fatto tre milioni di anni .
2025-05-28 09:42:53,886 - INFO - joeynmt.training - Example #1
2025-05-28 09:42:53,887 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:42:53,887 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:42:53,887 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'difficile', 'da', 'fare', 'il', 'punto', 'di', 'vista', ',', 'non', 'è', 'abbastanza', 'pot@@', '<unk>', '@', 'ente', ',', 'perché', 'non', 'lo', 'è', 'l&apos;', 'esist@@', '<unk>', '@', 'enza', 'del', 'ghi@@', '<unk>', '@', 'accio', ',', 'non', 'lo', 'è', 'l&apos;', 'idea', 'del', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'no', '.', '</s>']
2025-05-28 09:42:53,887 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:42:53,887 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:42:53,887 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza difficile da fare il punto di vista , non è abbastanza pot<unk> @ ente , perché non lo è l&apos; esist<unk> @ enza del ghi<unk> @ accio , non lo è l&apos; idea del di<unk> @ seg<unk> @ no .
2025-05-28 09:42:53,887 - INFO - joeynmt.training - Example #2
2025-05-28 09:42:53,888 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:42:53,888 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:42:53,888 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'certo', 'certo', 'senso', ',', 'è', 'la', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'va', 'è', 'il', 'cuore', 'ar@@', '<unk>', '@', 'to', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'del', 'nostro', 'sistema', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'vo', 'globale', '.', '</s>']
2025-05-28 09:42:53,888 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:42:53,888 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:42:53,888 - INFO - joeynmt.training - 	Hypothesis: In certo certo senso , è la c<unk> @ atti<unk> @ va è il cuore ar<unk> @ to ar<unk> @ t<unk> @ ico del nostro sistema c<unk> @ atti<unk> @ vo globale .
2025-05-28 09:42:53,889 - INFO - joeynmt.training - Example #3
2025-05-28 09:42:53,889 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:42:53,889 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:42:53,889 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'a', 'cres@@', '<unk>', '@', 'ci@@', '<unk>', '@', 'uta', 'in', 'vento', 'e', 'gi@@', '<unk>', '@', 'ù', '.', '</s>']
2025-05-28 09:42:53,889 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:42:53,889 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:42:53,889 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ a cres<unk> @ ci<unk> @ uta in vento e gi<unk> @ ù .
2025-05-28 09:42:53,889 - INFO - joeynmt.training - Example #4
2025-05-28 09:42:53,890 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:42:53,890 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:42:53,890 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', ',', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:42:53,890 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:42:53,890 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:42:53,890 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va , che vi mostr<unk> @ erò è successo negli ultimi 25 anni .
2025-05-28 09:42:57,312 - INFO - joeynmt.training - Epoch   5, Step:    30100, Batch Loss:     1.144650, Batch Acc: 0.652688, Tokens per Sec:    18464, Lr: 0.000300
2025-05-28 09:43:00,719 - INFO - joeynmt.training - Epoch   5, Step:    30200, Batch Loss:     1.252380, Batch Acc: 0.656142, Tokens per Sec:    21239, Lr: 0.000300
2025-05-28 09:43:04,107 - INFO - joeynmt.training - Epoch   5, Step:    30300, Batch Loss:     1.273630, Batch Acc: 0.650085, Tokens per Sec:    20779, Lr: 0.000300
2025-05-28 09:43:07,494 - INFO - joeynmt.training - Epoch   5, Step:    30400, Batch Loss:     1.138892, Batch Acc: 0.649002, Tokens per Sec:    20635, Lr: 0.000300
2025-05-28 09:43:10,884 - INFO - joeynmt.training - Epoch   5, Step:    30500, Batch Loss:     1.309942, Batch Acc: 0.652309, Tokens per Sec:    21552, Lr: 0.000300
2025-05-28 09:43:10,885 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:43:10,885 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:43:21,856 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.31, ppl:   3.70, acc:   0.65, generation: 10.9608[sec], evaluation: 0.0000[sec]
2025-05-28 09:43:21,856 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:43:22,466 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/28500.ckpt
2025-05-28 09:43:22,494 - INFO - joeynmt.training - Example #0
2025-05-28 09:43:22,495 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:43:22,495 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:43:22,495 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'ri@@', '<unk>', '@', 'durre', 'la', 'stessa', 'cosa', ',', 'che', 'i', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ali', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'che', 'ha', 'avuto', 'un', 'sacco', 'di', 's@@', '<unk>', '@', 'ov@@', '<unk>', '@', 'a', 'di', '4@@', '<unk>', '@', '8', 'milioni', 'di', 'anni', ',', 'per', 'cento', '.', '</s>']
2025-05-28 09:43:22,496 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:43:22,496 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:43:22,496 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per ri<unk> @ durre la stessa cosa , che i ghi<unk> @ accio ar<unk> @ t<unk> @ ic<unk> @ ali per tre milioni di anni , che ha avuto un sacco di s<unk> @ ov<unk> @ a di 4<unk> @ 8 milioni di anni , per cento .
2025-05-28 09:43:22,496 - INFO - joeynmt.training - Example #1
2025-05-28 09:43:22,496 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:43:22,496 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:43:22,496 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'da', 'da', 'poter', 'essere', 'abbastanza', 'abbastanza', 'da', 'da', 'poter', 'essere', 'la', 'sua', 'er@@', '<unk>', '@', 'edi@@', '<unk>', '@', 'tà', 'di', 'questo', 'problema', ',', 'non', 'è', 'l&apos;', 'er@@', '<unk>', '@', 'edi@@', '<unk>', '@', 'mento', 'del', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 09:43:22,497 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:43:22,497 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:43:22,497 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza da da poter essere abbastanza abbastanza da da poter essere la sua er<unk> @ edi<unk> @ tà di questo problema , non è l&apos; er<unk> @ edi<unk> @ mento del ghi<unk> @ accio .
2025-05-28 09:43:22,497 - INFO - joeynmt.training - Example #2
2025-05-28 09:43:22,497 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:43:22,497 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:43:22,497 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'è', 'la', 'c@@', '<unk>', '@', 'lasse', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'vo', 'del', 'nostro', 'sistema', 'globale', '.', '</s>']
2025-05-28 09:43:22,497 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:43:22,498 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:43:22,498 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , è la c<unk> @ lasse ar<unk> @ t<unk> @ ica , il cuore del nostro sistema c<unk> @ atti<unk> @ vo del nostro sistema globale .
2025-05-28 09:43:22,498 - INFO - joeynmt.training - Example #3
2025-05-28 09:43:22,498 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:43:22,498 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:43:22,498 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'volta', 'che', 'sta', 'cresc@@', '<unk>', '@', 'endo', 'il', 'vento', 'e', 's@@', '<unk>', '@', 'ov@@', '<unk>', '@', 'r@@', '<unk>', '@', 'ono', 'nel', 'vento', '.', '</s>']
2025-05-28 09:43:22,498 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:43:22,498 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:43:22,498 - INFO - joeynmt.training - 	Hypothesis: La prima volta che sta cresc<unk> @ endo il vento e s<unk> @ ov<unk> @ r<unk> @ ono nel vento .
2025-05-28 09:43:22,498 - INFO - joeynmt.training - Example #4
2025-05-28 09:43:22,499 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:43:22,499 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:43:22,499 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'consegu@@', '<unk>', '@', 'enza', ',', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'un', 'segn@@', '<unk>', '@', 'ale', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:43:22,499 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:43:22,499 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:43:22,499 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma consegu<unk> @ enza , che vi mostr<unk> @ erò è un segn<unk> @ ale che è successo negli ultimi 25 anni .
2025-05-28 09:43:25,910 - INFO - joeynmt.training - Epoch   5, Step:    30600, Batch Loss:     1.190879, Batch Acc: 0.649205, Tokens per Sec:    17827, Lr: 0.000300
2025-05-28 09:43:29,294 - INFO - joeynmt.training - Epoch   5, Step:    30700, Batch Loss:     1.162075, Batch Acc: 0.651891, Tokens per Sec:    20614, Lr: 0.000300
2025-05-28 09:43:32,654 - INFO - joeynmt.training - Epoch   5, Step:    30800, Batch Loss:     1.416726, Batch Acc: 0.654924, Tokens per Sec:    21176, Lr: 0.000300
2025-05-28 09:43:36,034 - INFO - joeynmt.training - Epoch   5, Step:    30900, Batch Loss:     1.394133, Batch Acc: 0.651293, Tokens per Sec:    20977, Lr: 0.000300
2025-05-28 09:43:39,437 - INFO - joeynmt.training - Epoch   5, Step:    31000, Batch Loss:     1.214936, Batch Acc: 0.653406, Tokens per Sec:    21249, Lr: 0.000300
2025-05-28 09:43:39,437 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:43:39,437 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:43:49,102 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.30, ppl:   3.68, acc:   0.65, generation: 9.6584[sec], evaluation: 0.0000[sec]
2025-05-28 09:43:49,103 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:43:49,687 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/28000.ckpt
2025-05-28 09:43:49,710 - INFO - joeynmt.training - Example #0
2025-05-28 09:43:49,711 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:43:49,711 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:43:49,711 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'due', 'consegu@@', '<unk>', '@', 'enze', 'per', 'la', 'con@@', '<unk>', '@', 'vers@@', '<unk>', '@', 'azione', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'che', 'il', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'è', 'che', 'il', 't@@', '<unk>', '@', 'asso', 'di', 'ag@@', '<unk>', '@', 'gr@@', '<unk>', '@', 'atore', 'di', 'tre', 'milioni', 'di', 'anni', ',', 'il', '40', '.', '</s>']
2025-05-28 09:43:49,712 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:43:49,712 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:43:49,712 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato queste due due consegu<unk> @ enze per la con<unk> @ vers<unk> @ azione ar<unk> @ t<unk> @ ica che il ghi<unk> @ accio ar<unk> @ t<unk> @ ico è che il t<unk> @ asso di ag<unk> @ gr<unk> @ atore di tre milioni di anni , il 40 .
2025-05-28 09:43:49,712 - INFO - joeynmt.training - Example #1
2025-05-28 09:43:49,712 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:43:49,712 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:43:49,712 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', ',', 'è', 'abbastanza', ',', 'il', 'punto', 'di', 'vista', ',', 'perché', 'non', 'è', 'un', 'punto', 'di', 'vista', 'del', 'problema', ',', 'perché', 'non', 'è', 'l&apos;', 'idea', 'del', 'fatto', 'che', 'non', 'è', 'l&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', 'mostra', 'l&apos;', 'idea', 'di', 'questo', 'problema', '.', '</s>']
2025-05-28 09:43:49,713 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:43:49,713 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:43:49,713 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza , è abbastanza , il punto di vista , perché non è un punto di vista del problema , perché non è l&apos; idea del fatto che non è l&apos; E<unk> @ is<unk> @ es mostra l&apos; idea di questo problema .
2025-05-28 09:43:49,713 - INFO - joeynmt.training - Example #2
2025-05-28 09:43:49,713 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:43:49,713 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:43:49,713 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'è', 'la', 'c@@', '<unk>', '@', 'ass@@', '<unk>', '@', 'om@@', '<unk>', '@', 'iglia', 'a', 'un', 'cuore', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ale', 'del', 'nostro', 'sistema', 'globale', '.', '</s>']
2025-05-28 09:43:49,714 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:43:49,714 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:43:49,714 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , è la c<unk> @ ass<unk> @ om<unk> @ iglia a un cuore ar<unk> @ t<unk> @ ic<unk> @ ale del nostro sistema globale .
2025-05-28 09:43:49,714 - INFO - joeynmt.training - Example #3
2025-05-28 09:43:49,714 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:43:49,714 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:43:49,714 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'a', 'cres@@', '<unk>', '@', 'ce', 'e', 's@@', '<unk>', '@', 'vol@@', '<unk>', '@', 'a', '.', '</s>']
2025-05-28 09:43:49,715 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:43:49,715 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:43:49,715 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ a cres<unk> @ ce e s<unk> @ vol<unk> @ a .
2025-05-28 09:43:49,715 - INFO - joeynmt.training - Example #4
2025-05-28 09:43:49,715 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:43:49,715 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:43:49,715 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'consegu@@', '<unk>', '@', 'enza', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'una', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'na', 'di', 'cosa', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:43:49,716 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:43:49,716 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:43:49,716 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma consegu<unk> @ enza che vi mostr<unk> @ erò è una di<unk> @ seg<unk> @ na di cosa negli ultimi 25 anni .
2025-05-28 09:43:53,095 - INFO - joeynmt.training - Epoch   5, Step:    31100, Batch Loss:     1.171605, Batch Acc: 0.652114, Tokens per Sec:    17306, Lr: 0.000300
2025-05-28 09:43:56,523 - INFO - joeynmt.training - Epoch   5, Step:    31200, Batch Loss:     1.259094, Batch Acc: 0.654947, Tokens per Sec:    21697, Lr: 0.000300
2025-05-28 09:43:59,921 - INFO - joeynmt.training - Epoch   5, Step:    31300, Batch Loss:     1.173796, Batch Acc: 0.654289, Tokens per Sec:    21046, Lr: 0.000300
2025-05-28 09:44:03,300 - INFO - joeynmt.training - Epoch   5, Step:    31400, Batch Loss:     1.359956, Batch Acc: 0.651566, Tokens per Sec:    21397, Lr: 0.000300
2025-05-28 09:44:06,170 - INFO - joeynmt.training - Epoch   5: total training loss 8032.32
2025-05-28 09:44:06,171 - INFO - joeynmt.training - EPOCH 6
2025-05-28 09:44:06,683 - INFO - joeynmt.training - Epoch   6, Step:    31500, Batch Loss:     1.338009, Batch Acc: 0.668627, Tokens per Sec:    22314, Lr: 0.000300
2025-05-28 09:44:06,683 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:44:06,683 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:44:18,345 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.30, ppl:   3.66, acc:   0.65, generation: 11.6508[sec], evaluation: 0.0000[sec]
2025-05-28 09:44:18,346 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:44:18,906 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/29000.ckpt
2025-05-28 09:44:18,934 - INFO - joeynmt.training - Example #0
2025-05-28 09:44:18,935 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:44:18,935 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:44:18,935 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'due', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'ri@@', '<unk>', '@', 'durre', 'la', 'stessa', 'cosa', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ana', ',', 'che', 'il', 't@@', '<unk>', '@', 'essu@@', '<unk>', '@', 'ale', 'è', 'stato', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', '.', '</s>']
2025-05-28 09:44:18,936 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:44:18,936 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:44:18,936 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato queste due due due di<unk> @ apos<unk> @ iti<unk> @ ve per ri<unk> @ durre la stessa cosa ar<unk> @ c<unk> @ ic<unk> @ ana , che il t<unk> @ essu<unk> @ ale è stato fatto per tre milioni di anni .
2025-05-28 09:44:18,936 - INFO - joeynmt.training - Example #1
2025-05-28 09:44:18,936 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:44:18,936 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:44:18,936 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', ',', 'per', 'il', 'fatto', 'che', 'la', 'nostra', 'pros@@', '<unk>', '@', 'p@@', '<unk>', '@', 'ettiva', 'di', 'questo', 'problema', 'speci@@', '<unk>', '@', 'ale', ',', 'perché', 'non', 'è', 'l&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 09:44:18,937 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:44:18,937 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:44:18,937 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza , per il fatto che la nostra pros<unk> @ p<unk> @ ettiva di questo problema speci<unk> @ ale , perché non è l&apos; E<unk> @ is<unk> @ es .
2025-05-28 09:44:18,937 - INFO - joeynmt.training - Example #2
2025-05-28 09:44:18,937 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:44:18,937 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:44:18,937 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'è', 'il', 'cuore', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ale', ',', 'il', 'cuore', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'del', 'nostro', 'sistema', 'globale', '.', '</s>']
2025-05-28 09:44:18,938 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:44:18,938 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:44:18,938 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , è il cuore ar<unk> @ t<unk> @ ic<unk> @ ale , il cuore ar<unk> @ t<unk> @ ico del nostro sistema globale .
2025-05-28 09:44:18,938 - INFO - joeynmt.training - Example #3
2025-05-28 09:44:18,938 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:44:18,938 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:44:18,938 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', '@', 'ol@@', '<unk>', '@', 'la', 'cres@@', '<unk>', '@', 'cia', 'e', 'si', 's@@', '<unk>', '@', 'v@@', '<unk>', '@', 'ag@@', '<unk>', '@', 'lia', '.', '</s>']
2025-05-28 09:44:18,939 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:44:18,939 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:44:18,939 - INFO - joeynmt.training - 	Hypothesis: C<unk> @ ol<unk> @ la cres<unk> @ cia e si s<unk> @ v<unk> @ ag<unk> @ lia .
2025-05-28 09:44:18,939 - INFO - joeynmt.training - Example #4
2025-05-28 09:44:18,939 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:44:18,939 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:44:18,939 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'consegu@@', '<unk>', '@', 'enza', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'una', 'segn@@', '<unk>', '@', 'ata', 'di', 'una', 'seg@@', '<unk>', '@', 'a', 'che', 'cosa', 'sta', 'succe@@', '<unk>', '@', 'dendo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:44:18,940 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:44:18,940 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:44:18,940 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma consegu<unk> @ enza che vi mostr<unk> @ erò è una segn<unk> @ ata di una seg<unk> @ a che cosa sta succe<unk> @ dendo negli ultimi 25 anni .
2025-05-28 09:44:22,344 - INFO - joeynmt.training - Epoch   6, Step:    31600, Batch Loss:     1.314854, Batch Acc: 0.666615, Tokens per Sec:    17806, Lr: 0.000300
2025-05-28 09:44:25,749 - INFO - joeynmt.training - Epoch   6, Step:    31700, Batch Loss:     1.169979, Batch Acc: 0.666935, Tokens per Sec:    21159, Lr: 0.000300
2025-05-28 09:44:29,127 - INFO - joeynmt.training - Epoch   6, Step:    31800, Batch Loss:     1.289820, Batch Acc: 0.663439, Tokens per Sec:    20766, Lr: 0.000300
2025-05-28 09:44:32,529 - INFO - joeynmt.training - Epoch   6, Step:    31900, Batch Loss:     1.198674, Batch Acc: 0.668384, Tokens per Sec:    21625, Lr: 0.000300
2025-05-28 09:44:35,913 - INFO - joeynmt.training - Epoch   6, Step:    32000, Batch Loss:     1.223596, Batch Acc: 0.669303, Tokens per Sec:    20404, Lr: 0.000300
2025-05-28 09:44:35,914 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:44:35,914 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:44:46,530 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.30, ppl:   3.68, acc:   0.65, generation: 10.6061[sec], evaluation: 0.0000[sec]
2025-05-28 09:44:46,916 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/30000.ckpt
2025-05-28 09:44:46,942 - INFO - joeynmt.training - Example #0
2025-05-28 09:44:46,943 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:44:46,943 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:44:46,943 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'ri@@', '<unk>', '@', 'vedere', 'che', 'i', 't@@', '<unk>', '@', 'es@@', '<unk>', '@', 'chi', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'es@@', '<unk>', '@', 'chi', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ici', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'la', 'dimen@@', '<unk>', '@', 'sione', 'dell&apos;', 'acqua', 'è', 'stato', 'in@@', '<unk>', '@', 'qu@@', '<unk>', '@', 'ato', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'la', 'dimen@@', '<unk>', '@', 'sione', 'dell&apos;', 'anno', ',', 'per', 'cento', 'di', 'tre', 'milioni', 'di', 'anni', ',', 'l&apos;', 'acqua', 'è', 'stato', 'in@@', '<unk>', '@', 'vi@@', '<unk>', '@', 'dente', '.', '</s>']
2025-05-28 09:44:46,943 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:44:46,943 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:44:46,943 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per ri<unk> @ vedere che i t<unk> @ es<unk> @ chi ar<unk> @ t<unk> @ es<unk> @ chi ar<unk> @ t<unk> @ ici per tre milioni di anni , la dimen<unk> @ sione dell&apos; acqua è stato in<unk> @ qu<unk> @ ato per tre milioni di anni , la dimen<unk> @ sione dell&apos; anno , per cento di tre milioni di anni , l&apos; acqua è stato in<unk> @ vi<unk> @ dente .
2025-05-28 09:44:46,943 - INFO - joeynmt.training - Example #1
2025-05-28 09:44:46,944 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:44:46,944 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:44:46,944 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', ',', 'la', 'st@@', '<unk>', '@', 'av@@', '<unk>', '@', 'ità', 'di', 'questo', 'particolare', 'problema', ',', 'perché', 'non', 'è', 'l&apos;', 'idea', 'di', 'questo', 'problema', ',', 'non', 'è', 'l&apos;', 'idea', 'di', 'questo', 'problema', ',', 'non', 'è', 'l&apos;', 'idea', 'dell&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 09:44:46,944 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:44:46,944 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:44:46,944 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza , la st<unk> @ av<unk> @ ità di questo particolare problema , perché non è l&apos; idea di questo problema , non è l&apos; idea di questo problema , non è l&apos; idea dell&apos; E<unk> @ is<unk> @ es .
2025-05-28 09:44:46,944 - INFO - joeynmt.training - Example #2
2025-05-28 09:44:46,945 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:44:46,945 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:44:46,945 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'cer@@', '<unk>', '@', 't@@', '<unk>', '@', 'amente', ',', 'i', 'sen@@', '<unk>', '@', 'si', 'è', 'un', 't@@', '<unk>', '@', 'ale', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'del', 'cuore', 'globale', '.', '</s>']
2025-05-28 09:44:46,945 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:44:46,945 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:44:46,945 - INFO - joeynmt.training - 	Hypothesis: In cer<unk> @ t<unk> @ amente , i sen<unk> @ si è un t<unk> @ ale ar<unk> @ t<unk> @ ico del cuore globale .
2025-05-28 09:44:46,945 - INFO - joeynmt.training - Example #3
2025-05-28 09:44:46,946 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:44:46,946 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:44:46,946 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'a', 'cres@@', '<unk>', '@', 'ce', 'e', 'la', 's@@', '<unk>', '@', 'om@@', '<unk>', '@', 'mer', '.', '</s>']
2025-05-28 09:44:46,946 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:44:46,946 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:44:46,946 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ a cres<unk> @ ce e la s<unk> @ om<unk> @ mer .
2025-05-28 09:44:46,946 - INFO - joeynmt.training - Example #4
2025-05-28 09:44:46,947 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:44:46,947 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:44:46,947 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'o', 'è', 'una', 'ri@@', '<unk>', '@', 'ma@@', '<unk>', '@', 'zione', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:44:46,947 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:44:46,947 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:44:46,947 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ o è una ri<unk> @ ma<unk> @ zione che è successo negli ultimi 25 anni .
2025-05-28 09:44:50,355 - INFO - joeynmt.training - Epoch   6, Step:    32100, Batch Loss:     1.231964, Batch Acc: 0.663510, Tokens per Sec:    18615, Lr: 0.000300
2025-05-28 09:44:53,734 - INFO - joeynmt.training - Epoch   6, Step:    32200, Batch Loss:     1.194203, Batch Acc: 0.666181, Tokens per Sec:    20720, Lr: 0.000300
2025-05-28 09:44:57,181 - INFO - joeynmt.training - Epoch   6, Step:    32300, Batch Loss:     1.125065, Batch Acc: 0.662200, Tokens per Sec:    21006, Lr: 0.000300
2025-05-28 09:45:00,567 - INFO - joeynmt.training - Epoch   6, Step:    32400, Batch Loss:     1.024059, Batch Acc: 0.666283, Tokens per Sec:    20515, Lr: 0.000300
2025-05-28 09:45:04,000 - INFO - joeynmt.training - Epoch   6, Step:    32500, Batch Loss:     1.224904, Batch Acc: 0.661960, Tokens per Sec:    21033, Lr: 0.000300
2025-05-28 09:45:04,000 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:45:04,000 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:45:14,461 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.30, ppl:   3.67, acc:   0.65, generation: 10.4544[sec], evaluation: 0.0000[sec]
2025-05-28 09:45:14,816 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/29500.ckpt
2025-05-28 09:45:14,841 - INFO - joeynmt.training - Example #0
2025-05-28 09:45:14,842 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:45:14,842 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:45:14,842 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'due', 'cose', 'che', 'hanno', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'cose', 'per', 'ri@@', '<unk>', '@', 'guard@@', '<unk>', '@', 'ando', 'che', 'i', 'g@@', '<unk>', '@', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'per', 'la', 's@@', '<unk>', '@', 'quad@@', '<unk>', '@', 'ra', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', 'di', 'gr@@', '<unk>', '@', 'azi@@', '<unk>', '@', 'end@@', '<unk>', '@', 'ale', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'che', 'è', 'stato', 'fatto', 'per', 'cento', 'di', 'tre', 'milioni', 'di', 'anni', '.', '</s>']
2025-05-28 09:45:14,843 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:45:14,843 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:45:14,843 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due due cose che hanno mostr<unk> @ ato queste due cose per ri<unk> @ guard<unk> @ ando che i g<unk> @ ar<unk> @ t<unk> @ ica per la s<unk> @ quad<unk> @ ra ar<unk> @ t<unk> @ ica che ha fatto per tre milioni di anni di gr<unk> @ azi<unk> @ end<unk> @ ale per tre milioni di anni , che è stato fatto per cento di tre milioni di anni .
2025-05-28 09:45:14,843 - INFO - joeynmt.training - Example #1
2025-05-28 09:45:14,843 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:45:14,843 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:45:14,843 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', ',', 'non', 'è', 'abbastanza', 'la', 'cre@@', '<unk>', '@', 'azione', 'di', 'questo', 'problema', 'speci@@', '<unk>', '@', 'ale', ',', 'non', 'è', 'abbastanza', 'da', 'quello', 'che', 'non', 'è', 'il', 'del@@', '<unk>', '@', 'u@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'co', 'dell&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 09:45:14,844 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:45:14,844 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:45:14,844 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza , non è abbastanza la cre<unk> @ azione di questo problema speci<unk> @ ale , non è abbastanza da quello che non è il del<unk> @ u<unk> @ ic<unk> @ co dell&apos; E<unk> @ is<unk> @ es .
2025-05-28 09:45:14,844 - INFO - joeynmt.training - Example #2
2025-05-28 09:45:14,844 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:45:14,844 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:45:14,844 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'va', 'è', 'la', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'va', 'del', 'nostro', 'sistema', 'di', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'vo', 'del', 'nostro', 'sistema', 'globale', '.', '</s>']
2025-05-28 09:45:14,845 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:45:14,845 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:45:14,845 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la c<unk> @ atti<unk> @ va è la c<unk> @ atti<unk> @ va del nostro sistema di c<unk> @ atti<unk> @ vo del nostro sistema globale .
2025-05-28 09:45:14,845 - INFO - joeynmt.training - Example #3
2025-05-28 09:45:14,845 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:45:14,845 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:45:14,845 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'ent@@', '<unk>', '@', 'a', 'cres@@', '<unk>', '@', 'ce', 'e', 'il', 'p@@', '<unk>', '@', 'ò', 'in', 'est@@', '<unk>', '@', 'ate', 'e', 's@@', '<unk>', '@', 'om@@', '<unk>', '@', 'ate', 'nel', 'vento', '.', '</s>']
2025-05-28 09:45:14,845 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:45:14,845 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:45:14,845 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ ent<unk> @ a cres<unk> @ ce e il p<unk> @ ò in est<unk> @ ate e s<unk> @ om<unk> @ ate nel vento .
2025-05-28 09:45:14,846 - INFO - joeynmt.training - Example #4
2025-05-28 09:45:14,846 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:45:14,846 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:45:14,846 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'una', 'seg@@', '<unk>', '@', 'ret@@', '<unk>', '@', 'ta', 'di', 'cosa', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:45:14,846 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:45:14,846 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:45:14,846 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ erò è una seg<unk> @ ret<unk> @ ta di cosa è successo negli ultimi 25 anni .
2025-05-28 09:45:18,207 - INFO - joeynmt.training - Epoch   6, Step:    32600, Batch Loss:     1.256251, Batch Acc: 0.662599, Tokens per Sec:    18905, Lr: 0.000300
2025-05-28 09:45:21,596 - INFO - joeynmt.training - Epoch   6, Step:    32700, Batch Loss:     1.143416, Batch Acc: 0.661421, Tokens per Sec:    21172, Lr: 0.000300
2025-05-28 09:45:24,993 - INFO - joeynmt.training - Epoch   6, Step:    32800, Batch Loss:     1.321149, Batch Acc: 0.662332, Tokens per Sec:    21147, Lr: 0.000300
2025-05-28 09:45:28,362 - INFO - joeynmt.training - Epoch   6, Step:    32900, Batch Loss:     1.155758, Batch Acc: 0.661906, Tokens per Sec:    20565, Lr: 0.000300
2025-05-28 09:45:31,756 - INFO - joeynmt.training - Epoch   6, Step:    33000, Batch Loss:     1.205154, Batch Acc: 0.667528, Tokens per Sec:    21434, Lr: 0.000300
2025-05-28 09:45:31,757 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:45:31,757 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:45:43,944 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.30, ppl:   3.66, acc:   0.65, generation: 12.1759[sec], evaluation: 0.0000[sec]
2025-05-28 09:45:43,945 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:45:44,527 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/30500.ckpt
2025-05-28 09:45:44,554 - INFO - joeynmt.training - Example #0
2025-05-28 09:45:44,555 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:45:44,555 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:45:44,555 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'due', 'cose', 'che', 'hanno', 'mostr@@', '<unk>', '@', 'ato', 'in', 'questa', 'stanza', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ale', ',', 'che', 'i', 'bu@@', '<unk>', '@', 'oni', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ali', 'che', 'si', 'in@@', '<unk>', '@', 'ser@@', '<unk>', '@', 'va', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'la', 'Gr@@', '<unk>', '@', 'an', 'B@@', '<unk>', '@', 'is@@', '<unk>', '@', 'c@@', '<unk>', '@', 'en@@', '<unk>', '@', 'ne', 'ha', 'fatto', 'tre', 'milioni', 'di', 'anni', 'per', 'cento', '.', '</s>']
2025-05-28 09:45:44,556 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:45:44,556 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:45:44,556 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due due cose che hanno mostr<unk> @ ato in questa stanza ar<unk> @ c<unk> @ ic<unk> @ ale , che i bu<unk> @ oni ar<unk> @ t<unk> @ ic<unk> @ ali che si in<unk> @ ser<unk> @ va per tre milioni di anni , la Gr<unk> @ an B<unk> @ is<unk> @ c<unk> @ en<unk> @ ne ha fatto tre milioni di anni per cento .
2025-05-28 09:45:44,556 - INFO - joeynmt.training - Example #1
2025-05-28 09:45:44,556 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:45:44,556 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:45:44,556 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', ',', 'la', 'nostra', 'cre@@', '<unk>', '@', 'azione', ',', 'perché', 'non', 'è', 'un', 'tr@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'passi@@', '<unk>', '@', 'on@@', '<unk>', '@', 'ato', 'di', 'questo', 'particolare', 'problema', ',', 'non', 'è', 'la', 'd@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ke', '.', '</s>']
2025-05-28 09:45:44,557 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:45:44,557 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:45:44,557 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza , la nostra cre<unk> @ azione , perché non è un tr<unk> @ ap<unk> @ passi<unk> @ on<unk> @ ato di questo particolare problema , non è la d<unk> @ ic<unk> @ ke .
2025-05-28 09:45:44,557 - INFO - joeynmt.training - Example #2
2025-05-28 09:45:44,557 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:45:44,557 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:45:44,557 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'qualche', 'certo', 'senso', ',', 'la', 'b@@', '<unk>', '@', 'ella', 'è', 'la', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'va', 'del', 'cuore', 'globale', '.', '</s>']
2025-05-28 09:45:44,558 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:45:44,558 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:45:44,558 - INFO - joeynmt.training - 	Hypothesis: In qualche certo senso , la b<unk> @ ella è la c<unk> @ atti<unk> @ va del cuore globale .
2025-05-28 09:45:44,558 - INFO - joeynmt.training - Example #3
2025-05-28 09:45:44,558 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:45:44,558 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:45:44,558 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Vi@@', '<unk>', '@', 'aggi@@', '<unk>', '@', 'a', 'nel', 'vento', 'e', 'il', 'vento', '.', '</s>']
2025-05-28 09:45:44,559 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:45:44,559 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:45:44,559 - INFO - joeynmt.training - 	Hypothesis: Vi<unk> @ aggi<unk> @ a nel vento e il vento .
2025-05-28 09:45:44,559 - INFO - joeynmt.training - Example #4
2025-05-28 09:45:44,559 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:45:44,559 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:45:44,559 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'consegu@@', '<unk>', '@', 'enza', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'un', 'segn@@', '<unk>', '@', 'ale', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:45:44,559 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:45:44,559 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:45:44,559 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma consegu<unk> @ enza che vi mostr<unk> @ erò è un segn<unk> @ ale che è successo negli ultimi 25 anni .
2025-05-28 09:45:47,974 - INFO - joeynmt.training - Epoch   6, Step:    33100, Batch Loss:     1.190474, Batch Acc: 0.663500, Tokens per Sec:    17454, Lr: 0.000300
2025-05-28 09:45:51,364 - INFO - joeynmt.training - Epoch   6, Step:    33200, Batch Loss:     1.201884, Batch Acc: 0.658355, Tokens per Sec:    20862, Lr: 0.000300
2025-05-28 09:45:54,768 - INFO - joeynmt.training - Epoch   6, Step:    33300, Batch Loss:     1.273167, Batch Acc: 0.663947, Tokens per Sec:    21748, Lr: 0.000300
2025-05-28 09:45:58,152 - INFO - joeynmt.training - Epoch   6, Step:    33400, Batch Loss:     1.335630, Batch Acc: 0.661600, Tokens per Sec:    20539, Lr: 0.000300
2025-05-28 09:46:01,548 - INFO - joeynmt.training - Epoch   6, Step:    33500, Batch Loss:     1.193762, Batch Acc: 0.662062, Tokens per Sec:    21430, Lr: 0.000300
2025-05-28 09:46:01,548 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:46:01,548 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:46:13,762 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.30, ppl:   3.67, acc:   0.65, generation: 12.2029[sec], evaluation: 0.0000[sec]
2025-05-28 09:46:14,171 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/31000.ckpt
2025-05-28 09:46:14,198 - INFO - joeynmt.training - Example #0
2025-05-28 09:46:14,198 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:46:14,198 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:46:14,198 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'cose', 'per', 'cui', 'le', 'cose', 'sono', 'state', 'di@@', '<unk>', '@', 'mostra', 'che', 'i', 'can@@', '<unk>', '@', 'ali', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'es@@', '<unk>', '@', 'co', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'che', 'l&apos;', 'anno', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'es@@', '<unk>', '@', 'co', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'per', 'la', 'guar@@', '<unk>', '@', 'di@@', '<unk>', '@', 'a', ',', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'il', '40', 'per', 'cento', '.', '</s>']
2025-05-28 09:46:14,199 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:46:14,199 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:46:14,199 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due cose per cui le cose sono state di<unk> @ mostra che i can<unk> @ ali ar<unk> @ t<unk> @ es<unk> @ co ar<unk> @ t<unk> @ ico che l&apos; anno ar<unk> @ t<unk> @ es<unk> @ co ar<unk> @ t<unk> @ ico per la guar<unk> @ di<unk> @ a , per tre milioni di anni , per il 40 per cento .
2025-05-28 09:46:14,199 - INFO - joeynmt.training - Example #1
2025-05-28 09:46:14,200 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:46:14,200 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:46:14,200 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', ',', 'la', 'sost@@', '<unk>', '@', 'anza', 'di', 'questo', 'problema', ',', 'la', 'sost@@', '<unk>', '@', 'anza', 'di', 'questo', 'problema', 'speci@@', '<unk>', '@', 'ale', ',', 'perché', 'non', 'lo', 'è', 'l&apos;', 'idea', 'di', 'questo', 'particolare', 'problema', ',', 'non', 'è', 'l&apos;', 'idea', 'di', 'un', 'livello', 'dell&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 09:46:14,200 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:46:14,200 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:46:14,200 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza , la sost<unk> @ anza di questo problema , la sost<unk> @ anza di questo problema speci<unk> @ ale , perché non lo è l&apos; idea di questo particolare problema , non è l&apos; idea di un livello dell&apos; E<unk> @ is<unk> @ es .
2025-05-28 09:46:14,200 - INFO - joeynmt.training - Example #2
2025-05-28 09:46:14,201 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:46:14,201 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:46:14,201 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'qualche', 'certo', 'senso', ',', 'la', 'sc@@', '<unk>', '@', 'app@@', '<unk>', '@', 'ola', ',', 'la', 'co@@', '<unk>', '@', 'per@@', '<unk>', '@', 'tura', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ale', ',', 'il', 'cuore', 'di', 'un', 'sistema', 'di', 'c@@', '<unk>', '@', 'ru@@', '<unk>', '@', 'ot@@', '<unk>', '@', 'ale', '.', '</s>']
2025-05-28 09:46:14,201 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:46:14,201 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:46:14,201 - INFO - joeynmt.training - 	Hypothesis: In qualche certo senso , la sc<unk> @ app<unk> @ ola , la co<unk> @ per<unk> @ tura ar<unk> @ t<unk> @ ale , il cuore di un sistema di c<unk> @ ru<unk> @ ot<unk> @ ale .
2025-05-28 09:46:14,201 - INFO - joeynmt.training - Example #3
2025-05-28 09:46:14,201 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:46:14,202 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:46:14,202 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'volta', 'che', 'la', 'sal@@', '<unk>', '@', 'a', 'e', 'la', 'b@@', '<unk>', '@', 'otti@@', '<unk>', '@', 'gli@@', '<unk>', '@', 'a', '.', '</s>']
2025-05-28 09:46:14,202 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:46:14,202 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:46:14,202 - INFO - joeynmt.training - 	Hypothesis: La prima volta che la sal<unk> @ a e la b<unk> @ otti<unk> @ gli<unk> @ a .
2025-05-28 09:46:14,202 - INFO - joeynmt.training - Example #4
2025-05-28 09:46:14,202 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:46:14,202 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:46:14,202 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'consegu@@', '<unk>', '@', 'enza', 'che', 'vi', 'mostr@@', '<unk>', '@', 'o', 'è', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'co@@', '<unk>', '@', 'dice', 'che', 'cosa', 'succede', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:46:14,203 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:46:14,203 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:46:14,203 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma consegu<unk> @ enza che vi mostr<unk> @ o è un seg<unk> @ no di un seg<unk> @ no di co<unk> @ dice che cosa succede negli ultimi 25 anni .
2025-05-28 09:46:17,630 - INFO - joeynmt.training - Epoch   6, Step:    33600, Batch Loss:     1.209535, Batch Acc: 0.661733, Tokens per Sec:    18499, Lr: 0.000300
2025-05-28 09:46:21,031 - INFO - joeynmt.training - Epoch   6, Step:    33700, Batch Loss:     1.257037, Batch Acc: 0.665589, Tokens per Sec:    20930, Lr: 0.000300
2025-05-28 09:46:24,427 - INFO - joeynmt.training - Epoch   6, Step:    33800, Batch Loss:     1.321143, Batch Acc: 0.663188, Tokens per Sec:    21284, Lr: 0.000300
2025-05-28 09:46:27,811 - INFO - joeynmt.training - Epoch   6, Step:    33900, Batch Loss:     1.284525, Batch Acc: 0.665277, Tokens per Sec:    21485, Lr: 0.000300
2025-05-28 09:46:31,205 - INFO - joeynmt.training - Epoch   6, Step:    34000, Batch Loss:     1.171161, Batch Acc: 0.661715, Tokens per Sec:    21224, Lr: 0.000300
2025-05-28 09:46:31,206 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:46:31,206 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:46:43,335 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.29, ppl:   3.63, acc:   0.65, generation: 12.1189[sec], evaluation: 0.0000[sec]
2025-05-28 09:46:43,336 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:46:43,930 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/32000.ckpt
2025-05-28 09:46:43,958 - INFO - joeynmt.training - Example #0
2025-05-28 09:46:43,958 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:46:43,958 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:46:43,958 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'cose', 'per', 'vedere', 'che', 'i', 'ghi@@', '<unk>', '@', 'acci@@', '<unk>', '@', 'ono', 'per', 'vedere', 'che', 'i', 'ghi@@', '<unk>', '@', 'acci@@', '<unk>', '@', 'ono', 'le', 'et@@', '<unk>', '@', 're', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ali', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ici', 'che', 'gli', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'et', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', '.', '</s>']
2025-05-28 09:46:43,959 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:46:43,959 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:46:43,959 - INFO - joeynmt.training - 	Hypothesis: Ho mostr<unk> @ ato queste due cose per vedere che i ghi<unk> @ acci<unk> @ ono per vedere che i ghi<unk> @ acci<unk> @ ono le et<unk> @ re ar<unk> @ t<unk> @ ic<unk> @ ali ar<unk> @ t<unk> @ ici che gli E<unk> @ is<unk> @ k<unk> @ et ha fatto per tre milioni di anni .
2025-05-28 09:46:43,959 - INFO - joeynmt.training - Example #1
2025-05-28 09:46:43,960 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:46:43,960 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:46:43,960 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'forte', ',', 'che', 'non', 'è', 'abbastanza', 'la', 'cre@@', '<unk>', '@', 'azione', 'di', 'questo', 'particolare', 'problema', ',', 'perché', 'non', 'è', 'l&apos;', 'idea', 'di', 'questo', 'particolare', 'problema', ',', 'non', 'è', 'l&apos;', 'idea', 'di', 'quello', 'che', 'mostra', '.', '</s>']
2025-05-28 09:46:43,960 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:46:43,960 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:46:43,960 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza forte , che non è abbastanza la cre<unk> @ azione di questo particolare problema , perché non è l&apos; idea di questo particolare problema , non è l&apos; idea di quello che mostra .
2025-05-28 09:46:43,960 - INFO - joeynmt.training - Example #2
2025-05-28 09:46:43,961 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:46:43,961 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:46:43,961 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'cer@@', '<unk>', '@', 'ca', 'di', 'cer@@', '<unk>', '@', 'ca', 'di', 'essere', 'il', 'cuore', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', '.', '</s>']
2025-05-28 09:46:43,961 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:46:43,961 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:46:43,961 - INFO - joeynmt.training - 	Hypothesis: In cer<unk> @ ca di cer<unk> @ ca di essere il cuore ar<unk> @ t<unk> @ ico .
2025-05-28 09:46:43,961 - INFO - joeynmt.training - Example #3
2025-05-28 09:46:43,962 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:46:43,962 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:46:43,962 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'en@@', '<unk>', '@', 'gono', 'nel', 'vento', 'e', 'il', 'vento', '.', '</s>']
2025-05-28 09:46:43,962 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:46:43,962 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:46:43,962 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ en<unk> @ gono nel vento e il vento .
2025-05-28 09:46:43,962 - INFO - joeynmt.training - Example #4
2025-05-28 09:46:43,962 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:46:43,963 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:46:43,963 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'una', 'ri@@', '<unk>', '@', 'ma@@', '<unk>', '@', 'zione', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:46:43,963 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:46:43,963 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:46:43,963 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ erò è una ri<unk> @ ma<unk> @ zione che è successo negli ultimi 25 anni .
2025-05-28 09:46:47,401 - INFO - joeynmt.training - Epoch   6, Step:    34100, Batch Loss:     1.276578, Batch Acc: 0.660931, Tokens per Sec:    17555, Lr: 0.000300
2025-05-28 09:46:50,807 - INFO - joeynmt.training - Epoch   6, Step:    34200, Batch Loss:     1.435705, Batch Acc: 0.664721, Tokens per Sec:    20929, Lr: 0.000300
2025-05-28 09:46:54,230 - INFO - joeynmt.training - Epoch   6, Step:    34300, Batch Loss:     1.196891, Batch Acc: 0.661522, Tokens per Sec:    21264, Lr: 0.000300
2025-05-28 09:46:57,651 - INFO - joeynmt.training - Epoch   6, Step:    34400, Batch Loss:     1.224865, Batch Acc: 0.665046, Tokens per Sec:    21101, Lr: 0.000300
2025-05-28 09:47:01,037 - INFO - joeynmt.training - Epoch   6, Step:    34500, Batch Loss:     1.236714, Batch Acc: 0.661820, Tokens per Sec:    20888, Lr: 0.000300
2025-05-28 09:47:01,037 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:47:01,037 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:47:12,601 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.29, ppl:   3.64, acc:   0.65, generation: 11.5564[sec], evaluation: 0.0000[sec]
2025-05-28 09:47:12,982 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/32500.ckpt
2025-05-28 09:47:13,008 - INFO - joeynmt.training - Example #0
2025-05-28 09:47:13,008 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:47:13,008 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:47:13,008 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'questa', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'per', 'vedere', 'che', 'il', 'ghi@@', '<unk>', '@', 'acci@@', '<unk>', '@', 'aio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', ',', 'che', 'i', 'ghi@@', '<unk>', '@', 'acci@@', '<unk>', '@', 'ono', 'per', 'la', 'l@@', '<unk>', '@', 'ett@@', '<unk>', '@', 'ura', 'di', 'cui', 'gli', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ano', ',', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'cento', 'di', 'l@@', '<unk>', '@', 'arg@@', '<unk>', '@', 'a', 'è', 'stato', '.', '</s>']
2025-05-28 09:47:13,009 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:47:13,009 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:47:13,009 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato questa di<unk> @ apos<unk> @ iti<unk> @ va per vedere che il ghi<unk> @ acci<unk> @ aio ar<unk> @ t<unk> @ ico , che i ghi<unk> @ acci<unk> @ ono per la l<unk> @ ett<unk> @ ura di cui gli E<unk> @ is<unk> @ c<unk> @ ano , per tre milioni di anni , per cento di l<unk> @ arg<unk> @ a è stato .
2025-05-28 09:47:13,009 - INFO - joeynmt.training - Example #1
2025-05-28 09:47:13,009 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:47:13,009 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:47:13,009 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'abbastanza', 'da', 'quanto', 'sia', 'una', 'cosa', 'che', 'non', 'sia', 'una', 'cosa', 'che', 'non', 'è', 'una', 'cosa', 'che', 'non', 'è', 'l&apos;', 'idea', 'di', 'questo', 'particolare', 'problema', ',', 'non', 'è', 'l&apos;', 'idea', 'di', 'questo', 'particolare', 'problema', ',', 'non', 'è', 'l&apos;', 'idea', 'dell&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 09:47:13,009 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:47:13,009 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:47:13,010 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza abbastanza da quanto sia una cosa che non sia una cosa che non è una cosa che non è l&apos; idea di questo particolare problema , non è l&apos; idea di questo particolare problema , non è l&apos; idea dell&apos; E<unk> @ is<unk> @ es .
2025-05-28 09:47:13,010 - INFO - joeynmt.training - Example #2
2025-05-28 09:47:13,010 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:47:13,010 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:47:13,010 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'cer@@', '<unk>', '@', 'ca', 'di', 'essere', 'un', 'certo', 'senso', 'ar@@', '<unk>', '@', 'to', 'di', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'are', 'il', 'cuore', 'di', 'un', 'sistema', 'di', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'mo', '.', '</s>']
2025-05-28 09:47:13,010 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:47:13,010 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:47:13,010 - INFO - joeynmt.training - 	Hypothesis: In cer<unk> @ ca di essere un certo senso ar<unk> @ to di ar<unk> @ t<unk> @ ic<unk> @ are il cuore di un sistema di c<unk> @ atti<unk> @ mo .
2025-05-28 09:47:13,010 - INFO - joeynmt.training - Example #3
2025-05-28 09:47:13,011 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:47:13,011 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:47:13,011 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'oi', 'cres@@', '<unk>', '@', 'ce', 'in', 'vento', 'e', 's@@', '<unk>', '@', 'v@@', '<unk>', '@', 'egli@@', '<unk>', '@', 'ò', 'in', 'est@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 09:47:13,011 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:47:13,011 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:47:13,011 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ oi cres<unk> @ ce in vento e s<unk> @ v<unk> @ egli<unk> @ ò in est<unk> @ ate .
2025-05-28 09:47:13,011 - INFO - joeynmt.training - Example #4
2025-05-28 09:47:13,012 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:47:13,012 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:47:13,012 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'una', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'ret@@', '<unk>', '@', 'a', 'che', 'cosa', 'acc@@', '<unk>', '@', 'ade', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:47:13,012 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:47:13,012 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:47:13,012 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ erò è una di<unk> @ seg<unk> @ ret<unk> @ a che cosa acc<unk> @ ade negli ultimi 25 anni .
2025-05-28 09:47:16,322 - INFO - joeynmt.training - Epoch   6, Step:    34600, Batch Loss:     1.164387, Batch Acc: 0.666190, Tokens per Sec:    18798, Lr: 0.000300
2025-05-28 09:47:19,622 - INFO - joeynmt.training - Epoch   6, Step:    34700, Batch Loss:     1.288860, Batch Acc: 0.665715, Tokens per Sec:    21336, Lr: 0.000300
2025-05-28 09:47:22,958 - INFO - joeynmt.training - Epoch   6, Step:    34800, Batch Loss:     1.197246, Batch Acc: 0.659239, Tokens per Sec:    21543, Lr: 0.000300
2025-05-28 09:47:26,254 - INFO - joeynmt.training - Epoch   6, Step:    34900, Batch Loss:     1.177053, Batch Acc: 0.662833, Tokens per Sec:    21007, Lr: 0.000300
2025-05-28 09:47:29,555 - INFO - joeynmt.training - Epoch   6, Step:    35000, Batch Loss:     1.306371, Batch Acc: 0.662820, Tokens per Sec:    21193, Lr: 0.000300
2025-05-28 09:47:29,555 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:47:29,555 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:47:40,535 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.29, ppl:   3.64, acc:   0.65, generation: 10.9692[sec], evaluation: 0.0000[sec]
2025-05-28 09:47:41,037 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/33500.ckpt
2025-05-28 09:47:41,063 - INFO - joeynmt.training - Example #0
2025-05-28 09:47:41,063 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:47:41,063 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:47:41,063 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'vedere', 'che', 'i', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'amente', ',', 'che', 'i', 'ghi@@', '<unk>', '@', 'acci@@', '<unk>', '@', 'ono', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'l&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'c@@', '<unk>', '@', 'en@@', '<unk>', '@', 'ne', ',', 'che', 'ha', 'avuto', 'il', '40', '%', 'di', 's@@', '<unk>', '@', 'ov@@', '<unk>', '@', 'a', '.', '</s>']
2025-05-28 09:47:41,064 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:47:41,064 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:47:41,064 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per vedere che i ghi<unk> @ accio ar<unk> @ t<unk> @ amente , che i ghi<unk> @ acci<unk> @ ono per tre milioni di anni , l&apos; E<unk> @ is<unk> @ c<unk> @ en<unk> @ ne , che ha avuto il 40 % di s<unk> @ ov<unk> @ a .
2025-05-28 09:47:41,064 - INFO - joeynmt.training - Example #1
2025-05-28 09:47:41,064 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:47:41,064 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:47:41,064 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'difficile', 'da', 'fare', 'la', 'prima', 'prima', 'prima', 'cosa', 'di', 'fare', 'è', 'che', 'la', 'giu@@', '<unk>', '@', 'st@@', '<unk>', '@', 'izia', ',', 'perché', 'non', 'è', 'il', 'di@@', '<unk>', '@', 'mostra', 'il', 'problema', 'di', 'questo', 'problema', ',', 'non', 'è', 'il', 'di@@', '<unk>', '@', 'mostra', 'il', 'l@@', '<unk>', '@', 'enzi@@', '<unk>', '@', 'o', 'del', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 09:47:41,065 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:47:41,065 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:47:41,065 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza difficile da fare la prima prima prima cosa di fare è che la giu<unk> @ st<unk> @ izia , perché non è il di<unk> @ mostra il problema di questo problema , non è il di<unk> @ mostra il l<unk> @ enzi<unk> @ o del ghi<unk> @ accio .
2025-05-28 09:47:41,065 - INFO - joeynmt.training - Example #2
2025-05-28 09:47:41,065 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:47:41,065 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:47:41,065 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'certo', 'senso', ',', 'è', 'la', 'c@@', '<unk>', '@', 'app@@', '<unk>', '@', 'ola', 'di', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'on@@', '<unk>', '@', 'ia', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'vo', 'globale', '.', '</s>']
2025-05-28 09:47:41,066 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:47:41,066 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:47:41,066 - INFO - joeynmt.training - 	Hypothesis: In certo senso , è la c<unk> @ app<unk> @ ola di ar<unk> @ t<unk> @ on<unk> @ ia ar<unk> @ t<unk> @ ica , il cuore del nostro sistema c<unk> @ atti<unk> @ vo globale .
2025-05-28 09:47:41,066 - INFO - joeynmt.training - Example #3
2025-05-28 09:47:41,066 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:47:41,066 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:47:41,066 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Vi@@', '<unk>', '@', 'ene', 'in', 'ci@@', '<unk>', '@', 'ma', ',', 'in', 'sal@@', '<unk>', '@', 'a', 'e', 's@@', '<unk>', '@', 'ale', 'nel', 'vento', '.', '</s>']
2025-05-28 09:47:41,067 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:47:41,067 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:47:41,067 - INFO - joeynmt.training - 	Hypothesis: Vi<unk> @ ene in ci<unk> @ ma , in sal<unk> @ a e s<unk> @ ale nel vento .
2025-05-28 09:47:41,067 - INFO - joeynmt.training - Example #4
2025-05-28 09:47:41,067 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:47:41,067 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:47:41,067 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'un', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'no', 'di', 'un', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'no', '.', '</s>']
2025-05-28 09:47:41,067 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:47:41,068 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:47:41,068 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ erò è un seg<unk> @ no di un di<unk> @ seg<unk> @ no di un di<unk> @ seg<unk> @ no .
2025-05-28 09:47:44,373 - INFO - joeynmt.training - Epoch   6, Step:    35100, Batch Loss:     1.277621, Batch Acc: 0.659270, Tokens per Sec:    18296, Lr: 0.000300
2025-05-28 09:47:47,698 - INFO - joeynmt.training - Epoch   6, Step:    35200, Batch Loss:     1.148760, Batch Acc: 0.657031, Tokens per Sec:    21343, Lr: 0.000300
2025-05-28 09:47:50,989 - INFO - joeynmt.training - Epoch   6, Step:    35300, Batch Loss:     1.297459, Batch Acc: 0.661227, Tokens per Sec:    21777, Lr: 0.000300
2025-05-28 09:47:54,292 - INFO - joeynmt.training - Epoch   6, Step:    35400, Batch Loss:     1.162151, Batch Acc: 0.661696, Tokens per Sec:    21325, Lr: 0.000300
2025-05-28 09:47:57,606 - INFO - joeynmt.training - Epoch   6, Step:    35500, Batch Loss:     1.081872, Batch Acc: 0.662584, Tokens per Sec:    21560, Lr: 0.000300
2025-05-28 09:47:57,607 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:47:57,607 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:48:08,290 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.29, ppl:   3.62, acc:   0.65, generation: 10.6731[sec], evaluation: 0.0000[sec]
2025-05-28 09:48:08,291 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:48:08,903 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/31500.ckpt
2025-05-28 09:48:08,932 - INFO - joeynmt.training - Example #0
2025-05-28 09:48:08,933 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:48:08,933 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:48:08,933 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'vedere', 'che', 'il', 't@@', '<unk>', '@', 'asso', 'di', 'ag@@', '<unk>', '@', 'ric@@', '<unk>', '@', 'o@@', '<unk>', '@', 'de', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'che', 'ha', 'avuto', 'il', 't@@', '<unk>', '@', 'asso', 'di', 'tre', 'milioni', 'di', 'anni', ',', 'il', 'gr@@', '<unk>', '@', 'asso', 'di', 'gr@@', '<unk>', '@', 'asso', 'di', 's@@', '<unk>', '@', 'ov@@', '<unk>', '@', 'a', 'tre', 'anni', '.', '</s>']
2025-05-28 09:48:08,933 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:48:08,934 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:48:08,934 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due due di<unk> @ apos<unk> @ iti<unk> @ ve per vedere che il t<unk> @ asso di ag<unk> @ ric<unk> @ o<unk> @ de ar<unk> @ t<unk> @ ico che ha avuto il t<unk> @ asso di tre milioni di anni , il gr<unk> @ asso di gr<unk> @ asso di s<unk> @ ov<unk> @ a tre anni .
2025-05-28 09:48:08,934 - INFO - joeynmt.training - Example #1
2025-05-28 09:48:08,934 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:48:08,934 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:48:08,934 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'così', 'che', 'è', 'più', 'forte', 'di', 'abbastanza', 'la', 'st@@', '<unk>', '@', 'a@@', '<unk>', '@', 'sione', 'di', 'questo', 'problema', ',', 'perché', 'non', 'è', 'il', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'no', '.', '</s>']
2025-05-28 09:48:08,935 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:48:08,935 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:48:08,935 - INFO - joeynmt.training - 	Hypothesis: Ma non è così che è più forte di abbastanza la st<unk> @ a<unk> @ sione di questo problema , perché non è il di<unk> @ seg<unk> @ no .
2025-05-28 09:48:08,935 - INFO - joeynmt.training - Example #2
2025-05-28 09:48:08,935 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:48:08,935 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:48:08,935 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'è', 'la', 'sc@@', '<unk>', '@', 'app@@', '<unk>', '@', 'a', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'è', 'la', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'va', 'dell&apos;', 'et@@', '<unk>', '@', 'izz@@', '<unk>', '@', 'azione', 'del', 'nostro', 'sistema', 'cli@@', '<unk>', '@', 'mat@@', '<unk>', '@', 'ico', '.', '</s>']
2025-05-28 09:48:08,936 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:48:08,936 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:48:08,936 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , è la sc<unk> @ app<unk> @ a ar<unk> @ t<unk> @ ica è la c<unk> @ atti<unk> @ va dell&apos; et<unk> @ izz<unk> @ azione del nostro sistema cli<unk> @ mat<unk> @ ico .
2025-05-28 09:48:08,936 - INFO - joeynmt.training - Example #3
2025-05-28 09:48:08,936 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:48:08,936 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:48:08,936 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'a', 'cresc@@', '<unk>', '@', 'ere', 'e', 's@@', '<unk>', '@', 'om@@', '<unk>', '@', 'p@@', '<unk>', '@', 'a', '.', '</s>']
2025-05-28 09:48:08,936 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:48:08,936 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:48:08,937 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ a cresc<unk> @ ere e s<unk> @ om<unk> @ p<unk> @ a .
2025-05-28 09:48:08,937 - INFO - joeynmt.training - Example #4
2025-05-28 09:48:08,937 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:48:08,937 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:48:08,937 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'consegu@@', '<unk>', '@', 'enza', 'che', 'vi', 'mostr@@', '<unk>', '@', 'o', 'è', 'un', 'seg@@', '<unk>', '@', 'ret@@', '<unk>', '@', 'o', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:48:08,937 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:48:08,937 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:48:08,937 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma consegu<unk> @ enza che vi mostr<unk> @ o è un seg<unk> @ ret<unk> @ o che è successo negli ultimi 25 anni .
2025-05-28 09:48:12,335 - INFO - joeynmt.training - Epoch   6, Step:    35600, Batch Loss:     1.174664, Batch Acc: 0.662566, Tokens per Sec:    17025, Lr: 0.000300
2025-05-28 09:48:15,722 - INFO - joeynmt.training - Epoch   6, Step:    35700, Batch Loss:     1.255523, Batch Acc: 0.665923, Tokens per Sec:    20767, Lr: 0.000300
2025-05-28 09:48:19,116 - INFO - joeynmt.training - Epoch   6, Step:    35800, Batch Loss:     1.217227, Batch Acc: 0.661994, Tokens per Sec:    21322, Lr: 0.000300
2025-05-28 09:48:22,518 - INFO - joeynmt.training - Epoch   6, Step:    35900, Batch Loss:     1.145227, Batch Acc: 0.667146, Tokens per Sec:    21246, Lr: 0.000300
2025-05-28 09:48:25,897 - INFO - joeynmt.training - Epoch   6, Step:    36000, Batch Loss:     1.266526, Batch Acc: 0.658723, Tokens per Sec:    21158, Lr: 0.000300
2025-05-28 09:48:25,897 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:48:25,897 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:48:36,549 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.28, ppl:   3.61, acc:   0.65, generation: 10.6419[sec], evaluation: 0.0000[sec]
2025-05-28 09:48:36,550 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:48:37,132 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/33000.ckpt
2025-05-28 09:48:37,159 - INFO - joeynmt.training - Example #0
2025-05-28 09:48:37,159 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:48:37,159 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:48:37,159 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'due', 'due', 'cose', 'per', 'cento', 'di', 'questa', 'serie', 'di', 'cose', 'per', 'ri@@', '<unk>', '@', 'guard@@', '<unk>', '@', 'ando', 'che', 'i', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ato', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'il', '4@@', '<unk>', '@', '8', '%', 'della', 'gr@@', '<unk>', '@', 'ave', 'è', 'stato', 'in@@', '<unk>', '@', 'ato', 'per', 'tre', 'milioni', 'di', 'anni', '.', '</s>']
2025-05-28 09:48:37,160 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:48:37,160 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:48:37,160 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due due due cose per cento di questa serie di cose per ri<unk> @ guard<unk> @ ando che i ghi<unk> @ accio ar<unk> @ t<unk> @ ic<unk> @ ato per tre milioni di anni , il 4<unk> @ 8 % della gr<unk> @ ave è stato in<unk> @ ato per tre milioni di anni .
2025-05-28 09:48:37,160 - INFO - joeynmt.training - Example #1
2025-05-28 09:48:37,160 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:48:37,160 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:48:37,160 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'forte', 'la', 'nostra', 'er@@', '<unk>', '@', 'n@@', '<unk>', '@', 'arr@@', '<unk>', '@', 'azione', ',', 'perché', 'non', 'è', 'il', 'del@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ato', 'di', 'questo', 'problema', 'speci@@', '<unk>', '@', 'ale', ',', 'perché', 'non', 'è', 'il', 'del@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'co', 'del', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 09:48:37,161 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:48:37,161 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:48:37,161 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte la nostra er<unk> @ n<unk> @ arr<unk> @ azione , perché non è il del<unk> @ ic<unk> @ ato di questo problema speci<unk> @ ale , perché non è il del<unk> @ ic<unk> @ co del ghi<unk> @ accio .
2025-05-28 09:48:37,161 - INFO - joeynmt.training - Example #2
2025-05-28 09:48:37,161 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:48:37,161 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:48:37,161 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'p@@', '<unk>', '@', 'aus@@', '<unk>', '@', 'a', 'è', 'la', 'p@@', '<unk>', '@', 'elle', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', ',', 'il', 'cuore', 'di', 'un', 'cuore', 'di', 'un', 'sistema', 'di', 'peggi@@', '<unk>', '@', 'ore', 'globale', '.', '</s>']
2025-05-28 09:48:37,162 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:48:37,162 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:48:37,162 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la p<unk> @ aus<unk> @ a è la p<unk> @ elle ar<unk> @ t<unk> @ ica , il cuore di un cuore di un sistema di peggi<unk> @ ore globale .
2025-05-28 09:48:37,162 - INFO - joeynmt.training - Example #3
2025-05-28 09:48:37,162 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:48:37,162 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:48:37,162 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', '@', 'e', 'cres@@', '<unk>', '@', 'ce', 'nel', 'vento', 'e', 'il', 'vento', 'in', 'est@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 09:48:37,162 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:48:37,162 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:48:37,163 - INFO - joeynmt.training - 	Hypothesis: C<unk> @ e cres<unk> @ ce nel vento e il vento in est<unk> @ ate .
2025-05-28 09:48:37,163 - INFO - joeynmt.training - Example #4
2025-05-28 09:48:37,163 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:48:37,163 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:48:37,163 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 's@@', '<unk>', '@', 'li@@', '<unk>', '@', 'de', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'una', 'seg@@', '<unk>', '@', 'ret@@', '<unk>', '@', 'a', 'che', 'cosa', 'succede', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:48:37,163 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:48:37,163 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:48:37,163 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma s<unk> @ li<unk> @ de che vi mostr<unk> @ erò è una seg<unk> @ ret<unk> @ a che cosa succede negli ultimi 25 anni .
2025-05-28 09:48:40,470 - INFO - joeynmt.training - Epoch   6, Step:    36100, Batch Loss:     1.159076, Batch Acc: 0.661548, Tokens per Sec:    18027, Lr: 0.000300
2025-05-28 09:48:43,871 - INFO - joeynmt.training - Epoch   6, Step:    36200, Batch Loss:     1.305971, Batch Acc: 0.659372, Tokens per Sec:    21437, Lr: 0.000300
2025-05-28 09:48:47,254 - INFO - joeynmt.training - Epoch   6, Step:    36300, Batch Loss:     1.174173, Batch Acc: 0.662094, Tokens per Sec:    21359, Lr: 0.000300
2025-05-28 09:48:50,662 - INFO - joeynmt.training - Epoch   6, Step:    36400, Batch Loss:     1.351561, Batch Acc: 0.662105, Tokens per Sec:    21277, Lr: 0.000300
2025-05-28 09:48:54,056 - INFO - joeynmt.training - Epoch   6, Step:    36500, Batch Loss:     1.281922, Batch Acc: 0.660306, Tokens per Sec:    20313, Lr: 0.000300
2025-05-28 09:48:54,056 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:48:54,056 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:49:04,319 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.29, ppl:   3.62, acc:   0.65, generation: 10.2565[sec], evaluation: 0.0000[sec]
2025-05-28 09:49:04,695 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/35000.ckpt
2025-05-28 09:49:04,721 - INFO - joeynmt.training - Example #0
2025-05-28 09:49:04,722 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:49:04,722 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:49:04,722 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'due', 'due', 'milioni', 'di', 'anni', 'per', 'vedere', 'che', 'i', 'can@@', '<unk>', '@', 'ali', 'per', 'vedere', 'che', 'i', 't@@', '<unk>', '@', 'op@@', '<unk>', '@', 'i', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ini', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ati', 'per', 'tre', 'anni', ',', 'il', '40', '.', '</s>']
2025-05-28 09:49:04,722 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:49:04,723 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:49:04,723 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato queste due due due milioni di anni per vedere che i can<unk> @ ali per vedere che i t<unk> @ op<unk> @ i ar<unk> @ c<unk> @ ini ar<unk> @ t<unk> @ ic<unk> @ ati per tre anni , il 40 .
2025-05-28 09:49:04,723 - INFO - joeynmt.training - Example #1
2025-05-28 09:49:04,723 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:49:04,723 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:49:04,723 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'forte', ',', 'perché', 'non', 'è', 'il', 'del@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'io', 'di', 'questo', 'problema', 'speci@@', '<unk>', '@', 'ale', ',', 'perché', 'non', 'è', 'il', 'del@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'co', 'dell&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 09:49:04,723 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:49:04,723 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:49:04,724 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte , perché non è il del<unk> @ ic<unk> @ io di questo problema speci<unk> @ ale , perché non è il del<unk> @ ic<unk> @ co dell&apos; E<unk> @ is<unk> @ es .
2025-05-28 09:49:04,724 - INFO - joeynmt.training - Example #2
2025-05-28 09:49:04,724 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:49:04,724 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:49:04,724 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'c@@', '<unk>', '@', 'app@@', '<unk>', '@', 'ola', 'è', 'la', 'c@@', '<unk>', '@', 'app@@', '<unk>', '@', 'ola', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'va', ',', 'il', 'cuore', 'è', 'il', 'cuore', 'che', 'è', 'il', 'cuore', 'del', 'nostro', 'sistema', 'globale', '.', '</s>']
2025-05-28 09:49:04,724 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:49:04,724 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:49:04,724 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la c<unk> @ app<unk> @ ola è la c<unk> @ app<unk> @ ola c<unk> @ atti<unk> @ va , il cuore è il cuore che è il cuore del nostro sistema globale .
2025-05-28 09:49:04,724 - INFO - joeynmt.training - Example #3
2025-05-28 09:49:04,725 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:49:04,725 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:49:04,725 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&apos;', 'una', 'volta', 'che', 'si', 's@@', '<unk>', '@', 'ente', 'e', 's@@', '<unk>', '@', 'ale', 'in', 'est@@', '<unk>', '@', 'ate', 'e', 's@@', '<unk>', '@', 'om@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 09:49:04,725 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:49:04,725 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:49:04,725 - INFO - joeynmt.training - 	Hypothesis: E &apos; una volta che si s<unk> @ ente e s<unk> @ ale in est<unk> @ ate e s<unk> @ om<unk> @ ate .
2025-05-28 09:49:04,725 - INFO - joeynmt.training - Example #4
2025-05-28 09:49:04,726 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:49:04,726 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:49:04,726 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'consegu@@', '<unk>', '@', 'enza', 'che', 'vi', 'mostr@@', '<unk>', '@', 'o', 'è', 'un', 'segn@@', '<unk>', '@', 'ale', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:49:04,726 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:49:04,726 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:49:04,726 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma consegu<unk> @ enza che vi mostr<unk> @ o è un segn<unk> @ ale che è successo negli ultimi 25 anni .
2025-05-28 09:49:08,015 - INFO - joeynmt.training - Epoch   6, Step:    36600, Batch Loss:     1.244092, Batch Acc: 0.662791, Tokens per Sec:    19550, Lr: 0.000300
2025-05-28 09:49:11,288 - INFO - joeynmt.training - Epoch   6, Step:    36700, Batch Loss:     1.264971, Batch Acc: 0.661350, Tokens per Sec:    22417, Lr: 0.000300
2025-05-28 09:49:14,536 - INFO - joeynmt.training - Epoch   6, Step:    36800, Batch Loss:     1.411274, Batch Acc: 0.663003, Tokens per Sec:    21741, Lr: 0.000300
2025-05-28 09:49:17,792 - INFO - joeynmt.training - Epoch   6, Step:    36900, Batch Loss:     1.280775, Batch Acc: 0.661283, Tokens per Sec:    22148, Lr: 0.000300
2025-05-28 09:49:21,019 - INFO - joeynmt.training - Epoch   6, Step:    37000, Batch Loss:     1.236410, Batch Acc: 0.660060, Tokens per Sec:    21628, Lr: 0.000300
2025-05-28 09:49:21,019 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:49:21,019 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:49:31,580 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.29, ppl:   3.63, acc:   0.65, generation: 10.5536[sec], evaluation: 0.0000[sec]
2025-05-28 09:49:31,935 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/34500.ckpt
2025-05-28 09:49:31,959 - INFO - joeynmt.training - Example #0
2025-05-28 09:49:31,960 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:49:31,960 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:49:31,960 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'due', 'due', 'due', 'cose', 'per', 'ri@@', '<unk>', '@', 'vel@@', '<unk>', '@', 'are', 'per', 'con@@', '<unk>', '@', 'durre', 'il', 't@@', '<unk>', '@', 'avolo', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', ',', 'che', 'ha', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'la', 'gr@@', '<unk>', '@', 'av@@', '<unk>', '@', 'ità', 'di', 'cui', 'si', 'in@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'na', 'di', 'terra', ',', 'il', '40', 'per@@', '<unk>', '@', 'cento', '.', '</s>']
2025-05-28 09:49:31,960 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:49:31,961 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:49:31,961 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due due due due cose per ri<unk> @ vel<unk> @ are per con<unk> @ durre il t<unk> @ avolo ar<unk> @ t<unk> @ ico ar<unk> @ t<unk> @ ico , che ha fatto per tre milioni di anni , la gr<unk> @ av<unk> @ ità di cui si in<unk> @ seg<unk> @ na di terra , il 40 per<unk> @ cento .
2025-05-28 09:49:31,961 - INFO - joeynmt.training - Example #1
2025-05-28 09:49:31,961 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:49:31,961 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:49:31,961 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'forte', 'la', 'cre@@', '<unk>', '@', 'ativ@@', '<unk>', '@', 'ità', 'di', 'questo', 'problema', 'speci@@', '<unk>', '@', 'ale', ',', 'perché', 'non', 'lo', 'ha', 'il', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'no', 'del', 'problema', ',', 'perché', 'non', 'è', 'il', 'di@@', '<unk>', '@', 'mostra', 'il', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'no', 'del', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 09:49:31,962 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:49:31,962 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:49:31,962 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte la cre<unk> @ ativ<unk> @ ità di questo problema speci<unk> @ ale , perché non lo ha il di<unk> @ seg<unk> @ no del problema , perché non è il di<unk> @ mostra il di<unk> @ seg<unk> @ no del ghi<unk> @ accio .
2025-05-28 09:49:31,962 - INFO - joeynmt.training - Example #2
2025-05-28 09:49:31,962 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:49:31,962 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:49:31,962 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'dei', 'sen@@', '<unk>', '@', 'si', 'è', 'il', 't@@', '<unk>', '@', 'asso', 'di', 'ag@@', '<unk>', '@', 'no', 'di', 'c@@', '<unk>', '@', 'ass@@', '<unk>', '@', 'ù', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'globale', 'globale', 'globale', '.', '</s>']
2025-05-28 09:49:31,963 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:49:31,963 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:49:31,963 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , il cuore dei sen<unk> @ si è il t<unk> @ asso di ag<unk> @ no di c<unk> @ ass<unk> @ ù , il cuore del nostro sistema globale globale globale .
2025-05-28 09:49:31,963 - INFO - joeynmt.training - Example #3
2025-05-28 09:49:31,963 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:49:31,963 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:49:31,963 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&apos;', 'una', 'volta', 'nel', 'vento', 'e', 's@@', '<unk>', '@', 'oc@@', '<unk>', '@', 'chi@@', '<unk>', '@', 'o@@', '<unk>', '@', 'te', 'nel', 'vento', 'e', 's@@', '<unk>', '@', 'om@@', '<unk>', '@', 'ate', 'nel', 'vento', '.', '</s>']
2025-05-28 09:49:31,963 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:49:31,963 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:49:31,964 - INFO - joeynmt.training - 	Hypothesis: E &apos; una volta nel vento e s<unk> @ oc<unk> @ chi<unk> @ o<unk> @ te nel vento e s<unk> @ om<unk> @ ate nel vento .
2025-05-28 09:49:31,964 - INFO - joeynmt.training - Example #4
2025-05-28 09:49:31,964 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:49:31,964 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:49:31,964 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'consegu@@', '<unk>', '@', 'enza', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'un', 'indi@@', '<unk>', '@', 'ce', 'di', 'cosa', 'succede', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:49:31,964 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:49:31,964 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:49:31,964 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma consegu<unk> @ enza che vi mostr<unk> @ erò è un indi<unk> @ ce di cosa succede negli ultimi 25 anni .
2025-05-28 09:49:35,380 - INFO - joeynmt.training - Epoch   6, Step:    37100, Batch Loss:     1.178447, Batch Acc: 0.662327, Tokens per Sec:    18719, Lr: 0.000300
2025-05-28 09:49:38,834 - INFO - joeynmt.training - Epoch   6, Step:    37200, Batch Loss:     1.096499, Batch Acc: 0.663900, Tokens per Sec:    21040, Lr: 0.000300
2025-05-28 09:49:42,244 - INFO - joeynmt.training - Epoch   6, Step:    37300, Batch Loss:     1.271776, Batch Acc: 0.659875, Tokens per Sec:    21292, Lr: 0.000300
2025-05-28 09:49:45,668 - INFO - joeynmt.training - Epoch   6, Step:    37400, Batch Loss:     1.263781, Batch Acc: 0.661698, Tokens per Sec:    21445, Lr: 0.000300
2025-05-28 09:49:49,131 - INFO - joeynmt.training - Epoch   6, Step:    37500, Batch Loss:     1.094431, Batch Acc: 0.662610, Tokens per Sec:    21357, Lr: 0.000300
2025-05-28 09:49:49,131 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:49:49,132 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:50:00,961 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.28, ppl:   3.61, acc:   0.65, generation: 11.8185[sec], evaluation: 0.0000[sec]
2025-05-28 09:50:01,412 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/34000.ckpt
2025-05-28 09:50:01,439 - INFO - joeynmt.training - Example #0
2025-05-28 09:50:01,440 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:50:01,440 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:50:01,440 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'ri@@', '<unk>', '@', 'durre', 'che', 'i', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ano', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'la', 'gr@@', '<unk>', '@', 'ati@@', '<unk>', '@', 'a', 'che', 'ha', 'avuto', 'la', 'gr@@', '<unk>', '@', 'ati@@', '<unk>', '@', 'a', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'cento', '.', '</s>']
2025-05-28 09:50:01,441 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:50:01,441 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:50:01,441 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato queste due due di<unk> @ apos<unk> @ iti<unk> @ ve per ri<unk> @ durre che i ghi<unk> @ accio ar<unk> @ t<unk> @ ic<unk> @ ano per tre milioni di anni , la gr<unk> @ ati<unk> @ a che ha avuto la gr<unk> @ ati<unk> @ a tre milioni di anni , per cento .
2025-05-28 09:50:01,441 - INFO - joeynmt.training - Example #1
2025-05-28 09:50:01,441 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:50:01,441 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:50:01,441 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'forte', 'la', 'cre@@', '<unk>', '@', 'ativ@@', '<unk>', '@', 'ità', 'di', 'questo', 'problema', ',', 'perché', 'non', 'è', 'l&apos;', 'in@@', '<unk>', '@', 'vi@@', '<unk>', '@', 'st@@', '<unk>', '@', 'ica', 'del', 'problema', ',', 'non', 'è', 'l&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', 'mostra', 'il', 'del@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'io', '.', '</s>']
2025-05-28 09:50:01,442 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:50:01,442 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:50:01,442 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte la cre<unk> @ ativ<unk> @ ità di questo problema , perché non è l&apos; in<unk> @ vi<unk> @ st<unk> @ ica del problema , non è l&apos; E<unk> @ is<unk> @ es mostra il del<unk> @ ic<unk> @ io .
2025-05-28 09:50:01,442 - INFO - joeynmt.training - Example #2
2025-05-28 09:50:01,442 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:50:01,442 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:50:01,443 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'c@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ona', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ale', 'è', 'la', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'va', 'del', 'cuore', 'del', 'nostro', 'sistema', 'globale', '.', '</s>']
2025-05-28 09:50:01,443 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:50:01,443 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:50:01,443 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la c<unk> @ ic<unk> @ ona ar<unk> @ c<unk> @ ic<unk> @ ale è la c<unk> @ atti<unk> @ va del cuore del nostro sistema globale .
2025-05-28 09:50:01,443 - INFO - joeynmt.training - Example #3
2025-05-28 09:50:01,443 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:50:01,443 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:50:01,443 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'e', 'si', 'ri@@', '<unk>', '@', 'fer@@', '<unk>', '@', 'isce', 'in', 'est@@', '<unk>', '@', 'ate', 'e', 's@@', '<unk>', '@', 'v@@', '<unk>', '@', 'egli@@', '<unk>', '@', 'ere', 'in', 'est@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 09:50:01,444 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:50:01,444 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:50:01,444 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ e si ri<unk> @ fer<unk> @ isce in est<unk> @ ate e s<unk> @ v<unk> @ egli<unk> @ ere in est<unk> @ ate .
2025-05-28 09:50:01,444 - INFO - joeynmt.training - Example #4
2025-05-28 09:50:01,444 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:50:01,444 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:50:01,444 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'ano', 'è', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'circa', '25', 'anni', '.', '</s>']
2025-05-28 09:50:01,445 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:50:01,445 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:50:01,445 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ ano è un seg<unk> @ no di un seg<unk> @ no di circa 25 anni .
2025-05-28 09:50:04,906 - INFO - joeynmt.training - Epoch   6, Step:    37600, Batch Loss:     1.205574, Batch Acc: 0.662140, Tokens per Sec:    18655, Lr: 0.000300
2025-05-28 09:50:08,304 - INFO - joeynmt.training - Epoch   6, Step:    37700, Batch Loss:     1.245070, Batch Acc: 0.662488, Tokens per Sec:    20782, Lr: 0.000300
2025-05-28 09:50:11,594 - INFO - joeynmt.training - Epoch   6: total training loss 7759.20
2025-05-28 09:50:11,595 - INFO - joeynmt.training - EPOCH 7
2025-05-28 09:50:11,699 - INFO - joeynmt.training - Epoch   7, Step:    37800, Batch Loss:     1.249251, Batch Acc: 0.658296, Tokens per Sec:    19790, Lr: 0.000300
2025-05-28 09:50:15,104 - INFO - joeynmt.training - Epoch   7, Step:    37900, Batch Loss:     1.236453, Batch Acc: 0.681143, Tokens per Sec:    20584, Lr: 0.000300
2025-05-28 09:50:18,483 - INFO - joeynmt.training - Epoch   7, Step:    38000, Batch Loss:     1.255798, Batch Acc: 0.679408, Tokens per Sec:    19904, Lr: 0.000300
2025-05-28 09:50:18,483 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:50:18,484 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:50:29,064 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.29, ppl:   3.64, acc:   0.65, generation: 10.5732[sec], evaluation: 0.0000[sec]
2025-05-28 09:50:29,075 - INFO - joeynmt.training - Example #0
2025-05-28 09:50:29,076 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:50:29,076 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:50:29,076 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'questi', 'due', 's@@', '<unk>', '@', 'li@@', '<unk>', '@', 'de', 'per', 'far', 'sì', 'che', 'i', 'ghi@@', '<unk>', '@', 'acci@@', '<unk>', '@', 'are', 'i', 'ghi@@', '<unk>', '@', 'acci@@', '<unk>', '@', 'ai', 'che', 'i', 'ghi@@', '<unk>', '@', 'acci@@', '<unk>', '@', 'ai', 'di', 'tre', 'milioni', 'di', 'anni', ',', 'la', 'gr@@', '<unk>', '@', 'av@@', '<unk>', '@', 'an@@', '<unk>', '@', 'sione', 'della', 'gr@@', '<unk>', '@', 'av@@', '<unk>', '@', 'an@@', '<unk>', '@', 'ea', ',', 'per', 'tre', 'milioni', 'di', 'anni', '.', '</s>']
2025-05-28 09:50:29,076 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:50:29,076 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:50:29,076 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato questi due s<unk> @ li<unk> @ de per far sì che i ghi<unk> @ acci<unk> @ are i ghi<unk> @ acci<unk> @ ai che i ghi<unk> @ acci<unk> @ ai di tre milioni di anni , la gr<unk> @ av<unk> @ an<unk> @ sione della gr<unk> @ av<unk> @ an<unk> @ ea , per tre milioni di anni .
2025-05-28 09:50:29,076 - INFO - joeynmt.training - Example #1
2025-05-28 09:50:29,076 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:50:29,076 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:50:29,076 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'forte', 'la', 'cre@@', '<unk>', '@', 'azione', 'di', 'questo', 'problema', ',', 'perché', 'non', 'è', 'che', 'non', 'è', 'il', 'punto', '<unk>', '@', 'par@@', '<unk>', '@', 'ten@@', '<unk>', '@', 'do', 'da', 'il', 'punto', 'di', 'vista', '.', '</s>']
2025-05-28 09:50:29,076 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:50:29,076 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:50:29,076 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte la cre<unk> @ azione di questo problema , perché non è che non è il punto <unk> @ par<unk> @ ten<unk> @ do da il punto di vista .
2025-05-28 09:50:29,076 - INFO - joeynmt.training - Example #2
2025-05-28 09:50:29,077 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:50:29,077 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:50:29,077 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'dei', 'ghi@@', '<unk>', '@', 'acci@@', '<unk>', '@', 'aio', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'vo', '.', '</s>']
2025-05-28 09:50:29,077 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:50:29,077 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:50:29,077 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , il cuore dei ghi<unk> @ acci<unk> @ aio , il cuore del nostro sistema c<unk> @ atti<unk> @ vo .
2025-05-28 09:50:29,077 - INFO - joeynmt.training - Example #3
2025-05-28 09:50:29,077 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:50:29,077 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:50:29,077 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'cres@@', '<unk>', '@', 'ce', 'nel', 'vento', 'e', 'il', 'vento', ',', 'in', 'est@@', '<unk>', '@', 'ate', 'e', 's@@', '<unk>', '@', 'otti@@', '<unk>', '@', 'le', '.', '</s>']
2025-05-28 09:50:29,077 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:50:29,077 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:50:29,077 - INFO - joeynmt.training - 	Hypothesis: E cres<unk> @ ce nel vento e il vento , in est<unk> @ ate e s<unk> @ otti<unk> @ le .
2025-05-28 09:50:29,077 - INFO - joeynmt.training - Example #4
2025-05-28 09:50:29,078 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:50:29,078 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:50:29,078 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'consegu@@', '<unk>', '@', 'enza', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'cosa', 'succede', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:50:29,078 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:50:29,078 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:50:29,078 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma consegu<unk> @ enza che vi mostr<unk> @ erò è un seg<unk> @ no di un seg<unk> @ no di cosa succede negli ultimi 25 anni .
2025-05-28 09:50:32,364 - INFO - joeynmt.training - Epoch   7, Step:    38100, Batch Loss:     1.147800, Batch Acc: 0.675664, Tokens per Sec:    22300, Lr: 0.000300
2025-05-28 09:50:35,609 - INFO - joeynmt.training - Epoch   7, Step:    38200, Batch Loss:     1.232827, Batch Acc: 0.678173, Tokens per Sec:    21641, Lr: 0.000300
2025-05-28 09:50:38,882 - INFO - joeynmt.training - Epoch   7, Step:    38300, Batch Loss:     1.121745, Batch Acc: 0.672479, Tokens per Sec:    22866, Lr: 0.000300
2025-05-28 09:50:42,165 - INFO - joeynmt.training - Epoch   7, Step:    38400, Batch Loss:     1.097968, Batch Acc: 0.676686, Tokens per Sec:    21663, Lr: 0.000300
2025-05-28 09:50:45,600 - INFO - joeynmt.training - Epoch   7, Step:    38500, Batch Loss:     1.227551, Batch Acc: 0.677868, Tokens per Sec:    21044, Lr: 0.000300
2025-05-28 09:50:45,600 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:50:45,600 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:50:57,377 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.29, ppl:   3.62, acc:   0.65, generation: 11.7659[sec], evaluation: 0.0000[sec]
2025-05-28 09:50:57,758 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/37000.ckpt
2025-05-28 09:50:57,784 - INFO - joeynmt.training - Example #0
2025-05-28 09:50:57,784 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:50:57,784 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:50:57,784 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'ri@@', '<unk>', '@', 'durre', 'la', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'i', 'ghi@@', '<unk>', '@', 'acci@@', '<unk>', '@', 'ano', 'che', 'i', 'ghi@@', '<unk>', '@', 'acci@@', '<unk>', '@', 'ano', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'la', 'gr@@', '<unk>', '@', 'av@@', '<unk>', '@', 'anz@@', '<unk>', '@', 'it@@', '<unk>', '@', 'ann@@', '<unk>', '@', 'a', ',', 'il', '40', '%', '.', '</s>']
2025-05-28 09:50:57,785 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:50:57,785 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:50:57,785 - INFO - joeynmt.training - 	Hypothesis: Ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per ri<unk> @ durre la di<unk> @ apos<unk> @ iti<unk> @ va che i ghi<unk> @ acci<unk> @ ano che i ghi<unk> @ acci<unk> @ ano per tre milioni di anni , la gr<unk> @ av<unk> @ anz<unk> @ it<unk> @ ann<unk> @ a , il 40 % .
2025-05-28 09:50:57,785 - INFO - joeynmt.training - Example #1
2025-05-28 09:50:57,785 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:50:57,785 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:50:57,786 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'forte', 'che', 'il', 'fatto', 'che', 'non', 'sia', 'abbastanza', 'forte', 'la', 'Terra', ',', 'perché', 'non', 'è', 'l&apos;', 'esplor@@', '<unk>', '@', 'azione', 'di', 'questo', 'problema', 'speci@@', '<unk>', '@', 'ale', ',', 'non', 'è', 'il', 'bu@@', '<unk>', '@', 'ff@@', '<unk>', '@', 'ico', '.', '</s>']
2025-05-28 09:50:57,786 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:50:57,786 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:50:57,786 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza forte che il fatto che non sia abbastanza forte la Terra , perché non è l&apos; esplor<unk> @ azione di questo problema speci<unk> @ ale , non è il bu<unk> @ ff<unk> @ ico .
2025-05-28 09:50:57,786 - INFO - joeynmt.training - Example #2
2025-05-28 09:50:57,786 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:50:57,786 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:50:57,786 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'dei', 'nostri', 'sistemi', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ani', 'è', 'la', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'va', 'del', 'nostro', 'sistema', 'globale', '.', '</s>']
2025-05-28 09:50:57,787 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:50:57,787 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:50:57,787 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , il cuore dei nostri sistemi ar<unk> @ c<unk> @ ic<unk> @ ani è la c<unk> @ atti<unk> @ va del nostro sistema globale .
2025-05-28 09:50:57,787 - INFO - joeynmt.training - Example #3
2025-05-28 09:50:57,787 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:50:57,787 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:50:57,787 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'volta', 'che', 'si', 'è', 'ri@@', '<unk>', '@', 'ma@@', '<unk>', '@', 'sta', 'e', 'in', 'est@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 09:50:57,788 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:50:57,788 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:50:57,788 - INFO - joeynmt.training - 	Hypothesis: La prima volta che si è ri<unk> @ ma<unk> @ sta e in est<unk> @ ate .
2025-05-28 09:50:57,788 - INFO - joeynmt.training - Example #4
2025-05-28 09:50:57,788 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:50:57,788 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:50:57,788 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 's@@', '<unk>', '@', 'li@@', '<unk>', '@', 'de', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'un', 'seg@@', '<unk>', '@', 'ret@@', '<unk>', '@', 'o', 'di', 'cosa', 'succede', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:50:57,788 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:50:57,788 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:50:57,788 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma s<unk> @ li<unk> @ de che vi mostr<unk> @ erò è un seg<unk> @ ret<unk> @ o di cosa succede negli ultimi 25 anni .
2025-05-28 09:51:01,124 - INFO - joeynmt.training - Epoch   7, Step:    38600, Batch Loss:     1.212089, Batch Acc: 0.676085, Tokens per Sec:    18780, Lr: 0.000300
2025-05-28 09:51:04,538 - INFO - joeynmt.training - Epoch   7, Step:    38700, Batch Loss:     1.456554, Batch Acc: 0.674008, Tokens per Sec:    20777, Lr: 0.000300
2025-05-28 09:51:07,943 - INFO - joeynmt.training - Epoch   7, Step:    38800, Batch Loss:     1.059103, Batch Acc: 0.674599, Tokens per Sec:    21211, Lr: 0.000300
2025-05-28 09:51:11,341 - INFO - joeynmt.training - Epoch   7, Step:    38900, Batch Loss:     1.205888, Batch Acc: 0.670346, Tokens per Sec:    20800, Lr: 0.000300
2025-05-28 09:51:14,775 - INFO - joeynmt.training - Epoch   7, Step:    39000, Batch Loss:     1.237727, Batch Acc: 0.671028, Tokens per Sec:    22175, Lr: 0.000300
2025-05-28 09:51:14,775 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:51:14,775 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:51:24,947 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.28, ppl:   3.60, acc:   0.65, generation: 10.1644[sec], evaluation: 0.0000[sec]
2025-05-28 09:51:24,947 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:51:25,582 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/36500.ckpt
2025-05-28 09:51:25,608 - INFO - joeynmt.training - Example #0
2025-05-28 09:51:25,609 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:51:25,609 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:51:25,609 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'ri@@', '<unk>', '@', 'pensare', 'che', 'l&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'h@@', '<unk>', '@', 't@@', '<unk>', '@', 'h', ',', 'che', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'la', 'gr@@', '<unk>', '@', 'av@@', '<unk>', '@', 'ità', 'è', 'stata', 'in@@', '<unk>', '@', 'ata', 'da', 'tre', 'milioni', 'di', 'anni', '.', '</s>']
2025-05-28 09:51:25,610 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:51:25,610 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:51:25,610 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per ri<unk> @ pensare che l&apos; E<unk> @ is<unk> @ k<unk> @ h<unk> @ t<unk> @ h , che per tre milioni di anni , la gr<unk> @ av<unk> @ ità è stata in<unk> @ ata da tre milioni di anni .
2025-05-28 09:51:25,610 - INFO - joeynmt.training - Example #1
2025-05-28 09:51:25,610 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:51:25,610 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:51:25,610 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'forte', 'la', 'st@@', '<unk>', '@', 'av@@', '<unk>', '@', 'ità', 'di', 'questo', 'problema', 'speci@@', '<unk>', '@', 'ale', ',', 'perché', 'non', 'è', 'il', 'di@@', '<unk>', '@', 'mostra', 'il', 'D@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ke', 'del', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 09:51:25,611 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:51:25,611 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:51:25,611 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza forte la st<unk> @ av<unk> @ ità di questo problema speci<unk> @ ale , perché non è il di<unk> @ mostra il D<unk> @ ic<unk> @ ke del E<unk> @ is<unk> @ es .
2025-05-28 09:51:25,611 - INFO - joeynmt.training - Example #2
2025-05-28 09:51:25,611 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:51:25,611 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:51:25,611 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'è', 'la', 'sc@@', '<unk>', '@', 'at@@', '<unk>', '@', 'app@@', '<unk>', '@', 'a', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'atore', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'mon@@', '<unk>', '@', 'diale', '.', '</s>']
2025-05-28 09:51:25,612 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:51:25,612 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:51:25,612 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , è la sc<unk> @ at<unk> @ app<unk> @ a ar<unk> @ c<unk> @ atore ar<unk> @ t<unk> @ ico , il cuore del nostro sistema mon<unk> @ diale .
2025-05-28 09:51:25,612 - INFO - joeynmt.training - Example #3
2025-05-28 09:51:25,612 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:51:25,612 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:51:25,612 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'di', 'tutto', ',', 'e', 'si', 's@@', '<unk>', '@', 'ente', 'in', 'est@@', '<unk>', '@', 'ate', 'e', 's@@', '<unk>', '@', 'os@@', '<unk>', '@', 'pes@@', '<unk>', '@', 'o', '.', '</s>']
2025-05-28 09:51:25,613 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:51:25,613 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:51:25,613 - INFO - joeynmt.training - 	Hypothesis: La prima di tutto , e si s<unk> @ ente in est<unk> @ ate e s<unk> @ os<unk> @ pes<unk> @ o .
2025-05-28 09:51:25,613 - INFO - joeynmt.training - Example #4
2025-05-28 09:51:25,613 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:51:25,613 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:51:25,613 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'seg@@', '<unk>', '@', 'no', 'di', 'cosa', 'sta', 'succe@@', '<unk>', '@', 'dendo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:51:25,613 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:51:25,613 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:51:25,613 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ erò è un seg<unk> @ no di seg<unk> @ no di cosa sta succe<unk> @ dendo negli ultimi 25 anni .
2025-05-28 09:51:29,024 - INFO - joeynmt.training - Epoch   7, Step:    39100, Batch Loss:     1.068153, Batch Acc: 0.670892, Tokens per Sec:    17962, Lr: 0.000300
2025-05-28 09:51:32,449 - INFO - joeynmt.training - Epoch   7, Step:    39200, Batch Loss:     1.132151, Batch Acc: 0.673720, Tokens per Sec:    21114, Lr: 0.000300
2025-05-28 09:51:35,899 - INFO - joeynmt.training - Epoch   7, Step:    39300, Batch Loss:     1.176079, Batch Acc: 0.670699, Tokens per Sec:    20517, Lr: 0.000300
2025-05-28 09:51:39,322 - INFO - joeynmt.training - Epoch   7, Step:    39400, Batch Loss:     1.130296, Batch Acc: 0.668148, Tokens per Sec:    20982, Lr: 0.000300
2025-05-28 09:51:42,708 - INFO - joeynmt.training - Epoch   7, Step:    39500, Batch Loss:     1.146309, Batch Acc: 0.672525, Tokens per Sec:    20671, Lr: 0.000300
2025-05-28 09:51:42,709 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:51:42,709 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:51:53,997 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.28, ppl:   3.61, acc:   0.65, generation: 11.2775[sec], evaluation: 0.0000[sec]
2025-05-28 09:51:54,433 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/38500.ckpt
2025-05-28 09:51:54,460 - INFO - joeynmt.training - Example #0
2025-05-28 09:51:54,461 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:51:54,461 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:51:54,461 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 's@@', '<unk>', '@', 'li@@', '<unk>', '@', 'de', 'per', 'ri@@', '<unk>', '@', 'durre', 'la', 'p@@', '<unk>', '@', 'elle', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ine', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ino', 'che', 'si', 'è', 'ri@@', '<unk>', '@', 'vel@@', '<unk>', '@', 'ato', 'per', 'tre', 'milioni', 'di', 'volte', 'la', 'gr@@', '<unk>', '@', 'av@@', '<unk>', '@', 'ità', '.', '</s>']
2025-05-28 09:51:54,462 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:51:54,462 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:51:54,462 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due s<unk> @ li<unk> @ de per ri<unk> @ durre la p<unk> @ elle ar<unk> @ c<unk> @ ine ar<unk> @ c<unk> @ ino che si è ri<unk> @ vel<unk> @ ato per tre milioni di volte la gr<unk> @ av<unk> @ ità .
2025-05-28 09:51:54,462 - INFO - joeynmt.training - Example #1
2025-05-28 09:51:54,462 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:51:54,462 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:51:54,462 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'forte', 'che', 'questo', 'problema', ',', 'perché', 'non', 'lo', 'fa', 'abbastanza', 'forte', 'da', 'questo', 'problema', ',', 'perché', 'non', 'lo', 'sa', 'di', 'questo', 'problema', ',', 'perché', 'non', 'lo', 'mostra', 'il', 'vi@@', '<unk>', '@', 'll@@', '<unk>', '@', 'aggio', 'del', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 09:51:54,463 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:51:54,463 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:51:54,463 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza forte che questo problema , perché non lo fa abbastanza forte da questo problema , perché non lo sa di questo problema , perché non lo mostra il vi<unk> @ ll<unk> @ aggio del ghi<unk> @ accio .
2025-05-28 09:51:54,463 - INFO - joeynmt.training - Example #2
2025-05-28 09:51:54,463 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:51:54,464 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:51:54,464 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'sc@@', '<unk>', '@', 'at@@', '<unk>', '@', 'ola', 'è', 'la', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'va', 'del', 'cuore', 'del', 'nostro', 'sistema', 'globale', '.', '</s>']
2025-05-28 09:51:54,464 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:51:54,464 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:51:54,464 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la sc<unk> @ at<unk> @ ola è la c<unk> @ atti<unk> @ va del cuore del nostro sistema globale .
2025-05-28 09:51:54,464 - INFO - joeynmt.training - Example #3
2025-05-28 09:51:54,464 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:51:54,465 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:51:54,465 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', '@', 'er@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ano', 'nel', 'vento', 'e', 'il', 'vento', 'in', 'est@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 09:51:54,465 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:51:54,465 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:51:54,465 - INFO - joeynmt.training - 	Hypothesis: C<unk> @ er<unk> @ c<unk> @ ano nel vento e il vento in est<unk> @ ate .
2025-05-28 09:51:54,465 - INFO - joeynmt.training - Example #4
2025-05-28 09:51:54,465 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:51:54,465 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:51:54,465 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'un', 'indi@@', '<unk>', '@', 'zio', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:51:54,466 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:51:54,466 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:51:54,466 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ erò è un seg<unk> @ no di un indi<unk> @ zio che è successo negli ultimi 25 anni .
2025-05-28 09:51:57,896 - INFO - joeynmt.training - Epoch   7, Step:    39600, Batch Loss:     1.242952, Batch Acc: 0.668709, Tokens per Sec:    18417, Lr: 0.000300
2025-05-28 09:52:01,326 - INFO - joeynmt.training - Epoch   7, Step:    39700, Batch Loss:     1.135217, Batch Acc: 0.674427, Tokens per Sec:    20936, Lr: 0.000300
2025-05-28 09:52:04,717 - INFO - joeynmt.training - Epoch   7, Step:    39800, Batch Loss:     1.280572, Batch Acc: 0.672757, Tokens per Sec:    20699, Lr: 0.000300
2025-05-28 09:52:08,129 - INFO - joeynmt.training - Epoch   7, Step:    39900, Batch Loss:     1.185019, Batch Acc: 0.672483, Tokens per Sec:    21302, Lr: 0.000300
2025-05-28 09:52:11,555 - INFO - joeynmt.training - Epoch   7, Step:    40000, Batch Loss:     1.125804, Batch Acc: 0.670559, Tokens per Sec:    20825, Lr: 0.000300
2025-05-28 09:52:11,556 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:52:11,556 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:52:22,467 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.28, ppl:   3.59, acc:   0.65, generation: 10.9041[sec], evaluation: 0.0000[sec]
2025-05-28 09:52:22,467 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:52:22,989 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/35500.ckpt
2025-05-28 09:52:23,009 - INFO - joeynmt.training - Example #0
2025-05-28 09:52:23,010 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:52:23,010 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:52:23,010 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 's@@', '<unk>', '@', 'li@@', '<unk>', '@', 'de', 'per', 'far', 'sì', 'che', 'i', 'propri@@', '<unk>', '@', 'et@@', '<unk>', '@', 'ari', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ini', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ini', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ini', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'cento', 'è', 'stato', 'in@@', '<unk>', '@', 'vi@@', '<unk>', '@', 'ato', '.', '</s>']
2025-05-28 09:52:23,010 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:52:23,010 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:52:23,010 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due s<unk> @ li<unk> @ de per far sì che i propri<unk> @ et<unk> @ ari ar<unk> @ c<unk> @ ini ar<unk> @ c<unk> @ ini ar<unk> @ c<unk> @ ini per tre milioni di anni , per cento è stato in<unk> @ vi<unk> @ ato .
2025-05-28 09:52:23,010 - INFO - joeynmt.training - Example #1
2025-05-28 09:52:23,011 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:52:23,011 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:52:23,011 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'forte', 'la', 'ser@@', '<unk>', '@', 'ia', 'di', 'questo', 'problema', ',', 'perché', 'non', 'è', 'abbastanza', 'la', 'di@@', '<unk>', '@', 'chiar@@', '<unk>', '@', 'azione', 'di', 'questo', 'problema', ',', 'perché', 'non', 'è', 'la', 'n@@', '<unk>', '@', 'ave', 'del', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 09:52:23,011 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:52:23,011 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:52:23,011 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte la ser<unk> @ ia di questo problema , perché non è abbastanza la di<unk> @ chiar<unk> @ azione di questo problema , perché non è la n<unk> @ ave del ghi<unk> @ accio .
2025-05-28 09:52:23,011 - INFO - joeynmt.training - Example #2
2025-05-28 09:52:23,012 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:52:23,012 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:52:23,012 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'sc@@', '<unk>', '@', 'at@@', '<unk>', '@', 'ola', 'è', 'la', 'c@@', '<unk>', '@', 'assa', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'va', 'il', 'cuore', 'del', 'nostro', 'sistema', 'globale', '.', '</s>']
2025-05-28 09:52:23,012 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:52:23,012 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:52:23,012 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la sc<unk> @ at<unk> @ ola è la c<unk> @ assa c<unk> @ atti<unk> @ va il cuore del nostro sistema globale .
2025-05-28 09:52:23,012 - INFO - joeynmt.training - Example #3
2025-05-28 09:52:23,012 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:52:23,013 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:52:23,013 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&apos;', 'una', 'volta', 'che', 'sta', 's@@', '<unk>', '@', 'om@@', '<unk>', '@', 'ate', 'e', 's@@', '<unk>', '@', 'om@@', '<unk>', '@', 'ate', 'in', 'est@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 09:52:23,013 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:52:23,013 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:52:23,013 - INFO - joeynmt.training - 	Hypothesis: E &apos; una volta che sta s<unk> @ om<unk> @ ate e s<unk> @ om<unk> @ ate in est<unk> @ ate .
2025-05-28 09:52:23,013 - INFO - joeynmt.training - Example #4
2025-05-28 09:52:23,013 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:52:23,013 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:52:23,014 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'consegu@@', '<unk>', '@', 'enza', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'seg@@', '<unk>', '@', 'no', 'di', 'cosa', 'succede', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:52:23,014 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:52:23,014 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:52:23,014 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma consegu<unk> @ enza che vi mostr<unk> @ erò è un seg<unk> @ no di seg<unk> @ no di cosa succede negli ultimi 25 anni .
2025-05-28 09:52:26,335 - INFO - joeynmt.training - Epoch   7, Step:    40100, Batch Loss:     1.231109, Batch Acc: 0.674543, Tokens per Sec:    18627, Lr: 0.000300
2025-05-28 09:52:29,615 - INFO - joeynmt.training - Epoch   7, Step:    40200, Batch Loss:     1.141866, Batch Acc: 0.669333, Tokens per Sec:    21765, Lr: 0.000300
2025-05-28 09:52:32,899 - INFO - joeynmt.training - Epoch   7, Step:    40300, Batch Loss:     1.215016, Batch Acc: 0.668687, Tokens per Sec:    21562, Lr: 0.000300
2025-05-28 09:52:36,167 - INFO - joeynmt.training - Epoch   7, Step:    40400, Batch Loss:     1.285422, Batch Acc: 0.667533, Tokens per Sec:    22040, Lr: 0.000300
2025-05-28 09:52:39,448 - INFO - joeynmt.training - Epoch   7, Step:    40500, Batch Loss:     1.199014, Batch Acc: 0.670456, Tokens per Sec:    21801, Lr: 0.000300
2025-05-28 09:52:39,448 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:52:39,448 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:52:49,564 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.28, ppl:   3.58, acc:   0.65, generation: 10.1089[sec], evaluation: 0.0000[sec]
2025-05-28 09:52:49,564 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:52:50,225 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/37500.ckpt
2025-05-28 09:52:50,250 - INFO - joeynmt.training - Example #0
2025-05-28 09:52:50,250 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:52:50,251 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:52:50,251 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'vedere', 'che', 'l&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'et', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'che', 'l&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ano', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'il', '40', '%', 'dei', 'paesi', ',', 'per', 'cento', '.', '</s>']
2025-05-28 09:52:50,251 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:52:50,251 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:52:50,251 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due due di<unk> @ apos<unk> @ iti<unk> @ ve per vedere che l&apos; E<unk> @ is<unk> @ k<unk> @ et ar<unk> @ t<unk> @ ico che l&apos; E<unk> @ is<unk> @ k<unk> @ i<unk> @ ano per tre milioni di anni , per il 40 % dei paesi , per cento .
2025-05-28 09:52:50,251 - INFO - joeynmt.training - Example #1
2025-05-28 09:52:50,251 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:52:50,251 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:52:50,252 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'forte', 'che', 'questo', 'sia', 'abbastanza', 'che', 'è', 'che', 'questo', 'problema', ',', 'che', 'non', 'è', 'l&apos;', 'av@@', '<unk>', '@', 'vis@@', '<unk>', '@', 'a', 'del', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 09:52:50,252 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:52:50,252 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:52:50,252 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte che questo sia abbastanza che è che questo problema , che non è l&apos; av<unk> @ vis<unk> @ a del ghi<unk> @ accio .
2025-05-28 09:52:50,252 - INFO - joeynmt.training - Example #2
2025-05-28 09:52:50,252 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:52:50,252 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:52:50,252 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'qualche', 'certo', 'senso', ',', 'la', 'sc@@', '<unk>', '@', 'at@@', '<unk>', '@', 'ola', 'è', 'la', 'c@@', '<unk>', '@', 'ass@@', '<unk>', '@', 'etta', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'va', 'è', 'la', 'p@@', '<unk>', '@', 'elle', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'va', 'del', 'nostro', 'sistema', 'mon@@', '<unk>', '@', 'diale', '.', '</s>']
2025-05-28 09:52:50,253 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:52:50,253 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:52:50,253 - INFO - joeynmt.training - 	Hypothesis: In qualche certo senso , la sc<unk> @ at<unk> @ ola è la c<unk> @ ass<unk> @ etta c<unk> @ atti<unk> @ va è la p<unk> @ elle c<unk> @ atti<unk> @ va del nostro sistema mon<unk> @ diale .
2025-05-28 09:52:50,253 - INFO - joeynmt.training - Example #3
2025-05-28 09:52:50,253 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:52:50,253 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:52:50,253 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', '@', 'e', 'cres@@', '<unk>', '@', 'ce', 'nel', 'vento', 'e', 's@@', '<unk>', '@', 'com@@', '<unk>', '@', 'in@@', '<unk>', '@', 'ciò', 'nel', 'vento', '.', '</s>']
2025-05-28 09:52:50,254 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:52:50,254 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:52:50,254 - INFO - joeynmt.training - 	Hypothesis: C<unk> @ e cres<unk> @ ce nel vento e s<unk> @ com<unk> @ in<unk> @ ciò nel vento .
2025-05-28 09:52:50,254 - INFO - joeynmt.training - Example #4
2025-05-28 09:52:50,254 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:52:50,254 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:52:50,254 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'a', 'è', 'una', 'ri@@', '<unk>', '@', 'ma@@', '<unk>', '@', 'zione', 'che', 'cosa', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:52:50,255 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:52:50,255 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:52:50,255 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ a è una ri<unk> @ ma<unk> @ zione che cosa negli ultimi 25 anni .
2025-05-28 09:52:53,564 - INFO - joeynmt.training - Epoch   7, Step:    40600, Batch Loss:     1.255494, Batch Acc: 0.670244, Tokens per Sec:    18570, Lr: 0.000300
2025-05-28 09:52:56,887 - INFO - joeynmt.training - Epoch   7, Step:    40700, Batch Loss:     1.150538, Batch Acc: 0.668743, Tokens per Sec:    21934, Lr: 0.000300
2025-05-28 09:53:00,205 - INFO - joeynmt.training - Epoch   7, Step:    40800, Batch Loss:     1.319952, Batch Acc: 0.673790, Tokens per Sec:    21781, Lr: 0.000300
2025-05-28 09:53:03,594 - INFO - joeynmt.training - Epoch   7, Step:    40900, Batch Loss:     1.281734, Batch Acc: 0.670724, Tokens per Sec:    21707, Lr: 0.000300
2025-05-28 09:53:06,978 - INFO - joeynmt.training - Epoch   7, Step:    41000, Batch Loss:     1.199805, Batch Acc: 0.667452, Tokens per Sec:    20822, Lr: 0.000300
2025-05-28 09:53:06,979 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:53:06,979 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:53:17,364 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.28, ppl:   3.59, acc:   0.65, generation: 10.3755[sec], evaluation: 0.0000[sec]
2025-05-28 09:53:17,770 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/39500.ckpt
2025-05-28 09:53:17,800 - INFO - joeynmt.training - Example #0
2025-05-28 09:53:17,800 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:53:17,801 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:53:17,801 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'che', 'per', 'poter', 'essere', 'il', '40', 'di', 'essere', 'umano', ',', 'che', 'l&apos;', 'et@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ch@@', '<unk>', '@', 'ett@@', '<unk>', '@', 'ico', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'che', 'è', 'stato', 'in@@', '<unk>', '@', 'ser@@', '<unk>', '@', 'ito', 'di', 'tre', 'milioni', 'di', 'anni', ',', 'che', 'è', 'stato', 'in@@', '<unk>', '@', 'ser@@', '<unk>', '@', 'ito', 'del', '40', '%', '.', '</s>']
2025-05-28 09:53:17,801 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:53:17,801 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:53:17,801 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ che per poter essere il 40 di essere umano , che l&apos; et<unk> @ i<unk> @ ch<unk> @ ett<unk> @ ico per tre milioni di anni , che è stato in<unk> @ ser<unk> @ ito di tre milioni di anni , che è stato in<unk> @ ser<unk> @ ito del 40 % .
2025-05-28 09:53:17,801 - INFO - joeynmt.training - Example #1
2025-05-28 09:53:17,802 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:53:17,802 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:53:17,802 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'forte', 'che', 'la', 'cre@@', '<unk>', '@', 'azione', 'di', 'questo', 'problema', 'speci@@', '<unk>', '@', 'ale', ',', 'perché', 'non', 'è', 'il', 'seg@@', '<unk>', '@', 'ret@@', '<unk>', '@', 'o', 'del', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 09:53:17,802 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:53:17,802 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:53:17,802 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza forte che la cre<unk> @ azione di questo problema speci<unk> @ ale , perché non è il seg<unk> @ ret<unk> @ o del ghi<unk> @ accio .
2025-05-28 09:53:17,802 - INFO - joeynmt.training - Example #2
2025-05-28 09:53:17,803 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:53:17,803 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:53:17,803 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 's@@', '<unk>', '@', 'ett@@', '<unk>', '@', 'ura', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'è', 'il', 'cuore', 'del', 'nostro', 'sistema', 'peggi@@', '<unk>', '@', 'ore', 'globale', '.', '</s>']
2025-05-28 09:53:17,803 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:53:17,803 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:53:17,803 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la s<unk> @ ett<unk> @ ura ar<unk> @ t<unk> @ ica è il cuore del nostro sistema peggi<unk> @ ore globale .
2025-05-28 09:53:17,804 - INFO - joeynmt.training - Example #3
2025-05-28 09:53:17,804 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:53:17,804 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:53:17,804 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'è', 'che', 'si', 'sta', 'cresc@@', '<unk>', '@', 'endo', 'in', 'vento', 'e', 'si', 'ri@@', '<unk>', '@', 'b@@', '<unk>', '@', 'oc@@', '<unk>', '@', 'ca', '.', '</s>']
2025-05-28 09:53:17,804 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:53:17,804 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:53:17,804 - INFO - joeynmt.training - 	Hypothesis: La prima è che si sta cresc<unk> @ endo in vento e si ri<unk> @ b<unk> @ oc<unk> @ ca .
2025-05-28 09:53:17,804 - INFO - joeynmt.training - Example #4
2025-05-28 09:53:17,805 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:53:17,805 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:53:17,805 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'consegu@@', '<unk>', '@', 'enza', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'una', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'ret@@', '<unk>', '@', 'a', 'che', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:53:17,805 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:53:17,805 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:53:17,805 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma consegu<unk> @ enza che vi mostr<unk> @ erò è una di<unk> @ seg<unk> @ ret<unk> @ a che negli ultimi 25 anni .
2025-05-28 09:53:21,209 - INFO - joeynmt.training - Epoch   7, Step:    41100, Batch Loss:     1.123597, Batch Acc: 0.668673, Tokens per Sec:    17718, Lr: 0.000300
2025-05-28 09:53:24,622 - INFO - joeynmt.training - Epoch   7, Step:    41200, Batch Loss:     1.277835, Batch Acc: 0.671408, Tokens per Sec:    21290, Lr: 0.000300
2025-05-28 09:53:28,024 - INFO - joeynmt.training - Epoch   7, Step:    41300, Batch Loss:     1.179846, Batch Acc: 0.670623, Tokens per Sec:    20956, Lr: 0.000300
2025-05-28 09:53:31,391 - INFO - joeynmt.training - Epoch   7, Step:    41400, Batch Loss:     1.356022, Batch Acc: 0.672569, Tokens per Sec:    20654, Lr: 0.000300
2025-05-28 09:53:34,783 - INFO - joeynmt.training - Epoch   7, Step:    41500, Batch Loss:     1.243567, Batch Acc: 0.670750, Tokens per Sec:    20898, Lr: 0.000300
2025-05-28 09:53:34,783 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:53:34,783 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:53:46,118 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.27, ppl:   3.57, acc:   0.66, generation: 11.3240[sec], evaluation: 0.0000[sec]
2025-05-28 09:53:46,118 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:53:46,917 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/36000.ckpt
2025-05-28 09:53:46,945 - INFO - joeynmt.training - Example #0
2025-05-28 09:53:46,946 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:53:46,946 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:53:46,946 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'vedere', 'che', 'l&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ci@@', '<unk>', '@', 'olo', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'che', 'ha', 'avuto', 'un', '4@@', '<unk>', '@', '8', '%', 'di', 'dimen@@', '<unk>', '@', 'sioni', 'di', 'tre', 'milioni', 'di', 'anni', 'per', 'cento', '.', '</s>']
2025-05-28 09:53:46,946 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:53:46,947 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:53:46,947 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per vedere che l&apos; E<unk> @ is<unk> @ c<unk> @ ci<unk> @ olo ar<unk> @ t<unk> @ ico che ha avuto un 4<unk> @ 8 % di dimen<unk> @ sioni di tre milioni di anni per cento .
2025-05-28 09:53:46,947 - INFO - joeynmt.training - Example #1
2025-05-28 09:53:46,947 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:53:46,947 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:53:46,947 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'abbastanza', 'da', 'la', 'Terra', ',', 'perché', 'non', 'è', 'abbastanza', 'la', 'prima', 'cosa', 'che', 'non', 'è', 'abbastanza', 'da', 'questo', 'problema', ',', 'perché', 'non', 'lo', 'fa', 'il', 'vi@@', '<unk>', '@', 'aggi@@', '<unk>', '@', 'are', 'il', 'vi@@', '<unk>', '@', 'll@@', '<unk>', '@', 'aggio', '.', '</s>']
2025-05-28 09:53:46,948 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:53:46,948 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:53:46,948 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza abbastanza da la Terra , perché non è abbastanza la prima cosa che non è abbastanza da questo problema , perché non lo fa il vi<unk> @ aggi<unk> @ are il vi<unk> @ ll<unk> @ aggio .
2025-05-28 09:53:46,948 - INFO - joeynmt.training - Example #2
2025-05-28 09:53:46,948 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:53:46,948 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:53:46,948 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'c@@', '<unk>', '@', 'ass@@', '<unk>', '@', 'or@@', '<unk>', '@', 'osa', 'è', 'il', 'cuore', 'del', 'sistema', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'vo', '.', '</s>']
2025-05-28 09:53:46,949 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:53:46,949 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:53:46,949 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la c<unk> @ ass<unk> @ or<unk> @ osa è il cuore del sistema c<unk> @ atti<unk> @ vo .
2025-05-28 09:53:46,949 - INFO - joeynmt.training - Example #3
2025-05-28 09:53:46,949 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:53:46,949 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:53:46,949 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&apos;', 'in', 'un', 'vento', 'e', 'il', 'vento', 'in', 'est@@', '<unk>', '@', 'ate', 'e', 's@@', '<unk>', '@', 'om@@', '<unk>', '@', 'ate', 'in', 'est@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 09:53:46,949 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:53:46,950 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:53:46,950 - INFO - joeynmt.training - 	Hypothesis: E &apos; in un vento e il vento in est<unk> @ ate e s<unk> @ om<unk> @ ate in est<unk> @ ate .
2025-05-28 09:53:46,950 - INFO - joeynmt.training - Example #4
2025-05-28 09:53:46,950 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:53:46,950 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:53:46,950 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'consegu@@', '<unk>', '@', 'enza', 'che', 'vi', 'mostr@@', '<unk>', '@', 'ano', 'è', 'un', 'segn@@', '<unk>', '@', 'ale', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:53:46,950 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:53:46,951 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:53:46,951 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma consegu<unk> @ enza che vi mostr<unk> @ ano è un segn<unk> @ ale che è successo negli ultimi 25 anni .
2025-05-28 09:53:50,368 - INFO - joeynmt.training - Epoch   7, Step:    41600, Batch Loss:     1.165059, Batch Acc: 0.668830, Tokens per Sec:    16247, Lr: 0.000300
2025-05-28 09:53:53,758 - INFO - joeynmt.training - Epoch   7, Step:    41700, Batch Loss:     1.325283, Batch Acc: 0.674834, Tokens per Sec:    20943, Lr: 0.000300
2025-05-28 09:53:57,163 - INFO - joeynmt.training - Epoch   7, Step:    41800, Batch Loss:     1.119828, Batch Acc: 0.667363, Tokens per Sec:    20533, Lr: 0.000300
2025-05-28 09:54:00,560 - INFO - joeynmt.training - Epoch   7, Step:    41900, Batch Loss:     1.219203, Batch Acc: 0.673932, Tokens per Sec:    20966, Lr: 0.000300
2025-05-28 09:54:03,943 - INFO - joeynmt.training - Epoch   7, Step:    42000, Batch Loss:     1.337471, Batch Acc: 0.670147, Tokens per Sec:    20984, Lr: 0.000300
2025-05-28 09:54:03,943 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:54:03,943 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:54:15,159 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.27, ppl:   3.57, acc:   0.66, generation: 11.2048[sec], evaluation: 0.0000[sec]
2025-05-28 09:54:15,573 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/39000.ckpt
2025-05-28 09:54:15,602 - INFO - joeynmt.training - Example #0
2025-05-28 09:54:15,603 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:54:15,603 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:54:15,603 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'vedere', 'che', 'i', 'ghi@@', '<unk>', '@', 'acci@@', '<unk>', '@', 'aio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ici', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ani', 'che', 'sono', 'stati', 'tre', 'milioni', 'di', 'anni', 'per', 'cento', '.', '</s>']
2025-05-28 09:54:15,603 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:54:15,603 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:54:15,603 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per vedere che i ghi<unk> @ acci<unk> @ aio ar<unk> @ t<unk> @ ici ar<unk> @ t<unk> @ ic<unk> @ ani che sono stati tre milioni di anni per cento .
2025-05-28 09:54:15,604 - INFO - joeynmt.training - Example #1
2025-05-28 09:54:15,604 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:54:15,604 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:54:15,604 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'forte', 'la', 'nostra', 'er@@', '<unk>', '@', 'n@@', '<unk>', '@', 'ità', 'di', 'questo', 'particolare', 'problema', ',', 'perché', 'non', 'è', 'il', 'di@@', '<unk>', '@', 'mostra', 'il', 'vi@@', '<unk>', '@', 'gi@@', '<unk>', '@', 'le', '.', '</s>']
2025-05-28 09:54:15,604 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:54:15,605 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:54:15,605 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte la nostra er<unk> @ n<unk> @ ità di questo particolare problema , perché non è il di<unk> @ mostra il vi<unk> @ gi<unk> @ le .
2025-05-28 09:54:15,605 - INFO - joeynmt.training - Example #2
2025-05-28 09:54:15,605 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:54:15,605 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:54:15,605 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'sc@@', '<unk>', '@', 'app@@', '<unk>', '@', 'a', 'è', 'la', 'c@@', '<unk>', '@', 'assa', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'va', 'il', 'cuore', 'del', 'nostro', 'sistema', 'globale', '.', '</s>']
2025-05-28 09:54:15,605 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:54:15,606 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:54:15,606 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la sc<unk> @ app<unk> @ a è la c<unk> @ assa c<unk> @ atti<unk> @ va il cuore del nostro sistema globale .
2025-05-28 09:54:15,606 - INFO - joeynmt.training - Example #3
2025-05-28 09:54:15,606 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:54:15,606 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:54:15,606 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'a', 'cresc@@', '<unk>', '@', 'ere', 'e', 'il', 'p@@', '<unk>', '@', 'om@@', '<unk>', '@', 'ate', 'in', 'est@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 09:54:15,606 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:54:15,606 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:54:15,606 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ a cresc<unk> @ ere e il p<unk> @ om<unk> @ ate in est<unk> @ ate .
2025-05-28 09:54:15,606 - INFO - joeynmt.training - Example #4
2025-05-28 09:54:15,607 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:54:15,607 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:54:15,607 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'o', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:54:15,607 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:54:15,607 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:54:15,607 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ o è successo negli ultimi 25 anni .
2025-05-28 09:54:19,035 - INFO - joeynmt.training - Epoch   7, Step:    42100, Batch Loss:     1.153233, Batch Acc: 0.676580, Tokens per Sec:    18240, Lr: 0.000300
2025-05-28 09:54:22,485 - INFO - joeynmt.training - Epoch   7, Step:    42200, Batch Loss:     1.370967, Batch Acc: 0.666047, Tokens per Sec:    20885, Lr: 0.000300
2025-05-28 09:54:25,900 - INFO - joeynmt.training - Epoch   7, Step:    42300, Batch Loss:     1.200388, Batch Acc: 0.667513, Tokens per Sec:    20546, Lr: 0.000300
2025-05-28 09:54:29,315 - INFO - joeynmt.training - Epoch   7, Step:    42400, Batch Loss:     1.184167, Batch Acc: 0.668483, Tokens per Sec:    20905, Lr: 0.000300
2025-05-28 09:54:32,719 - INFO - joeynmt.training - Epoch   7, Step:    42500, Batch Loss:     1.218572, Batch Acc: 0.667488, Tokens per Sec:    20270, Lr: 0.000300
2025-05-28 09:54:32,720 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:54:32,720 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:54:44,447 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.27, ppl:   3.56, acc:   0.66, generation: 11.7166[sec], evaluation: 0.0000[sec]
2025-05-28 09:54:44,448 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:54:45,083 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/40000.ckpt
2025-05-28 09:54:45,111 - INFO - joeynmt.training - Example #0
2025-05-28 09:54:45,111 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:54:45,112 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:54:45,112 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'che', ',', 'per', 'guardare', 'il', 'livello', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'che', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'il', 'grande', 'grande', 'è', 'stato', '.', '</s>']
2025-05-28 09:54:45,112 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:54:45,112 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:54:45,112 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ che , per guardare il livello ar<unk> @ t<unk> @ ico ar<unk> @ t<unk> @ ico ar<unk> @ t<unk> @ ico che per tre milioni di anni , il grande grande è stato .
2025-05-28 09:54:45,112 - INFO - joeynmt.training - Example #1
2025-05-28 09:54:45,113 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:54:45,113 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:54:45,113 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'forte', 'da', 'fare', 'questo', 'problema', ',', 'perché', 'non', 'è', 'un', 'problema', 'di', 'in@@', '<unk>', '@', 'dag@@', '<unk>', '@', 'ine', 'che', 'non', 'è', 'il', 'D@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ke', 'dell&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 09:54:45,113 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:54:45,113 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:54:45,113 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza forte da fare questo problema , perché non è un problema di in<unk> @ dag<unk> @ ine che non è il D<unk> @ ic<unk> @ ke dell&apos; E<unk> @ is<unk> @ es .
2025-05-28 09:54:45,114 - INFO - joeynmt.training - Example #2
2025-05-28 09:54:45,114 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:54:45,114 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:54:45,114 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'c@@', '<unk>', '@', 'app@@', '<unk>', '@', 'ola', 'è', 'la', 'c@@', '<unk>', '@', 'app@@', '<unk>', '@', 'ola', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'del', 'nostro', 'sistema', 'globale', '.', '</s>']
2025-05-28 09:54:45,114 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:54:45,114 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:54:45,115 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la c<unk> @ app<unk> @ ola è la c<unk> @ app<unk> @ ola ar<unk> @ t<unk> @ ica del nostro sistema globale .
2025-05-28 09:54:45,115 - INFO - joeynmt.training - Example #3
2025-05-28 09:54:45,115 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:54:45,115 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:54:45,115 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'tratta', 'di', 'un', 'vento', 'e', 'la', 's@@', '<unk>', '@', 'om@@', '<unk>', '@', 'ate', 'in', 'est@@', '<unk>', '@', 'ate', 'e', 's@@', '<unk>', '@', 'om@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 09:54:45,115 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:54:45,115 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:54:45,115 - INFO - joeynmt.training - 	Hypothesis: Si tratta di un vento e la s<unk> @ om<unk> @ ate in est<unk> @ ate e s<unk> @ om<unk> @ ate .
2025-05-28 09:54:45,116 - INFO - joeynmt.training - Example #4
2025-05-28 09:54:45,116 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:54:45,116 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:54:45,116 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'ano', 'è', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'cosa', 'succede', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:54:45,116 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:54:45,116 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:54:45,116 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ ano è un seg<unk> @ no di cosa succede negli ultimi 25 anni .
2025-05-28 09:54:48,522 - INFO - joeynmt.training - Epoch   7, Step:    42600, Batch Loss:     1.074985, Batch Acc: 0.668694, Tokens per Sec:    17268, Lr: 0.000300
2025-05-28 09:54:51,916 - INFO - joeynmt.training - Epoch   7, Step:    42700, Batch Loss:     1.287315, Batch Acc: 0.667972, Tokens per Sec:    21082, Lr: 0.000300
2025-05-28 09:54:55,297 - INFO - joeynmt.training - Epoch   7, Step:    42800, Batch Loss:     1.169113, Batch Acc: 0.670941, Tokens per Sec:    21118, Lr: 0.000300
2025-05-28 09:54:58,676 - INFO - joeynmt.training - Epoch   7, Step:    42900, Batch Loss:     1.266724, Batch Acc: 0.666393, Tokens per Sec:    20930, Lr: 0.000300
2025-05-28 09:55:02,075 - INFO - joeynmt.training - Epoch   7, Step:    43000, Batch Loss:     1.460295, Batch Acc: 0.669387, Tokens per Sec:    21454, Lr: 0.000300
2025-05-28 09:55:02,076 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:55:02,076 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:55:13,223 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.27, ppl:   3.55, acc:   0.66, generation: 11.1364[sec], evaluation: 0.0000[sec]
2025-05-28 09:55:13,224 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:55:13,862 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/41000.ckpt
2025-05-28 09:55:13,889 - INFO - joeynmt.training - Example #0
2025-05-28 09:55:13,890 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:55:13,890 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:55:13,890 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'vedere', 'che', 'i', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ali', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ici', 'che', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'che', 'sono', 'stati', 'in', 'grado', 'di', 'chiam@@', '<unk>', '@', 'are', 'i', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ale', ',', 'per', 'tre', 'milioni', 'di', 'anni', '.', '</s>']
2025-05-28 09:55:13,891 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:55:13,891 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:55:13,891 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per vedere che i ghi<unk> @ accio ar<unk> @ t<unk> @ ic<unk> @ ali ar<unk> @ t<unk> @ ici che per tre milioni di anni , che sono stati in grado di chiam<unk> @ are i ghi<unk> @ accio ar<unk> @ t<unk> @ ic<unk> @ ale , per tre milioni di anni .
2025-05-28 09:55:13,891 - INFO - joeynmt.training - Example #1
2025-05-28 09:55:13,891 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:55:13,891 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:55:13,892 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'un', 'modo', 'di', 'essere', 'abbastanza', 'forte', 'per', 'la', 'Terra', ',', 'perché', 'non', 'è', 'un', 'po', '&apos;', 'di', 'più', ',', 'perché', 'non', 'è', 'il', 'D@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ke', 'del', 'fatto', '.', '</s>']
2025-05-28 09:55:13,892 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:55:13,892 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:55:13,892 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è un modo di essere abbastanza forte per la Terra , perché non è un po &apos; di più , perché non è il D<unk> @ ic<unk> @ ke del fatto .
2025-05-28 09:55:13,892 - INFO - joeynmt.training - Example #2
2025-05-28 09:55:13,892 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:55:13,892 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:55:13,893 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'è', 'il', 'cuore', 'di', 'un', 'senso', 'di', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ale', 'del', 'nostro', 'sistema', 'di', 'peggi@@', '<unk>', '@', 'ore', 'globale', '.', '</s>']
2025-05-28 09:55:13,893 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:55:13,893 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:55:13,893 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , è il cuore di un senso di ar<unk> @ t<unk> @ ic<unk> @ ale del nostro sistema di peggi<unk> @ ore globale .
2025-05-28 09:55:13,893 - INFO - joeynmt.training - Example #3
2025-05-28 09:55:13,893 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:55:13,893 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:55:13,894 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'a', 'cresc@@', '<unk>', '@', 'ere', 'e', 'si', 's@@', '<unk>', '@', 'ale', 'in', 'vento', ',', 'e', 'si', 's@@', '<unk>', '@', 'ale', 'in', 'est@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 09:55:13,894 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:55:13,894 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:55:13,894 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ a cresc<unk> @ ere e si s<unk> @ ale in vento , e si s<unk> @ ale in est<unk> @ ate .
2025-05-28 09:55:13,894 - INFO - joeynmt.training - Example #4
2025-05-28 09:55:13,894 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:55:13,894 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:55:13,894 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'o', 'è', 'una', 'di@@', '<unk>', '@', 'ret@@', '<unk>', '@', 'ta', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:55:13,895 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:55:13,895 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:55:13,895 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ o è una di<unk> @ ret<unk> @ ta che è successo negli ultimi 25 anni .
2025-05-28 09:55:17,312 - INFO - joeynmt.training - Epoch   7, Step:    43100, Batch Loss:     1.189737, Batch Acc: 0.667954, Tokens per Sec:    17548, Lr: 0.000300
2025-05-28 09:55:20,707 - INFO - joeynmt.training - Epoch   7, Step:    43200, Batch Loss:     1.223471, Batch Acc: 0.667081, Tokens per Sec:    21356, Lr: 0.000300
2025-05-28 09:55:24,094 - INFO - joeynmt.training - Epoch   7, Step:    43300, Batch Loss:     1.218155, Batch Acc: 0.666818, Tokens per Sec:    20860, Lr: 0.000300
2025-05-28 09:55:27,507 - INFO - joeynmt.training - Epoch   7, Step:    43400, Batch Loss:     1.211723, Batch Acc: 0.669843, Tokens per Sec:    20850, Lr: 0.000300
2025-05-28 09:55:30,888 - INFO - joeynmt.training - Epoch   7, Step:    43500, Batch Loss:     1.217004, Batch Acc: 0.668294, Tokens per Sec:    20785, Lr: 0.000300
2025-05-28 09:55:30,888 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:55:30,888 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:55:42,571 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.27, ppl:   3.55, acc:   0.66, generation: 11.6716[sec], evaluation: 0.0000[sec]
2025-05-28 09:55:42,571 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:55:43,188 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/40500.ckpt
2025-05-28 09:55:43,216 - INFO - joeynmt.training - Example #0
2025-05-28 09:55:43,216 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:55:43,216 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:55:43,217 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'che', 'per', 'es@@', '<unk>', '@', 'amin@@', '<unk>', '@', 'are', 'che', 'i', 'g@@', '<unk>', '@', 'on@@', '<unk>', '@', 'g@@', '<unk>', '@', 'ano', 'per', 'il', '40', 'milioni', 'di', 'anni', 'di', 'l@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'enzi@@', '<unk>', '@', 'o', 'di', 'cui', 'i', '&apos;', '4@@', '<unk>', '@', '8', 'milioni', 'di', 'anni', ',', 'il', '40', '.', '</s>']
2025-05-28 09:55:43,217 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:55:43,217 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:55:43,217 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ che per es<unk> @ amin<unk> @ are che i g<unk> @ on<unk> @ g<unk> @ ano per il 40 milioni di anni di l<unk> @ ic<unk> @ enzi<unk> @ o di cui i &apos; 4<unk> @ 8 milioni di anni , il 40 .
2025-05-28 09:55:43,217 - INFO - joeynmt.training - Example #1
2025-05-28 09:55:43,218 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:55:43,218 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:55:43,218 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'così', 'che', 'non', 'è', 'mor@@', '<unk>', '@', 'to', 'abbastanza', 'la', 'Terra', 'abbastanza', 'la', 'di@@', '<unk>', '@', 'mostra', 'di', 'questo', 'problema', ',', 'perché', 'non', 'è', 'il', 'de@@', '<unk>', '@', 'st@@', '<unk>', '@', 'in@@', '<unk>', '@', 'to', '.', '</s>']
2025-05-28 09:55:43,218 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:55:43,218 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:55:43,218 - INFO - joeynmt.training - 	Hypothesis: Ma non è così che non è mor<unk> @ to abbastanza la Terra abbastanza la di<unk> @ mostra di questo problema , perché non è il de<unk> @ st<unk> @ in<unk> @ to .
2025-05-28 09:55:43,218 - INFO - joeynmt.training - Example #2
2025-05-28 09:55:43,219 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:55:43,219 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:55:43,219 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'cer@@', '<unk>', '@', 't@@', '<unk>', '@', 'amente', ',', 'la', 'c@@', '<unk>', '@', 'app@@', '<unk>', '@', 'a', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'are', 'il', 'cuore', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', '.', '</s>']
2025-05-28 09:55:43,219 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:55:43,219 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:55:43,219 - INFO - joeynmt.training - 	Hypothesis: In cer<unk> @ t<unk> @ amente , la c<unk> @ app<unk> @ a ar<unk> @ t<unk> @ ic<unk> @ are il cuore ar<unk> @ t<unk> @ ico .
2025-05-28 09:55:43,219 - INFO - joeynmt.training - Example #3
2025-05-28 09:55:43,220 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:55:43,220 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:55:43,220 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'a', 'cres@@', '<unk>', '@', 'ce', 'nel', 'vento', 'e', 's@@', '<unk>', '@', 'om@@', '<unk>', '@', 'ba', '.', '</s>']
2025-05-28 09:55:43,220 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:55:43,220 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:55:43,220 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ a cres<unk> @ ce nel vento e s<unk> @ om<unk> @ ba .
2025-05-28 09:55:43,220 - INFO - joeynmt.training - Example #4
2025-05-28 09:55:43,220 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:55:43,220 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:55:43,221 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'consegu@@', '<unk>', '@', 'enza', 'che', 'vi', 'mostr@@', '<unk>', '@', 'o', 'è', 'una', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'ret@@', '<unk>', '@', 'a', 'che', 'cosa', 'sta', 'succe@@', '<unk>', '@', 'dendo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:55:43,221 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:55:43,221 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:55:43,221 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma consegu<unk> @ enza che vi mostr<unk> @ o è una di<unk> @ seg<unk> @ ret<unk> @ a che cosa sta succe<unk> @ dendo negli ultimi 25 anni .
2025-05-28 09:55:46,630 - INFO - joeynmt.training - Epoch   7, Step:    43600, Batch Loss:     1.208945, Batch Acc: 0.670585, Tokens per Sec:    17608, Lr: 0.000300
2025-05-28 09:55:50,041 - INFO - joeynmt.training - Epoch   7, Step:    43700, Batch Loss:     1.162796, Batch Acc: 0.672933, Tokens per Sec:    21173, Lr: 0.000300
2025-05-28 09:55:53,438 - INFO - joeynmt.training - Epoch   7, Step:    43800, Batch Loss:     1.119652, Batch Acc: 0.668170, Tokens per Sec:    21015, Lr: 0.000300
2025-05-28 09:55:56,838 - INFO - joeynmt.training - Epoch   7, Step:    43900, Batch Loss:     1.184969, Batch Acc: 0.670852, Tokens per Sec:    21179, Lr: 0.000300
2025-05-28 09:56:00,228 - INFO - joeynmt.training - Epoch   7, Step:    44000, Batch Loss:     1.032563, Batch Acc: 0.671529, Tokens per Sec:    20508, Lr: 0.000300
2025-05-28 09:56:00,228 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:56:00,229 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:56:10,851 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.26, ppl:   3.54, acc:   0.66, generation: 10.6115[sec], evaluation: 0.0000[sec]
2025-05-28 09:56:10,851 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:56:11,420 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/42000.ckpt
2025-05-28 09:56:11,441 - INFO - joeynmt.training - Example #0
2025-05-28 09:56:11,442 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:56:11,442 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:56:11,442 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'ri@@', '<unk>', '@', 'durre', 'che', 'i', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ano', 'per', 'la', 'c@@', '<unk>', '@', 'lasse', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'che', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'cento', '.', '</s>']
2025-05-28 09:56:11,442 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:56:11,443 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:56:11,443 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due due di<unk> @ apos<unk> @ iti<unk> @ ve per ri<unk> @ durre che i ghi<unk> @ accio ar<unk> @ t<unk> @ ic<unk> @ ano per la c<unk> @ lasse ar<unk> @ t<unk> @ ica che per tre milioni di anni , per cento .
2025-05-28 09:56:11,443 - INFO - joeynmt.training - Example #1
2025-05-28 09:56:11,443 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:56:11,443 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:56:11,443 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'forte', 'che', 'la', 'di@@', '<unk>', '@', 'stri@@', '<unk>', '@', 'bu@@', '<unk>', '@', 'zione', 'di', 'questo', 'particolare', 'problema', ',', 'perché', 'non', 'è', 'il', 'di@@', '<unk>', '@', 'mostra', 'l&apos;', 'et@@', '<unk>', '@', 'n@@', '<unk>', '@', 'ice', 'dell&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 09:56:11,443 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:56:11,443 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:56:11,443 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza forte che la di<unk> @ stri<unk> @ bu<unk> @ zione di questo particolare problema , perché non è il di<unk> @ mostra l&apos; et<unk> @ n<unk> @ ice dell&apos; E<unk> @ is<unk> @ es .
2025-05-28 09:56:11,444 - INFO - joeynmt.training - Example #2
2025-05-28 09:56:11,444 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:56:11,444 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:56:11,444 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'c@@', '<unk>', '@', 'app@@', '<unk>', '@', 'a', 'è', 'la', 'c@@', '<unk>', '@', 'ris@@', '<unk>', '@', 'o', 'del', 'nostro', 'sistema', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'vo', '.', '</s>']
2025-05-28 09:56:11,444 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:56:11,444 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:56:11,444 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la c<unk> @ app<unk> @ a è la c<unk> @ ris<unk> @ o del nostro sistema c<unk> @ atti<unk> @ vo .
2025-05-28 09:56:11,444 - INFO - joeynmt.training - Example #3
2025-05-28 09:56:11,445 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:56:11,445 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:56:11,445 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'volta', 'che', 'si', 'è', 'ri@@', '<unk>', '@', 'un@@', '<unk>', '@', 'ci@@', '<unk>', '@', 'ata', 'in', 'est@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 09:56:11,445 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:56:11,445 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:56:11,445 - INFO - joeynmt.training - 	Hypothesis: La prima volta che si è ri<unk> @ un<unk> @ ci<unk> @ ata in est<unk> @ ate .
2025-05-28 09:56:11,445 - INFO - joeynmt.training - Example #4
2025-05-28 09:56:11,445 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:56:11,445 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:56:11,445 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'a', 'è', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'un', 'indi@@', '<unk>', '@', 'ce', 'che', 'è', 'acca@@', '<unk>', '@', 'duto', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:56:11,446 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:56:11,446 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:56:11,446 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ a è un seg<unk> @ no di un indi<unk> @ ce che è acca<unk> @ duto negli ultimi 25 anni .
2025-05-28 09:56:14,834 - INFO - joeynmt.training - Epoch   7, Step:    44100, Batch Loss:     1.127636, Batch Acc: 0.669278, Tokens per Sec:    18526, Lr: 0.000300
2025-05-28 09:56:15,105 - INFO - joeynmt.training - Epoch   7: total training loss 7540.72
2025-05-28 09:56:15,106 - INFO - joeynmt.training - EPOCH 8
2025-05-28 09:56:18,238 - INFO - joeynmt.training - Epoch   8, Step:    44200, Batch Loss:     1.213387, Batch Acc: 0.682740, Tokens per Sec:    21474, Lr: 0.000300
2025-05-28 09:56:21,655 - INFO - joeynmt.training - Epoch   8, Step:    44300, Batch Loss:     1.021168, Batch Acc: 0.689405, Tokens per Sec:    20605, Lr: 0.000300
2025-05-28 09:56:25,055 - INFO - joeynmt.training - Epoch   8, Step:    44400, Batch Loss:     1.145231, Batch Acc: 0.688750, Tokens per Sec:    21572, Lr: 0.000300
2025-05-28 09:56:28,470 - INFO - joeynmt.training - Epoch   8, Step:    44500, Batch Loss:     1.226312, Batch Acc: 0.680694, Tokens per Sec:    21212, Lr: 0.000300
2025-05-28 09:56:28,470 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:56:28,471 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:56:38,793 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.26, ppl:   3.53, acc:   0.66, generation: 10.3149[sec], evaluation: 0.0000[sec]
2025-05-28 09:56:38,794 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:56:39,300 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/41500.ckpt
2025-05-28 09:56:39,320 - INFO - joeynmt.training - Example #0
2025-05-28 09:56:39,320 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:56:39,321 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:56:39,321 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'vedere', 'che', 'i', 'ghi@@', '<unk>', '@', 'acci@@', '<unk>', '@', 'aio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ate', 'che', 'i', 'g@@', '<unk>', '@', 'on@@', '<unk>', '@', 'n@@', '<unk>', '@', 'd', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'il', 'grande', 'grande', 'grande', 'grande', '40', '%', '.', '</s>']
2025-05-28 09:56:39,321 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:56:39,321 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:56:39,321 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per vedere che i ghi<unk> @ acci<unk> @ aio ar<unk> @ t<unk> @ ic<unk> @ ate che i g<unk> @ on<unk> @ n<unk> @ d per tre milioni di anni , il grande grande grande grande 40 % .
2025-05-28 09:56:39,321 - INFO - joeynmt.training - Example #1
2025-05-28 09:56:39,321 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:56:39,321 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:56:39,321 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'la', 'ter@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'fic@@', '<unk>', '@', 'enza', 'di', 'questo', 'particolare', 'problema', ',', 'perché', 'non', 'è', 'il', 'del@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'co', 'dell&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 09:56:39,322 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:56:39,322 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:56:39,322 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza la ter<unk> @ ri<unk> @ fic<unk> @ enza di questo particolare problema , perché non è il del<unk> @ ic<unk> @ ic<unk> @ co dell&apos; E<unk> @ is<unk> @ es .
2025-05-28 09:56:39,322 - INFO - joeynmt.training - Example #2
2025-05-28 09:56:39,322 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:56:39,322 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:56:39,322 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'b@@', '<unk>', '@', 'oc@@', '<unk>', '@', 'ca', 'è', 'la', 'c@@', '<unk>', '@', 'accia', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ana', 'che', 'è', 'il', 'cuore', 'di', 'un', 'sistema', 'globale', '.', '</s>']
2025-05-28 09:56:39,323 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:56:39,323 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:56:39,323 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la b<unk> @ oc<unk> @ ca è la c<unk> @ accia ar<unk> @ t<unk> @ ic<unk> @ ana che è il cuore di un sistema globale .
2025-05-28 09:56:39,323 - INFO - joeynmt.training - Example #3
2025-05-28 09:56:39,323 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:56:39,323 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:56:39,323 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Pri@@', '<unk>', '@', 'ma', 'di', 'tutto', 'il', 'vento', 'e', 's@@', '<unk>', '@', 'om@@', '<unk>', '@', 'ate', 'in', 'est@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 09:56:39,323 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:56:39,323 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:56:39,323 - INFO - joeynmt.training - 	Hypothesis: Pri<unk> @ ma di tutto il vento e s<unk> @ om<unk> @ ate in est<unk> @ ate .
2025-05-28 09:56:39,323 - INFO - joeynmt.training - Example #4
2025-05-28 09:56:39,323 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:56:39,324 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:56:39,324 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'tempo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:56:39,324 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:56:39,324 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:56:39,324 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ erò è un seg<unk> @ no di tempo negli ultimi 25 anni .
2025-05-28 09:56:42,589 - INFO - joeynmt.training - Epoch   8, Step:    44600, Batch Loss:     1.327081, Batch Acc: 0.682272, Tokens per Sec:    18627, Lr: 0.000300
2025-05-28 09:56:45,924 - INFO - joeynmt.training - Epoch   8, Step:    44700, Batch Loss:     1.117149, Batch Acc: 0.684298, Tokens per Sec:    21909, Lr: 0.000300
2025-05-28 09:56:49,325 - INFO - joeynmt.training - Epoch   8, Step:    44800, Batch Loss:     1.144081, Batch Acc: 0.682865, Tokens per Sec:    21286, Lr: 0.000300
2025-05-28 09:56:52,718 - INFO - joeynmt.training - Epoch   8, Step:    44900, Batch Loss:     1.195166, Batch Acc: 0.682495, Tokens per Sec:    20944, Lr: 0.000300
2025-05-28 09:56:56,132 - INFO - joeynmt.training - Epoch   8, Step:    45000, Batch Loss:     1.212934, Batch Acc: 0.681210, Tokens per Sec:    20986, Lr: 0.000300
2025-05-28 09:56:56,133 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:56:56,133 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:57:06,199 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.27, ppl:   3.54, acc:   0.66, generation: 10.0594[sec], evaluation: 0.0000[sec]
2025-05-28 09:57:06,548 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/42500.ckpt
2025-05-28 09:57:06,567 - INFO - joeynmt.training - Example #0
2025-05-28 09:57:06,567 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:57:06,567 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:57:06,567 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'capire', 'che', 'i', 'v@@', '<unk>', '@', 'ari', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'elli', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ici', 'che', 'gli', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'on@@', '<unk>', '@', 'g@@', '<unk>', '@', 'hi', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'che', 'è', 'stato', 'il', '40', 'per', 'cento', 'degli', 'stati', 'in@@', '<unk>', '@', 'i@@', '<unk>', '@', 'vi', '.', '</s>']
2025-05-28 09:57:06,568 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:57:06,568 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:57:06,568 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per capire che i v<unk> @ ari ar<unk> @ t<unk> @ elli ar<unk> @ t<unk> @ ici che gli ar<unk> @ t<unk> @ on<unk> @ g<unk> @ hi per tre milioni di anni , che è stato il 40 per cento degli stati in<unk> @ i<unk> @ vi .
2025-05-28 09:57:06,568 - INFO - joeynmt.training - Example #1
2025-05-28 09:57:06,568 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:57:06,568 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:57:06,568 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'forte', ',', 'perché', 'non', 'è', 'abbastanza', 'la', 'pot@@', '<unk>', '@', 'enza', 'di', 'questo', 'problema', ',', 'perché', 'non', 'lo', 'mostra', 'il', 'del@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'co', 'del', 'problema', ',', 'perché', 'non', 'lo', 'mostra', 'il', 'bu@@', '<unk>', '@', 'io', '.', '</s>']
2025-05-28 09:57:06,568 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:57:06,568 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:57:06,569 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte , perché non è abbastanza la pot<unk> @ enza di questo problema , perché non lo mostra il del<unk> @ ic<unk> @ co del problema , perché non lo mostra il bu<unk> @ io .
2025-05-28 09:57:06,569 - INFO - joeynmt.training - Example #2
2025-05-28 09:57:06,569 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:57:06,569 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:57:06,569 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'co@@', '<unk>', '@', 'per@@', '<unk>', '@', 'ta', 'è', 'la', 'co@@', '<unk>', '@', 'per@@', '<unk>', '@', 'ta', 'di', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'l@@', '<unk>', '@', 'etta', ',', 'il', 'cuore', 'globale', '.', '</s>']
2025-05-28 09:57:06,569 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:57:06,569 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:57:06,569 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la co<unk> @ per<unk> @ ta è la co<unk> @ per<unk> @ ta di ar<unk> @ t<unk> @ ic<unk> @ l<unk> @ etta , il cuore globale .
2025-05-28 09:57:06,569 - INFO - joeynmt.training - Example #3
2025-05-28 09:57:06,570 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:57:06,570 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:57:06,570 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'a', 'cresc@@', '<unk>', '@', 'ere', 'nel', 'vento', 'e', 'b@@', '<unk>', '@', 'ast@@', '<unk>', '@', 'ico', '.', '</s>']
2025-05-28 09:57:06,570 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:57:06,570 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:57:06,570 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ a cresc<unk> @ ere nel vento e b<unk> @ ast<unk> @ ico .
2025-05-28 09:57:06,570 - INFO - joeynmt.training - Example #4
2025-05-28 09:57:06,570 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:57:06,570 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:57:06,570 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'o', 'è', 'un', 'seg@@', '<unk>', '@', 'ret@@', '<unk>', '@', 'o', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:57:06,571 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:57:06,571 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:57:06,571 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ o è un seg<unk> @ ret<unk> @ o che è successo negli ultimi 25 anni .
2025-05-28 09:57:09,910 - INFO - joeynmt.training - Epoch   8, Step:    45100, Batch Loss:     1.137626, Batch Acc: 0.680353, Tokens per Sec:    19222, Lr: 0.000300
2025-05-28 09:57:13,256 - INFO - joeynmt.training - Epoch   8, Step:    45200, Batch Loss:     1.096244, Batch Acc: 0.680286, Tokens per Sec:    21256, Lr: 0.000300
2025-05-28 09:57:16,574 - INFO - joeynmt.training - Epoch   8, Step:    45300, Batch Loss:     1.193107, Batch Acc: 0.684916, Tokens per Sec:    20843, Lr: 0.000300
2025-05-28 09:57:19,896 - INFO - joeynmt.training - Epoch   8, Step:    45400, Batch Loss:     1.057276, Batch Acc: 0.679108, Tokens per Sec:    21557, Lr: 0.000300
2025-05-28 09:57:23,202 - INFO - joeynmt.training - Epoch   8, Step:    45500, Batch Loss:     1.258216, Batch Acc: 0.675069, Tokens per Sec:    21597, Lr: 0.000300
2025-05-28 09:57:23,202 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:57:23,202 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:57:33,225 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.26, ppl:   3.54, acc:   0.66, generation: 10.0127[sec], evaluation: 0.0000[sec]
2025-05-28 09:57:33,637 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/43000.ckpt
2025-05-28 09:57:33,664 - INFO - joeynmt.training - Example #0
2025-05-28 09:57:33,664 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:57:33,664 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:57:33,664 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'mostr@@', '<unk>', '@', 'are', 'che', 'la', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'che', 'è', 'stata', 'la', 'chiamata', 'dell&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'ing', ',', 'per', 'tre', 'milioni', 'di', 'anni', '.', '</s>']
2025-05-28 09:57:33,664 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:57:33,665 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:57:33,665 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due due di<unk> @ apos<unk> @ iti<unk> @ ve per mostr<unk> @ are che la ghi<unk> @ accio ar<unk> @ t<unk> @ ica ar<unk> @ t<unk> @ ica che è stata la chiamata dell&apos; E<unk> @ is<unk> @ k<unk> @ ing , per tre milioni di anni .
2025-05-28 09:57:33,665 - INFO - joeynmt.training - Example #1
2025-05-28 09:57:33,665 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:57:33,665 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:57:33,665 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'la', 'prima', 'cosa', 'che', 'è', 'abbastanza', 'la', 'sost@@', '<unk>', '@', 'anza', 'di', 'questo', 'problema', ',', 'perché', 'non', 'ci', 'mostra', 'la', 'di@@', '<unk>', '@', 'mostra', 'l&apos;', 'u@@', '<unk>', '@', 'mi@@', '<unk>', '@', 'st@@', '<unk>', '@', 'ità', 'del', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 09:57:33,665 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:57:33,665 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:57:33,665 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza la prima cosa che è abbastanza la sost<unk> @ anza di questo problema , perché non ci mostra la di<unk> @ mostra l&apos; u<unk> @ mi<unk> @ st<unk> @ ità del ghi<unk> @ accio .
2025-05-28 09:57:33,665 - INFO - joeynmt.training - Example #2
2025-05-28 09:57:33,666 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:57:33,666 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:57:33,666 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'c@@', '<unk>', '@', 'ass@@', '<unk>', '@', 'etta', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'è', 'la', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'va', 'del', 'nostro', 'sistema', 'peggi@@', '<unk>', '@', 'o', 'globale', '.', '</s>']
2025-05-28 09:57:33,666 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:57:33,666 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:57:33,666 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la c<unk> @ ass<unk> @ etta ar<unk> @ t<unk> @ ica è la c<unk> @ atti<unk> @ va del nostro sistema peggi<unk> @ o globale .
2025-05-28 09:57:33,666 - INFO - joeynmt.training - Example #3
2025-05-28 09:57:33,666 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:57:33,667 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:57:33,667 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', 'cres@@', '<unk>', '@', 'ce', 'nel', 'vento', 'e', 's@@', '<unk>', '@', 'fondo', '.', '</s>']
2025-05-28 09:57:33,667 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:57:33,667 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:57:33,667 - INFO - joeynmt.training - 	Hypothesis: E cres<unk> @ ce nel vento e s<unk> @ fondo .
2025-05-28 09:57:33,667 - INFO - joeynmt.training - Example #4
2025-05-28 09:57:33,667 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:57:33,667 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:57:33,667 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'o', 'è', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'più', 'di', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:57:33,668 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:57:33,668 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:57:33,668 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ o è un seg<unk> @ no di più di negli ultimi 25 anni .
2025-05-28 09:57:36,998 - INFO - joeynmt.training - Epoch   8, Step:    45600, Batch Loss:     1.263769, Batch Acc: 0.678397, Tokens per Sec:    18914, Lr: 0.000300
2025-05-28 09:57:40,306 - INFO - joeynmt.training - Epoch   8, Step:    45700, Batch Loss:     1.177585, Batch Acc: 0.678151, Tokens per Sec:    21637, Lr: 0.000300
2025-05-28 09:57:43,631 - INFO - joeynmt.training - Epoch   8, Step:    45800, Batch Loss:     1.170168, Batch Acc: 0.680998, Tokens per Sec:    21732, Lr: 0.000300
2025-05-28 09:57:46,943 - INFO - joeynmt.training - Epoch   8, Step:    45900, Batch Loss:     1.081819, Batch Acc: 0.674884, Tokens per Sec:    21664, Lr: 0.000300
2025-05-28 09:57:50,271 - INFO - joeynmt.training - Epoch   8, Step:    46000, Batch Loss:     1.052907, Batch Acc: 0.676083, Tokens per Sec:    22172, Lr: 0.000300
2025-05-28 09:57:50,271 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:57:50,271 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:58:00,970 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.26, ppl:   3.53, acc:   0.66, generation: 10.6917[sec], evaluation: 0.0000[sec]
2025-05-28 09:58:00,971 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:58:01,494 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/43500.ckpt
2025-05-28 09:58:01,519 - INFO - joeynmt.training - Example #0
2025-05-28 09:58:01,520 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:58:01,520 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:58:01,520 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'mostr@@', '<unk>', '@', 'are', 'che', 'i', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ino', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'che', 'si', 'è', 'ri@@', '<unk>', '@', 'ma@@', '<unk>', '@', 'sta', 'in', 'mezzo', 'al', '40', 'di', 'sotto', 'del', '4@@', '<unk>', '@', '8', ',', 'il', '40', '%', '.', '</s>']
2025-05-28 09:58:01,520 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:58:01,520 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:58:01,520 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due due di<unk> @ apos<unk> @ iti<unk> @ ve per mostr<unk> @ are che i ghi<unk> @ accio ar<unk> @ c<unk> @ ino per tre milioni di anni , che si è ri<unk> @ ma<unk> @ sta in mezzo al 40 di sotto del 4<unk> @ 8 , il 40 % .
2025-05-28 09:58:01,520 - INFO - joeynmt.training - Example #1
2025-05-28 09:58:01,521 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:58:01,521 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:58:01,521 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'forte', 'da', 'essere', 'il', 'problema', 'di', 'quanto', 'sia', 'la', 'cre@@', '<unk>', '@', 'azione', 'di', 'questo', 'particolare', 'problema', ',', 'perché', 'non', 'è', 'il', 'soff@@', '<unk>', '@', 'ri@@', '<unk>', '@', 're', 'del', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 09:58:01,521 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:58:01,521 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:58:01,521 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte da essere il problema di quanto sia la cre<unk> @ azione di questo particolare problema , perché non è il soff<unk> @ ri<unk> @ re del ghi<unk> @ accio .
2025-05-28 09:58:01,521 - INFO - joeynmt.training - Example #2
2025-05-28 09:58:01,521 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:58:01,521 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:58:01,521 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'è', 'il', 'g@@', '<unk>', '@', 'as', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', ',', 'il', 'cuore', 'intelli@@', '<unk>', '@', 'gente', 'del', 'nostro', 'sistema', 'intelli@@', '<unk>', '@', 'gente', '.', '</s>']
2025-05-28 09:58:01,522 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:58:01,522 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:58:01,522 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , il cuore è il g<unk> @ as ar<unk> @ t<unk> @ ico ar<unk> @ t<unk> @ ico , il cuore intelli<unk> @ gente del nostro sistema intelli<unk> @ gente .
2025-05-28 09:58:01,522 - INFO - joeynmt.training - Example #3
2025-05-28 09:58:01,522 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:58:01,522 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:58:01,522 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'sta', 'cresc@@', '<unk>', '@', 'endo', 'in', 'sal@@', '<unk>', '@', 'a', 'e', 'b@@', '<unk>', '@', 'oc@@', '<unk>', '@', 'ca', 'nel', 'sal@@', '<unk>', '@', 'o', '.', '</s>']
2025-05-28 09:58:01,522 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:58:01,522 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:58:01,522 - INFO - joeynmt.training - 	Hypothesis: Si sta cresc<unk> @ endo in sal<unk> @ a e b<unk> @ oc<unk> @ ca nel sal<unk> @ o .
2025-05-28 09:58:01,522 - INFO - joeynmt.training - Example #4
2025-05-28 09:58:01,523 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:58:01,523 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:58:01,523 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'o', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:58:01,523 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:58:01,523 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:58:01,523 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ o è successo negli ultimi 25 anni .
2025-05-28 09:58:04,899 - INFO - joeynmt.training - Epoch   8, Step:    46100, Batch Loss:     1.317710, Batch Acc: 0.681031, Tokens per Sec:    18180, Lr: 0.000300
2025-05-28 09:58:08,319 - INFO - joeynmt.training - Epoch   8, Step:    46200, Batch Loss:     1.212020, Batch Acc: 0.680953, Tokens per Sec:    21368, Lr: 0.000300
2025-05-28 09:58:11,736 - INFO - joeynmt.training - Epoch   8, Step:    46300, Batch Loss:     1.142770, Batch Acc: 0.679380, Tokens per Sec:    21007, Lr: 0.000300
2025-05-28 09:58:15,157 - INFO - joeynmt.training - Epoch   8, Step:    46400, Batch Loss:     1.156533, Batch Acc: 0.676228, Tokens per Sec:    21364, Lr: 0.000300
2025-05-28 09:58:18,562 - INFO - joeynmt.training - Epoch   8, Step:    46500, Batch Loss:     1.287073, Batch Acc: 0.675813, Tokens per Sec:    20835, Lr: 0.000300
2025-05-28 09:58:18,562 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:58:18,562 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:58:30,758 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.27, ppl:   3.55, acc:   0.66, generation: 12.1848[sec], evaluation: 0.0000[sec]
2025-05-28 09:58:30,768 - INFO - joeynmt.training - Example #0
2025-05-28 09:58:30,769 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:58:30,769 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:58:30,769 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 's@@', '<unk>', '@', 'li@@', '<unk>', '@', 'de', 'per', 'capire', 'che', 'la', 'fr@@', '<unk>', '@', 'ase', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'la', 'gr@@', '<unk>', '@', 'av@@', '<unk>', '@', 'ola', 'di', '4@@', '<unk>', '@', '8', '%', '.', '</s>']
2025-05-28 09:58:30,769 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:58:30,770 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:58:30,770 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due s<unk> @ li<unk> @ de per capire che la fr<unk> @ ase ar<unk> @ t<unk> @ ica ar<unk> @ t<unk> @ ica ar<unk> @ t<unk> @ ica ar<unk> @ t<unk> @ ica per tre milioni di anni , la gr<unk> @ av<unk> @ ola di 4<unk> @ 8 % .
2025-05-28 09:58:30,770 - INFO - joeynmt.training - Example #1
2025-05-28 09:58:30,770 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:58:30,770 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:58:30,770 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'forte', 'per', 'la', 'sost@@', '<unk>', '@', 'anza', 'di', 'questo', 'problema', 'speci@@', '<unk>', '@', 'fico', ',', 'perché', 'non', 'è', 'il', 'soff@@', '<unk>', '@', 'itto', 'di', 'questo', 'problema', 'speci@@', '<unk>', '@', 'fico', ',', 'perché', 'non', 'è', 'il', 'soff@@', '<unk>', '@', 'itto', '.', '</s>']
2025-05-28 09:58:30,771 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:58:30,771 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:58:30,771 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza forte per la sost<unk> @ anza di questo problema speci<unk> @ fico , perché non è il soff<unk> @ itto di questo problema speci<unk> @ fico , perché non è il soff<unk> @ itto .
2025-05-28 09:58:30,771 - INFO - joeynmt.training - Example #2
2025-05-28 09:58:30,771 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:58:30,771 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:58:30,771 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'di', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'app@@', '<unk>', '@', 'e', ',', 'il', 'cuore', 'del', 'sistema', 'intelli@@', '<unk>', '@', 'gente', '.', '</s>']
2025-05-28 09:58:30,772 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:58:30,772 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:58:30,772 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , il cuore di E<unk> @ is<unk> @ k<unk> @ app<unk> @ e , il cuore del sistema intelli<unk> @ gente .
2025-05-28 09:58:30,772 - INFO - joeynmt.training - Example #3
2025-05-28 09:58:30,772 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:58:30,772 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:58:30,772 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'è', 'fer@@', '<unk>', '@', 'mato', 'in', 'in@@', '<unk>', '@', 'ver@@', '<unk>', '@', 'no', 'e', 's@@', '<unk>', '@', 'v@@', '<unk>', '@', 'egli@@', '<unk>', '@', 'ato', 'nell&apos;', 'est@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 09:58:30,773 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:58:30,773 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:58:30,773 - INFO - joeynmt.training - 	Hypothesis: Si è fer<unk> @ mato in in<unk> @ ver<unk> @ no e s<unk> @ v<unk> @ egli<unk> @ ato nell&apos; est<unk> @ ate .
2025-05-28 09:58:30,773 - INFO - joeynmt.training - Example #4
2025-05-28 09:58:30,773 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:58:30,773 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:58:30,773 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'i', 'è', 'ciò', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:58:30,774 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:58:30,774 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:58:30,774 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ i è ciò che è successo negli ultimi 25 anni .
2025-05-28 09:58:34,177 - INFO - joeynmt.training - Epoch   8, Step:    46600, Batch Loss:     1.288599, Batch Acc: 0.679869, Tokens per Sec:    20386, Lr: 0.000300
2025-05-28 09:58:37,583 - INFO - joeynmt.training - Epoch   8, Step:    46700, Batch Loss:     1.234707, Batch Acc: 0.678841, Tokens per Sec:    21269, Lr: 0.000300
2025-05-28 09:58:41,009 - INFO - joeynmt.training - Epoch   8, Step:    46800, Batch Loss:     1.309868, Batch Acc: 0.680682, Tokens per Sec:    20816, Lr: 0.000300
2025-05-28 09:58:44,406 - INFO - joeynmt.training - Epoch   8, Step:    46900, Batch Loss:     1.148278, Batch Acc: 0.677602, Tokens per Sec:    20959, Lr: 0.000300
2025-05-28 09:58:47,792 - INFO - joeynmt.training - Epoch   8, Step:    47000, Batch Loss:     1.358131, Batch Acc: 0.676562, Tokens per Sec:    20609, Lr: 0.000300
2025-05-28 09:58:47,792 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:58:47,792 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:58:59,401 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.26, ppl:   3.54, acc:   0.66, generation: 11.5976[sec], evaluation: 0.0000[sec]
2025-05-28 09:58:59,825 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/45000.ckpt
2025-05-28 09:58:59,855 - INFO - joeynmt.training - Example #0
2025-05-28 09:58:59,855 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:58:59,855 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:58:59,855 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'due', 'due', 's@@', '<unk>', '@', 'li@@', '<unk>', '@', 'de', 'per', 'ri@@', '<unk>', '@', 'durre', 'che', 'la', 'mat@@', '<unk>', '@', 'eria', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'che', 'ha', 'avuto', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'che', 'è', 'stato', 'chiamato', 'il', '40', '%', 'della', 'gr@@', '<unk>', '@', 'av@@', '<unk>', '@', 'ità', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'del', '40', 'per@@', '<unk>', '@', 'cento', '.', '</s>']
2025-05-28 09:58:59,856 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:58:59,856 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:58:59,856 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due due due s<unk> @ li<unk> @ de per ri<unk> @ durre che la mat<unk> @ eria ar<unk> @ t<unk> @ ica ar<unk> @ t<unk> @ ica ar<unk> @ t<unk> @ ica che ha avuto per tre milioni di anni , che è stato chiamato il 40 % della gr<unk> @ av<unk> @ ità ar<unk> @ t<unk> @ ica del 40 per<unk> @ cento .
2025-05-28 09:58:59,856 - INFO - joeynmt.training - Example #1
2025-05-28 09:58:59,856 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:58:59,856 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:58:59,857 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'forte', 'da', 'essere', 'una', 'cosa', 'di', 'cui', 'la', 'cosa', 'di', 'cui', 'la', 'loro', 'esperienza', 'è', 'una', 'cosa', 'che', 'non', 'è', 'il', 'di@@', '<unk>', '@', 'mostra', 'il', 'tri@@', '<unk>', '@', 'b@@', '<unk>', '@', 'bi@@', '<unk>', '@', 'o', 'del', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 09:58:59,857 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:58:59,857 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:58:59,857 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte da essere una cosa di cui la cosa di cui la loro esperienza è una cosa che non è il di<unk> @ mostra il tri<unk> @ b<unk> @ bi<unk> @ o del ghi<unk> @ accio .
2025-05-28 09:58:59,857 - INFO - joeynmt.training - Example #2
2025-05-28 09:58:59,857 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:58:59,858 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:58:59,858 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'di', 'una', 'b@@', '<unk>', '@', 'otti@@', '<unk>', '@', 'gli@@', '<unk>', '@', 'a', 'di', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'del', 'nostro', 'sistema', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'vo', 'globale', '.', '</s>']
2025-05-28 09:58:59,858 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:58:59,858 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:58:59,858 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , il cuore di una b<unk> @ otti<unk> @ gli<unk> @ a di ghi<unk> @ accio ar<unk> @ t<unk> @ ico del nostro sistema c<unk> @ atti<unk> @ vo globale .
2025-05-28 09:58:59,858 - INFO - joeynmt.training - Example #3
2025-05-28 09:58:59,858 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:58:59,858 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:58:59,859 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'volta', 'che', 'si', 'è', 'ri@@', '<unk>', '@', 'fer@@', '<unk>', '@', 'ita', 'e', 'si', 's@@', '<unk>', '@', 'vol@@', '<unk>', '@', 'ge', 'in', 'est@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 09:58:59,859 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:58:59,859 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:58:59,859 - INFO - joeynmt.training - 	Hypothesis: La prima volta che si è ri<unk> @ fer<unk> @ ita e si s<unk> @ vol<unk> @ ge in est<unk> @ ate .
2025-05-28 09:58:59,859 - INFO - joeynmt.training - Example #4
2025-05-28 09:58:59,859 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:58:59,859 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:58:59,860 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'consegu@@', '<unk>', '@', 'enza', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'una', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'na', 'di', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'seg@@', '<unk>', '@', 'no', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:58:59,860 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:58:59,860 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:58:59,860 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma consegu<unk> @ enza che vi mostr<unk> @ erò è una di<unk> @ seg<unk> @ na di un seg<unk> @ no di seg<unk> @ no negli ultimi 25 anni .
2025-05-28 09:59:03,324 - INFO - joeynmt.training - Epoch   8, Step:    47100, Batch Loss:     1.180357, Batch Acc: 0.676206, Tokens per Sec:    18592, Lr: 0.000300
2025-05-28 09:59:06,726 - INFO - joeynmt.training - Epoch   8, Step:    47200, Batch Loss:     1.185239, Batch Acc: 0.673541, Tokens per Sec:    20473, Lr: 0.000300
2025-05-28 09:59:10,132 - INFO - joeynmt.training - Epoch   8, Step:    47300, Batch Loss:     1.208488, Batch Acc: 0.675298, Tokens per Sec:    20800, Lr: 0.000300
2025-05-28 09:59:13,526 - INFO - joeynmt.training - Epoch   8, Step:    47400, Batch Loss:     1.140400, Batch Acc: 0.676606, Tokens per Sec:    21441, Lr: 0.000300
2025-05-28 09:59:16,932 - INFO - joeynmt.training - Epoch   8, Step:    47500, Batch Loss:     1.194407, Batch Acc: 0.676236, Tokens per Sec:    20775, Lr: 0.000300
2025-05-28 09:59:16,933 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:59:16,933 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:59:28,199 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.26, ppl:   3.52, acc:   0.66, generation: 11.2590[sec], evaluation: 0.0000[sec]
2025-05-28 09:59:28,199 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:59:28,726 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/45500.ckpt
2025-05-28 09:59:28,751 - INFO - joeynmt.training - Example #0
2025-05-28 09:59:28,751 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:59:28,751 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:59:28,751 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'es@@', '<unk>', '@', 'i@@', '<unk>', '@', 'de', 'per', 'vedere', 'che', 'i', 't@@', '<unk>', '@', 'es@@', '<unk>', '@', 'chi', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'es@@', '<unk>', '@', 'chi', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'es@@', '<unk>', '@', 'chi', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'la', 'gr@@', '<unk>', '@', 'av@@', '<unk>', '@', 'ità', ',', 'che', 'è', 'stato', 'in@@', '<unk>', '@', 'fer@@', '<unk>', '@', 'mato', 'il', '40', '%', '.', '</s>']
2025-05-28 09:59:28,752 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:59:28,752 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:59:28,752 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due es<unk> @ i<unk> @ de per vedere che i t<unk> @ es<unk> @ chi ar<unk> @ t<unk> @ es<unk> @ chi ar<unk> @ t<unk> @ es<unk> @ chi per tre milioni di anni , la gr<unk> @ av<unk> @ ità , che è stato in<unk> @ fer<unk> @ mato il 40 % .
2025-05-28 09:59:28,752 - INFO - joeynmt.training - Example #1
2025-05-28 09:59:28,752 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:59:28,752 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:59:28,752 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'forte', ',', 'la', 'sost@@', '<unk>', '@', 'anza', 'di', 'questo', 'particolare', 'problema', ',', 'perché', 'non', 'lo', 'fa', 'il', 'l@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ke', '.', '</s>']
2025-05-28 09:59:28,753 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:59:28,753 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:59:28,753 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte , la sost<unk> @ anza di questo particolare problema , perché non lo fa il l<unk> @ ic<unk> @ ke .
2025-05-28 09:59:28,753 - INFO - joeynmt.training - Example #2
2025-05-28 09:59:28,753 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:59:28,753 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:59:28,754 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'ris@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ale', 'è', 'il', 'cuore', 'dell&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'c@@', '<unk>', '@', 'app@@', '<unk>', '@', 'ola', 'il', 'cuore', 'della', 'nostra', 'c@@', '<unk>', '@', 'ru@@', '<unk>', '@', 'zione', 'del', 'nostro', 'sistema', 'globale', '.', '</s>']
2025-05-28 09:59:28,754 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:59:28,754 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:59:28,754 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la ris<unk> @ i<unk> @ ale è il cuore dell&apos; E<unk> @ is<unk> @ c<unk> @ app<unk> @ ola il cuore della nostra c<unk> @ ru<unk> @ zione del nostro sistema globale .
2025-05-28 09:59:28,754 - INFO - joeynmt.training - Example #3
2025-05-28 09:59:28,754 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:59:28,754 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:59:28,754 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&apos;', 'cres@@', '<unk>', '@', 'ci@@', '<unk>', '@', 'uta', 'in', 'vento', 'e', 's@@', '<unk>', '@', 'os@@', '<unk>', '@', 'so', 'nel', 'vento', '.', '</s>']
2025-05-28 09:59:28,755 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:59:28,755 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:59:28,755 - INFO - joeynmt.training - 	Hypothesis: E &apos; cres<unk> @ ci<unk> @ uta in vento e s<unk> @ os<unk> @ so nel vento .
2025-05-28 09:59:28,755 - INFO - joeynmt.training - Example #4
2025-05-28 09:59:28,755 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:59:28,755 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:59:28,755 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 's@@', '<unk>', '@', 'li@@', '<unk>', '@', 'de', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'una', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'ret@@', '<unk>', '@', 'ta', 'cosa', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:59:28,755 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:59:28,756 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:59:28,756 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma s<unk> @ li<unk> @ de che vi mostr<unk> @ erò è una di<unk> @ seg<unk> @ ret<unk> @ ta cosa è successo negli ultimi 25 anni .
2025-05-28 09:59:32,136 - INFO - joeynmt.training - Epoch   8, Step:    47600, Batch Loss:     1.145152, Batch Acc: 0.679134, Tokens per Sec:    18755, Lr: 0.000300
2025-05-28 09:59:35,563 - INFO - joeynmt.training - Epoch   8, Step:    47700, Batch Loss:     1.210604, Batch Acc: 0.676832, Tokens per Sec:    21661, Lr: 0.000300
2025-05-28 09:59:38,967 - INFO - joeynmt.training - Epoch   8, Step:    47800, Batch Loss:     1.257925, Batch Acc: 0.677788, Tokens per Sec:    20592, Lr: 0.000300
2025-05-28 09:59:42,377 - INFO - joeynmt.training - Epoch   8, Step:    47900, Batch Loss:     1.198875, Batch Acc: 0.677704, Tokens per Sec:    21111, Lr: 0.000300
2025-05-28 09:59:45,772 - INFO - joeynmt.training - Epoch   8, Step:    48000, Batch Loss:     1.111606, Batch Acc: 0.681653, Tokens per Sec:    21272, Lr: 0.000300
2025-05-28 09:59:45,773 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 09:59:45,773 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 09:59:57,094 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.26, ppl:   3.52, acc:   0.66, generation: 11.3108[sec], evaluation: 0.0000[sec]
2025-05-28 09:59:57,094 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 09:59:57,677 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/47000.ckpt
2025-05-28 09:59:57,705 - INFO - joeynmt.training - Example #0
2025-05-28 09:59:57,706 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 09:59:57,706 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 09:59:57,706 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'per', 'poter', 'controll@@', '<unk>', '@', 'are', 'che', 'i', 'ghi@@', '<unk>', '@', 'acci@@', '<unk>', '@', 'ai', ',', 'che', 'i', 'sin@@', '<unk>', '@', 'cer@@', '<unk>', '@', 'ini', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ici', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'il', 'più', 'grande', 'di', '4@@', '<unk>', '@', '8', 'stati', ',', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'la', 'ver@@', '<unk>', '@', 'sione', 'dell&apos;', 'in@@', '<unk>', '@', 'dag@@', '<unk>', '@', 'ine', 'di', 'tre', 'milioni', 'di', 'anni', '.', '</s>']
2025-05-28 09:59:57,707 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 09:59:57,707 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 09:59:57,707 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ va per poter controll<unk> @ are che i ghi<unk> @ acci<unk> @ ai , che i sin<unk> @ cer<unk> @ ini ar<unk> @ t<unk> @ ici per tre milioni di anni , il più grande di 4<unk> @ 8 stati , per tre milioni di anni , per la ver<unk> @ sione dell&apos; in<unk> @ dag<unk> @ ine di tre milioni di anni .
2025-05-28 09:59:57,707 - INFO - joeynmt.training - Example #1
2025-05-28 09:59:57,707 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 09:59:57,707 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 09:59:57,707 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'così', 'che', 'ci', 'fa', 'abbastanza', 'forte', 'da', 'essere', 'la', 'ser@@', '<unk>', '@', 'i@@', '<unk>', '@', 'età', ',', 'perché', 'non', 'ci', 'mostra', 'la', 'forza', 'di', 'questo', 'particolare', 'problema', ',', 'perché', 'non', 'ci', 'mostra', 'la', 'mos@@', '<unk>', '@', 'sa', 'dell&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 09:59:57,708 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 09:59:57,708 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 09:59:57,708 - INFO - joeynmt.training - 	Hypothesis: Ma non è così che ci fa abbastanza forte da essere la ser<unk> @ i<unk> @ età , perché non ci mostra la forza di questo particolare problema , perché non ci mostra la mos<unk> @ sa dell&apos; E<unk> @ is<unk> @ es .
2025-05-28 09:59:57,708 - INFO - joeynmt.training - Example #2
2025-05-28 09:59:57,708 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 09:59:57,708 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 09:59:57,708 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'è', 'il', 'cuore', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', ',', 'il', 'cuore', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'del', 'nostro', 'sistema', 'globale', 'globale', '.', '</s>']
2025-05-28 09:59:57,709 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 09:59:57,709 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 09:59:57,709 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , è il cuore ar<unk> @ t<unk> @ ico ar<unk> @ t<unk> @ ico , il cuore ar<unk> @ t<unk> @ ico del nostro sistema globale globale .
2025-05-28 09:59:57,709 - INFO - joeynmt.training - Example #3
2025-05-28 09:59:57,709 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 09:59:57,709 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 09:59:57,709 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&apos;', 'in', 'un', 'in@@', '<unk>', '@', 'ver@@', '<unk>', '@', 'no', 'e', 's@@', '<unk>', '@', 'for@@', '<unk>', '@', 'a', 'nel', 'vento', 'e', 's@@', '<unk>', '@', 'om@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 09:59:57,710 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 09:59:57,710 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 09:59:57,710 - INFO - joeynmt.training - 	Hypothesis: E &apos; in un in<unk> @ ver<unk> @ no e s<unk> @ for<unk> @ a nel vento e s<unk> @ om<unk> @ ate .
2025-05-28 09:59:57,710 - INFO - joeynmt.training - Example #4
2025-05-28 09:59:57,710 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 09:59:57,710 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 09:59:57,710 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'consegu@@', '<unk>', '@', 'enza', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'una', 'co@@', '<unk>', '@', 'per@@', '<unk>', '@', 'ta', 'cosa', 'acc@@', '<unk>', '@', 'ade', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 09:59:57,710 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 09:59:57,711 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 09:59:57,711 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma consegu<unk> @ enza che vi mostr<unk> @ erò è una co<unk> @ per<unk> @ ta cosa acc<unk> @ ade negli ultimi 25 anni .
2025-05-28 10:00:01,122 - INFO - joeynmt.training - Epoch   8, Step:    48100, Batch Loss:     1.120796, Batch Acc: 0.674857, Tokens per Sec:    17541, Lr: 0.000300
2025-05-28 10:00:04,460 - INFO - joeynmt.training - Epoch   8, Step:    48200, Batch Loss:     1.200309, Batch Acc: 0.681317, Tokens per Sec:    21047, Lr: 0.000300
2025-05-28 10:00:07,775 - INFO - joeynmt.training - Epoch   8, Step:    48300, Batch Loss:     1.240851, Batch Acc: 0.680254, Tokens per Sec:    21374, Lr: 0.000300
2025-05-28 10:00:11,103 - INFO - joeynmt.training - Epoch   8, Step:    48400, Batch Loss:     1.104988, Batch Acc: 0.674481, Tokens per Sec:    21325, Lr: 0.000300
2025-05-28 10:00:14,427 - INFO - joeynmt.training - Epoch   8, Step:    48500, Batch Loss:     1.209955, Batch Acc: 0.675836, Tokens per Sec:    21904, Lr: 0.000300
2025-05-28 10:00:14,427 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:00:14,427 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:00:25,602 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.26, ppl:   3.51, acc:   0.66, generation: 11.1640[sec], evaluation: 0.0000[sec]
2025-05-28 10:00:25,603 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 10:00:26,282 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/44000.ckpt
2025-05-28 10:00:26,301 - INFO - joeynmt.training - Example #0
2025-05-28 10:00:26,302 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:00:26,302 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:00:26,302 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'vedere', 'che', 'i', 'ghi@@', '<unk>', '@', 'acci@@', '<unk>', '@', 'ai', ',', 'che', 'i', 'sin@@', '<unk>', '@', 'on@@', '<unk>', '@', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ini', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ici', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'il', 'l@@', '<unk>', '@', 'or@@', '<unk>', '@', 'n@@', '<unk>', '@', 'azioni', 'di', 'circa', 'il', '4@@', '<unk>', '@', '8', 'anni', '.', '</s>']
2025-05-28 10:00:26,302 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:00:26,302 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:00:26,302 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per vedere che i ghi<unk> @ acci<unk> @ ai , che i sin<unk> @ on<unk> @ ar<unk> @ c<unk> @ ini ar<unk> @ t<unk> @ ici per tre milioni di anni , il l<unk> @ or<unk> @ n<unk> @ azioni di circa il 4<unk> @ 8 anni .
2025-05-28 10:00:26,302 - INFO - joeynmt.training - Example #1
2025-05-28 10:00:26,303 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:00:26,303 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:00:26,303 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'che', 'la', 'sost@@', '<unk>', '@', 'anza', 'di', 'questo', 'problema', ',', 'che', 'non', 'è', 'abbastanza', 'la', 'sost@@', '<unk>', '@', 'anza', 'di', 'questo', 'problema', ',', 'perché', 'non', 'è', 'il', 'di@@', '<unk>', '@', 'mostra', 'il', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'no', '.', '</s>']
2025-05-28 10:00:26,303 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:00:26,303 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:00:26,303 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza che la sost<unk> @ anza di questo problema , che non è abbastanza la sost<unk> @ anza di questo problema , perché non è il di<unk> @ mostra il di<unk> @ seg<unk> @ no .
2025-05-28 10:00:26,303 - INFO - joeynmt.training - Example #2
2025-05-28 10:00:26,303 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:00:26,303 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:00:26,304 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'cor@@', '<unk>', '@', 'sa', 'di', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'è', 'la', 'cor@@', '<unk>', '@', 'rispon@@', '<unk>', '@', 'denza', 'di', 'un', 'sistema', 'di', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'vo', 'globale', '.', '</s>']
2025-05-28 10:00:26,304 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:00:26,304 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:00:26,304 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la cor<unk> @ sa di ghi<unk> @ accio ar<unk> @ t<unk> @ ica è la cor<unk> @ rispon<unk> @ denza di un sistema di c<unk> @ atti<unk> @ vo globale .
2025-05-28 10:00:26,304 - INFO - joeynmt.training - Example #3
2025-05-28 10:00:26,304 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:00:26,304 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:00:26,304 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Vi@@', '<unk>', '@', 'ene', 'nel', 'vento', 'e', 's@@', '<unk>', '@', 'guar@@', '<unk>', '@', 'do', 'nel', 'vento', 'e', 's@@', '<unk>', '@', 'guar@@', '<unk>', '@', 'do', 'nel', 'vento', '.', '</s>']
2025-05-28 10:00:26,305 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:00:26,305 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:00:26,305 - INFO - joeynmt.training - 	Hypothesis: Vi<unk> @ ene nel vento e s<unk> @ guar<unk> @ do nel vento e s<unk> @ guar<unk> @ do nel vento .
2025-05-28 10:00:26,305 - INFO - joeynmt.training - Example #4
2025-05-28 10:00:26,305 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:00:26,305 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:00:26,305 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'co@@', '<unk>', '@', 'per@@', '<unk>', '@', 'to', 'di', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 10:00:26,306 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:00:26,306 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:00:26,306 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ erò è un seg<unk> @ no di co<unk> @ per<unk> @ to di quello che è successo negli ultimi 25 anni .
2025-05-28 10:00:29,688 - INFO - joeynmt.training - Epoch   8, Step:    48600, Batch Loss:     1.087156, Batch Acc: 0.677040, Tokens per Sec:    17809, Lr: 0.000300
2025-05-28 10:00:33,097 - INFO - joeynmt.training - Epoch   8, Step:    48700, Batch Loss:     1.105694, Batch Acc: 0.672452, Tokens per Sec:    20792, Lr: 0.000300
2025-05-28 10:00:36,546 - INFO - joeynmt.training - Epoch   8, Step:    48800, Batch Loss:     0.965989, Batch Acc: 0.676931, Tokens per Sec:    20901, Lr: 0.000300
2025-05-28 10:00:39,979 - INFO - joeynmt.training - Epoch   8, Step:    48900, Batch Loss:     1.133255, Batch Acc: 0.674933, Tokens per Sec:    20518, Lr: 0.000300
2025-05-28 10:00:43,385 - INFO - joeynmt.training - Epoch   8, Step:    49000, Batch Loss:     1.116739, Batch Acc: 0.678240, Tokens per Sec:    20712, Lr: 0.000300
2025-05-28 10:00:43,385 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:00:43,385 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:00:54,409 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.25, ppl:   3.50, acc:   0.66, generation: 11.0125[sec], evaluation: 0.0000[sec]
2025-05-28 10:00:54,409 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 10:00:55,058 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/44500.ckpt
2025-05-28 10:00:55,096 - INFO - joeynmt.training - Example #0
2025-05-28 10:00:55,096 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:00:55,096 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:00:55,097 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'guardare', 'a', 'guardare', 'il', 't@@', '<unk>', '@', 'oc@@', '<unk>', '@', 'co', 'ar@@', '<unk>', '@', 'kt@@', '<unk>', '@', 'ica', ',', 'che', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'i', 'con@@', '<unk>', '@', 'fr@@', '<unk>', '@', 'ont@@', '<unk>', '@', 'ari', 'che', 'si', 'era', 'chiam@@', '<unk>', '@', 'ava', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'il', 'gr@@', '<unk>', '@', 'asso', 'di', 's@@', '<unk>', '@', 'ov@@', '<unk>', '@', 'e', ',', 'per', 'cento', 'di', '40', 'stati', 's@@', '<unk>', '@', 'fr@@', '<unk>', '@', 'utt@@', '<unk>', '@', 'ori', '.', '</s>']
2025-05-28 10:00:55,097 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:00:55,097 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:00:55,097 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per guardare a guardare il t<unk> @ oc<unk> @ co ar<unk> @ kt<unk> @ ica , che per tre milioni di anni , i con<unk> @ fr<unk> @ ont<unk> @ ari che si era chiam<unk> @ ava per tre milioni di anni , il gr<unk> @ asso di s<unk> @ ov<unk> @ e , per cento di 40 stati s<unk> @ fr<unk> @ utt<unk> @ ori .
2025-05-28 10:00:55,097 - INFO - joeynmt.training - Example #1
2025-05-28 10:00:55,098 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:00:55,098 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:00:55,098 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'che', 'non', 'è', 'abbastanza', 'la', 'ser@@', '<unk>', '@', 'i@@', '<unk>', '@', 'enza', 'di', 'questo', 'speci@@', '<unk>', '@', 'fico', ',', 'perché', 'non', 'lo', 'mostra', 'il', 'soff@@', '<unk>', '@', 'itto', 'che', 'non', 'ci', 'mostra', 'la', 'D@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ke', '.', '</s>']
2025-05-28 10:00:55,098 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:00:55,098 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:00:55,098 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza che non è abbastanza la ser<unk> @ i<unk> @ enza di questo speci<unk> @ fico , perché non lo mostra il soff<unk> @ itto che non ci mostra la D<unk> @ ic<unk> @ ke .
2025-05-28 10:00:55,098 - INFO - joeynmt.training - Example #2
2025-05-28 10:00:55,099 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:00:55,099 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:00:55,099 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'di', 'un', 'sistema', 'di', 'mat@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'mon@@', '<unk>', '@', 'io', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', ',', 'il', 'cuore', 'di', 'un', 'sistema', 'di', 'ris@@', '<unk>', '@', 'cal@@', '<unk>', '@', 'd@@', '<unk>', '@', 'amento', 'globale', '.', '</s>']
2025-05-28 10:00:55,099 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:00:55,099 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:00:55,099 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , il cuore di un sistema di mat<unk> @ ri<unk> @ mon<unk> @ io ar<unk> @ t<unk> @ ica , il cuore di un sistema di ris<unk> @ cal<unk> @ d<unk> @ amento globale .
2025-05-28 10:00:55,099 - INFO - joeynmt.training - Example #3
2025-05-28 10:00:55,100 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:00:55,100 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:00:55,100 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['cres@@', '<unk>', '@', 'ce', 'in', 'un', 'vento', 'e', 's@@', '<unk>', '@', 'om@@', '<unk>', '@', 't@@', '<unk>', '@', 'ito', 'nella', 'sal@@', '<unk>', '@', 'a', '.', '</s>']
2025-05-28 10:00:55,100 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:00:55,100 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:00:55,100 - INFO - joeynmt.training - 	Hypothesis: cres<unk> @ ce in un vento e s<unk> @ om<unk> @ t<unk> @ ito nella sal<unk> @ a .
2025-05-28 10:00:55,100 - INFO - joeynmt.training - Example #4
2025-05-28 10:00:55,101 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:00:55,101 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:00:55,101 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'consegu@@', '<unk>', '@', 'enza', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'seg@@', '<unk>', '@', 'no', 'di', 'un', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'no', 'di', 'co@@', '<unk>', '@', '-@@', '<unk>', '@', 'anni', '.', '</s>']
2025-05-28 10:00:55,101 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:00:55,101 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:00:55,101 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma consegu<unk> @ enza che vi mostr<unk> @ erò è un seg<unk> @ no di seg<unk> @ no di un di<unk> @ seg<unk> @ no di co<unk> @ -<unk> @ anni .
2025-05-28 10:00:58,505 - INFO - joeynmt.training - Epoch   8, Step:    49100, Batch Loss:     1.194262, Batch Acc: 0.677663, Tokens per Sec:    17367, Lr: 0.000300
2025-05-28 10:01:01,890 - INFO - joeynmt.training - Epoch   8, Step:    49200, Batch Loss:     1.272986, Batch Acc: 0.674633, Tokens per Sec:    21115, Lr: 0.000300
2025-05-28 10:01:05,294 - INFO - joeynmt.training - Epoch   8, Step:    49300, Batch Loss:     1.184800, Batch Acc: 0.674863, Tokens per Sec:    20434, Lr: 0.000300
2025-05-28 10:01:08,689 - INFO - joeynmt.training - Epoch   8, Step:    49400, Batch Loss:     1.104564, Batch Acc: 0.673731, Tokens per Sec:    21757, Lr: 0.000300
2025-05-28 10:01:12,078 - INFO - joeynmt.training - Epoch   8, Step:    49500, Batch Loss:     1.304230, Batch Acc: 0.672376, Tokens per Sec:    20990, Lr: 0.000300
2025-05-28 10:01:12,078 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:01:12,078 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:01:23,289 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.25, ppl:   3.50, acc:   0.66, generation: 11.2007[sec], evaluation: 0.0000[sec]
2025-05-28 10:01:23,696 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/46000.ckpt
2025-05-28 10:01:23,717 - INFO - joeynmt.training - Example #0
2025-05-28 10:01:23,717 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:01:23,717 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:01:23,717 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'vedere', 'che', 'i', 'ghi@@', '<unk>', '@', 'acci@@', '<unk>', '@', 'ai', 'che', 'i', 'gi@@', '<unk>', '@', 'ard@@', '<unk>', '@', 'ino', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'la', 'maggior', 'parte', 'degli', 'stati', ',', 'il', '4@@', '<unk>', '@', '8', 'anni', ',', 'per', 'cento', '.', '</s>']
2025-05-28 10:01:23,718 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:01:23,718 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:01:23,718 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per vedere che i ghi<unk> @ acci<unk> @ ai che i gi<unk> @ ard<unk> @ ino ar<unk> @ t<unk> @ ico per tre milioni di anni , la maggior parte degli stati , il 4<unk> @ 8 anni , per cento .
2025-05-28 10:01:23,718 - INFO - joeynmt.training - Example #1
2025-05-28 10:01:23,719 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:01:23,719 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:01:23,719 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'forte', 'che', 'la', 'ser@@', '<unk>', '@', 'i@@', '<unk>', '@', 'età', 'di', 'questo', 'particolare', 'problema', ',', 'non', 'è', 'che', 'la', 'di@@', '<unk>', '@', 'mostra', 'che', 'non', 'è', 'il', 'bu@@', '<unk>', '@', 'io', ',', 'perché', 'non', 'è', 'il', 'bu@@', '<unk>', '@', 'io', 'mostr@@', '<unk>', '@', 'o', 'l&apos;', 'u@@', '<unk>', '@', 'ov@@', '<unk>', '@', 'a', 'dell&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 10:01:23,719 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:01:23,719 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:01:23,719 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza forte che la ser<unk> @ i<unk> @ età di questo particolare problema , non è che la di<unk> @ mostra che non è il bu<unk> @ io , perché non è il bu<unk> @ io mostr<unk> @ o l&apos; u<unk> @ ov<unk> @ a dell&apos; E<unk> @ is<unk> @ es .
2025-05-28 10:01:23,719 - INFO - joeynmt.training - Example #2
2025-05-28 10:01:23,720 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:01:23,720 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:01:23,720 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'qualche', 'certo', 'senso', ',', 'la', 'c@@', '<unk>', '@', 'ass@@', '<unk>', '@', 'etta', 'è', 'la', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'va', 'del', 'nostro', 'sistema', 'di', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'vo', '.', '</s>']
2025-05-28 10:01:23,720 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:01:23,720 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:01:23,720 - INFO - joeynmt.training - 	Hypothesis: In qualche certo senso , la c<unk> @ ass<unk> @ etta è la c<unk> @ atti<unk> @ va del nostro sistema di c<unk> @ atti<unk> @ vo .
2025-05-28 10:01:23,720 - INFO - joeynmt.training - Example #3
2025-05-28 10:01:23,720 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:01:23,721 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:01:23,721 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'a', 'cres@@', '<unk>', '@', 'ce', 'nel', 'vento', 'e', 's@@', '<unk>', '@', 'fondo', '.', '</s>']
2025-05-28 10:01:23,721 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:01:23,721 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:01:23,721 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ a cres<unk> @ ce nel vento e s<unk> @ fondo .
2025-05-28 10:01:23,721 - INFO - joeynmt.training - Example #4
2025-05-28 10:01:23,721 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:01:23,721 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:01:23,721 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'tempo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 10:01:23,722 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:01:23,722 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:01:23,722 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ erò è un seg<unk> @ no di tempo negli ultimi 25 anni .
2025-05-28 10:01:27,146 - INFO - joeynmt.training - Epoch   8, Step:    49600, Batch Loss:     1.079701, Batch Acc: 0.681906, Tokens per Sec:    18437, Lr: 0.000300
2025-05-28 10:01:30,546 - INFO - joeynmt.training - Epoch   8, Step:    49700, Batch Loss:     1.128416, Batch Acc: 0.677136, Tokens per Sec:    20959, Lr: 0.000300
2025-05-28 10:01:33,911 - INFO - joeynmt.training - Epoch   8, Step:    49800, Batch Loss:     1.107083, Batch Acc: 0.677480, Tokens per Sec:    20700, Lr: 0.000300
2025-05-28 10:01:37,336 - INFO - joeynmt.training - Epoch   8, Step:    49900, Batch Loss:     1.338146, Batch Acc: 0.676267, Tokens per Sec:    20746, Lr: 0.000300
2025-05-28 10:01:40,759 - INFO - joeynmt.training - Epoch   8, Step:    50000, Batch Loss:     1.103270, Batch Acc: 0.669969, Tokens per Sec:    21533, Lr: 0.000300
2025-05-28 10:01:40,759 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:01:40,759 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:01:51,491 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.25, ppl:   3.49, acc:   0.66, generation: 10.7212[sec], evaluation: 0.0000[sec]
2025-05-28 10:01:51,491 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 10:01:52,074 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/47500.ckpt
2025-05-28 10:01:52,102 - INFO - joeynmt.training - Example #0
2025-05-28 10:01:52,102 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:01:52,102 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:01:52,102 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'vedere', 'che', 'i', 'ghi@@', '<unk>', '@', 'acci@@', '<unk>', '@', 'ai', ',', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'che', 'la', 'ver@@', '<unk>', '@', 'sione', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'la', 'risol@@', '<unk>', '@', 'uzione', 'del', '4@@', '<unk>', '@', '8', 'per', 'cento', 'degli', 'stati', 'in@@', '<unk>', '@', 'qu@@', '<unk>', '@', 'anti', 'del', '4@@', '<unk>', '@', '8', '%', '.', '</s>']
2025-05-28 10:01:52,103 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:01:52,103 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:01:52,103 - INFO - joeynmt.training - 	Hypothesis: Ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per vedere che i ghi<unk> @ acci<unk> @ ai , per tre milioni di anni , che la ver<unk> @ sione ar<unk> @ t<unk> @ ica per tre milioni di anni , la risol<unk> @ uzione del 4<unk> @ 8 per cento degli stati in<unk> @ qu<unk> @ anti del 4<unk> @ 8 % .
2025-05-28 10:01:52,103 - INFO - joeynmt.training - Example #1
2025-05-28 10:01:52,103 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:01:52,104 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:01:52,104 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'forte', 'la', 's@@', '<unk>', '@', 'con@@', '<unk>', '@', 'sap@@', '<unk>', '@', 'evol@@', '<unk>', '@', 'i', 'di', 'questo', 'particolare', 'problema', 'speci@@', '<unk>', '@', 'ale', ',', 'non', 'è', 'la', 'di@@', '<unk>', '@', 'mostra', 'l&apos;', 'et@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ch@@', '<unk>', '@', 'etto', '.', '</s>']
2025-05-28 10:01:52,104 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:01:52,104 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:01:52,104 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte la s<unk> @ con<unk> @ sap<unk> @ evol<unk> @ i di questo particolare problema speci<unk> @ ale , non è la di<unk> @ mostra l&apos; et<unk> @ i<unk> @ ch<unk> @ etto .
2025-05-28 10:01:52,104 - INFO - joeynmt.training - Example #2
2025-05-28 10:01:52,104 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:01:52,105 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:01:52,105 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'cer@@', '<unk>', '@', 'ti', 'sen@@', '<unk>', '@', 'si', 'è', 'la', 'c@@', '<unk>', '@', 'assa', 'di', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'il', 'cuore', 'di', 'un', 'sistema', 'di', 'com@@', '<unk>', '@', 'posto', ',', 'il', 'nostro', 'sistema', 'di', 'com@@', '<unk>', '@', 'bu@@', '<unk>', '@', 'sti@@', '<unk>', '@', 'bile', 'globale', '.', '</s>']
2025-05-28 10:01:52,105 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:01:52,105 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:01:52,105 - INFO - joeynmt.training - 	Hypothesis: In cer<unk> @ ti sen<unk> @ si è la c<unk> @ assa di ar<unk> @ t<unk> @ ica il cuore di un sistema di com<unk> @ posto , il nostro sistema di com<unk> @ bu<unk> @ sti<unk> @ bile globale .
2025-05-28 10:01:52,105 - INFO - joeynmt.training - Example #3
2025-05-28 10:01:52,105 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:01:52,105 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:01:52,105 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', '@', 'in@@', '<unk>', '@', 'ta', ',', 'nel', 'vento', 'e', 'si', 's@@', '<unk>', '@', 'v@@', '<unk>', '@', 'egli@@', '<unk>', '@', 'ò', 'nell&apos;', 'est@@', '<unk>', '@', 'in@@', '<unk>', '@', 'ver@@', '<unk>', '@', 'no', '.', '</s>']
2025-05-28 10:01:52,106 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:01:52,106 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:01:52,106 - INFO - joeynmt.training - 	Hypothesis: C<unk> @ in<unk> @ ta , nel vento e si s<unk> @ v<unk> @ egli<unk> @ ò nell&apos; est<unk> @ in<unk> @ ver<unk> @ no .
2025-05-28 10:01:52,106 - INFO - joeynmt.training - Example #4
2025-05-28 10:01:52,106 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:01:52,106 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:01:52,106 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'o', 'è', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'seg@@', '<unk>', '@', 'no', 'di', 'circa', '25', 'anni', '.', '</s>']
2025-05-28 10:01:52,107 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:01:52,107 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:01:52,107 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ o è un seg<unk> @ no di seg<unk> @ no di circa 25 anni .
2025-05-28 10:01:55,536 - INFO - joeynmt.training - Epoch   8, Step:    50100, Batch Loss:     1.144874, Batch Acc: 0.678913, Tokens per Sec:    17505, Lr: 0.000300
2025-05-28 10:01:58,924 - INFO - joeynmt.training - Epoch   8, Step:    50200, Batch Loss:     1.181906, Batch Acc: 0.674992, Tokens per Sec:    20593, Lr: 0.000300
2025-05-28 10:02:02,323 - INFO - joeynmt.training - Epoch   8, Step:    50300, Batch Loss:     1.119732, Batch Acc: 0.676577, Tokens per Sec:    21062, Lr: 0.000300
2025-05-28 10:02:05,732 - INFO - joeynmt.training - Epoch   8, Step:    50400, Batch Loss:     1.191580, Batch Acc: 0.671379, Tokens per Sec:    20877, Lr: 0.000300
2025-05-28 10:02:06,034 - INFO - joeynmt.training - Epoch   8: total training loss 7352.30
2025-05-28 10:02:06,035 - INFO - joeynmt.training - EPOCH 9
2025-05-28 10:02:09,155 - INFO - joeynmt.training - Epoch   9, Step:    50500, Batch Loss:     1.192256, Batch Acc: 0.691710, Tokens per Sec:    21000, Lr: 0.000300
2025-05-28 10:02:09,155 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:02:09,155 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:02:19,196 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.25, ppl:   3.51, acc:   0.66, generation: 10.0289[sec], evaluation: 0.0000[sec]
2025-05-28 10:02:19,603 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/48000.ckpt
2025-05-28 10:02:19,625 - INFO - joeynmt.training - Example #0
2025-05-28 10:02:19,625 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:02:19,625 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:02:19,626 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'mostr@@', '<unk>', '@', 'are', 'che', 'gli', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ano', 'che', 'i', 'ghi@@', '<unk>', '@', 'acci@@', '<unk>', '@', 'ai', 'che', ',', 'che', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'il', 'ver@@', '<unk>', '@', 'de', 'del', '4@@', '<unk>', '@', '8', '%', '.', '</s>']
2025-05-28 10:02:19,626 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:02:19,626 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:02:19,626 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per mostr<unk> @ are che gli E<unk> @ is<unk> @ k<unk> @ i<unk> @ ano che i ghi<unk> @ acci<unk> @ ai che , che per tre milioni di anni , il ver<unk> @ de del 4<unk> @ 8 % .
2025-05-28 10:02:19,626 - INFO - joeynmt.training - Example #1
2025-05-28 10:02:19,627 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:02:19,627 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:02:19,627 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'forte', 'che', 'non', 'è', 'abbastanza', 'la', 'ser@@', '<unk>', '@', 'a', 'di', 'quanto', 'sia', 'stato', ',', 'perché', 'non', 'è', 'un', 'ti@@', '<unk>', '@', 'zio', 'che', ',', 'perché', 'non', 'è', 'l&apos;', 'u@@', '<unk>', '@', 'dito', 'del', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 10:02:19,627 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:02:19,627 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:02:19,627 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte che non è abbastanza la ser<unk> @ a di quanto sia stato , perché non è un ti<unk> @ zio che , perché non è l&apos; u<unk> @ dito del ghi<unk> @ accio .
2025-05-28 10:02:19,627 - INFO - joeynmt.training - Example #2
2025-05-28 10:02:19,628 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:02:19,628 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:02:19,628 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'qualche', 'certo', 'senso', ',', 'è', 'la', 'car@@', '<unk>', '@', 't@@', '<unk>', '@', 'ella', 'di', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ale', 'del', 'nostro', 'sistema', 'globale', '.', '</s>']
2025-05-28 10:02:19,628 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:02:19,628 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:02:19,628 - INFO - joeynmt.training - 	Hypothesis: In qualche certo senso , è la car<unk> @ t<unk> @ ella di ghi<unk> @ accio ar<unk> @ t<unk> @ ic<unk> @ ale del nostro sistema globale .
2025-05-28 10:02:19,628 - INFO - joeynmt.training - Example #3
2025-05-28 10:02:19,629 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:02:19,629 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:02:19,629 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', '@', 'er@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ano', 'in', 'in@@', '<unk>', '@', 'ver@@', '<unk>', '@', 'no', 'e', 's@@', '<unk>', '@', 'con@@', '<unk>', '@', 'f@@', '<unk>', '@', 'ia', 'nel', 'vento', '.', '</s>']
2025-05-28 10:02:19,629 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:02:19,629 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:02:19,629 - INFO - joeynmt.training - 	Hypothesis: C<unk> @ er<unk> @ c<unk> @ ano in in<unk> @ ver<unk> @ no e s<unk> @ con<unk> @ f<unk> @ ia nel vento .
2025-05-28 10:02:19,629 - INFO - joeynmt.training - Example #4
2025-05-28 10:02:19,630 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:02:19,630 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:02:19,630 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'seg@@', '<unk>', '@', 'no', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 10:02:19,630 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:02:19,630 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:02:19,630 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ erò è un seg<unk> @ no di seg<unk> @ no negli ultimi 25 anni .
2025-05-28 10:02:23,040 - INFO - joeynmt.training - Epoch   9, Step:    50600, Batch Loss:     1.146902, Batch Acc: 0.692008, Tokens per Sec:    18812, Lr: 0.000300
2025-05-28 10:02:26,429 - INFO - joeynmt.training - Epoch   9, Step:    50700, Batch Loss:     1.385632, Batch Acc: 0.690587, Tokens per Sec:    21219, Lr: 0.000300
2025-05-28 10:02:29,810 - INFO - joeynmt.training - Epoch   9, Step:    50800, Batch Loss:     1.109120, Batch Acc: 0.691228, Tokens per Sec:    21148, Lr: 0.000300
2025-05-28 10:02:33,194 - INFO - joeynmt.training - Epoch   9, Step:    50900, Batch Loss:     1.133804, Batch Acc: 0.685155, Tokens per Sec:    21589, Lr: 0.000300
2025-05-28 10:02:36,584 - INFO - joeynmt.training - Epoch   9, Step:    51000, Batch Loss:     1.294308, Batch Acc: 0.690826, Tokens per Sec:    21547, Lr: 0.000300
2025-05-28 10:02:36,584 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:02:36,585 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:02:48,362 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.25, ppl:   3.51, acc:   0.66, generation: 11.7671[sec], evaluation: 0.0000[sec]
2025-05-28 10:02:48,766 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/48500.ckpt
2025-05-28 10:02:48,793 - INFO - joeynmt.training - Example #0
2025-05-28 10:02:48,794 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:02:48,794 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:02:48,794 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'mostr@@', '<unk>', '@', 'are', 'che', 'i', 'ghi@@', '<unk>', '@', 'acci@@', '<unk>', '@', 'ai', ',', 'che', 'i', 't@@', '<unk>', '@', 'an@@', '<unk>', '@', 'ci@@', '<unk>', '@', 'oli', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ini', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ati', 'per', 'tre', 'milioni', 'di', 'anni', '.', '</s>']
2025-05-28 10:02:48,794 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:02:48,794 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:02:48,795 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per mostr<unk> @ are che i ghi<unk> @ acci<unk> @ ai , che i t<unk> @ an<unk> @ ci<unk> @ oli ar<unk> @ c<unk> @ ini ar<unk> @ t<unk> @ ic<unk> @ ati per tre milioni di anni .
2025-05-28 10:02:48,795 - INFO - joeynmt.training - Example #1
2025-05-28 10:02:48,795 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:02:48,795 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:02:48,795 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'per', 'il', 'tr@@', '<unk>', '@', 'av@@', '<unk>', '@', 'ol@@', '<unk>', '@', 'ast@@', '<unk>', '@', 'ico', 'di', 'questo', 'particolare', 'problema', ',', 'non', 'è', 'che', 'mostra', 'il', 'bu@@', '<unk>', '@', 'io', ',', 'perché', 'non', 'è', 'il', 'bu@@', '<unk>', '@', 'one', 'del', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 10:02:48,796 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:02:48,796 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:02:48,796 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza per il tr<unk> @ av<unk> @ ol<unk> @ ast<unk> @ ico di questo particolare problema , non è che mostra il bu<unk> @ io , perché non è il bu<unk> @ one del ghi<unk> @ accio .
2025-05-28 10:02:48,796 - INFO - joeynmt.training - Example #2
2025-05-28 10:02:48,796 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:02:48,796 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:02:48,796 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'qualche', 'certo', 'senso', ',', 'è', 'la', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'va', 'è', 'la', 'car@@', '<unk>', '@', 't@@', '<unk>', '@', 'ella', 'del', 'nostro', 'sistema', 'di', 'com@@', '<unk>', '@', 'bu@@', '<unk>', '@', 'sti@@', '<unk>', '@', 'bile', '.', '</s>']
2025-05-28 10:02:48,797 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:02:48,797 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:02:48,797 - INFO - joeynmt.training - 	Hypothesis: In qualche certo senso , è la c<unk> @ atti<unk> @ va è la car<unk> @ t<unk> @ ella del nostro sistema di com<unk> @ bu<unk> @ sti<unk> @ bile .
2025-05-28 10:02:48,797 - INFO - joeynmt.training - Example #3
2025-05-28 10:02:48,797 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:02:48,797 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:02:48,797 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'a', 'cres@@', '<unk>', '@', 'ce', 'e', 's@@', '<unk>', '@', 'com@@', '<unk>', '@', 'in@@', '<unk>', '@', 'ciò', 'in', 'est@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 10:02:48,797 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:02:48,797 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:02:48,798 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ a cres<unk> @ ce e s<unk> @ com<unk> @ in<unk> @ ciò in est<unk> @ ate .
2025-05-28 10:02:48,798 - INFO - joeynmt.training - Example #4
2025-05-28 10:02:48,798 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:02:48,798 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:02:48,798 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'a', 'è', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'un', 'indi@@', '<unk>', '@', 'zio', 'che', 'è', 'acca@@', '<unk>', '@', 'duto', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 10:02:48,798 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:02:48,798 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:02:48,798 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ a è un seg<unk> @ no di un indi<unk> @ zio che è acca<unk> @ duto negli ultimi 25 anni .
2025-05-28 10:02:52,197 - INFO - joeynmt.training - Epoch   9, Step:    51100, Batch Loss:     1.147245, Batch Acc: 0.690919, Tokens per Sec:    19393, Lr: 0.000300
2025-05-28 10:02:55,589 - INFO - joeynmt.training - Epoch   9, Step:    51200, Batch Loss:     1.005645, Batch Acc: 0.686741, Tokens per Sec:    21732, Lr: 0.000300
2025-05-28 10:02:58,944 - INFO - joeynmt.training - Epoch   9, Step:    51300, Batch Loss:     1.087818, Batch Acc: 0.685600, Tokens per Sec:    20864, Lr: 0.000300
2025-05-28 10:03:02,295 - INFO - joeynmt.training - Epoch   9, Step:    51400, Batch Loss:     1.034691, Batch Acc: 0.690528, Tokens per Sec:    20800, Lr: 0.000300
2025-05-28 10:03:05,673 - INFO - joeynmt.training - Epoch   9, Step:    51500, Batch Loss:     1.044195, Batch Acc: 0.688891, Tokens per Sec:    21012, Lr: 0.000300
2025-05-28 10:03:05,673 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:03:05,673 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:03:17,499 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.26, ppl:   3.51, acc:   0.66, generation: 11.8131[sec], evaluation: 0.0000[sec]
2025-05-28 10:03:17,512 - INFO - joeynmt.training - Example #0
2025-05-28 10:03:17,513 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:03:17,513 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:03:17,513 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'per', 'ri@@', '<unk>', '@', 'durre', 'che', 'gli', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'app@@', '<unk>', '@', 'a', 'per', 'cui', 'i', 't@@', '<unk>', '@', 'oc@@', '<unk>', '@', 'cor@@', '<unk>', '@', 're', 'le', 'mat@@', '<unk>', '@', 'erie', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'di', 'tre', 'anni', ',', 'per', 'cento', '.', '</s>']
2025-05-28 10:03:17,514 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:03:17,514 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:03:17,514 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ va per ri<unk> @ durre che gli E<unk> @ is<unk> @ k<unk> @ app<unk> @ a per cui i t<unk> @ oc<unk> @ cor<unk> @ re le mat<unk> @ erie ar<unk> @ t<unk> @ ica di tre anni , per cento .
2025-05-28 10:03:17,514 - INFO - joeynmt.training - Example #1
2025-05-28 10:03:17,514 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:03:17,514 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:03:17,514 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'forte', 'la', 'ser@@', '<unk>', '@', 'ità', 'di', 'er@@', '<unk>', '@', 'edi@@', '<unk>', '@', 'tà', 'di', 'questo', 'particolare', 'problema', ',', 'non', 'lo', 'è', 'il', 'soff@@', '<unk>', '@', 'itto', ',', 'perché', 'non', 'lo', 'mostra', 'la', 'soff@@', '<unk>', '@', 'er@@', '<unk>', '@', 'enza', 'del', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 10:03:17,515 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:03:17,515 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:03:17,515 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte la ser<unk> @ ità di er<unk> @ edi<unk> @ tà di questo particolare problema , non lo è il soff<unk> @ itto , perché non lo mostra la soff<unk> @ er<unk> @ enza del ghi<unk> @ accio .
2025-05-28 10:03:17,515 - INFO - joeynmt.training - Example #2
2025-05-28 10:03:17,515 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:03:17,515 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:03:17,515 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'di', 'un', 'mi@@', '<unk>', '@', 'st@@', '<unk>', '@', 'ero', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'di', 'ghi@@', '<unk>', '@', 'accio', ',', 'il', 'cuore', 'di', 'un', 'sistema', 'peggi@@', '<unk>', '@', 'ore', 'globale', '.', '</s>']
2025-05-28 10:03:17,516 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:03:17,516 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:03:17,516 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , il cuore di un mi<unk> @ st<unk> @ ero ar<unk> @ t<unk> @ ico di ghi<unk> @ accio , il cuore di un sistema peggi<unk> @ ore globale .
2025-05-28 10:03:17,516 - INFO - joeynmt.training - Example #3
2025-05-28 10:03:17,516 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:03:17,516 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:03:17,516 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'a', 'cres@@', '<unk>', '@', 'ce', 'in', 'est@@', '<unk>', '@', 'ate', 'e', 's@@', '<unk>', '@', 'com@@', '<unk>', '@', 'par@@', '<unk>', '@', 'sa', 'in', 'est@@', '<unk>', '@', 'ate', 'e', 's@@', '<unk>', '@', 'com@@', '<unk>', '@', 'par@@', '<unk>', '@', 'sa', '.', '</s>']
2025-05-28 10:03:17,517 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:03:17,517 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:03:17,517 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ a cres<unk> @ ce in est<unk> @ ate e s<unk> @ com<unk> @ par<unk> @ sa in est<unk> @ ate e s<unk> @ com<unk> @ par<unk> @ sa .
2025-05-28 10:03:17,517 - INFO - joeynmt.training - Example #4
2025-05-28 10:03:17,517 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:03:17,517 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:03:17,517 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'tempo', '.', '</s>']
2025-05-28 10:03:17,518 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:03:17,518 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:03:17,518 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ erò è un seg<unk> @ no di tempo .
2025-05-28 10:03:20,908 - INFO - joeynmt.training - Epoch   9, Step:    51600, Batch Loss:     1.106941, Batch Acc: 0.684523, Tokens per Sec:    20659, Lr: 0.000300
2025-05-28 10:03:24,297 - INFO - joeynmt.training - Epoch   9, Step:    51700, Batch Loss:     1.056779, Batch Acc: 0.678910, Tokens per Sec:    21398, Lr: 0.000300
2025-05-28 10:03:27,689 - INFO - joeynmt.training - Epoch   9, Step:    51800, Batch Loss:     1.018097, Batch Acc: 0.687177, Tokens per Sec:    20664, Lr: 0.000300
2025-05-28 10:03:31,094 - INFO - joeynmt.training - Epoch   9, Step:    51900, Batch Loss:     1.282684, Batch Acc: 0.687171, Tokens per Sec:    21357, Lr: 0.000300
2025-05-28 10:03:34,477 - INFO - joeynmt.training - Epoch   9, Step:    52000, Batch Loss:     1.097044, Batch Acc: 0.686445, Tokens per Sec:    20974, Lr: 0.000300
2025-05-28 10:03:34,477 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:03:34,477 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:03:45,395 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.26, ppl:   3.52, acc:   0.66, generation: 10.9069[sec], evaluation: 0.0000[sec]
2025-05-28 10:03:45,408 - INFO - joeynmt.training - Example #0
2025-05-28 10:03:45,408 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:03:45,408 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:03:45,409 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'capire', 'che', 'la', 'bar@@', '<unk>', '@', 'att@@', '<unk>', '@', 'u@@', '<unk>', '@', 'almente', 'l&apos;', 'et@@', '<unk>', '@', 'izz@@', '<unk>', '@', 'azione', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', ',', 'che', 'è', 'il', 't@@', '<unk>', '@', 'ale', 'di', 'circa', 'il', '4@@', '<unk>', '@', '8', 'per', 'cento', '.', '</s>']
2025-05-28 10:03:45,409 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:03:45,409 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:03:45,409 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per capire che la bar<unk> @ att<unk> @ u<unk> @ almente l&apos; et<unk> @ izz<unk> @ azione ar<unk> @ t<unk> @ ica , che è il t<unk> @ ale di circa il 4<unk> @ 8 per cento .
2025-05-28 10:03:45,409 - INFO - joeynmt.training - Example #1
2025-05-28 10:03:45,409 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:03:45,410 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:03:45,410 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'così', 'che', 'questo', 'è', 'un', 'po', '&apos;', 'di', 'fatto', 'che', 'non', 'è', 'un', 'problema', 'di', 'quanto', 'non', 'sia', 'la', 'di@@', '<unk>', '@', 'mostra', 'questo', 'problema', 'speci@@', '<unk>', '@', 'ale', ',', 'perché', 'non', 'è', 'il', 'vi@@', '<unk>', '@', 'vo', 'del', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 10:03:45,410 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:03:45,410 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:03:45,410 - INFO - joeynmt.training - 	Hypothesis: Ma non è così che questo è un po &apos; di fatto che non è un problema di quanto non sia la di<unk> @ mostra questo problema speci<unk> @ ale , perché non è il vi<unk> @ vo del ghi<unk> @ accio .
2025-05-28 10:03:45,410 - INFO - joeynmt.training - Example #2
2025-05-28 10:03:45,411 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:03:45,411 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:03:45,411 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'è', 'il', 'cuore', 'di', 'un', 'sistema', 'di', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ano', 'il', 'cuore', 'del', 'nostro', 'sistema', 'peggi@@', '<unk>', '@', 'o', '.', '</s>']
2025-05-28 10:03:45,411 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:03:45,411 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:03:45,411 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , è il cuore di un sistema di ghi<unk> @ accio ar<unk> @ t<unk> @ ic<unk> @ ano il cuore del nostro sistema peggi<unk> @ o .
2025-05-28 10:03:45,411 - INFO - joeynmt.training - Example #3
2025-05-28 10:03:45,412 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:03:45,412 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:03:45,412 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Vi@@', '<unk>', '@', 'ene', 'cres@@', '<unk>', '@', 'ce', 'in', 'est@@', '<unk>', '@', 'ate', 'e', 's@@', '<unk>', '@', 'con@@', '<unk>', '@', 'vol@@', '<unk>', '@', 'e', '.', '</s>']
2025-05-28 10:03:45,412 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:03:45,412 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:03:45,412 - INFO - joeynmt.training - 	Hypothesis: Vi<unk> @ ene cres<unk> @ ce in est<unk> @ ate e s<unk> @ con<unk> @ vol<unk> @ e .
2025-05-28 10:03:45,412 - INFO - joeynmt.training - Example #4
2025-05-28 10:03:45,413 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:03:45,413 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:03:45,413 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'i', 'è', 'una', 'ri@@', '<unk>', '@', 'vel@@', '<unk>', '@', 'azione', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 10:03:45,413 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:03:45,413 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:03:45,413 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ i è una ri<unk> @ vel<unk> @ azione che è successo negli ultimi 25 anni .
2025-05-28 10:03:48,828 - INFO - joeynmt.training - Epoch   9, Step:    52100, Batch Loss:     1.112927, Batch Acc: 0.689275, Tokens per Sec:    20963, Lr: 0.000300
2025-05-28 10:03:52,214 - INFO - joeynmt.training - Epoch   9, Step:    52200, Batch Loss:     1.050096, Batch Acc: 0.684189, Tokens per Sec:    20521, Lr: 0.000300
2025-05-28 10:03:55,599 - INFO - joeynmt.training - Epoch   9, Step:    52300, Batch Loss:     1.176336, Batch Acc: 0.687833, Tokens per Sec:    20827, Lr: 0.000300
2025-05-28 10:03:59,000 - INFO - joeynmt.training - Epoch   9, Step:    52400, Batch Loss:     1.112512, Batch Acc: 0.684712, Tokens per Sec:    20788, Lr: 0.000300
2025-05-28 10:04:02,381 - INFO - joeynmt.training - Epoch   9, Step:    52500, Batch Loss:     1.181479, Batch Acc: 0.681470, Tokens per Sec:    20715, Lr: 0.000300
2025-05-28 10:04:02,381 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:04:02,382 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:04:15,204 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.25, ppl:   3.50, acc:   0.66, generation: 12.8116[sec], evaluation: 0.0000[sec]
2025-05-28 10:04:15,602 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/50500.ckpt
2025-05-28 10:04:15,629 - INFO - joeynmt.training - Example #0
2025-05-28 10:04:15,630 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:04:15,630 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:04:15,630 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'vedere', 'che', 'i', 'dirit@@', '<unk>', '@', 'ti', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ici', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ici', 'che', 'gli', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ini', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ici', 'che', 'erano', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'cento', 'del', '40', '%', '.', '</s>']
2025-05-28 10:04:15,630 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:04:15,630 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:04:15,630 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste di<unk> @ apos<unk> @ iti<unk> @ ve per vedere che i dirit<unk> @ ti ar<unk> @ t<unk> @ ici ar<unk> @ t<unk> @ ici che gli E<unk> @ is<unk> @ c<unk> @ ini ar<unk> @ t<unk> @ ici che erano tre milioni di anni , per cento del 40 % .
2025-05-28 10:04:15,630 - INFO - joeynmt.training - Example #1
2025-05-28 10:04:15,631 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:04:15,631 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:04:15,631 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'un', 'modo', 'di', 'essere', 'abbastanza', 'forte', 'da', 'essere', 'la', 'nostra', 'esperienza', ',', 'perché', 'non', 'è', 'l&apos;', 'et@@', '<unk>', '@', 'i@@', '<unk>', '@', 'pot@@', '<unk>', '@', 'enza', 'di', 'questo', 'problema', ',', 'perché', 'non', 'lo', 'mostra', 'dell&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 10:04:15,631 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:04:15,631 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:04:15,631 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è un modo di essere abbastanza forte da essere la nostra esperienza , perché non è l&apos; et<unk> @ i<unk> @ pot<unk> @ enza di questo problema , perché non lo mostra dell&apos; E<unk> @ is<unk> @ es .
2025-05-28 10:04:15,631 - INFO - joeynmt.training - Example #2
2025-05-28 10:04:15,632 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:04:15,632 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:04:15,632 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'b@@', '<unk>', '@', 'otti@@', '<unk>', '@', 'gli@@', '<unk>', '@', 'a', 'è', 'la', 'b@@', '<unk>', '@', 'oc@@', '<unk>', '@', 'ca', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'va', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'globale', '.', '</s>']
2025-05-28 10:04:15,632 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:04:15,632 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:04:15,632 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la b<unk> @ otti<unk> @ gli<unk> @ a è la b<unk> @ oc<unk> @ ca ar<unk> @ c<unk> @ atti<unk> @ va , il cuore del nostro sistema globale .
2025-05-28 10:04:15,632 - INFO - joeynmt.training - Example #3
2025-05-28 10:04:15,633 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:04:15,633 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:04:15,633 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'a', 'cres@@', '<unk>', '@', 'ce', 'e', 'si', 's@@', '<unk>', '@', 'v@@', '<unk>', '@', 'egli@@', '<unk>', '@', 'ano', 'nell&apos;', 'est@@', '<unk>', '@', 'ate', 'e', 's@@', '<unk>', '@', 'om@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 10:04:15,633 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:04:15,633 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:04:15,633 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ a cres<unk> @ ce e si s<unk> @ v<unk> @ egli<unk> @ ano nell&apos; est<unk> @ ate e s<unk> @ om<unk> @ ate .
2025-05-28 10:04:15,633 - INFO - joeynmt.training - Example #4
2025-05-28 10:04:15,634 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:04:15,634 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:04:15,634 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'o', 'è', 'un', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'no', 'di', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'na', 'che', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 10:04:15,634 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:04:15,634 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:04:15,634 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ o è un di<unk> @ seg<unk> @ no di di<unk> @ seg<unk> @ na che negli ultimi 25 anni .
2025-05-28 10:04:19,038 - INFO - joeynmt.training - Epoch   9, Step:    52600, Batch Loss:     1.001094, Batch Acc: 0.687692, Tokens per Sec:    18788, Lr: 0.000300
2025-05-28 10:04:22,464 - INFO - joeynmt.training - Epoch   9, Step:    52700, Batch Loss:     1.092275, Batch Acc: 0.686006, Tokens per Sec:    21569, Lr: 0.000300
2025-05-28 10:04:25,869 - INFO - joeynmt.training - Epoch   9, Step:    52800, Batch Loss:     1.173215, Batch Acc: 0.682075, Tokens per Sec:    21258, Lr: 0.000300
2025-05-28 10:04:29,267 - INFO - joeynmt.training - Epoch   9, Step:    52900, Batch Loss:     1.051029, Batch Acc: 0.687604, Tokens per Sec:    21261, Lr: 0.000300
2025-05-28 10:04:32,646 - INFO - joeynmt.training - Epoch   9, Step:    53000, Batch Loss:     1.273005, Batch Acc: 0.685304, Tokens per Sec:    21140, Lr: 0.000300
2025-05-28 10:04:32,646 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:04:32,646 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:04:44,734 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.25, ppl:   3.50, acc:   0.66, generation: 12.0775[sec], evaluation: 0.0000[sec]
2025-05-28 10:04:45,112 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/51000.ckpt
2025-05-28 10:04:45,137 - INFO - joeynmt.training - Example #0
2025-05-28 10:04:45,137 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:04:45,137 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:04:45,138 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'vedere', 'che', 'i', 'ric@@', '<unk>', '@', 'o@@', '<unk>', '@', 'stru@@', '<unk>', '@', 'g@@', '<unk>', '@', 'gere', 'la', 'gr@@', '<unk>', '@', 'ave', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ica', 'che', 'aveva', 'per', 'tre', 'anni', ',', 'il', '40', 'per', 'cento', 'dei', 'paesi', '.', '</s>']
2025-05-28 10:04:45,138 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:04:45,138 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:04:45,138 - INFO - joeynmt.training - 	Hypothesis: Ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per vedere che i ric<unk> @ o<unk> @ stru<unk> @ g<unk> @ gere la gr<unk> @ ave ar<unk> @ c<unk> @ ica che aveva per tre anni , il 40 per cento dei paesi .
2025-05-28 10:04:45,138 - INFO - joeynmt.training - Example #1
2025-05-28 10:04:45,139 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:04:45,139 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:04:45,139 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'forte', 'che', 'sia', 'la', 'sost@@', '<unk>', '@', 'anza', 'di', 'questo', 'speci@@', '<unk>', '@', 'fico', ',', 'perché', 'non', 'ci', 'mostra', 'il', 'vi@@', '<unk>', '@', 'll@@', '<unk>', '@', 'aggio', 'di', 'questo', 'speci@@', '<unk>', '@', 'fico', ',', 'perché', 'non', 'ci', 'mostra', 'il', 'soff@@', '<unk>', '@', 'itto', 'di', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 10:04:45,139 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:04:45,139 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:04:45,139 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte che sia la sost<unk> @ anza di questo speci<unk> @ fico , perché non ci mostra il vi<unk> @ ll<unk> @ aggio di questo speci<unk> @ fico , perché non ci mostra il soff<unk> @ itto di ghi<unk> @ accio .
2025-05-28 10:04:45,139 - INFO - joeynmt.training - Example #2
2025-05-28 10:04:45,140 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:04:45,140 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:04:45,140 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'il', 't@@', '<unk>', '@', 'es@@', '<unk>', '@', 'per@@', '<unk>', '@', 'o', 'è', 'il', 'ben@@', '<unk>', '@', 'issimo', 'cuore', 'ar@@', '<unk>', '@', 'ico', 'che', 'il', 'cuore', 'di', 'un', 'sistema', 'di', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'vo', 'globale', '.', '</s>']
2025-05-28 10:04:45,140 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:04:45,140 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:04:45,140 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , il t<unk> @ es<unk> @ per<unk> @ o è il ben<unk> @ issimo cuore ar<unk> @ ico che il cuore di un sistema di c<unk> @ atti<unk> @ vo globale .
2025-05-28 10:04:45,141 - INFO - joeynmt.training - Example #3
2025-05-28 10:04:45,141 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:04:45,141 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:04:45,141 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'e', 'cres@@', '<unk>', '@', 'ce', 'nel', 'vento', 'e', 'si', 's@@', '<unk>', '@', 'ale', 'in', 'est@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 10:04:45,141 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:04:45,141 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:04:45,141 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ e cres<unk> @ ce nel vento e si s<unk> @ ale in est<unk> @ ate .
2025-05-28 10:04:45,141 - INFO - joeynmt.training - Example #4
2025-05-28 10:04:45,142 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:04:45,142 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:04:45,142 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'ano', 'è', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'più', 'di', '25', 'anni', '.', '</s>']
2025-05-28 10:04:45,142 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:04:45,142 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:04:45,142 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ ano è un seg<unk> @ no di più di 25 anni .
2025-05-28 10:04:48,560 - INFO - joeynmt.training - Epoch   9, Step:    53100, Batch Loss:     1.192115, Batch Acc: 0.681905, Tokens per Sec:    19022, Lr: 0.000300
2025-05-28 10:04:51,959 - INFO - joeynmt.training - Epoch   9, Step:    53200, Batch Loss:     1.031690, Batch Acc: 0.685461, Tokens per Sec:    20603, Lr: 0.000300
2025-05-28 10:04:55,348 - INFO - joeynmt.training - Epoch   9, Step:    53300, Batch Loss:     1.118863, Batch Acc: 0.681098, Tokens per Sec:    20791, Lr: 0.000300
2025-05-28 10:04:58,790 - INFO - joeynmt.training - Epoch   9, Step:    53400, Batch Loss:     1.147001, Batch Acc: 0.685607, Tokens per Sec:    20884, Lr: 0.000300
2025-05-28 10:05:02,180 - INFO - joeynmt.training - Epoch   9, Step:    53500, Batch Loss:     1.023154, Batch Acc: 0.687160, Tokens per Sec:    20854, Lr: 0.000300
2025-05-28 10:05:02,181 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:05:02,181 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:05:13,288 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.25, ppl:   3.50, acc:   0.66, generation: 11.0963[sec], evaluation: 0.0000[sec]
2025-05-28 10:05:13,848 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/49500.ckpt
2025-05-28 10:05:13,876 - INFO - joeynmt.training - Example #0
2025-05-28 10:05:13,877 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:05:13,877 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:05:13,877 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'guardare', 'il', 'fatto', 'che', 'i', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ale', 'che', 'il', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'la', 'ver@@', '<unk>', '@', 'sione', 'del', '40', '%', 'dei', 'paesi', 'che', 'aveva', 'fatto', 'per', 'cui', 'il', '40', '%', 'delle', 'dimen@@', '<unk>', '@', 'sioni', 'di', 'questi', 'qu@@', '<unk>', '@', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ini', 'di', '40', '%', '.', '</s>']
2025-05-28 10:05:13,878 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:05:13,878 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:05:13,878 - INFO - joeynmt.training - 	Hypothesis: Ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per guardare il fatto che i ghi<unk> @ accio ar<unk> @ c<unk> @ ic<unk> @ ale che il ghi<unk> @ accio ar<unk> @ t<unk> @ ico per tre milioni di anni , la ver<unk> @ sione del 40 % dei paesi che aveva fatto per cui il 40 % delle dimen<unk> @ sioni di questi qu<unk> @ ar<unk> @ c<unk> @ ini di 40 % .
2025-05-28 10:05:13,878 - INFO - joeynmt.training - Example #1
2025-05-28 10:05:13,878 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:05:13,878 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:05:13,879 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'da', 'da', 'un', 'punto', 'di', 'vista', 'che', 'non', 'è', 'abbastanza', 'per', 'la', 'cre@@', '<unk>', '@', 'azione', ',', 'perché', 'non', 'lo', 'mostra', 'il', 'vi@@', '<unk>', '@', 'll@@', '<unk>', '@', 'aggio', 'che', 'mostra', 'il', 'bu@@', '<unk>', '@', 'io', 'del', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 10:05:13,879 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:05:13,879 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:05:13,879 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza da da un punto di vista che non è abbastanza per la cre<unk> @ azione , perché non lo mostra il vi<unk> @ ll<unk> @ aggio che mostra il bu<unk> @ io del ghi<unk> @ accio .
2025-05-28 10:05:13,879 - INFO - joeynmt.training - Example #2
2025-05-28 10:05:13,880 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:05:13,880 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:05:13,880 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'è', 'il', 'cuore', 'di', 'un', 'sistema', 'di', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ale', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'globale', '.', '</s>']
2025-05-28 10:05:13,880 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:05:13,880 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:05:13,880 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , è il cuore di un sistema di ghi<unk> @ accio ar<unk> @ t<unk> @ ic<unk> @ ale , il cuore del nostro sistema globale .
2025-05-28 10:05:13,880 - INFO - joeynmt.training - Example #3
2025-05-28 10:05:13,880 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:05:13,880 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:05:13,881 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', '@', 'in@@', '<unk>', '@', 'ta', 'nel', 'vento', 'e', 'si', 's@@', '<unk>', '@', 'g@@', '<unk>', '@', 'ur@@', '<unk>', '@', 'ra', 'nel', 'vento', 'e', 'lo', 's@@', '<unk>', '@', 'om@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 10:05:13,881 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:05:13,881 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:05:13,881 - INFO - joeynmt.training - 	Hypothesis: C<unk> @ in<unk> @ ta nel vento e si s<unk> @ g<unk> @ ur<unk> @ ra nel vento e lo s<unk> @ om<unk> @ ate .
2025-05-28 10:05:13,881 - INFO - joeynmt.training - Example #4
2025-05-28 10:05:13,881 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:05:13,881 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:05:13,881 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'consegu@@', '<unk>', '@', 'enza', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'tempo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 10:05:13,882 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:05:13,882 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:05:13,882 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma consegu<unk> @ enza che vi mostr<unk> @ erò è un seg<unk> @ no di tempo negli ultimi 25 anni .
2025-05-28 10:05:17,304 - INFO - joeynmt.training - Epoch   9, Step:    53600, Batch Loss:     1.092879, Batch Acc: 0.682847, Tokens per Sec:    17704, Lr: 0.000300
2025-05-28 10:05:20,708 - INFO - joeynmt.training - Epoch   9, Step:    53700, Batch Loss:     1.080028, Batch Acc: 0.682487, Tokens per Sec:    21519, Lr: 0.000300
2025-05-28 10:05:24,088 - INFO - joeynmt.training - Epoch   9, Step:    53800, Batch Loss:     1.383611, Batch Acc: 0.684674, Tokens per Sec:    21168, Lr: 0.000300
2025-05-28 10:05:27,468 - INFO - joeynmt.training - Epoch   9, Step:    53900, Batch Loss:     1.174655, Batch Acc: 0.685012, Tokens per Sec:    20869, Lr: 0.000300
2025-05-28 10:05:30,881 - INFO - joeynmt.training - Epoch   9, Step:    54000, Batch Loss:     1.114848, Batch Acc: 0.683761, Tokens per Sec:    21915, Lr: 0.000300
2025-05-28 10:05:30,881 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:05:30,881 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:05:43,185 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.25, ppl:   3.48, acc:   0.66, generation: 12.2930[sec], evaluation: 0.0000[sec]
2025-05-28 10:05:43,186 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 10:05:43,785 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/52500.ckpt
2025-05-28 10:05:43,814 - INFO - joeynmt.training - Example #0
2025-05-28 10:05:43,814 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:05:43,815 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:05:43,815 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'vedere', 'che', 'i', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ano', 'che', 'i', 'gi@@', '<unk>', '@', 'g@@', '<unk>', '@', 'ano', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'il', '40', 'per', 'cento', 'dei', '40', 'stati', 's@@', '<unk>', '@', 'ed@@', '<unk>', '@', 'uti', 'in', 'cui', 'si', 'è', 'ri@@', '<unk>', '@', 'ma@@', '<unk>', '@', 'sto', 'per', 'tre', 'anni', ',', '40', 'per', 'cento', 'di', '40', 'stati', 's@@', '<unk>', '@', 'com@@', '<unk>', '@', 'par@@', '<unk>', '@', 'si', '.', '</s>']
2025-05-28 10:05:43,815 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:05:43,815 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:05:43,815 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per vedere che i ghi<unk> @ accio ar<unk> @ t<unk> @ ic<unk> @ ano che i gi<unk> @ g<unk> @ ano per tre milioni di anni , il 40 per cento dei 40 stati s<unk> @ ed<unk> @ uti in cui si è ri<unk> @ ma<unk> @ sto per tre anni , 40 per cento di 40 stati s<unk> @ com<unk> @ par<unk> @ si .
2025-05-28 10:05:43,815 - INFO - joeynmt.training - Example #1
2025-05-28 10:05:43,816 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:05:43,816 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:05:43,816 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'forte', 'la', 'sost@@', '<unk>', '@', 'anza', 'di', 'questo', 'particolare', 'problema', ',', 'che', 'non', 'è', 'abbastanza', 'per', 'la', 'sost@@', '<unk>', '@', 'anza', 'di', 'questo', 'particolare', 'problema', ',', 'non', 'è', 'il', 'vi@@', '<unk>', '@', 'et@@', '<unk>', '@', 'o', '.', '</s>']
2025-05-28 10:05:43,816 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:05:43,816 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:05:43,816 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza forte la sost<unk> @ anza di questo particolare problema , che non è abbastanza per la sost<unk> @ anza di questo particolare problema , non è il vi<unk> @ et<unk> @ o .
2025-05-28 10:05:43,817 - INFO - joeynmt.training - Example #2
2025-05-28 10:05:43,817 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:05:43,817 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:05:43,817 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'c@@', '<unk>', '@', 'att@@', '<unk>', '@', 'ura', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'è', 'la', 'causa', 'dell&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'c@@', '<unk>', '@', 'enza', 'del', 'nostro', 'sistema', 'globale', '.', '</s>']
2025-05-28 10:05:43,817 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:05:43,817 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:05:43,818 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la c<unk> @ att<unk> @ ura ar<unk> @ t<unk> @ ica è la causa dell&apos; E<unk> @ is<unk> @ c<unk> @ enza del nostro sistema globale .
2025-05-28 10:05:43,818 - INFO - joeynmt.training - Example #3
2025-05-28 10:05:43,818 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:05:43,818 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:05:43,818 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Si', 'è', 'in@@', '<unk>', '@', 'ver@@', '<unk>', '@', 'no', 'in', 'in@@', '<unk>', '@', 'ver@@', '<unk>', '@', 'no', 'e', 's@@', '<unk>', '@', 'vol@@', '<unk>', '@', 'e', '.', '</s>']
2025-05-28 10:05:43,818 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:05:43,818 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:05:43,819 - INFO - joeynmt.training - 	Hypothesis: Si è in<unk> @ ver<unk> @ no in in<unk> @ ver<unk> @ no e s<unk> @ vol<unk> @ e .
2025-05-28 10:05:43,819 - INFO - joeynmt.training - Example #4
2025-05-28 10:05:43,819 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:05:43,819 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:05:43,819 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'un', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'no', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 10:05:43,819 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:05:43,819 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:05:43,820 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ erò è un di<unk> @ seg<unk> @ no negli ultimi 25 anni .
2025-05-28 10:05:47,215 - INFO - joeynmt.training - Epoch   9, Step:    54100, Batch Loss:     1.184134, Batch Acc: 0.683546, Tokens per Sec:    17440, Lr: 0.000300
2025-05-28 10:05:50,594 - INFO - joeynmt.training - Epoch   9, Step:    54200, Batch Loss:     1.157977, Batch Acc: 0.686496, Tokens per Sec:    20538, Lr: 0.000300
2025-05-28 10:05:53,994 - INFO - joeynmt.training - Epoch   9, Step:    54300, Batch Loss:     1.068898, Batch Acc: 0.680322, Tokens per Sec:    21060, Lr: 0.000300
2025-05-28 10:05:57,375 - INFO - joeynmt.training - Epoch   9, Step:    54400, Batch Loss:     1.071587, Batch Acc: 0.681097, Tokens per Sec:    20745, Lr: 0.000300
2025-05-28 10:06:00,774 - INFO - joeynmt.training - Epoch   9, Step:    54500, Batch Loss:     1.364857, Batch Acc: 0.684294, Tokens per Sec:    21032, Lr: 0.000300
2025-05-28 10:06:00,774 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:06:00,774 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:06:12,093 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.25, ppl:   3.49, acc:   0.66, generation: 11.3079[sec], evaluation: 0.0000[sec]
2025-05-28 10:06:12,497 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/53500.ckpt
2025-05-28 10:06:12,524 - INFO - joeynmt.training - Example #0
2025-05-28 10:06:12,525 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:06:12,525 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:06:12,525 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'vedere', 'che', 'i', 'ghi@@', '<unk>', '@', 'acci@@', '<unk>', '@', 'atori', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ici', ',', 'che', 'i', 'ghi@@', '<unk>', '@', 'acci@@', '<unk>', '@', 'ano', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'il', 'mondo', ',', 'che', 'ha', 'avuto', 'il', '40', '%', 'della', 'gr@@', '<unk>', '@', 'av@@', '<unk>', '@', 'ola', 'di', 'anni', 'per', 'la', 'risol@@', '<unk>', '@', 'uzione', 'del', '4@@', '<unk>', '@', '8', ',', 'per', 'cento', 'del', '40', '%', 'è', 'stato', 'sp@@', '<unk>', '@', 'av@@', '<unk>', '@', 'ent@@', '<unk>', '@', 'ato', '.', '</s>']
2025-05-28 10:06:12,525 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:06:12,525 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:06:12,525 - INFO - joeynmt.training - 	Hypothesis: Ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per vedere che i ghi<unk> @ acci<unk> @ atori ar<unk> @ t<unk> @ ici , che i ghi<unk> @ acci<unk> @ ano per tre milioni di anni , il mondo , che ha avuto il 40 % della gr<unk> @ av<unk> @ ola di anni per la risol<unk> @ uzione del 4<unk> @ 8 , per cento del 40 % è stato sp<unk> @ av<unk> @ ent<unk> @ ato .
2025-05-28 10:06:12,525 - INFO - joeynmt.training - Example #1
2025-05-28 10:06:12,526 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:06:12,526 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:06:12,526 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'forte', 'da', 'essere', 'la', 'cre@@', '<unk>', '@', 'ativ@@', '<unk>', '@', 'ità', 'di', 'questo', 'particolare', 'problema', ',', 'perché', 'non', 'è', 'che', 'non', 'ci', 'mostra', 'il', 'D@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ke', 'di', 'questo', 'problema', ',', 'non', 'ci', 'mostra', 'il', 'D@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ke', 'del', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 10:06:12,526 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:06:12,526 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:06:12,526 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte da essere la cre<unk> @ ativ<unk> @ ità di questo particolare problema , perché non è che non ci mostra il D<unk> @ ic<unk> @ ke di questo problema , non ci mostra il D<unk> @ ic<unk> @ ke del ghi<unk> @ accio .
2025-05-28 10:06:12,526 - INFO - joeynmt.training - Example #2
2025-05-28 10:06:12,527 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:06:12,527 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:06:12,527 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'b@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ona', 'di', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'è', 'la', 'co@@', '<unk>', '@', 'per@@', '<unk>', '@', 't@@', '<unk>', '@', 'ina', 'di', 'oggi', '.', '</s>']
2025-05-28 10:06:12,527 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:06:12,527 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:06:12,527 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la b<unk> @ ic<unk> @ ona di ghi<unk> @ accio ar<unk> @ t<unk> @ ico è la co<unk> @ per<unk> @ t<unk> @ ina di oggi .
2025-05-28 10:06:12,527 - INFO - joeynmt.training - Example #3
2025-05-28 10:06:12,527 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:06:12,528 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:06:12,528 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&apos;', 'una', 'volta', 'che', 'si', 'sta', 's@@', '<unk>', '@', 'os@@', '<unk>', '@', 'ci@@', '<unk>', '@', 'ando', 'nel', 'vento', 'e', 'lo', 's@@', '<unk>', '@', 'om@@', '<unk>', '@', 'ate', 'in', 'est@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 10:06:12,528 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:06:12,528 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:06:12,528 - INFO - joeynmt.training - 	Hypothesis: E &apos; una volta che si sta s<unk> @ os<unk> @ ci<unk> @ ando nel vento e lo s<unk> @ om<unk> @ ate in est<unk> @ ate .
2025-05-28 10:06:12,528 - INFO - joeynmt.training - Example #4
2025-05-28 10:06:12,528 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:06:12,528 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:06:12,528 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'una', 'di@@', '<unk>', '@', 'ret@@', '<unk>', '@', 'ta', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 10:06:12,529 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:06:12,529 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:06:12,529 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ erò è una di<unk> @ ret<unk> @ ta che è successo negli ultimi 25 anni .
2025-05-28 10:06:15,945 - INFO - joeynmt.training - Epoch   9, Step:    54600, Batch Loss:     1.176379, Batch Acc: 0.685292, Tokens per Sec:    18837, Lr: 0.000300
2025-05-28 10:06:19,344 - INFO - joeynmt.training - Epoch   9, Step:    54700, Batch Loss:     1.411831, Batch Acc: 0.677686, Tokens per Sec:    21262, Lr: 0.000300
2025-05-28 10:06:22,731 - INFO - joeynmt.training - Epoch   9, Step:    54800, Batch Loss:     1.170640, Batch Acc: 0.682033, Tokens per Sec:    20840, Lr: 0.000300
2025-05-28 10:06:26,134 - INFO - joeynmt.training - Epoch   9, Step:    54900, Batch Loss:     1.192533, Batch Acc: 0.681373, Tokens per Sec:    20991, Lr: 0.000300
2025-05-28 10:06:29,512 - INFO - joeynmt.training - Epoch   9, Step:    55000, Batch Loss:     1.163155, Batch Acc: 0.680888, Tokens per Sec:    20768, Lr: 0.000300
2025-05-28 10:06:29,513 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:06:29,513 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:06:40,121 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.25, ppl:   3.49, acc:   0.66, generation: 10.6016[sec], evaluation: 0.0000[sec]
2025-05-28 10:06:40,494 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/53000.ckpt
2025-05-28 10:06:40,522 - INFO - joeynmt.training - Example #0
2025-05-28 10:06:40,522 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:06:40,522 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:06:40,522 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 's@@', '<unk>', '@', 'li@@', '<unk>', '@', 'de', 'per', 'guardare', 'a', 'questo', ',', 'per', 'guardare', 'a', 'che', 'il', 'mat@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'mon@@', '<unk>', '@', 'io', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', ',', 'che', 'per', 'cui', 'il', 'ann@@', '<unk>', '@', 'eg@@', '<unk>', '@', 'o@@', '<unk>', '@', 'de', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'cui', 'è', 'stato', 'chiamato', 'il', 'numero', 'di', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'la', 'risol@@', '<unk>', '@', 'uzione', 'del', '40', '%', '.', '</s>']
2025-05-28 10:06:40,523 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:06:40,523 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:06:40,523 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due s<unk> @ li<unk> @ de per guardare a questo , per guardare a che il mat<unk> @ ri<unk> @ mon<unk> @ io ar<unk> @ t<unk> @ ico , che per cui il ann<unk> @ eg<unk> @ o<unk> @ de per tre milioni di anni , per cui è stato chiamato il numero di tre milioni di anni , per la risol<unk> @ uzione del 40 % .
2025-05-28 10:06:40,523 - INFO - joeynmt.training - Example #1
2025-05-28 10:06:40,523 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:06:40,524 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:06:40,524 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'pot@@', '<unk>', '@', 'ente', 'che', 'la', 'sost@@', '<unk>', '@', 'anza', 'di', 'questo', 'problema', ',', 'perché', 'non', 'ci', 'sono', 'i', 'det@@', '<unk>', '@', 'tag@@', '<unk>', '@', 'li', '.', '</s>']
2025-05-28 10:06:40,524 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:06:40,524 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:06:40,524 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza pot<unk> @ ente che la sost<unk> @ anza di questo problema , perché non ci sono i det<unk> @ tag<unk> @ li .
2025-05-28 10:06:40,524 - INFO - joeynmt.training - Example #2
2025-05-28 10:06:40,524 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:06:40,524 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:06:40,525 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'cat@@', '<unk>', '@', 'ena', 'di', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'vo', 'è', 'la', 'cat@@', '<unk>', '@', 'ena', 'di', 'cli@@', '<unk>', '@', 'mat@@', '<unk>', '@', 'ica', 'globale', '.', '</s>']
2025-05-28 10:06:40,525 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:06:40,525 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:06:40,525 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la cat<unk> @ ena di ghi<unk> @ accio ar<unk> @ c<unk> @ atti<unk> @ vo è la cat<unk> @ ena di cli<unk> @ mat<unk> @ ica globale .
2025-05-28 10:06:40,525 - INFO - joeynmt.training - Example #3
2025-05-28 10:06:40,525 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:06:40,525 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:06:40,525 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', '@', 'in@@', '<unk>', '@', 'ce', 'nel', 'vento', 'e', 'si', 's@@', '<unk>', '@', 'ale', 'in', 'est@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 10:06:40,526 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:06:40,526 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:06:40,526 - INFO - joeynmt.training - 	Hypothesis: C<unk> @ in<unk> @ ce nel vento e si s<unk> @ ale in est<unk> @ ate .
2025-05-28 10:06:40,526 - INFO - joeynmt.training - Example #4
2025-05-28 10:06:40,526 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:06:40,526 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:06:40,526 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 's@@', '<unk>', '@', 'li@@', '<unk>', '@', 'de', 'che', 'vi', 'mostr@@', '<unk>', '@', 'o', 'è', 'una', 'seg@@', '<unk>', '@', 'ret@@', '<unk>', '@', 'a', 'che', 'negli', 'ultimi', '25', 'anni', 'è', 'successo', '.', '</s>']
2025-05-28 10:06:40,527 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:06:40,527 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:06:40,527 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma s<unk> @ li<unk> @ de che vi mostr<unk> @ o è una seg<unk> @ ret<unk> @ a che negli ultimi 25 anni è successo .
2025-05-28 10:06:43,947 - INFO - joeynmt.training - Epoch   9, Step:    55100, Batch Loss:     1.279313, Batch Acc: 0.680534, Tokens per Sec:    18493, Lr: 0.000300
2025-05-28 10:06:47,333 - INFO - joeynmt.training - Epoch   9, Step:    55200, Batch Loss:     1.253128, Batch Acc: 0.680013, Tokens per Sec:    20771, Lr: 0.000300
2025-05-28 10:06:50,715 - INFO - joeynmt.training - Epoch   9, Step:    55300, Batch Loss:     0.989828, Batch Acc: 0.682870, Tokens per Sec:    21594, Lr: 0.000300
2025-05-28 10:06:54,106 - INFO - joeynmt.training - Epoch   9, Step:    55400, Batch Loss:     0.966939, Batch Acc: 0.679659, Tokens per Sec:    21159, Lr: 0.000300
2025-05-28 10:06:57,529 - INFO - joeynmt.training - Epoch   9, Step:    55500, Batch Loss:     1.241761, Batch Acc: 0.679684, Tokens per Sec:    20830, Lr: 0.000300
2025-05-28 10:06:57,530 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:06:57,530 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:07:08,545 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.25, ppl:   3.48, acc:   0.66, generation: 11.0045[sec], evaluation: 0.0000[sec]
2025-05-28 10:07:08,983 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/49000.ckpt
2025-05-28 10:07:09,012 - INFO - joeynmt.training - Example #0
2025-05-28 10:07:09,013 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:07:09,013 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:07:09,013 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'vedere', 'che', 'il', 'can@@', '<unk>', '@', 'e', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'che', 'il', 'gi@@', '<unk>', '@', 'g@@', '<unk>', '@', 'ur@@', '<unk>', '@', 'ale', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'il', 'l@@', '<unk>', '@', 'or@@', '<unk>', '@', 'to', 'di', 'tras@@', '<unk>', '@', 'porto', 'con', 'il', '4@@', '<unk>', '@', '8', ',', 'è', 'stata', 'de@@', '<unk>', '@', 'termin@@', '<unk>', '@', 'ata', 'per', 'tre', 'milioni', 'di', 'anni', '.', '</s>']
2025-05-28 10:07:09,014 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:07:09,014 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:07:09,014 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per vedere che il can<unk> @ e ar<unk> @ t<unk> @ ico che il gi<unk> @ g<unk> @ ur<unk> @ ale per tre milioni di anni , il l<unk> @ or<unk> @ to di tras<unk> @ porto con il 4<unk> @ 8 , è stata de<unk> @ termin<unk> @ ata per tre milioni di anni .
2025-05-28 10:07:09,014 - INFO - joeynmt.training - Example #1
2025-05-28 10:07:09,014 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:07:09,014 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:07:09,014 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'forte', 'la', 's@@', '<unk>', '@', 'ogli@@', '<unk>', '@', 'a', 'di', 'questo', 'particolare', 'problema', ',', 'perché', 'non', 'è', 'l&apos;', 'esist@@', '<unk>', '@', 'enza', 'di', 'questo', 'particolare', 'problema', ',', 'non', 'è', 'il', 'bu@@', '<unk>', '@', 'io', '.', '</s>']
2025-05-28 10:07:09,014 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:07:09,015 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:07:09,015 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte la s<unk> @ ogli<unk> @ a di questo particolare problema , perché non è l&apos; esist<unk> @ enza di questo particolare problema , non è il bu<unk> @ io .
2025-05-28 10:07:09,015 - INFO - joeynmt.training - Example #2
2025-05-28 10:07:09,015 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:07:09,015 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:07:09,015 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'il', 'ben@@', '<unk>', '@', 'e@@', '<unk>', '@', 'ficio', 'è', 'il', 'ben@@', '<unk>', '@', 'e@@', '<unk>', '@', 'ficio', 'che', 'il', 'nostro', 'sistema', 'peggi@@', '<unk>', '@', 'ore', 'globale', '.', '</s>']
2025-05-28 10:07:09,016 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:07:09,016 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:07:09,016 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , il ben<unk> @ e<unk> @ ficio è il ben<unk> @ e<unk> @ ficio che il nostro sistema peggi<unk> @ ore globale .
2025-05-28 10:07:09,016 - INFO - joeynmt.training - Example #3
2025-05-28 10:07:09,016 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:07:09,016 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:07:09,016 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', '@', 'ambi@@', '<unk>', '@', 'ano', 'nel', 'vento', 'e', 'lo', 's@@', '<unk>', '@', 'om@@', '<unk>', '@', 'ate', 'in', 'est@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 10:07:09,016 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:07:09,016 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:07:09,017 - INFO - joeynmt.training - 	Hypothesis: C<unk> @ ambi<unk> @ ano nel vento e lo s<unk> @ om<unk> @ ate in est<unk> @ ate .
2025-05-28 10:07:09,017 - INFO - joeynmt.training - Example #4
2025-05-28 10:07:09,017 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:07:09,017 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:07:09,017 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 's@@', '<unk>', '@', 'li@@', '<unk>', '@', 'de', 'che', 'vi', 'mostr@@', '<unk>', '@', 'i', 'è', 'una', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'na', 'di', 'seg@@', '<unk>', '@', 'no', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 10:07:09,017 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:07:09,017 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:07:09,017 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma s<unk> @ li<unk> @ de che vi mostr<unk> @ i è una di<unk> @ seg<unk> @ na di seg<unk> @ no negli ultimi 25 anni .
2025-05-28 10:07:12,445 - INFO - joeynmt.training - Epoch   9, Step:    55600, Batch Loss:     1.060714, Batch Acc: 0.679007, Tokens per Sec:    18122, Lr: 0.000300
2025-05-28 10:07:15,838 - INFO - joeynmt.training - Epoch   9, Step:    55700, Batch Loss:     1.182612, Batch Acc: 0.685201, Tokens per Sec:    20576, Lr: 0.000300
2025-05-28 10:07:19,221 - INFO - joeynmt.training - Epoch   9, Step:    55800, Batch Loss:     1.259386, Batch Acc: 0.681351, Tokens per Sec:    20985, Lr: 0.000300
2025-05-28 10:07:22,641 - INFO - joeynmt.training - Epoch   9, Step:    55900, Batch Loss:     1.142677, Batch Acc: 0.678114, Tokens per Sec:    21290, Lr: 0.000300
2025-05-28 10:07:26,047 - INFO - joeynmt.training - Epoch   9, Step:    56000, Batch Loss:     1.119807, Batch Acc: 0.677856, Tokens per Sec:    21127, Lr: 0.000300
2025-05-28 10:07:26,048 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:07:26,048 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:07:36,752 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.24, ppl:   3.46, acc:   0.66, generation: 10.6968[sec], evaluation: 0.0000[sec]
2025-05-28 10:07:36,752 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 10:07:37,475 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/50000.ckpt
2025-05-28 10:07:37,505 - INFO - joeynmt.training - Example #0
2025-05-28 10:07:37,506 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:07:37,506 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:07:37,506 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'vedere', 'che', 'la', 'gr@@', '<unk>', '@', 'av@@', '<unk>', '@', 'ità', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'che', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'che', 'è', 'stata', 'la', 'più', 'alta', 'di', '4@@', '<unk>', '@', '8', ',', 'per', 'cento', 'di', '40', 'stati', 'tras@@', '<unk>', '@', 'fer@@', '<unk>', '@', 'ir@@', '<unk>', '@', 'si', 'a', '40', 'stati', 'in', '40', 'anni', ',', 'per', 'cento', '.', '</s>']
2025-05-28 10:07:37,507 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:07:37,507 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:07:37,507 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per vedere che la gr<unk> @ av<unk> @ ità ar<unk> @ t<unk> @ ica ar<unk> @ t<unk> @ ica che per tre milioni di anni , che è stata la più alta di 4<unk> @ 8 , per cento di 40 stati tras<unk> @ fer<unk> @ ir<unk> @ si a 40 stati in 40 anni , per cento .
2025-05-28 10:07:37,507 - INFO - joeynmt.training - Example #1
2025-05-28 10:07:37,507 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:07:37,507 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:07:37,507 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'da', 'quanto', 'sia', 'or@@', '<unk>', '@', 'mai', 'abbastanza', 'da', 'da', 'da', 'da', 'quel', 'problema', ',', 'perché', 'non', 'lo', 'è', 'il', 'soff@@', '<unk>', '@', 'itto', 'di', 'questo', 'problema', ',', 'non', 'è', 'il', 'soff@@', '<unk>', '@', 'itto', 'di', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 10:07:37,508 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:07:37,508 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:07:37,508 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza da quanto sia or<unk> @ mai abbastanza da da da da quel problema , perché non lo è il soff<unk> @ itto di questo problema , non è il soff<unk> @ itto di ghi<unk> @ accio .
2025-05-28 10:07:37,508 - INFO - joeynmt.training - Example #2
2025-05-28 10:07:37,508 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:07:37,508 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:07:37,509 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'di', 'un', 'sistema', 'di', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', ',', 'il', 'cuore', 'di', 'un', 'sistema', 'peggi@@', '<unk>', '@', 'ore', 'globale', '.', '</s>']
2025-05-28 10:07:37,509 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:07:37,509 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:07:37,509 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , il cuore di un sistema di ghi<unk> @ accio ar<unk> @ t<unk> @ ico , il cuore di un sistema peggi<unk> @ ore globale .
2025-05-28 10:07:37,509 - INFO - joeynmt.training - Example #3
2025-05-28 10:07:37,509 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:07:37,509 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:07:37,509 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'volta', 'che', 'si', 'è', 'in@@', '<unk>', '@', 'ver@@', '<unk>', '@', 'no', 'e', 's@@', '<unk>', '@', 'v@@', '<unk>', '@', 'eg@@', '<unk>', '@', 'lio', '.', '</s>']
2025-05-28 10:07:37,510 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:07:37,510 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:07:37,510 - INFO - joeynmt.training - 	Hypothesis: La prima volta che si è in<unk> @ ver<unk> @ no e s<unk> @ v<unk> @ eg<unk> @ lio .
2025-05-28 10:07:37,510 - INFO - joeynmt.training - Example #4
2025-05-28 10:07:37,510 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:07:37,510 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:07:37,510 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 's@@', '<unk>', '@', 'li@@', '<unk>', '@', 'de', 'che', 'vi', 'mostr@@', '<unk>', '@', 'i', 'è', 'una', 'ri@@', '<unk>', '@', 'un@@', '<unk>', '@', 'ione', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 10:07:37,511 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:07:37,511 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:07:37,511 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma s<unk> @ li<unk> @ de che vi mostr<unk> @ i è una ri<unk> @ un<unk> @ ione che è successo negli ultimi 25 anni .
2025-05-28 10:07:40,924 - INFO - joeynmt.training - Epoch   9, Step:    56100, Batch Loss:     1.068778, Batch Acc: 0.682915, Tokens per Sec:    17271, Lr: 0.000300
2025-05-28 10:07:44,315 - INFO - joeynmt.training - Epoch   9, Step:    56200, Batch Loss:     1.356279, Batch Acc: 0.682710, Tokens per Sec:    21303, Lr: 0.000300
2025-05-28 10:07:47,701 - INFO - joeynmt.training - Epoch   9, Step:    56300, Batch Loss:     1.332883, Batch Acc: 0.680473, Tokens per Sec:    21185, Lr: 0.000300
2025-05-28 10:07:51,114 - INFO - joeynmt.training - Epoch   9, Step:    56400, Batch Loss:     1.248060, Batch Acc: 0.682703, Tokens per Sec:    20855, Lr: 0.000300
2025-05-28 10:07:54,494 - INFO - joeynmt.training - Epoch   9, Step:    56500, Batch Loss:     1.210536, Batch Acc: 0.677840, Tokens per Sec:    21253, Lr: 0.000300
2025-05-28 10:07:54,495 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:07:54,495 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:08:05,726 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.24, ppl:   3.46, acc:   0.66, generation: 11.2209[sec], evaluation: 0.0000[sec]
2025-05-28 10:08:05,727 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 10:08:06,484 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/54500.ckpt
2025-05-28 10:08:06,512 - INFO - joeynmt.training - Example #0
2025-05-28 10:08:06,513 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:08:06,513 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:08:06,513 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'ri@@', '<unk>', '@', 'durre', 'che', 'l&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ano', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'che', 'è', 'stato', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'che', 'è', 'stato', 'stato', 'fatto', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'per', 's@@', '<unk>', '@', 'fr@@', '<unk>', '@', 'utt@@', '<unk>', '@', 'are', 'il', '40', 'anni', 'per', 'cento', '.', '</s>']
2025-05-28 10:08:06,513 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:08:06,513 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:08:06,514 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per ri<unk> @ durre che l&apos; E<unk> @ is<unk> @ k<unk> @ i<unk> @ ano per tre milioni di anni , che è stato fatto per tre milioni di anni , che è stato stato fatto per tre milioni di anni , per s<unk> @ fr<unk> @ utt<unk> @ are il 40 anni per cento .
2025-05-28 10:08:06,514 - INFO - joeynmt.training - Example #1
2025-05-28 10:08:06,514 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:08:06,514 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:08:06,514 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'forte', ',', 'che', 'è', 'abbastanza', 'la', 'cre@@', '<unk>', '@', 'ativ@@', '<unk>', '@', 'ità', 'di', 'questo', 'particolare', 'problema', ',', 'perché', 'non', 'è', 'la', 'd@@', '<unk>', '@', 'à', 'l&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 10:08:06,514 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:08:06,514 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:08:06,515 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza forte , che è abbastanza la cre<unk> @ ativ<unk> @ ità di questo particolare problema , perché non è la d<unk> @ à l&apos; E<unk> @ is<unk> @ es .
2025-05-28 10:08:06,515 - INFO - joeynmt.training - Example #2
2025-05-28 10:08:06,515 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:08:06,515 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:08:06,515 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'di', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ur@@', '<unk>', '@', 'ale', 'è', 'la', 'mal@@', '<unk>', '@', 'ata', 'su@@', '<unk>', '@', 'peri@@', '<unk>', '@', 'ore', 'del', 'nostro', 'sistema', 'globale', '.', '</s>']
2025-05-28 10:08:06,515 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:08:06,516 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:08:06,516 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , il cuore di ghi<unk> @ accio ar<unk> @ t<unk> @ ur<unk> @ ale è la mal<unk> @ ata su<unk> @ peri<unk> @ ore del nostro sistema globale .
2025-05-28 10:08:06,516 - INFO - joeynmt.training - Example #3
2025-05-28 10:08:06,516 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:08:06,516 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:08:06,516 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', '@', 'e', 'cres@@', '<unk>', '@', 'ce', 'nel', 'vento', 'e', 'si', 's@@', '<unk>', '@', 'v@@', '<unk>', '@', 'egli@@', '<unk>', '@', 'ano', 'nell&apos;', 'est@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 10:08:06,516 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:08:06,516 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:08:06,517 - INFO - joeynmt.training - 	Hypothesis: C<unk> @ e cres<unk> @ ce nel vento e si s<unk> @ v<unk> @ egli<unk> @ ano nell&apos; est<unk> @ ate .
2025-05-28 10:08:06,517 - INFO - joeynmt.training - Example #4
2025-05-28 10:08:06,517 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:08:06,517 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:08:06,517 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'o', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 10:08:06,517 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:08:06,517 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:08:06,518 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ o è successo negli ultimi 25 anni .
2025-05-28 10:08:09,923 - INFO - joeynmt.training - Epoch   9, Step:    56600, Batch Loss:     1.175494, Batch Acc: 0.678246, Tokens per Sec:    17023, Lr: 0.000300
2025-05-28 10:08:13,312 - INFO - joeynmt.training - Epoch   9, Step:    56700, Batch Loss:     1.222183, Batch Acc: 0.680626, Tokens per Sec:    20912, Lr: 0.000300
2025-05-28 10:08:13,712 - INFO - joeynmt.training - Epoch   9: total training loss 7205.24
2025-05-28 10:08:13,712 - INFO - joeynmt.training - EPOCH 10
2025-05-28 10:08:16,704 - INFO - joeynmt.training - Epoch  10, Step:    56800, Batch Loss:     0.924040, Batch Acc: 0.692840, Tokens per Sec:    20966, Lr: 0.000300
2025-05-28 10:08:20,100 - INFO - joeynmt.training - Epoch  10, Step:    56900, Batch Loss:     1.203300, Batch Acc: 0.698098, Tokens per Sec:    20717, Lr: 0.000300
2025-05-28 10:08:23,487 - INFO - joeynmt.training - Epoch  10, Step:    57000, Batch Loss:     1.033442, Batch Acc: 0.691478, Tokens per Sec:    21194, Lr: 0.000300
2025-05-28 10:08:23,487 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:08:23,487 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:08:34,934 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.25, ppl:   3.47, acc:   0.66, generation: 11.4347[sec], evaluation: 0.0000[sec]
2025-05-28 10:08:35,490 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/55000.ckpt
2025-05-28 10:08:35,518 - INFO - joeynmt.training - Example #0
2025-05-28 10:08:35,518 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:08:35,519 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:08:35,519 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'dimostr@@', '<unk>', '@', 'are', 'che', 'il', 'livello', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', ',', 'che', 'è', 'stato', 'il', 't@@', '<unk>', '@', 'es@@', '<unk>', '@', 'atto', 'di', 'tre', 'milioni', 'di', 'anni', ',', 'il', '40', '%', 'è', 'stato', 'sc@@', '<unk>', '@', 'att@@', '<unk>', '@', 'ac@@', '<unk>', '@', 'olo', '.', '</s>']
2025-05-28 10:08:35,519 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:08:35,519 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:08:35,519 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per dimostr<unk> @ are che il livello ar<unk> @ t<unk> @ ico ar<unk> @ t<unk> @ ico , che è stato il t<unk> @ es<unk> @ atto di tre milioni di anni , il 40 % è stato sc<unk> @ att<unk> @ ac<unk> @ olo .
2025-05-28 10:08:35,519 - INFO - joeynmt.training - Example #1
2025-05-28 10:08:35,520 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:08:35,520 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:08:35,520 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', ',', 'che', 'questo', 'è', 'abbastanza', ',', 'perché', 'non', 'è', 'un', 'di@@', '<unk>', '@', 'mostra', 'il', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'no', '.', '</s>']
2025-05-28 10:08:35,520 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:08:35,520 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:08:35,520 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza , che questo è abbastanza , perché non è un di<unk> @ mostra il di<unk> @ seg<unk> @ no .
2025-05-28 10:08:35,520 - INFO - joeynmt.training - Example #2
2025-05-28 10:08:35,521 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:08:35,521 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:08:35,521 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'è', 'il', 'c@@', '<unk>', '@', 'app@@', '<unk>', '@', 'ello', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'del', 'nostro', 'sistema', 'peggi@@', '<unk>', '@', 'o', 'globale', '.', '</s>']
2025-05-28 10:08:35,521 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:08:35,521 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:08:35,521 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , il cuore è il c<unk> @ app<unk> @ ello ar<unk> @ t<unk> @ ico del nostro sistema peggi<unk> @ o globale .
2025-05-28 10:08:35,521 - INFO - joeynmt.training - Example #3
2025-05-28 10:08:35,522 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:08:35,522 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:08:35,522 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', '@', 'ir@@', '<unk>', '@', 'ca', 'nel', 'vento', 'e', 'la', 'p@@', '<unk>', '@', 'om@@', '<unk>', '@', 'ate', 'in', 'est@@', '<unk>', '@', 'ate', 'e', 's@@', '<unk>', '@', 'ale', '.', '</s>']
2025-05-28 10:08:35,522 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:08:35,522 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:08:35,522 - INFO - joeynmt.training - 	Hypothesis: C<unk> @ ir<unk> @ ca nel vento e la p<unk> @ om<unk> @ ate in est<unk> @ ate e s<unk> @ ale .
2025-05-28 10:08:35,522 - INFO - joeynmt.training - Example #4
2025-05-28 10:08:35,522 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:08:35,523 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:08:35,523 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'i', 'è', 'un', 'segn@@', '<unk>', '@', 'ale', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 10:08:35,523 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:08:35,523 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:08:35,523 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ i è un segn<unk> @ ale che è successo negli ultimi 25 anni .
2025-05-28 10:08:38,945 - INFO - joeynmt.training - Epoch  10, Step:    57100, Batch Loss:     1.109082, Batch Acc: 0.698841, Tokens per Sec:    17427, Lr: 0.000300
2025-05-28 10:08:42,352 - INFO - joeynmt.training - Epoch  10, Step:    57200, Batch Loss:     0.962149, Batch Acc: 0.695572, Tokens per Sec:    20658, Lr: 0.000300
2025-05-28 10:08:45,749 - INFO - joeynmt.training - Epoch  10, Step:    57300, Batch Loss:     1.143429, Batch Acc: 0.691987, Tokens per Sec:    20606, Lr: 0.000300
2025-05-28 10:08:49,137 - INFO - joeynmt.training - Epoch  10, Step:    57400, Batch Loss:     1.150960, Batch Acc: 0.696239, Tokens per Sec:    21376, Lr: 0.000300
2025-05-28 10:08:52,533 - INFO - joeynmt.training - Epoch  10, Step:    57500, Batch Loss:     1.116607, Batch Acc: 0.699365, Tokens per Sec:    21665, Lr: 0.000300
2025-05-28 10:08:52,534 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:08:52,534 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:09:03,096 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.24, ppl:   3.47, acc:   0.67, generation: 10.5550[sec], evaluation: 0.0000[sec]
2025-05-28 10:09:03,482 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/55500.ckpt
2025-05-28 10:09:03,511 - INFO - joeynmt.training - Example #0
2025-05-28 10:09:03,512 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:09:03,512 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:09:03,512 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'mostr@@', '<unk>', '@', 'are', 'che', 'i', 't@@', '<unk>', '@', 'av@@', '<unk>', '@', 'oli', 'dell&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'et', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'che', 'è', 'stato', 'in@@', '<unk>', '@', 'vi@@', '<unk>', '@', 'dente', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'cento', '.', '</s>']
2025-05-28 10:09:03,512 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:09:03,512 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:09:03,512 - INFO - joeynmt.training - 	Hypothesis: Ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per mostr<unk> @ are che i t<unk> @ av<unk> @ oli dell&apos; E<unk> @ is<unk> @ k<unk> @ et per tre milioni di anni , che è stato in<unk> @ vi<unk> @ dente per tre milioni di anni , per cento .
2025-05-28 10:09:03,513 - INFO - joeynmt.training - Example #1
2025-05-28 10:09:03,513 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:09:03,513 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:09:03,513 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'forte', 'che', 'la', 'ser@@', '<unk>', '@', 'i@@', '<unk>', '@', 'enza', 'di', 'questo', 'particolare', 'problema', 'speci@@', '<unk>', '@', 'ale', ',', 'non', 'è', 'il', 'di@@', '<unk>', '@', 'mostra', 'il', 'vi@@', '<unk>', '@', 'll@@', '<unk>', '@', 'aggio', 'dell&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'es', '.', '</s>']
2025-05-28 10:09:03,513 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:09:03,513 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:09:03,514 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza forte che la ser<unk> @ i<unk> @ enza di questo particolare problema speci<unk> @ ale , non è il di<unk> @ mostra il vi<unk> @ ll<unk> @ aggio dell&apos; E<unk> @ is<unk> @ es .
2025-05-28 10:09:03,514 - INFO - joeynmt.training - Example #2
2025-05-28 10:09:03,514 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:09:03,514 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:09:03,514 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'è', 'il', 'cuore', 'delle', 'ghi@@', '<unk>', '@', 'accio', 'su@@', '<unk>', '@', 'peri@@', '<unk>', '@', 'ore', 'del', 'nostro', 'sistema', 'peggi@@', '<unk>', '@', 'o', 'globale', '.', '</s>']
2025-05-28 10:09:03,514 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:09:03,514 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:09:03,515 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , il cuore è il cuore delle ghi<unk> @ accio su<unk> @ peri<unk> @ ore del nostro sistema peggi<unk> @ o globale .
2025-05-28 10:09:03,515 - INFO - joeynmt.training - Example #3
2025-05-28 10:09:03,515 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:09:03,515 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:09:03,515 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['cres@@', '<unk>', '@', 'ce', 'nel', 'vento', 'e', 'il', 'v@@', '<unk>', '@', 'ant@@', '<unk>', '@', 'aggio', 'in', 'est@@', '<unk>', '@', 'in@@', '<unk>', '@', 'ver@@', '<unk>', '@', 'no', '.', '</s>']
2025-05-28 10:09:03,515 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:09:03,515 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:09:03,516 - INFO - joeynmt.training - 	Hypothesis: cres<unk> @ ce nel vento e il v<unk> @ ant<unk> @ aggio in est<unk> @ in<unk> @ ver<unk> @ no .
2025-05-28 10:09:03,516 - INFO - joeynmt.training - Example #4
2025-05-28 10:09:03,516 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:09:03,516 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:09:03,516 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'o', 'è', 'un', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'no', 'di', 'un', 'indi@@', '<unk>', '@', 'g@@', '<unk>', '@', 'no', 'di', 'cosa', 'succede', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 10:09:03,516 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:09:03,516 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:09:03,516 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ o è un di<unk> @ seg<unk> @ no di un indi<unk> @ g<unk> @ no di cosa succede negli ultimi 25 anni .
2025-05-28 10:09:06,927 - INFO - joeynmt.training - Epoch  10, Step:    57600, Batch Loss:     1.235229, Batch Acc: 0.690244, Tokens per Sec:    18452, Lr: 0.000300
2025-05-28 10:09:10,312 - INFO - joeynmt.training - Epoch  10, Step:    57700, Batch Loss:     1.178906, Batch Acc: 0.695688, Tokens per Sec:    21085, Lr: 0.000300
2025-05-28 10:09:13,684 - INFO - joeynmt.training - Epoch  10, Step:    57800, Batch Loss:     1.012462, Batch Acc: 0.694115, Tokens per Sec:    21080, Lr: 0.000300
2025-05-28 10:09:17,065 - INFO - joeynmt.training - Epoch  10, Step:    57900, Batch Loss:     1.159843, Batch Acc: 0.695216, Tokens per Sec:    20735, Lr: 0.000300
2025-05-28 10:09:20,459 - INFO - joeynmt.training - Epoch  10, Step:    58000, Batch Loss:     1.086990, Batch Acc: 0.687317, Tokens per Sec:    21147, Lr: 0.000300
2025-05-28 10:09:20,459 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:09:20,459 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:09:33,503 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.25, ppl:   3.50, acc:   0.66, generation: 13.0316[sec], evaluation: 0.0000[sec]
2025-05-28 10:09:33,515 - INFO - joeynmt.training - Example #0
2025-05-28 10:09:33,516 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:09:33,516 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:09:33,516 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'creare', 'che', 'l&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ano', 'che', 'l&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ano', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'cento', 'della', 'dimen@@', '<unk>', '@', 'sione', 'è', 'stato', 'sp@@', '<unk>', '@', 'os@@', '<unk>', '@', 'ato', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'cento', 'di', '40', 'stati', 'tras@@', '<unk>', '@', 'fer@@', '<unk>', '@', 'ti@@', '<unk>', '@', 'lizz@@', '<unk>', '@', 'ati', 'al', '40', '%', '.', '</s>']
2025-05-28 10:09:33,517 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:09:33,517 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:09:33,517 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per creare che l&apos; E<unk> @ is<unk> @ k<unk> @ i<unk> @ ano che l&apos; E<unk> @ is<unk> @ k<unk> @ i<unk> @ ano per tre milioni di anni , per cento della dimen<unk> @ sione è stato sp<unk> @ os<unk> @ ato per tre milioni di anni , per cento di 40 stati tras<unk> @ fer<unk> @ ti<unk> @ lizz<unk> @ ati al 40 % .
2025-05-28 10:09:33,517 - INFO - joeynmt.training - Example #1
2025-05-28 10:09:33,517 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:09:33,517 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:09:33,517 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'da', 'essere', 'la', 'ser@@', '<unk>', '@', 'i@@', '<unk>', '@', 'età', 'di', 'questo', 'problema', 'speci@@', '<unk>', '@', 'ale', ',', 'perché', 'non', 'è', 'la', 'sig@@', '<unk>', '@', 'n@@', '<unk>', '@', 'ora', 'che', 'mostra', 'la', 'sig@@', '<unk>', '@', 'n@@', '<unk>', '@', 'ora', '.', '</s>']
2025-05-28 10:09:33,518 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:09:33,518 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:09:33,518 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza da essere la ser<unk> @ i<unk> @ età di questo problema speci<unk> @ ale , perché non è la sig<unk> @ n<unk> @ ora che mostra la sig<unk> @ n<unk> @ ora .
2025-05-28 10:09:33,518 - INFO - joeynmt.training - Example #2
2025-05-28 10:09:33,518 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:09:33,518 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:09:33,519 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'c@@', '<unk>', '@', 'ris@@', '<unk>', '@', 'i', 'è', 'la', 'no@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ci@@', '<unk>', '@', 'enza', 'di', 'ghi@@', '<unk>', '@', 'accio', 'globale', '.', '</s>']
2025-05-28 10:09:33,519 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:09:33,519 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:09:33,519 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la c<unk> @ ris<unk> @ i è la no<unk> @ c<unk> @ ci<unk> @ enza di ghi<unk> @ accio globale .
2025-05-28 10:09:33,519 - INFO - joeynmt.training - Example #3
2025-05-28 10:09:33,519 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:09:33,519 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:09:33,520 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'a', 'cresc@@', '<unk>', '@', 'ere', 'e', '&apos;', 'in@@', '<unk>', '@', 'ver@@', '<unk>', '@', 'no', 'in', 'est@@', '<unk>', '@', 'ate', 'e', 's@@', '<unk>', '@', 'cos@@', '<unk>', '@', 'ci@@', '<unk>', '@', 'ente', '.', '</s>']
2025-05-28 10:09:33,520 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:09:33,520 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:09:33,520 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ a cresc<unk> @ ere e &apos; in<unk> @ ver<unk> @ no in est<unk> @ ate e s<unk> @ cos<unk> @ ci<unk> @ ente .
2025-05-28 10:09:33,520 - INFO - joeynmt.training - Example #4
2025-05-28 10:09:33,520 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:09:33,520 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:09:33,521 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'o', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 10:09:33,521 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:09:33,521 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:09:33,521 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ o è successo negli ultimi 25 anni .
2025-05-28 10:09:36,928 - INFO - joeynmt.training - Epoch  10, Step:    58100, Batch Loss:     1.041019, Batch Acc: 0.693423, Tokens per Sec:    20325, Lr: 0.000300
2025-05-28 10:09:40,320 - INFO - joeynmt.training - Epoch  10, Step:    58200, Batch Loss:     1.073585, Batch Acc: 0.696011, Tokens per Sec:    20813, Lr: 0.000300
2025-05-28 10:09:43,720 - INFO - joeynmt.training - Epoch  10, Step:    58300, Batch Loss:     1.107406, Batch Acc: 0.691287, Tokens per Sec:    21065, Lr: 0.000300
2025-05-28 10:09:47,113 - INFO - joeynmt.training - Epoch  10, Step:    58400, Batch Loss:     1.056638, Batch Acc: 0.694743, Tokens per Sec:    21322, Lr: 0.000300
2025-05-28 10:09:50,488 - INFO - joeynmt.training - Epoch  10, Step:    58500, Batch Loss:     1.062414, Batch Acc: 0.692872, Tokens per Sec:    20524, Lr: 0.000300
2025-05-28 10:09:50,489 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:09:50,489 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:10:01,872 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.25, ppl:   3.49, acc:   0.66, generation: 11.3725[sec], evaluation: 0.0000[sec]
2025-05-28 10:10:01,883 - INFO - joeynmt.training - Example #0
2025-05-28 10:10:01,884 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:10:01,884 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:10:01,884 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'cose', 'per', 'cui', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'cose', 'per', 'far', 'sì', 'che', 'i', 'ghi@@', '<unk>', '@', 'acci@@', '<unk>', '@', 'ai', 'che', 'gli', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ano', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'che', 'è', 'stato', 'in@@', '<unk>', '@', 'ser@@', '<unk>', '@', 'ito', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'cui', 'è', 'stato', 'in@@', '<unk>', '@', 'vi@@', '<unk>', '@', 'aggi@@', '<unk>', '@', 'ù', 'il', '4@@', '<unk>', '@', '8', '%', '.', '</s>']
2025-05-28 10:10:01,885 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:10:01,885 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:10:01,885 - INFO - joeynmt.training - 	Hypothesis: Ho mostr<unk> @ ato queste due cose per cui ho mostr<unk> @ ato queste due cose per far sì che i ghi<unk> @ acci<unk> @ ai che gli E<unk> @ is<unk> @ c<unk> @ ano per tre milioni di anni , che è stato in<unk> @ ser<unk> @ ito per tre milioni di anni , per cui è stato in<unk> @ vi<unk> @ aggi<unk> @ ù il 4<unk> @ 8 % .
2025-05-28 10:10:01,885 - INFO - joeynmt.training - Example #1
2025-05-28 10:10:01,885 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:10:01,885 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:10:01,885 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'forte', 'la', 'ser@@', '<unk>', '@', 'i@@', '<unk>', '@', 'enza', 'di', 'questo', 'particolare', 'problema', ',', 'perché', 'non', 'è', 'la', 'di@@', '<unk>', '@', 'mostra', 'del', 'problema', 'speci@@', '<unk>', '@', 'ale', ',', 'perché', 'non', 'lo', 'mostra', 'del', 'ghi@@', '<unk>', '@', 'accio', 'del', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 10:10:01,886 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:10:01,886 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:10:01,886 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza forte la ser<unk> @ i<unk> @ enza di questo particolare problema , perché non è la di<unk> @ mostra del problema speci<unk> @ ale , perché non lo mostra del ghi<unk> @ accio del ghi<unk> @ accio .
2025-05-28 10:10:01,886 - INFO - joeynmt.training - Example #2
2025-05-28 10:10:01,886 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:10:01,886 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:10:01,887 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'sc@@', '<unk>', '@', 'app@@', '<unk>', '@', 'ola', 'è', 'la', 'corr@@', '<unk>', '@', 'ente', 'dell&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'app@@', '<unk>', '@', 'a', 'a', 'dor@@', '<unk>', '@', 'mi@@', '<unk>', '@', 're', 'il', 'cuore', 'del', 'nostro', 'sistema', 'globale', '.', '</s>']
2025-05-28 10:10:01,887 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:10:01,887 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:10:01,887 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la sc<unk> @ app<unk> @ ola è la corr<unk> @ ente dell&apos; E<unk> @ is<unk> @ k<unk> @ app<unk> @ a a dor<unk> @ mi<unk> @ re il cuore del nostro sistema globale .
2025-05-28 10:10:01,887 - INFO - joeynmt.training - Example #3
2025-05-28 10:10:01,887 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:10:01,887 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:10:01,887 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', '@', 'res@@', '<unk>', '@', 'ce', 'nel', 'vento', 'e', 'la', 'g@@', '<unk>', '@', 'on@@', '<unk>', '@', 'na', 'nell&apos;', 'est@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 10:10:01,888 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:10:01,888 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:10:01,888 - INFO - joeynmt.training - 	Hypothesis: C<unk> @ res<unk> @ ce nel vento e la g<unk> @ on<unk> @ na nell&apos; est<unk> @ ate .
2025-05-28 10:10:01,888 - INFO - joeynmt.training - Example #4
2025-05-28 10:10:01,888 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:10:01,888 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:10:01,888 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 's@@', '<unk>', '@', 'li@@', '<unk>', '@', 'de', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'un', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'no', 'di', 'co@@', '<unk>', '@', 'per@@', '<unk>', '@', 't@@', '<unk>', '@', 'ell@@', '<unk>', '@', 'etta', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 10:10:01,889 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:10:01,889 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:10:01,889 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma s<unk> @ li<unk> @ de che vi mostr<unk> @ erò è un di<unk> @ seg<unk> @ no di co<unk> @ per<unk> @ t<unk> @ ell<unk> @ etta negli ultimi 25 anni .
2025-05-28 10:10:05,170 - INFO - joeynmt.training - Epoch  10, Step:    58600, Batch Loss:     1.174236, Batch Acc: 0.690497, Tokens per Sec:    21591, Lr: 0.000300
2025-05-28 10:10:08,495 - INFO - joeynmt.training - Epoch  10, Step:    58700, Batch Loss:     1.234683, Batch Acc: 0.685035, Tokens per Sec:    21962, Lr: 0.000300
2025-05-28 10:10:11,750 - INFO - joeynmt.training - Epoch  10, Step:    58800, Batch Loss:     1.136755, Batch Acc: 0.691748, Tokens per Sec:    21793, Lr: 0.000300
2025-05-28 10:10:15,005 - INFO - joeynmt.training - Epoch  10, Step:    58900, Batch Loss:     1.135121, Batch Acc: 0.691835, Tokens per Sec:    21416, Lr: 0.000300
2025-05-28 10:10:18,286 - INFO - joeynmt.training - Epoch  10, Step:    59000, Batch Loss:     1.160499, Batch Acc: 0.690875, Tokens per Sec:    21579, Lr: 0.000300
2025-05-28 10:10:18,287 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:10:18,287 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:10:28,951 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.25, ppl:   3.49, acc:   0.66, generation: 10.6536[sec], evaluation: 0.0000[sec]
2025-05-28 10:10:28,963 - INFO - joeynmt.training - Example #0
2025-05-28 10:10:28,964 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:10:28,964 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:10:28,964 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'ri@@', '<unk>', '@', 'vel@@', '<unk>', '@', 'are', 'che', 'i', 'g@@', '<unk>', '@', 'ett@@', '<unk>', '@', 'ori', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ini', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ino', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'il', '40', ',', 'che', 'è', 'stato', 'sp@@', '<unk>', '@', 'av@@', '<unk>', '@', 'ent@@', '<unk>', '@', 'ato', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'il', '40', ',', 'il', '40', 'per', 'cento', 'di', '40', '%', '.', '</s>']
2025-05-28 10:10:28,964 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:10:28,964 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:10:28,965 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per ri<unk> @ vel<unk> @ are che i g<unk> @ ett<unk> @ ori ar<unk> @ c<unk> @ ini ar<unk> @ c<unk> @ ino per tre milioni di anni , il 40 , che è stato sp<unk> @ av<unk> @ ent<unk> @ ato per tre milioni di anni , il 40 , il 40 per cento di 40 % .
2025-05-28 10:10:28,965 - INFO - joeynmt.training - Example #1
2025-05-28 10:10:28,965 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:10:28,965 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:10:28,965 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', ',', 'che', 'non', 'è', 'abbastanza', 'il', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'no', 'di', 'questo', 'problema', 'speci@@', '<unk>', '@', 'ale', ',', 'perché', 'non', 'ci', 'sono', 'il', 'vi@@', '<unk>', '@', 've', 'di', 'questo', 'problema', 'speci@@', '<unk>', '@', 'ale', ',', 'non', 'ci', 'sono', 'il', 'vi@@', '<unk>', '@', 've', 'del', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 10:10:28,966 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:10:28,966 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:10:28,966 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza , che non è abbastanza il di<unk> @ seg<unk> @ no di questo problema speci<unk> @ ale , perché non ci sono il vi<unk> @ ve di questo problema speci<unk> @ ale , non ci sono il vi<unk> @ ve del ghi<unk> @ accio .
2025-05-28 10:10:28,966 - INFO - joeynmt.training - Example #2
2025-05-28 10:10:28,966 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:10:28,966 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:10:28,966 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'di', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ino', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', ',', 'il', 'cuore', 'del', 'nostro', 'sistema', 'peggi@@', '<unk>', '@', 'ore', 'globale', '.', '</s>']
2025-05-28 10:10:28,967 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:10:28,967 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:10:28,967 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , il cuore di ghi<unk> @ accio ar<unk> @ c<unk> @ ino ar<unk> @ t<unk> @ ico , il cuore del nostro sistema peggi<unk> @ ore globale .
2025-05-28 10:10:28,967 - INFO - joeynmt.training - Example #3
2025-05-28 10:10:28,967 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:10:28,967 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:10:28,967 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Pri@@', '<unk>', '@', 'ma', 'di', 'tutto', ',', 'il', 'p@@', '<unk>', '@', 'ane', 'e', 'lo', 's@@', '<unk>', '@', 'fondo', 'nell&apos;', 'est@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 10:10:28,968 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:10:28,968 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:10:28,968 - INFO - joeynmt.training - 	Hypothesis: Pri<unk> @ ma di tutto , il p<unk> @ ane e lo s<unk> @ fondo nell&apos; est<unk> @ ate .
2025-05-28 10:10:28,968 - INFO - joeynmt.training - Example #4
2025-05-28 10:10:28,968 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:10:28,968 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:10:28,968 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'o', 'è', 'un', 'di@@', '<unk>', '@', 'seg@@', '<unk>', '@', 'no', 'di', 'un', 'indi@@', '<unk>', '@', 'g@@', '<unk>', '@', 'n@@', '<unk>', '@', 'ato', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 10:10:28,969 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:10:28,969 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:10:28,969 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ o è un di<unk> @ seg<unk> @ no di un indi<unk> @ g<unk> @ n<unk> @ ato che è successo negli ultimi 25 anni .
2025-05-28 10:10:32,385 - INFO - joeynmt.training - Epoch  10, Step:    59100, Batch Loss:     1.072465, Batch Acc: 0.691217, Tokens per Sec:    20864, Lr: 0.000300
2025-05-28 10:10:35,774 - INFO - joeynmt.training - Epoch  10, Step:    59200, Batch Loss:     1.055485, Batch Acc: 0.685424, Tokens per Sec:    20927, Lr: 0.000300
2025-05-28 10:10:39,175 - INFO - joeynmt.training - Epoch  10, Step:    59300, Batch Loss:     0.972418, Batch Acc: 0.687957, Tokens per Sec:    21232, Lr: 0.000300
2025-05-28 10:10:42,554 - INFO - joeynmt.training - Epoch  10, Step:    59400, Batch Loss:     1.076579, Batch Acc: 0.686142, Tokens per Sec:    20520, Lr: 0.000300
2025-05-28 10:10:45,951 - INFO - joeynmt.training - Epoch  10, Step:    59500, Batch Loss:     1.085925, Batch Acc: 0.689885, Tokens per Sec:    20903, Lr: 0.000300
2025-05-28 10:10:45,951 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:10:45,952 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:10:56,986 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.25, ppl:   3.48, acc:   0.66, generation: 11.0240[sec], evaluation: 0.0000[sec]
2025-05-28 10:10:57,393 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/54000.ckpt
2025-05-28 10:10:57,421 - INFO - joeynmt.training - Example #0
2025-05-28 10:10:57,421 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:10:57,421 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:10:57,422 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'che', 'per', 'guardare', 'i', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ino', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'che', 'si', 'trov@@', '<unk>', '@', 'ava', 'in', 'modo', 'che', 'il', 'gi@@', '<unk>', '@', 'g@@', '<unk>', '@', 'ano', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'la', 'gr@@', '<unk>', '@', 'av@@', '<unk>', '@', 'ità', 'che', 'si', 'chiama', 'la', 'gr@@', '<unk>', '@', 'av@@', '<unk>', '@', 'ola', 'di', 'circa', 'il', '40', '%', 'dei', '40', 'per@@', '<unk>', '@', 'cento', '.', '</s>']
2025-05-28 10:10:57,422 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:10:57,422 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:10:57,422 - INFO - joeynmt.training - 	Hypothesis: Ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ che per guardare i ghi<unk> @ accio ar<unk> @ c<unk> @ ino ar<unk> @ t<unk> @ ico che si trov<unk> @ ava in modo che il gi<unk> @ g<unk> @ ano per tre milioni di anni , la gr<unk> @ av<unk> @ ità che si chiama la gr<unk> @ av<unk> @ ola di circa il 40 % dei 40 per<unk> @ cento .
2025-05-28 10:10:57,422 - INFO - joeynmt.training - Example #1
2025-05-28 10:10:57,423 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:10:57,423 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:10:57,423 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'abbastanza', 'la', 'n@@', '<unk>', '@', 'ave', 'per', 'la', 'prima', 'cosa', 'che', 'mostra', ',', 'non', 'è', 'il', 'vi@@', '<unk>', '@', 'st@@', '<unk>', '@', 'ore', 'mostra', 'il', 'vi@@', '<unk>', '@', 'll@@', '<unk>', '@', 'aggio', 'di', 'questo', 'particolare', 'problema', ',', 'non', 'è', 'il', 'vi@@', '<unk>', '@', 'll@@', '<unk>', '@', 'aggio', 'del', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 10:10:57,423 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:10:57,423 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:10:57,423 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza abbastanza la n<unk> @ ave per la prima cosa che mostra , non è il vi<unk> @ st<unk> @ ore mostra il vi<unk> @ ll<unk> @ aggio di questo particolare problema , non è il vi<unk> @ ll<unk> @ aggio del ghi<unk> @ accio .
2025-05-28 10:10:57,423 - INFO - joeynmt.training - Example #2
2025-05-28 10:10:57,424 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:10:57,424 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:10:57,424 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'pres@@', '<unk>', '@', 'enza', 'è', 'la', 'pres@@', '<unk>', '@', 'enza', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'atti@@', '<unk>', '@', 'va', 'il', 'cuore', 'del', 'nostro', 'sistema', 'peggi@@', '<unk>', '@', 'o', 'globale', '.', '</s>']
2025-05-28 10:10:57,424 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:10:57,424 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:10:57,424 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la pres<unk> @ enza è la pres<unk> @ enza ar<unk> @ c<unk> @ atti<unk> @ va il cuore del nostro sistema peggi<unk> @ o globale .
2025-05-28 10:10:57,424 - INFO - joeynmt.training - Example #3
2025-05-28 10:10:57,425 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:10:57,425 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:10:57,425 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'a', 'cresc@@', '<unk>', '@', 'ere', 'nel', 'vento', 'e', 'il', 'g@@', '<unk>', '@', 'all@@', '<unk>', '@', 'one', '.', '</s>']
2025-05-28 10:10:57,425 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:10:57,425 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:10:57,425 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ a cresc<unk> @ ere nel vento e il g<unk> @ all<unk> @ one .
2025-05-28 10:10:57,425 - INFO - joeynmt.training - Example #4
2025-05-28 10:10:57,426 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:10:57,426 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:10:57,426 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'a', 'è', 'quello', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 10:10:57,426 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:10:57,426 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:10:57,426 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ a è quello che è successo negli ultimi 25 anni .
2025-05-28 10:11:00,849 - INFO - joeynmt.training - Epoch  10, Step:    59600, Batch Loss:     1.076909, Batch Acc: 0.688771, Tokens per Sec:    18783, Lr: 0.000300
2025-05-28 10:11:04,238 - INFO - joeynmt.training - Epoch  10, Step:    59700, Batch Loss:     1.176496, Batch Acc: 0.688490, Tokens per Sec:    20453, Lr: 0.000300
2025-05-28 10:11:07,621 - INFO - joeynmt.training - Epoch  10, Step:    59800, Batch Loss:     1.150172, Batch Acc: 0.685180, Tokens per Sec:    20828, Lr: 0.000300
2025-05-28 10:11:11,003 - INFO - joeynmt.training - Epoch  10, Step:    59900, Batch Loss:     1.033101, Batch Acc: 0.688389, Tokens per Sec:    21499, Lr: 0.000300
2025-05-28 10:11:14,396 - INFO - joeynmt.training - Epoch  10, Step:    60000, Batch Loss:     1.325384, Batch Acc: 0.688675, Tokens per Sec:    21015, Lr: 0.000300
2025-05-28 10:11:14,396 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:11:14,396 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:11:25,372 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.24, ppl:   3.47, acc:   0.66, generation: 10.9648[sec], evaluation: 0.0000[sec]
2025-05-28 10:11:25,797 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/59500.ckpt
2025-05-28 10:11:25,826 - INFO - joeynmt.training - Example #0
2025-05-28 10:11:25,826 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:11:25,826 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:11:25,826 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'vedere', 'che', 'l&apos;', 'ic@@', '<unk>', '@', 'ona', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ato', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'che', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'per', 'cento', '.', '</s>']
2025-05-28 10:11:25,827 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:11:25,827 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:11:25,827 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per vedere che l&apos; ic<unk> @ ona ar<unk> @ t<unk> @ ic<unk> @ ato ar<unk> @ t<unk> @ ico che per tre milioni di anni , per cento .
2025-05-28 10:11:25,827 - INFO - joeynmt.training - Example #1
2025-05-28 10:11:25,827 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:11:25,827 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:11:25,828 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', ',', 'ma', 'il', 'tr@@', '<unk>', '@', 'uc@@', '<unk>', '@', 'co', 'di', 'questo', 'problema', 'speci@@', '<unk>', '@', 'ale', ',', 'perché', 'non', 'lo', 'mostra', 'il', 'soff@@', '<unk>', '@', 'itto', 'di', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 10:11:25,828 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:11:25,828 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:11:25,828 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza , ma il tr<unk> @ uc<unk> @ co di questo problema speci<unk> @ ale , perché non lo mostra il soff<unk> @ itto di ghi<unk> @ accio .
2025-05-28 10:11:25,828 - INFO - joeynmt.training - Example #2
2025-05-28 10:11:25,828 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:11:25,828 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:11:25,829 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'cer@@', '<unk>', '@', 'ti', 'sen@@', '<unk>', '@', 'si', ',', 'il', 'cuore', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'è', 'il', 'cuore', 'di', 'un', 'sistema', 'di', 'cat@@', '<unk>', '@', 'ap@@', '<unk>', '@', 'i@@', '<unk>', '@', 'anto', 'del', 'nostro', 'sistema', 'globale', '.', '</s>']
2025-05-28 10:11:25,829 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:11:25,829 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:11:25,829 - INFO - joeynmt.training - 	Hypothesis: In cer<unk> @ ti sen<unk> @ si , il cuore ar<unk> @ t<unk> @ ico è il cuore di un sistema di cat<unk> @ ap<unk> @ i<unk> @ anto del nostro sistema globale .
2025-05-28 10:11:25,829 - INFO - joeynmt.training - Example #3
2025-05-28 10:11:25,829 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:11:25,829 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:11:25,829 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'è', 'che', 'cres@@', '<unk>', '@', 'ce', 'nel', 'vento', 'e', 's@@', '<unk>', '@', 'os@@', '<unk>', '@', 'so', 'nel', 'sal@@', '<unk>', '@', 'to', 'e', 's@@', '<unk>', '@', 'cos@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ente', '.', '</s>']
2025-05-28 10:11:25,830 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:11:25,830 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:11:25,830 - INFO - joeynmt.training - 	Hypothesis: La prima è che cres<unk> @ ce nel vento e s<unk> @ os<unk> @ so nel sal<unk> @ to e s<unk> @ cos<unk> @ c<unk> @ ente .
2025-05-28 10:11:25,830 - INFO - joeynmt.training - Example #4
2025-05-28 10:11:25,830 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:11:25,830 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:11:25,830 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'a', 'è', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'cosa', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 10:11:25,831 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:11:25,831 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:11:25,831 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ a è un seg<unk> @ no di cosa è successo negli ultimi 25 anni .
2025-05-28 10:11:29,244 - INFO - joeynmt.training - Epoch  10, Step:    60100, Batch Loss:     1.188798, Batch Acc: 0.690161, Tokens per Sec:    17871, Lr: 0.000300
2025-05-28 10:11:32,629 - INFO - joeynmt.training - Epoch  10, Step:    60200, Batch Loss:     1.137451, Batch Acc: 0.685339, Tokens per Sec:    21324, Lr: 0.000300
2025-05-28 10:11:36,011 - INFO - joeynmt.training - Epoch  10, Step:    60300, Batch Loss:     1.221134, Batch Acc: 0.686779, Tokens per Sec:    20690, Lr: 0.000300
2025-05-28 10:11:39,422 - INFO - joeynmt.training - Epoch  10, Step:    60400, Batch Loss:     1.196828, Batch Acc: 0.689611, Tokens per Sec:    21117, Lr: 0.000300
2025-05-28 10:11:42,808 - INFO - joeynmt.training - Epoch  10, Step:    60500, Batch Loss:     1.188135, Batch Acc: 0.686346, Tokens per Sec:    21080, Lr: 0.000300
2025-05-28 10:11:42,809 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:11:42,809 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:11:54,428 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.24, ppl:   3.47, acc:   0.66, generation: 11.6087[sec], evaluation: 0.0000[sec]
2025-05-28 10:11:55,004 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/57000.ckpt
2025-05-28 10:11:55,026 - INFO - joeynmt.training - Example #0
2025-05-28 10:11:55,026 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:11:55,026 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:11:55,026 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'vedere', 'che', 'la', 'gi@@', '<unk>', '@', 'ù', 'per', 'la', 'quale', 'i', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ava', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'la', 'dimen@@', '<unk>', '@', 'sione', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ale', 'che', 'ha', 'studi@@', '<unk>', '@', 'ato', 'il', '4@@', '<unk>', '@', '8', 'stati', 's@@', '<unk>', '@', 'otti@@', '<unk>', '@', 'li', '.', '</s>']
2025-05-28 10:11:55,027 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:11:55,027 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:11:55,027 - INFO - joeynmt.training - 	Hypothesis: Ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per vedere che la gi<unk> @ ù per la quale i ghi<unk> @ accio ar<unk> @ c<unk> @ ic<unk> @ ava per tre milioni di anni , la dimen<unk> @ sione ar<unk> @ t<unk> @ ale che ha studi<unk> @ ato il 4<unk> @ 8 stati s<unk> @ otti<unk> @ li .
2025-05-28 10:11:55,027 - INFO - joeynmt.training - Example #1
2025-05-28 10:11:55,028 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:11:55,028 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:11:55,028 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'un', 'sacco', 'di', 'cose', 'che', 'non', 'è', 'abbastanza', 'il', 'tr@@', '<unk>', '@', 'uc@@', '<unk>', '@', 'co', 'di', 'questo', 'problema', 'speci@@', '<unk>', '@', 'ale', ',', 'perché', 'non', 'lo', 'mostra', 'il', 'vi@@', '<unk>', '@', 'vo', 'del', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 10:11:55,028 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:11:55,028 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:11:55,028 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è un sacco di cose che non è abbastanza il tr<unk> @ uc<unk> @ co di questo problema speci<unk> @ ale , perché non lo mostra il vi<unk> @ vo del ghi<unk> @ accio .
2025-05-28 10:11:55,028 - INFO - joeynmt.training - Example #2
2025-05-28 10:11:55,029 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:11:55,029 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:11:55,029 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'di', 'un', 'sistema', 'di', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', ',', 'il', 'cuore', 'di', 'un', 'sistema', 'di', 'mat@@', '<unk>', '@', 'ri@@', '<unk>', '@', 'mon@@', '<unk>', '@', 'io', 'globale', '.', '</s>']
2025-05-28 10:11:55,029 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:11:55,029 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:11:55,029 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , il cuore di un sistema di ghi<unk> @ accio ar<unk> @ t<unk> @ ico , il cuore di un sistema di mat<unk> @ ri<unk> @ mon<unk> @ io globale .
2025-05-28 10:11:55,029 - INFO - joeynmt.training - Example #3
2025-05-28 10:11:55,030 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:11:55,030 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:11:55,030 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prima', 'volta', 'che', 'il', 'vento', 'e', 's@@', '<unk>', '@', 'ur@@', '<unk>', '@', 'no', '.', '</s>']
2025-05-28 10:11:55,030 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:11:55,030 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:11:55,030 - INFO - joeynmt.training - 	Hypothesis: La prima volta che il vento e s<unk> @ ur<unk> @ no .
2025-05-28 10:11:55,030 - INFO - joeynmt.training - Example #4
2025-05-28 10:11:55,030 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:11:55,030 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:11:55,030 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 's@@', '<unk>', '@', 'li@@', '<unk>', '@', 'de', 'che', 'vi', 'mostr@@', '<unk>', '@', 'i', 'è', 'un', 'segn@@', '<unk>', '@', 'ale', 'che', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 10:11:55,031 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:11:55,031 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:11:55,031 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma s<unk> @ li<unk> @ de che vi mostr<unk> @ i è un segn<unk> @ ale che è successo negli ultimi 25 anni .
2025-05-28 10:11:58,461 - INFO - joeynmt.training - Epoch  10, Step:    60600, Batch Loss:     1.193599, Batch Acc: 0.686658, Tokens per Sec:    18016, Lr: 0.000300
2025-05-28 10:12:01,832 - INFO - joeynmt.training - Epoch  10, Step:    60700, Batch Loss:     1.064490, Batch Acc: 0.687452, Tokens per Sec:    21075, Lr: 0.000300
2025-05-28 10:12:05,207 - INFO - joeynmt.training - Epoch  10, Step:    60800, Batch Loss:     1.139088, Batch Acc: 0.685687, Tokens per Sec:    21309, Lr: 0.000300
2025-05-28 10:12:08,560 - INFO - joeynmt.training - Epoch  10, Step:    60900, Batch Loss:     1.061037, Batch Acc: 0.684654, Tokens per Sec:    20858, Lr: 0.000300
2025-05-28 10:12:11,929 - INFO - joeynmt.training - Epoch  10, Step:    61000, Batch Loss:     1.212224, Batch Acc: 0.686627, Tokens per Sec:    21478, Lr: 0.000300
2025-05-28 10:12:11,929 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:12:11,929 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:12:23,789 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.24, ppl:   3.46, acc:   0.67, generation: 11.8484[sec], evaluation: 0.0000[sec]
2025-05-28 10:12:24,245 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/60500.ckpt
2025-05-28 10:12:24,276 - INFO - joeynmt.training - Example #0
2025-05-28 10:12:24,277 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:12:24,277 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:12:24,277 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 's@@', '<unk>', '@', 'li@@', '<unk>', '@', 'de', 'per', 'vedere', 'che', 'i', 't@@', '<unk>', '@', 'av@@', '<unk>', '@', 'ent@@', '<unk>', '@', 'emente', 'per', 'la', 'quale', 'la', 'parte', 'del', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 'to', 'che', 'i', 'ghi@@', '<unk>', '@', 'accio', 'che', 'si', 'chiam@@', '<unk>', '@', 'ava', 'la', 'gr@@', '<unk>', '@', 'ad@@', '<unk>', '@', 'de', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'il', '40', 'per', 'cento', 'dei', '40', 'anni', '.', '</s>']
2025-05-28 10:12:24,277 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:12:24,278 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:12:24,278 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due s<unk> @ li<unk> @ de per vedere che i t<unk> @ av<unk> @ ent<unk> @ emente per la quale la parte del ghi<unk> @ accio ar<unk> @ to che i ghi<unk> @ accio che si chiam<unk> @ ava la gr<unk> @ ad<unk> @ de per tre milioni di anni , il 40 per cento dei 40 anni .
2025-05-28 10:12:24,278 - INFO - joeynmt.training - Example #1
2025-05-28 10:12:24,278 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:12:24,278 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:12:24,278 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'forte', 'che', 'la', 'sost@@', '<unk>', '@', 'anza', 'di', 'questo', 'problema', 'speci@@', '<unk>', '@', 'ale', ',', 'perché', 'non', 'è', 'il', 'bu@@', '<unk>', '@', 'ff@@', '<unk>', '@', 'ico', 'che', 'mostra', 'la', 'di@@', '<unk>', '@', 'mostra', 'di', 'questo', 'problema', 'che', 'non', 'è', 'il', 'bu@@', '<unk>', '@', 'io', 'del', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 10:12:24,279 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:12:24,279 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:12:24,279 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza forte che la sost<unk> @ anza di questo problema speci<unk> @ ale , perché non è il bu<unk> @ ff<unk> @ ico che mostra la di<unk> @ mostra di questo problema che non è il bu<unk> @ io del ghi<unk> @ accio .
2025-05-28 10:12:24,279 - INFO - joeynmt.training - Example #2
2025-05-28 10:12:24,279 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:12:24,279 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:12:24,279 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'di', 'un', 'sistema', 'di', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ino', 'è', 'il', 'cuore', 'del', 'nostro', 'sistema', 'globale', '.', '</s>']
2025-05-28 10:12:24,280 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:12:24,280 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:12:24,280 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , il cuore di un sistema di ghi<unk> @ accio ar<unk> @ c<unk> @ ino è il cuore del nostro sistema globale .
2025-05-28 10:12:24,280 - INFO - joeynmt.training - Example #3
2025-05-28 10:12:24,280 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:12:24,280 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:12:24,280 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&apos;', 'una', 'volta', 'che', 'si', 'sta', 'in@@', '<unk>', '@', 'ver@@', '<unk>', '@', 'no', ',', 'nel', 's@@', '<unk>', '@', 'fondo', '.', '</s>']
2025-05-28 10:12:24,281 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:12:24,281 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:12:24,281 - INFO - joeynmt.training - 	Hypothesis: E &apos; una volta che si sta in<unk> @ ver<unk> @ no , nel s<unk> @ fondo .
2025-05-28 10:12:24,281 - INFO - joeynmt.training - Example #4
2025-05-28 10:12:24,281 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:12:24,281 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:12:24,281 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 's@@', '<unk>', '@', 'li@@', '<unk>', '@', 'de', 'che', 'vi', 'mostr@@', '<unk>', '@', 'i', 'è', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'tempo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 10:12:24,281 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:12:24,282 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:12:24,282 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma s<unk> @ li<unk> @ de che vi mostr<unk> @ i è un seg<unk> @ no di tempo negli ultimi 25 anni .
2025-05-28 10:12:27,701 - INFO - joeynmt.training - Epoch  10, Step:    61100, Batch Loss:     1.153717, Batch Acc: 0.684966, Tokens per Sec:    18408, Lr: 0.000210
2025-05-28 10:12:31,094 - INFO - joeynmt.training - Epoch  10, Step:    61200, Batch Loss:     1.092648, Batch Acc: 0.691206, Tokens per Sec:    21451, Lr: 0.000210
2025-05-28 10:12:34,511 - INFO - joeynmt.training - Epoch  10, Step:    61300, Batch Loss:     1.127844, Batch Acc: 0.691587, Tokens per Sec:    21045, Lr: 0.000210
2025-05-28 10:12:37,890 - INFO - joeynmt.training - Epoch  10, Step:    61400, Batch Loss:     1.173527, Batch Acc: 0.692141, Tokens per Sec:    21036, Lr: 0.000210
2025-05-28 10:12:41,283 - INFO - joeynmt.training - Epoch  10, Step:    61500, Batch Loss:     1.083120, Batch Acc: 0.693562, Tokens per Sec:    21256, Lr: 0.000210
2025-05-28 10:12:41,283 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:12:41,283 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:12:52,890 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.23, ppl:   3.41, acc:   0.67, generation: 11.5957[sec], evaluation: 0.0000[sec]
2025-05-28 10:12:52,890 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 10:12:53,686 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/57500.ckpt
2025-05-28 10:12:53,715 - INFO - joeynmt.training - Example #0
2025-05-28 10:12:53,716 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:12:53,716 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:12:53,716 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 's@@', '<unk>', '@', 'li@@', '<unk>', '@', 'de', 'per', 'vedere', 'che', 'il', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'che', 'il', 'ghi@@', '<unk>', '@', 'accio', 'della', 'terra', 'ha', 'studi@@', '<unk>', '@', 'ato', 'il', 't@@', '<unk>', '@', 'asso', 'di', 's@@', '<unk>', '@', 'ett@@', '<unk>', '@', 'em@@', '<unk>', '@', 'po', ',', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'il', '40', ',', '40', '%', '.', '</s>']
2025-05-28 10:12:53,717 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:12:53,717 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:12:53,717 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due s<unk> @ li<unk> @ de per vedere che il ghi<unk> @ accio ar<unk> @ t<unk> @ ico ar<unk> @ t<unk> @ ico che il ghi<unk> @ accio della terra ha studi<unk> @ ato il t<unk> @ asso di s<unk> @ ett<unk> @ em<unk> @ po , per tre milioni di anni , il 40 , 40 % .
2025-05-28 10:12:53,717 - INFO - joeynmt.training - Example #1
2025-05-28 10:12:53,717 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:12:53,718 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:12:53,718 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'questo', 'non', 'è', 'abbastanza', 'forte', 'che', 'la', 'sost@@', '<unk>', '@', 'anzi@@', '<unk>', '@', 'almente', 'la', 'paura', 'di', 'questo', 'problema', 'speci@@', '<unk>', '@', 'ale', ',', 'perché', 'non', 'è', 'il', 'vi@@', '<unk>', '@', 'st@@', '<unk>', '@', 'izia', '.', '</s>']
2025-05-28 10:12:53,718 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:12:53,718 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:12:53,718 - INFO - joeynmt.training - 	Hypothesis: Ma questo non è abbastanza forte che la sost<unk> @ anzi<unk> @ almente la paura di questo problema speci<unk> @ ale , perché non è il vi<unk> @ st<unk> @ izia .
2025-05-28 10:12:53,718 - INFO - joeynmt.training - Example #2
2025-05-28 10:12:53,719 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:12:53,719 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:12:53,719 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'di', 'un', 'c@@', '<unk>', '@', 'ru@@', '<unk>', '@', 'olo', ',', 'il', 'cuore', 'di', 'un', 'sistema', 'intelli@@', '<unk>', '@', 'gente', 'che', 'fa', 'il', 'cuore', 'del', 'nostro', 'sistema', 'di', 'cat@@', '<unk>', '@', 'ena', 'globale', '.', '</s>']
2025-05-28 10:12:53,719 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:12:53,719 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:12:53,719 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , il cuore di un c<unk> @ ru<unk> @ olo , il cuore di un sistema intelli<unk> @ gente che fa il cuore del nostro sistema di cat<unk> @ ena globale .
2025-05-28 10:12:53,719 - INFO - joeynmt.training - Example #3
2025-05-28 10:12:53,720 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:12:53,720 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:12:53,720 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E', '&apos;', 'che', 'cres@@', '<unk>', '@', 'ce', 'nel', 'vento', 'e', 's@@', '<unk>', '@', 'fondo', 'in', 'est@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 10:12:53,720 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:12:53,720 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:12:53,720 - INFO - joeynmt.training - 	Hypothesis: E &apos; che cres<unk> @ ce nel vento e s<unk> @ fondo in est<unk> @ ate .
2025-05-28 10:12:53,720 - INFO - joeynmt.training - Example #4
2025-05-28 10:12:53,720 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:12:53,720 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:12:53,720 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 's@@', '<unk>', '@', 'li@@', '<unk>', '@', 'de', 'che', 'vi', 'mostr@@', '<unk>', '@', 'i', 'è', 'un', 'seg@@', '<unk>', '@', 'no', 'di', 'tempo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 10:12:53,721 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:12:53,721 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:12:53,721 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma s<unk> @ li<unk> @ de che vi mostr<unk> @ i è un seg<unk> @ no di tempo negli ultimi 25 anni .
2025-05-28 10:12:57,172 - INFO - joeynmt.training - Epoch  10, Step:    61600, Batch Loss:     1.220577, Batch Acc: 0.689845, Tokens per Sec:    17231, Lr: 0.000210
2025-05-28 10:13:00,561 - INFO - joeynmt.training - Epoch  10, Step:    61700, Batch Loss:     1.085123, Batch Acc: 0.689902, Tokens per Sec:    21237, Lr: 0.000210
2025-05-28 10:13:03,961 - INFO - joeynmt.training - Epoch  10, Step:    61800, Batch Loss:     1.049735, Batch Acc: 0.690548, Tokens per Sec:    21091, Lr: 0.000210
2025-05-28 10:13:07,366 - INFO - joeynmt.training - Epoch  10, Step:    61900, Batch Loss:     0.962747, Batch Acc: 0.691816, Tokens per Sec:    20897, Lr: 0.000210
2025-05-28 10:13:10,753 - INFO - joeynmt.training - Epoch  10, Step:    62000, Batch Loss:     1.064100, Batch Acc: 0.692897, Tokens per Sec:    21278, Lr: 0.000210
2025-05-28 10:13:10,753 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:13:10,754 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:13:21,861 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.22, ppl:   3.38, acc:   0.67, generation: 11.0973[sec], evaluation: 0.0000[sec]
2025-05-28 10:13:21,862 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 10:13:22,454 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/60000.ckpt
2025-05-28 10:13:22,481 - INFO - joeynmt.training - Example #0
2025-05-28 10:13:22,482 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:13:22,482 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:13:22,482 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'vedere', 'che', 'i', 'ghi@@', '<unk>', '@', 'accio', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ic@@', '<unk>', '@', 'ale', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ico', 'che', 'ha', 'in@@', '<unk>', '@', 'cor@@', '<unk>', '@', 're', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'la', 'gr@@', '<unk>', '@', 'av@@', '<unk>', '@', 'ola', 'di', '4@@', '<unk>', '@', '8', 'stati', 'in@@', '<unk>', '@', 'vi@@', '<unk>', '@', 'ati', '.', '</s>']
2025-05-28 10:13:22,482 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:13:22,482 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:13:22,482 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per vedere che i ghi<unk> @ accio ar<unk> @ t<unk> @ ic<unk> @ ale ar<unk> @ t<unk> @ ico che ha in<unk> @ cor<unk> @ re per tre milioni di anni , la gr<unk> @ av<unk> @ ola di 4<unk> @ 8 stati in<unk> @ vi<unk> @ ati .
2025-05-28 10:13:22,482 - INFO - joeynmt.training - Example #1
2025-05-28 10:13:22,483 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:13:22,483 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:13:22,483 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'forte', ',', 'perché', 'non', 'è', 'abbastanza', 'il', 'vi@@', '<unk>', '@', 'st@@', '<unk>', '@', 'ico', 'che', 'mostra', 'questo', 'problema', 'speci@@', '<unk>', '@', 'ale', ',', 'non', 'è', 'il', 'vi@@', '<unk>', '@', 'll@@', '<unk>', '@', 'aggio', 'del', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 10:13:22,483 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:13:22,483 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:13:22,483 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte , perché non è abbastanza il vi<unk> @ st<unk> @ ico che mostra questo problema speci<unk> @ ale , non è il vi<unk> @ ll<unk> @ aggio del ghi<unk> @ accio .
2025-05-28 10:13:22,483 - INFO - joeynmt.training - Example #2
2025-05-28 10:13:22,484 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:13:22,484 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:13:22,484 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ur@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ano', 'è', 'il', 'cuore', 'ar@@', '<unk>', '@', 'ico', 'del', 'nostro', 'sistema', 'globale', 'globale', '.', '</s>']
2025-05-28 10:13:22,484 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:13:22,484 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:13:22,484 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , il cuore ar<unk> @ t<unk> @ ur<unk> @ i<unk> @ ano è il cuore ar<unk> @ ico del nostro sistema globale globale .
2025-05-28 10:13:22,484 - INFO - joeynmt.training - Example #3
2025-05-28 10:13:22,485 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:13:22,485 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:13:22,485 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', '@', 'er@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ano', 'nel', 'vento', 'e', 'si', 's@@', '<unk>', '@', 'oc@@', '<unk>', '@', 'corso', 'in', 'est@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 10:13:22,485 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:13:22,485 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:13:22,485 - INFO - joeynmt.training - 	Hypothesis: C<unk> @ er<unk> @ c<unk> @ ano nel vento e si s<unk> @ oc<unk> @ corso in est<unk> @ ate .
2025-05-28 10:13:22,485 - INFO - joeynmt.training - Example #4
2025-05-28 10:13:22,486 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:13:22,486 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:13:22,486 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'i', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 10:13:22,486 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:13:22,486 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:13:22,486 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ i è successo negli ultimi 25 anni .
2025-05-28 10:13:25,882 - INFO - joeynmt.training - Epoch  10, Step:    62100, Batch Loss:     0.935900, Batch Acc: 0.693392, Tokens per Sec:    18083, Lr: 0.000210
2025-05-28 10:13:29,287 - INFO - joeynmt.training - Epoch  10, Step:    62200, Batch Loss:     1.179908, Batch Acc: 0.690752, Tokens per Sec:    21452, Lr: 0.000210
2025-05-28 10:13:32,695 - INFO - joeynmt.training - Epoch  10, Step:    62300, Batch Loss:     1.172634, Batch Acc: 0.688941, Tokens per Sec:    21339, Lr: 0.000210
2025-05-28 10:13:36,085 - INFO - joeynmt.training - Epoch  10, Step:    62400, Batch Loss:     1.198507, Batch Acc: 0.693299, Tokens per Sec:    21053, Lr: 0.000210
2025-05-28 10:13:39,490 - INFO - joeynmt.training - Epoch  10, Step:    62500, Batch Loss:     1.029456, Batch Acc: 0.694736, Tokens per Sec:    21245, Lr: 0.000210
2025-05-28 10:13:39,491 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:13:39,491 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:13:50,305 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.21, ppl:   3.36, acc:   0.67, generation: 10.8039[sec], evaluation: 0.0000[sec]
2025-05-28 10:13:50,306 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 10:13:50,896 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/56000.ckpt
2025-05-28 10:13:50,924 - INFO - joeynmt.training - Example #0
2025-05-28 10:13:50,925 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:13:50,925 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:13:50,925 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', ',', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 's@@', '<unk>', '@', 'li@@', '<unk>', '@', 'de', 'per', 'vedere', 'che', 'la', 'c@@', '<unk>', '@', 'av@@', '<unk>', '@', 'ola', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'che', 'era', 'chiam@@', '<unk>', '@', 'ata', 'l&apos;', 'E@@', '<unk>', '@', 'is@@', '<unk>', '@', 'k@@', '<unk>', '@', 'et', 'per', 'tre', 'milioni', 'di', 'anni', ',', 'il', '40', 'per@@', '<unk>', '@', 'cento', '.', '</s>']
2025-05-28 10:13:50,926 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:13:50,926 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:13:50,926 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso , ho mostr<unk> @ ato queste due s<unk> @ li<unk> @ de per vedere che la c<unk> @ av<unk> @ ola ar<unk> @ t<unk> @ ica ar<unk> @ t<unk> @ ica che era chiam<unk> @ ata l&apos; E<unk> @ is<unk> @ k<unk> @ et per tre milioni di anni , il 40 per<unk> @ cento .
2025-05-28 10:13:50,926 - INFO - joeynmt.training - Example #1
2025-05-28 10:13:50,926 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:13:50,926 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:13:50,927 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'forte', 'che', 'la', 'sost@@', '<unk>', '@', 'anza', 'di', 'questo', 'particolare', 'problema', ',', 'perché', 'non', 'è', 'il', 'di@@', '<unk>', '@', 'mostra', 'il', 'vi@@', '<unk>', '@', 'st@@', '<unk>', '@', 'ico', 'che', 'mostra', 'il', 'vi@@', '<unk>', '@', 'vo', '.', '</s>']
2025-05-28 10:13:50,927 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:13:50,927 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:13:50,927 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte che la sost<unk> @ anza di questo particolare problema , perché non è il di<unk> @ mostra il vi<unk> @ st<unk> @ ico che mostra il vi<unk> @ vo .
2025-05-28 10:13:50,927 - INFO - joeynmt.training - Example #2
2025-05-28 10:13:50,927 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:13:50,928 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:13:50,928 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ur@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ano', 'il', 'cuore', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ur@@', '<unk>', '@', 'i@@', '<unk>', '@', 'ana', '.', '</s>']
2025-05-28 10:13:50,928 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:13:50,928 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:13:50,928 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , il cuore ar<unk> @ t<unk> @ ur<unk> @ i<unk> @ ano il cuore ar<unk> @ t<unk> @ ur<unk> @ i<unk> @ ana .
2025-05-28 10:13:50,928 - INFO - joeynmt.training - Example #3
2025-05-28 10:13:50,928 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:13:50,928 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:13:50,929 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['C@@', '<unk>', '@', 'in@@', '<unk>', '@', 'ce', 'nel', 'vento', 'e', 'si', 's@@', '<unk>', '@', 'ente', 'in', 'est@@', '<unk>', '@', 'ate', 'e', 's@@', '<unk>', '@', 'os@@', '<unk>', '@', 'c@@', '<unk>', '@', 'ano', '.', '</s>']
2025-05-28 10:13:50,929 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:13:50,929 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:13:50,929 - INFO - joeynmt.training - 	Hypothesis: C<unk> @ in<unk> @ ce nel vento e si s<unk> @ ente in est<unk> @ ate e s<unk> @ os<unk> @ c<unk> @ ano .
2025-05-28 10:13:50,929 - INFO - joeynmt.training - Example #4
2025-05-28 10:13:50,929 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:13:50,929 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:13:50,929 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 's@@', '<unk>', '@', 'li@@', '<unk>', '@', 'de', 'che', 'vi', 'mostr@@', '<unk>', '@', 'erò', 'è', 'una', 'ri@@', '<unk>', '@', 'vel@@', '<unk>', '@', 'azione', 'di', 'cosa', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 10:13:50,930 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:13:50,930 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:13:50,930 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma s<unk> @ li<unk> @ de che vi mostr<unk> @ erò è una ri<unk> @ vel<unk> @ azione di cosa è successo negli ultimi 25 anni .
2025-05-28 10:13:54,323 - INFO - joeynmt.training - Epoch  10, Step:    62600, Batch Loss:     0.939365, Batch Acc: 0.696516, Tokens per Sec:    17813, Lr: 0.000210
2025-05-28 10:13:57,695 - INFO - joeynmt.training - Epoch  10, Step:    62700, Batch Loss:     1.037012, Batch Acc: 0.689456, Tokens per Sec:    20742, Lr: 0.000210
2025-05-28 10:14:01,061 - INFO - joeynmt.training - Epoch  10, Step:    62800, Batch Loss:     1.004199, Batch Acc: 0.691946, Tokens per Sec:    20388, Lr: 0.000210
2025-05-28 10:14:04,458 - INFO - joeynmt.training - Epoch  10, Step:    62900, Batch Loss:     1.204264, Batch Acc: 0.691250, Tokens per Sec:    21005, Lr: 0.000210
2025-05-28 10:14:07,869 - INFO - joeynmt.training - Epoch  10, Step:    63000, Batch Loss:     1.269353, Batch Acc: 0.698513, Tokens per Sec:    21253, Lr: 0.000210
2025-05-28 10:14:07,870 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:14:07,870 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:14:18,925 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.21, ppl:   3.36, acc:   0.67, generation: 11.0448[sec], evaluation: 0.0000[sec]
2025-05-28 10:14:18,926 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2025-05-28 10:14:19,503 - INFO - joeynmt.helpers - delete models/bpe_5k_de_it/61000.ckpt
2025-05-28 10:14:19,527 - INFO - joeynmt.training - Example #0
2025-05-28 10:14:19,528 - DEBUG - joeynmt.training - 	Tokenized source:     ['L@@', '@@@', '@', 'etz@@', '@@@', '@', 'tes', 'Jahr', 'habe', 'ich', 'diese', 'beiden', 'Fol@@', '@@@', '@', 'ien', 'ge@@', '@@@', '@', 'zeigt', ',', 'um', 'zu', 'ver@@', '@@@', '@', 'an@@', '@@@', '@', 'schau@@', '@@@', '@', 'lichen', ',', 'dass', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', ',', 'die', 'für', 'ann@@', '@@@', '@', 'ä@@', '@@@', '@', 'her@@', '@@@', '@', 'n@@', '@@@', '@', 'd', 'drei', 'Millionen', 'Jahre', 'die', 'Gr@@', '@@@', '@', 'ös@@', '@@@', '@', 'se', 'der', 'unter@@', '@@@', '@', 'en', '4@@', '@@@', '@', '8', 'Staaten', 'hatte', ',', 'um', '40', 'Prozent', 'gesch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'ist', '.']
2025-05-28 10:14:19,528 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '@@@', '@', 'ato', 'queste', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 've', 'per', 'dimostr@@', '@@@', '@', 'are', 'che', 'la', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', ',', 'che', 'per', 'quasi', 'tre', 'milioni', 'di', 'anni', 'ha', 'avuto', 'le', 'dimen@@', '@@@', '@', 'sioni', 'dei', '4@@', '@@@', '@', '8', 'Stati', 'Uniti', 'cont@@', '@@@', '@', 'in@@', '@@@', '@', 'ent@@', '@@@', '@', 'ali', ',', 'si', 'è', 'ri@@', '@@@', '@', 'str@@', '@@@', '@', 'etta', 'del', '40', '%', '.']
2025-05-28 10:14:19,528 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L&apos;', 'anno', 'scorso', 'ho', 'mostr@@', '<unk>', '@', 'ato', 'queste', 'due', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 've', 'per', 'vedere', 'che', 'la', 'gr@@', '<unk>', '@', 'av@@', '<unk>', '@', 'ità', 'ar@@', '<unk>', '@', 't@@', '<unk>', '@', 'ica', 'che', 'ha', 'avuto', 'la', 'causa', 'dell&apos;', 'ic@@', '<unk>', '@', 'ona', 'del', '4@@', '<unk>', '@', '8', '%', 'di', 'terra', ',', 'e', '&apos;', '40', '%', 'di', 'circa', 'il', '40', '%', '.', '</s>']
2025-05-28 10:14:19,529 - INFO - joeynmt.training - 	Source:     L@@ etz@@ tes Jahr habe ich diese beiden Fol@@ ien ge@@ zeigt , um zu ver@@ an@@ schau@@ lichen , dass die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e , die für ann@@ ä@@ her@@ n@@ d drei Millionen Jahre die Gr@@ ös@@ se der unter@@ en 4@@ 8 Staaten hatte , um 40 Prozent gesch@@ r@@ um@@ p@@ ft ist .
2025-05-28 10:14:19,529 - INFO - joeynmt.training - 	Reference:  L&apos; anno scorso ho mostr@@ ato queste di@@ apos@@ iti@@ ve per dimostr@@ are che la cal@@ otta gl@@ a@@ ci@@ ale art@@ ica , che per quasi tre milioni di anni ha avuto le dimen@@ sioni dei 4@@ 8 Stati Uniti cont@@ in@@ ent@@ ali , si è ri@@ str@@ etta del 40 % .
2025-05-28 10:14:19,529 - INFO - joeynmt.training - 	Hypothesis: L&apos; anno scorso ho mostr<unk> @ ato queste due di<unk> @ apos<unk> @ iti<unk> @ ve per vedere che la gr<unk> @ av<unk> @ ità ar<unk> @ t<unk> @ ica che ha avuto la causa dell&apos; ic<unk> @ ona del 4<unk> @ 8 % di terra , e &apos; 40 % di circa il 40 % .
2025-05-28 10:14:19,529 - INFO - joeynmt.training - Example #1
2025-05-28 10:14:19,529 - DEBUG - joeynmt.training - 	Tokenized source:     ['Aber', 'dies', 'dr@@', '@@@', '@', 'ückt', 'nicht', 'star@@', '@@@', '@', 'k', 'genug', 'die', 'Er@@', '@@@', '@', 'n@@', '@@@', '@', 'st@@', '@@@', '@', 'ha@@', '@@@', '@', 'f@@', '@@@', '@', 'tigkeit', 'dieses', 'spezi@@', '@@@', '@', 'ellen', 'Probl@@', '@@@', '@', 'em@@', '@@@', '@', 's', 'aus', ',', 'da', 'es', 'nicht', 'die', 'D@@', '@@@', '@', 'ic@@', '@@@', '@', 'ke', 'des', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'es', 'zeigt', '.']
2025-05-28 10:14:19,529 - DEBUG - joeynmt.training - 	Tokenized reference:  ['T@@', '@@@', '@', 'utt@@', '@@@', '@', 'a@@', '@@@', '@', 'via', 'questo', 'sott@@', '@@@', '@', 'ov@@', '@@@', '@', 'al@@', '@@@', '@', 'uta', 'la', 'gr@@', '@@@', '@', 'av@@', '@@@', '@', 'ità', 'del', 'problema', 'perché', 'non', 'mostra', 'lo', 'sp@@', '@@@', '@', 'ess@@', '@@@', '@', 'ore', 'del', 'ghi@@', '@@@', '@', 'accio', '.']
2025-05-28 10:14:19,530 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['Ma', 'non', 'è', 'abbastanza', 'forte', 'la', 'sost@@', '<unk>', '@', 'anza', 'di', 'questo', 'particolare', 'problema', ',', 'non', 'è', 'abbastanza', 'la', 'sost@@', '<unk>', '@', 'anza', 'di', 'questo', 'particolare', 'problema', ',', 'non', 'lo', 'è', 'il', 'vi@@', '<unk>', '@', 'll@@', '<unk>', '@', 'aggio', 'del', 'ghi@@', '<unk>', '@', 'accio', '.', '</s>']
2025-05-28 10:14:19,530 - INFO - joeynmt.training - 	Source:     Aber dies dr@@ ückt nicht star@@ k genug die Er@@ n@@ st@@ ha@@ f@@ tigkeit dieses spezi@@ ellen Probl@@ em@@ s aus , da es nicht die D@@ ic@@ ke des E@@ is@@ es zeigt .
2025-05-28 10:14:19,530 - INFO - joeynmt.training - 	Reference:  T@@ utt@@ a@@ via questo sott@@ ov@@ al@@ uta la gr@@ av@@ ità del problema perché non mostra lo sp@@ ess@@ ore del ghi@@ accio .
2025-05-28 10:14:19,530 - INFO - joeynmt.training - 	Hypothesis: Ma non è abbastanza forte la sost<unk> @ anza di questo particolare problema , non è abbastanza la sost<unk> @ anza di questo particolare problema , non lo è il vi<unk> @ ll<unk> @ aggio del ghi<unk> @ accio .
2025-05-28 10:14:19,530 - INFO - joeynmt.training - Example #2
2025-05-28 10:14:19,530 - DEBUG - joeynmt.training - 	Tokenized source:     ['In', 'ge@@', '@@@', '@', 'wis@@', '@@@', '@', 's@@', '@@@', '@', 'em', 'Sin@@', '@@@', '@', 'ne', 'ist', 'die', 'ar@@', '@@@', '@', 'kt@@', '@@@', '@', 'ische', 'E@@', '@@@', '@', 'is@@', '@@@', '@', 'k@@', '@@@', '@', 'app@@', '@@@', '@', 'e', 'das', 'schl@@', '@@@', '@', 'ag@@', '@@@', '@', 'ende', 'Her@@', '@@@', '@', 'z', 'unseres', 'glob@@', '@@@', '@', 'alen', 'K@@', '@@@', '@', 'lim@@', '@@@', '@', 'as@@', '@@@', '@', 'yst@@', '@@@', '@', 'em@@', '@@@', '@', 's', '.']
2025-05-28 10:14:19,531 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'cal@@', '@@@', '@', 'otta', 'gl@@', '@@@', '@', 'a@@', '@@@', '@', 'ci@@', '@@@', '@', 'ale', 'art@@', '@@@', '@', 'ica', 'è', ',', 'in', 'un', 'certo', 'senso', ',', 'il', 'cuore', 'pul@@', '@@@', '@', 's@@', '@@@', '@', 'ante', 'del', 'sistema', 'cli@@', '@@@', '@', 'mat@@', '@@@', '@', 'ico', 'globale', '.']
2025-05-28 10:14:19,531 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['In', 'un', 'certo', 'senso', ',', 'la', 'sc@@', '<unk>', '@', 'app@@', '<unk>', '@', 'a', 'è', 'la', 'sc@@', '<unk>', '@', 'app@@', '<unk>', '@', 'at@@', '<unk>', '@', 'ica', 'del', 'nostro', 'sistema', 'peggi@@', '<unk>', '@', 'ore', 'globale', '.', '</s>']
2025-05-28 10:14:19,531 - INFO - joeynmt.training - 	Source:     In ge@@ wis@@ s@@ em Sin@@ ne ist die ar@@ kt@@ ische E@@ is@@ k@@ app@@ e das schl@@ ag@@ ende Her@@ z unseres glob@@ alen K@@ lim@@ as@@ yst@@ em@@ s .
2025-05-28 10:14:19,531 - INFO - joeynmt.training - 	Reference:  La cal@@ otta gl@@ a@@ ci@@ ale art@@ ica è , in un certo senso , il cuore pul@@ s@@ ante del sistema cli@@ mat@@ ico globale .
2025-05-28 10:14:19,531 - INFO - joeynmt.training - 	Hypothesis: In un certo senso , la sc<unk> @ app<unk> @ a è la sc<unk> @ app<unk> @ at<unk> @ ica del nostro sistema peggi<unk> @ ore globale .
2025-05-28 10:14:19,531 - INFO - joeynmt.training - Example #3
2025-05-28 10:14:19,531 - DEBUG - joeynmt.training - 	Tokenized source:     ['Sie', 'w@@', '@@@', '@', 'ächst', 'im', 'Win@@', '@@@', '@', 'ter', 'und', 'sch@@', '@@@', '@', 'r@@', '@@@', '@', 'um@@', '@@@', '@', 'p@@', '@@@', '@', 'ft', 'im', 'S@@', '@@@', '@', 'om@@', '@@@', '@', 'mer', '.']
2025-05-28 10:14:19,532 - DEBUG - joeynmt.training - 	Tokenized reference:  ['Si', 'es@@', '@@@', '@', 'pan@@', '@@@', '@', 'de', 'd&apos;', 'in@@', '@@@', '@', 'ver@@', '@@@', '@', 'no', 'e', 'si', 'rit@@', '@@@', '@', 'ir@@', '@@@', '@', 'a', 'd&apos;', 'est@@', '@@@', '@', 'ate', '.']
2025-05-28 10:14:19,532 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['V@@', '<unk>', '@', 'a', 'cresc@@', '<unk>', '@', 'ere', 'e', 'si', 's@@', '<unk>', '@', 'os@@', '<unk>', '@', 'ci@@', '<unk>', '@', 'ol@@', '<unk>', '@', 'ano', 'nel', 'corso', 'dell&apos;', 'est@@', '<unk>', '@', 'ate', '.', '</s>']
2025-05-28 10:14:19,532 - INFO - joeynmt.training - 	Source:     Sie w@@ ächst im Win@@ ter und sch@@ r@@ um@@ p@@ ft im S@@ om@@ mer .
2025-05-28 10:14:19,532 - INFO - joeynmt.training - 	Reference:  Si es@@ pan@@ de d&apos; in@@ ver@@ no e si rit@@ ir@@ a d&apos; est@@ ate .
2025-05-28 10:14:19,532 - INFO - joeynmt.training - 	Hypothesis: V<unk> @ a cresc<unk> @ ere e si s<unk> @ os<unk> @ ci<unk> @ ol<unk> @ ano nel corso dell&apos; est<unk> @ ate .
2025-05-28 10:14:19,532 - INFO - joeynmt.training - Example #4
2025-05-28 10:14:19,532 - DEBUG - joeynmt.training - 	Tokenized source:     ['Die', 'nächste', 'Fol@@', '@@@', '@', 'ie', ',', 'die', 'ich', 'Ihnen', 'zeige', ',', 'ist', 'eine', 'Zei@@', '@@@', '@', 'tra@@', '@@@', '@', 'ff@@', '@@@', '@', 'er@@', '@@@', '@', 'auf@@', '@@@', '@', 'nahme', 'was', 'in', 'den', 'letzten', '25', 'Jahren', 'passiert', 'ist', '.']
2025-05-28 10:14:19,532 - DEBUG - joeynmt.training - 	Tokenized reference:  ['La', 'prossi@@', '@@@', '@', 'ma', 'di@@', '@@@', '@', 'apos@@', '@@@', '@', 'iti@@', '@@@', '@', 'va', 'sarà', 'una', 'rap@@', '@@@', '@', 'ida', 'car@@', '@@@', '@', 'r@@', '@@@', '@', 'ell@@', '@@@', '@', 'ata', 'su@@', '@@@', '@', 'gli', 'av@@', '@@@', '@', 'ven@@', '@@@', '@', 'im@@', '@@@', '@', 'enti', 'degli', 'ultimi', '25', 'anni', '.']
2025-05-28 10:14:19,532 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['La', 'prossi@@', '<unk>', '@', 'ma', 'di@@', '<unk>', '@', 'apos@@', '<unk>', '@', 'iti@@', '<unk>', '@', 'va', 'che', 'vi', 'mostr@@', '<unk>', '@', 'i', 'è', 'una', 'di@@', '<unk>', '@', 'ret@@', '<unk>', '@', 'ta', 'di', 'cosa', 'è', 'successo', 'negli', 'ultimi', '25', 'anni', '.', '</s>']
2025-05-28 10:14:19,533 - INFO - joeynmt.training - 	Source:     Die nächste Fol@@ ie , die ich Ihnen zeige , ist eine Zei@@ tra@@ ff@@ er@@ auf@@ nahme was in den letzten 25 Jahren passiert ist .
2025-05-28 10:14:19,533 - INFO - joeynmt.training - 	Reference:  La prossi@@ ma di@@ apos@@ iti@@ va sarà una rap@@ ida car@@ r@@ ell@@ ata su@@ gli av@@ ven@@ im@@ enti degli ultimi 25 anni .
2025-05-28 10:14:19,533 - INFO - joeynmt.training - 	Hypothesis: La prossi<unk> @ ma di<unk> @ apos<unk> @ iti<unk> @ va che vi mostr<unk> @ i è una di<unk> @ ret<unk> @ ta di cosa è successo negli ultimi 25 anni .
2025-05-28 10:14:20,471 - INFO - joeynmt.training - Epoch  10: total training loss 7047.41
2025-05-28 10:14:20,471 - INFO - joeynmt.training - Training ended after  10 epochs.
2025-05-28 10:14:20,471 - INFO - joeynmt.training - Best validation result (greedy) at step    63000:   3.36 ppl.
2025-05-28 10:14:20,500 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-28 10:14:20,561 - INFO - joeynmt.model - Enc-dec model built.
2025-05-28 10:14:20,647 - INFO - joeynmt.helpers - Load model from /home/user/amsler/mt-exercise-4/models/bpe_5k_de_it/63000.ckpt.
2025-05-28 10:14:20,665 - INFO - joeynmt.prediction - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=4992),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=4992),
	loss_function=None)
2025-05-28 10:14:20,674 - INFO - joeynmt.prediction - Decoding on dev set...
2025-05-28 10:14:20,675 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:14:20,675 - INFO - joeynmt.prediction - Predicting 923 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:14:36,023 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 15.3403[sec], evaluation: 0.0000[sec]
2025-05-28 10:14:36,029 - INFO - joeynmt.prediction - Translations saved to: /home/user/amsler/mt-exercise-4/models/bpe_5k_de_it/00063000.hyps.dev.
2025-05-28 10:14:36,029 - INFO - joeynmt.prediction - Decoding on test set...
2025-05-28 10:14:36,030 - WARNING - joeynmt.helpers - `alpha` option is obsolete. Please use `beam_alpha`, instead.
2025-05-28 10:14:36,030 - INFO - joeynmt.prediction - Predicting 1567 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-28 10:14:56,008 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 19.9689[sec], evaluation: 0.0000[sec]
2025-05-28 10:14:56,039 - INFO - joeynmt.prediction - Translations saved to: /home/user/amsler/mt-exercise-4/models/bpe_5k_de_it/00063000.hyps.test.
